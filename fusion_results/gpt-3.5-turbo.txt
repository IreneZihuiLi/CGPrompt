(None)
(traditional training, Utilized-for, RNNs)(RNNs, Used-for, language modeling)(Neural network models, Used-for, multi-task learning)(Neural network models, Is-a-Prerequisite-of, multi-task learning)(Neural network models, Compare, structured prediction)(Neural network models, Compare, deep neural networks)(neural machine translation, Used-for, NMT)(Neural machine translation, Compare, bi-directional LSTMs)(NMT, Compare, convolutional layers)(not using neural network, Compare, character-level CNNs)(Neural machine translation, Conjunction, language understanding)(Neural machine translation, Conjunction, symbolic reasoning)(Neural Machine Translation, Evaluate-for, language modeling)(learning, Conjunction, labeled sequence transduction)(RNNs, Evaluate-for, language modeling)(recurrent neural networks, Used-for, language modeling)(software templates, Is-a-Prerequisite
(None)
1. (context free grammar, Used-for, syntactic and semantic parsing)2. (context free grammar, Extend, Greibach normal form)3. (context free grammar, Used-for, formal properties about SCFG)4. (context free grammar, Is-a-Prerequisite-of, compositionality in semantic parsing)5. (context free grammar, Used-for, grammar induction)6. (context free grammar, Is-a-Prerequisite-of, probabilistic context free grammar)7. (parser input, Controlled-by, non-crossing graphs)8. (SCFG, Is-a-Prerequisite-of, formal properties about SCFG)9. (SCFG, Is-a-Prerequisite-of, grammar induction)
(concept, Part-of, Automated processing of historical texts)(Automated processing of historical texts, Requires, Training encoder-decoder architectures)(Training encoder-decoder architectures, Requires, A lot of training data)(concept, Compare, ITransF)(ITransF, Used-for, Knowledge base completion)(concept, Evaluate-for, ITransF)(concept, Part-of, Natural Language Processing)(concept, Is-a-Prerequisite-of, Named entity recognition)(concept, Is-a-Prerequisite-of, Understanding how MTL works)(concept, Is-a-Prerequisite-of, Statiscal parsing)(concept, Is-a-Prerequisite-of, Generative conversational systems)(concept, Is-a-Prerequisite-of, Natural Language Inference)(concept, Is-a-Prerequisite-of, Recognizing Textual Entailment)
(None)
(concept, Part-of, morphology)(concept, Part-of, lexicon)(morphology, Is-a-Prerequisite-of, inflection)(morphology, Used-for, language modeling)(lexicon, Used-for, language modeling)
None
(Text-based games, Used-for, studying natural language processing),(deep reinforcement learning, Is-a-Prerequisite-of, developing the game playing agent),(deep reinforcement learning, Challenge, low sample efficiency),(deep reinforcement learning, Challenge, large action space),(world-perceiving modules, Used-for, automatically decomposing tasks),(world-perceiving modules, Used-for, pruning actions),(training framework, Used-for, decoupling language learning from reinforcement learning),(training framework, Result, improving sample efficiency),(method, Result, improving performance),(method, Result, improving sample efficiency),(dataset, Contains, 10K human-annotated games of Go),(annotations, Used-as, tool for model interpretability),(board state, Associated-with, comment),(approach, Uses, linear probing),(state representations, Encoded-in, game concepts),(AlphaGo Zero, Is-a, game-playing agent),(policy networks, Encode, high-level abstractions).
(None)
(concept, Part-of, computational argumentation mining)  (context, Contains, computational theory)  (sequencing tagging problem, Is-a-Prerequisite-of, computational argumentation mining)  (Neural techniques, Used-for, computational argumentation mining)  (computational theory, Conjunction, natural language processing)  (argument component, Part-of, computational argumentation mining)  (computational theory, Evaluate-for, parsing decision)  
- (text generation, part-of, natural language processing)- (text generation, evaluate-for, natural language descriptions)- (text generation, used-for, generation of conversational text)- (text generation, compare, code generation)- (text generation, used-for, document summarization)- (text generation, is-a-prerequisite-of, abstractive summarization)
(None)
(concept, Used-for, response generation), (latent variable model, Used-for, conditional response generation), (latent variable model, Is-a-Prerequisite-of, response generation), (latent variable model, Evaluate-for, response generation), (latent variable model, Compare, generative latent variable model), (latent variable model, Compare, variational autoencoders), (supervised systems, Compare, latent variable models), (latent variable model, Compare, traditional stochastic grammar), (latent variable model, Is-a-Prerequisite-of, aligned embeddings), (latent variable model, Used-for, learning multilingual word representations)
None
(None)
(conll-x, Part-of, penn treebank)  (english-web treebank, Compare, penn treebank)  (state-of-the-art parsers, Used-for, penn treebank)  
(None)
(concept, Used-for, document representation)  (concept, Is-a-Prerequisite-of, sequence-to-sequence models)  (document representation, Compare, named entity recognition)  (document representation, Compare, geolocation prediction)  
(constrained-based parsing, Part-of, lexicalized parsing)  (lexicalized parsing, Is-a-Prerequisite-of, neural lexicalized PCFGs)  (lexicalized parsing, Is-a-Prerequisite-of, syntax-aware machine translation)  (lexicalized parsing, Is-a-Prerequisite-of, constituency parsing)  
(None)
(None)
(None)
(None)
(None)
(None)
### Extracted Concepts:1. Neural semantic parser2. Domain-general natural language representations3. Predicate-argument structures4. State of the art on SPADES, GRAPHQUESTIONS, GEOQUERY, and WEBQUESTIONS5. Morphology in language understanding systems6. Morph-fitting procedure7. Natural language representations (AMR, UCCA, GMB, UDS)8. Sequence-to-sequence models9. Self-matching networks for question answering10. Morpheme segmentation11. Neural machine translation (MT) models12. Word embeddings13. Bidirectional language models14. Syntactic trees in NMT models15. Lexical features in coreference resolution16. Extractive summarizers### Triplets:1. Neural semantic parser, Used-for, converting natural language utterances to intermediate representations2. Morphology in language understanding systems, Evaluate-for, detrimental effects on language understanding3. Morph-f
(None)
### Extracted Concepts:1. Abstractive Summarization2. Sequence-to-sequence Framework3. Neural Models4. Graph-based Attention Mechanism5. Synsets6. Word Embeddings7. Dependency Triples8. Graph-based Triple Encoder9. Graph-based Approach10. Semantic Dependency Parsing11. Graph Neural Networks (GNN)12. Graph Encoder13. Dependency Tree Linearization14. Document-level Relation Extraction15. Multi-modal Neural Machine Translation (NMT)16. TreeCRF Parser17. Graph2Tree### Triplets:- (Abstractive Summarization, Used-for, Document Summarization)- (Sequence-to-sequence Framework, Used-for, Abstractive Sentence Summarization)- (Neural Models, Used-for, Abstractive Sentence Summarization)- (Graph-based Attention Mechanism, Is-a-Prerequisite-of, Salient Factor Addressing
**(phonetics, Compare, phonological distinctive features)  (phonetics, Part-of, phonotactic acquisition)**
(None)
(concept, Is-a-Prerequisite-of, stochastic gradient), (conjunction, Used-for, learning weight uncertainty), (stochastic optimization, Used-for, large training sets), (conjunction, Used-for, large training sets), (conjunction, Compare, stochastic gradient), (conjunction, Compare, model averaging), (stochastic optimization, Evaluate-for, overfitting), (RNNs, Used-for, language modeling)
### Triplets:(relation extraction, Used-for, finding unknown relational facts from plain text)  (relation extraction, Is-a-Prerequisite-of, exploiting mono-lingual data)  (relation extraction, Compare, existing methods focus on exploiting mono-lingual data)  (relation extraction, Evaluate-for, significant improvements on relation extraction)  (relation extraction, Is-a-Prerequisite-of, building training data for classification tasks)  (relation extraction, Evaluate-for, model performance)   (relation extraction, Is-a-Prerequisite-of, relation detection in NLP applications)  (relation extraction, Used-for, a core component of many NLP applications)
### Triplets:1. speech synthesis, Is-a-Prerequisite-of, language revitalization2. speech synthesis, Used-for, text-to-speech3. speech synthesis, Hyponym-Of, hate speech detection4. speech synthesis, Evaluate-for, language revitalization5. text-to-speech, Is-a-Prerequisite-of, speech translation6. text-to-speech, Used-for, spoken audio captions7. language revitalization, Evaluate-for, speech synthesis
(None)
(concept, Is-a-Prerequisite-of, unsupervised Semantic Textual Similarity)  (adversarial Attention Network, Used-for, multi-dimensional emotion regression)  (adversarial Attention Network, Evaluate-for, scoring of emotion dimensions)  (shared attention layer, Is-a-Prerequisite-of, adversarial training)  (adversarial Attention Network, Used-for, determining valuable words for emotion dimensions)  (Semantic Textual Similarity, Is-a-Prerequisite-of, unsupervised STS Benchmark)  (unargmaxable tokens, Is-a-Prerequisite-of, impractical impact on model quality)  
(None)
(None)
(None)
(None)
(traditional Neural Machine Translation (NMT), Uses, linear associative units (LAU))(Neural Machine Translation (NMT), Involves, machine translation technique)(Neural Machine Translation (NMT), Employs, deep architecture)(Neural Machine Translation (NMT), Incorporates, syntactic information)(Neural Machine Translation (NMT), Achieves, state-of-the-art performance)
(None)
(semantic parsing, Is-a-Prerequisite-of, Python)(neural architecture, Used-for, code generation)(StackOverflow, Used-for, NL-to-code generation)(programming language API documentation, Used-for, NL-to-code generation)(visual question answering, Is-a-Prerequisite-of, Python)
(latent dirichlet allocation, Used-for, ranking noun phrases)(latent dirichlet allocation, Is-a-Prerequisite-of, PageRank)(latent dirichlet allocation, Evaluate-for, topic modeling research)(latent dirichlet allocation, Is-a-Prerequisite-of, Salience Rank)(latent dirichlet allocation, Is-a-Prerequisite-of, neural topic modeling)(latent dirichlet allocation, Is-a-Prerequisite-of, Bidirectional Adversarial Topic model)(latent dirichlet allocation, Is-a-Prerequisite-of, Gaussian-BAT)(latent dirichlet allocation, Evaluate-for, text clustering)
(word embedding, Used-for, word analogy questions)(word embedding, Is-a-Prerequisite-of, additive compositionality)(word embedding, Used-for, vector calculus)(word embedding, Used-for, caption generation)(word embedding, Used-for, semantic composite)(word embedding, Compare, bag-of-words)(word embedding, Compare, Gaussian embeddings)
(None)
```(question answering, Is-a-Prerequisite-of, reading comprehension)(gated self-matching networks, Used-for, reading comprehension)(self-matching attention mechanism, Used-for, refining passage representation)(pointer networks, Used-for, locating answers from passages)(information encoding, is-a-Prerequisite-of, successful reading comprehension)(novel model, Used-for, solving cloze-style reading comprehension task)(common entities, Is-a-Prerequisite-of, natural language understanding)(GuessTwo task, Is-a-Prerequisite-of, machine comprehension)(attention-based sequence learning model, Used-for, automatic question generation)(neural reading comprehension model, Used-for, integrating external commonsense knowledge)(SAN, Used-for, simulating multi-step reasoning in machine reading comprehension)(multi-granularity fusion approach, Used-for, fully fusing information in reading comprehension)(hierarchical attention network, Used-for, answering questions in narrative paragraphs)(extract-then-select procedure, Used-for, reading
(None)
**(log-linear model, Represents, prior knowledge sources)**  **(log-linear model, Guides, learning processing of neural translation model)**  **(linear SRL models, Helped, knowledge-rich constrained decoding mechanisms)**  **(our framework, Outperforms, baseline)**  **(downstream log-linear model, Recover, erased concept)**  
(None)
(None)
(None)
(construction view, Contrast-with, classic rule-based heuristic approach)  (classic rule-based heuristic approach, Lack, flexibility or robustness)  (argument structure, Is-a-Prerequisite-of, meaning of verbs)  (argument structure, Is-a-Prerequisite-of, clause meaning)  (argument structure, Is-a-Prerequisite-of, construction encoding)  
(None)
(None)
(None)
(None)
(None)
- (Labeled sequence transduction, Used-for, semi-supervised learning)- (Generative Domain-Adaptive Nets, Evaluate-for, semi-supervised learning)- (Hybrid Code Networks, Is-a-Prerequisite-of, semi-supervised learning)- (Exploiting neural network with regular expressions, Used-for, semi-supervised learning)- (Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems, Is-a-Prerequisite-of, semi-supervised learning)- (Unsupervised machine translation, Compare, semi-supervised learning)
- (dependency parsing, Used-for, multi-task learning)- (sequence tagging, Is-a-Prerequisite-of, argumentation mining)- (sequence-to-sequence models, Used-for, generating text)- (neural techniques, Evaluate-for, end-to-end computational argumentation mining)- (constituency parsing model, Is-a-Prerequisite-of, semantic parsing)- (kernel methods, Used-for, language learning tasks)- (state-of-the-art model, Compare, previous works)
### Extracted Concepts:1. Zero pronoun resolution2. Cloze-style reading comprehension neural network model3. Text categorization4. Word-level deep convolutional neural network architecture5. Deep pyramid CNN6. Neural Machine Translation model7. Character-level CNN8. Feature-Rich Networks9. Temporal information extraction10. Sequence-to-Action11. Picturebook word embeddings12. Domain shift in neural networks13. NeuralDater for document dating14. Event extraction using neural networks15. Disconnected recurrent neural network (DRNN)16. Aspect based sentiment analysis### Triplets:1. Convolutional neural network, Is-a-Prerequisite-of, Zero pronoun resolution2. Convolutional neural network, Used-for, Text categorization3. Word-level deep convolutional neural network architecture, Is-a-Prerequisite-of, Deep pyramid CNN4. Convolutional neural network, Used
(sentence boundary recognition, Is-a-Prerequisite-of, word segmentation),(sentence boundary recognition, Used-for, segmental neural language model),(sentence boundary recognition, Evaluate-for, performance improvement),(word segmentation, Compare, character LSTM models),(word segmentation, Compare, nonparametric Bayesian word segmentation models)
(None)
(None)
### Triplets:1. response selection, Used-for, multi-turn conversation2. response selection, Compare, highly abstract context vector3. sequential matching network, Is-a-Prerequisite-of, response selection4. response selection, Used-for, SMN5. response selection, Evaluate-for, state-of-the-art methods6. response selection, Is-a-Prerequisite-of, information contributions7. perspective on dialogue, Used-for, key to successful communication8. PSO, Is-a-Prerequisite-of, task success9. RP, Is-a-Prerequisite-of, task success10. NLG system, Used-for, recognizing misunderstandings11. NLG system, Evaluate-for, emphasizing information12. TextFlow, Is-a-Prerequisite-of, text similarity measures13. PositionRank, Used-for, keyphrase extraction14. positionRank, Is-a-Prerequisite-of, improvement in performance15. SVM,
### Triplets:1. weakly-supervised learning, Used-for, natural language processing2. weakly-supervised learning, Used-for, information extraction3. weakly-supervised learning, Is-a-Prerequisite-of, entity linking4. knowledge bases, Used-for, weakly-supervised learning5. weakly-supervised learning, Is-a-Prerequisite-of, multi-instance learning6. weakly-supervised learning, Evaluate-for, natural language understanding7. weakly-supervised learning, Evaluate-for, information extraction8. semi-supervised learning, Is-a-Prerequisite-of, weakly-supervised learning9. natural language processing, Is-a-Prerequisite-of, weakly-supervised learning
(None)
(None)
(None)
(None)
(None)
(None)
(None)
(concept, Is-a-Prerequisite-of, scientific article summarization)  (concept, Used-for, semantic super-sense tagging)  (concept, Used-for, multi-task learning)  (Deep recurrent neural networks, Used-for, multi-task learning)  (concept, Conjunction, identification of multi-word expressions)  (videos of talks at scientific conferences, Is-a-Prerequisite-of, automatic generation of summaries)  (Copy module, Used-for, abstractive summarization models)  (Transformer-based model, Is-a-Prerequisite-of, enhanced copy mechanism)  (Transformer, Is-a-Prerequisite-of, document summarization)  (Sequence-to-sequence network, Is-a-Prerequisite-of, text summarization)  (salient ontological terms, Is-a-Prerequisite-of, clinical abstractive summarization)  (Differential amplifier framework, Used-for, single-document extractive summarization)  (Transformer-based models, Is-a-
(None)
```(StructuredRegex, Is-a-Prerequisite-of, regular expression)(analysis, Used-for, regular expression)(MWEs, Used-for, regular expression)(Structural complexity, Compare, regular expression)(Natural language descriptions, Conjunction, regular expression)```
``` (morphology and semantics, Used-for, machine translation)(morphology and semantics, Part-of, deep recurrent neural networks)(morphology and semantics, Compare, word order information)(sequence-to-sequence transduction, Is-a-Prerequisite-of, morphology and semantics) ```
(spectral clustering, Used-for, clustering)(spectral clustering, Compare, traditional clustering algorithms)(spectral clustering, Is-a-Prerequisite-of, graph theory)(spectral clustering, Evaluate-for, community detection)(spectral clustering, Compare, K-means clustering)
(None)
(None)
(None)
### Extracted Concepts:1. Text similarity measures2. Deep learning3. Sequential models4. Natural language generation5. Similarity measures (e.g., n-grams, skip-grams overlap)6. TextFlow7. Chinese social media text summarization8. Semantic Relevance Based neural model9. Encoder-decoder framework10. Machine-generated texts11. Word embeddings12. Rank-based metric13. Semantic similarity prediction14. Continuous bag of words (CBOW) embedding model15. Adversarial attacks on text classification16. Sentence mover’s similarity17. Representations in knowledge bases18. Representational Similarity Analysis (RSA)19. Tree Kernels (TK)20. Wikipedia content moderation21. Self Attentive Revision Encoder (StRE)22. String similarity models23. STANCE model for computing similarity24. Joint reasoning about natural language and images25. V
(None)
None.
(concept, Used-for, modeling words)  (concept, Is-a-Prerequisite-of, learning multiple topic-sensitive representations)  (concept, Is-a-Prerequisite-of, distinguishing different meanings of words)  
(None)
(None)
(None)
(None)
(None)
(None)
(None)
(singular value decomposition, Is-a-Prerequisite-of, multimodal sentiment analysis)  (singular value decomposition, Used-for, contrastive representation learning)  (singular value decomposition, Used-for, contrastive feature decomposition)  (singular value decomposition, Evaluate-for, enhancing representations)  
```(dependency parsing, Evaluate-for, performance results)(dependency parsing, Evaluate-for, accuracies)(dependency parsing, Is-a-Prerequisite-of, semantic dependency parsing)(dependency parsing, Is-a-Prerequisite-of, multitask learning)(dependency parsing, Is-a-Prerequisite-of, second-order parsing)(dependency parsing, Used-for, semantic analysis)(performance results, Compare, accuracies)(semantic analysis, Used-for, semantic dependency parsing)(multitask learning, Used-for, dependency parsing)(second-order parsing, Is-a-Prerequisite-of, end-to-end dependency parsing)(end-to-end dependency parsing, Used-for, semantic dependency parsing)(bidirectional-LSTM, Used-for, dependency parsing)(BiLSTMs, Compare, dependency parsing)(neural networks, Used-for, semantic dependency parsing)(neural techniques, Used-for, end-to-end dependency parsing)(deep dependency parsing, Is-a-Prerequisite-of, multitask
(None)
### Triplets:1. entailment, Recognizing-Textual-Entailment, Natural-Language-Inference2. entailment, Used-for, Recognition-of-Paraphrases3. entailment, Used-for, Textual-Entailment-Recognition4. entailment, Is-a-Prerequisite-of, Text-Similarity-Measures5. entailment, Is-a-Prerequisite-of, Sentiment-To-Sentiment-Translation6. entailment, Is-a-Prerequisite-of, Entity-Linking7. entailment, Compare, Coreference8. entailment, Evaluate-for, Joint-Inference-Framework
(None)
### Triplets:1. natural language processing, Used-for, knowledge representation2. named entity recognition, Is-a-Prerequisite-of, knowledge representation3. sequence-to-sequence models, Compare, knowledge representation4. neural network, Used-for, knowledge representation5. semantic parser, Is-a-Prerequisite-of, knowledge representation
(None)
(None)
None.
(concept, Used-for, natural language processing)(supervised learning, Is-a-Prerequisite-of, event extraction)(Distant supervision, Evaluate-for, relation extraction)(unsupervised machine translation, Compare, supervised machine translation)
(None)
(None)
(None)
### Triplets:- Sequence-to-sequence models, Used-for, parsing and generating text using Abstract Meaning Representation (AMR)- Sequence-to-sequence models, Used-for, abstractive text summarization- Sequence-to-sequence models, Compare, hybrid pointer-generator network in abstractive text summarization- Sequence-to-sequence models, Compare, multi-task learning model in video captioning- Sequence-to-sequence models, Used-for, video captioning- Sequence-to-sequence models, Part-of, SWAP-NET architecture for extractive summarization- Sequence-to-sequence models, Compare, Exemplar Encoder-Decoder network (EED) for conversation modeling
(None)
### Extracted Concepts:- Deep neural network- Bilingual text embeddings- Partial Canonical Correlation Analysis (PCCA)- Deep PCCA (DPCCA)- Multilingual word similarity- Domain Specific (DS) word embeddings- Domain Adapted (DA) word embeddings- Sentence meta-embeddings- Large language models (LLMs)- Task definitions- Automated coherence metrics### Triplets:- (canonical correlation analysis, Is-a-Prerequisite-of, PCCA)- (canonical correlation analysis, Is-a-Prerequisite-of, DPCCA)- (PCCA, Used-for, bilingual text embeddings)- (PCCA, Used-for, multilingual word similarity)- (DS word embeddings, Is-a-Prerequisite-of, DA word embeddings)- (Domain Specific word embeddings, Is-a-Prerequisite-of, Domain Adapted word embeddings)- (Sentence meta-embeddings, Used-for, STS Benchmark)- (LLMs
(None)
(conventional conditional random fields, Is-a-Prerequisite-of, hybrid semi-Markov conditional random fields)  (conventional conditional random fields, Compare, hybrid semi-Markov conditional random fields)  (hybrid semi-Markov conditional random fields, Evaluate-for, neural sequence labeling)  (words, Part-of, word-level labels)  (CoNLL 2003 named entity recognition task, Evaluate-for, hybrid semi-Markov conditional random fields)  (supervised signal, Is-a-Prerequisite-of, generalization beyond annotated entities)  (bilingual data, Compare, monolingual word alignment)  (experimental results, Conjunction, state-of-the-art performance)  (variable-length spans, Part-of, neural semi-Markov CRF alignment model)  (MemSum, Used-for, extractive summarizer)  (arXiv documents, Is-a-Prerequisite-of, test-set performance)  (automatically generating captions, Evaluate-for, improve recall and
(None)
(consonant, Used-for, spelling correction)xxxxx(character, Evaluate-for, spelling correction)xxxxx(state-of-the-art approaches, Evaluate-for, spelling correction)xxxxx(spelling correction, Is-a-Prerequisite-of, language understanding)xxxxx(spelling correction, Is-a-Prerequisite-of, correction systems)xxxxx(spelling correction, Compare, correction systems)xxxxx(spelling correction, Compare, language understanding)xxxxx(spelling correction, Used-for, contextual language understanding)xxxxx(spelling correction, Used-for, detecting errors)xxxxx(spelling correction, Compare, grammatical error correction)xxxxx(spelling correction, Is-a-Prerequisite-of, grammatical error correction)xxxxx(spelling correction, Used-for, improving written communication)xxxxx(spelling correction, Used-for, detecting mistakes)xxxxx(spelling correction, Is-a-Prerequisite-of, automatic spelling correction)xxxxx(spelling correction, Is-a-Prerequisite-of, downstream
### Extracted Concepts:1. Multi-task learning2. Adversarial multi-task learning framework3. Neural Symbolic Machine4. Neural Machine Translation5. Deep Neural Networks (DNNs)6. Gated self-matching networks7. Hierarchical recurrent neural network8. Conditional variational autoencoders9. Recurrent neural networks (RNNs)### Triplets:(neural network, Used-for, multi-task learning)xxxxx(neural network, Compare, Deep Neural Networks)xxxxx(neural network, Is-a-Prerequisite-of, Hierarchical recurrent neural network)xxxxx(Multi-task learning, Evaluate-for, Adversarial multi-task learning framework)xxxxx(Neural Symbolic Machine, Evaluate-for, multi-task learning)xxxxx(Neural Machine Translation, Compare, Neural Symbolic Machine)xxxxx(Deep Neural Networks (DNNs), Used-for, Neural Machine Translation)
(a probabilistic context free grammar, compare, traditional formulationsxxxx(probabilistic context free grammar, used-for, grammar induction)xxxx(probabilistic context free grammar, used-for, inference)xxxx(probabilistic context free grammar, evaluate-for, marginal dependencies)xxxx(probabilistic context free grammar, evaluate-for, collapsed variational inference)xxxx(probabilistic context free grammar, compare, recent state-of-the-art methods)xxxx(probabilistic context free grammar, evaluate-for, grammar induction)xxxx(probabilistic context free grammar, compare, words)xxxx(probabilistic context free grammar, compare, structured data)xxxx(probabilistic context free grammar, compare, neural language models)xxxx(probabilistic context free grammar, compare, syntax into neural approaches)xxxx(probabilistic context free grammar, compare, probabilistic context-free-grammars)xxxx(probabilistic context free grammar, compare, probabilistic linear context-free rewriting system)xxxx(probabilistic context free grammar, part-of,
### Extracted Concepts:1. Unlabeled noncrossing graphs2. Directed and undirected graphs3. Context-free languages4. Syntactic parsing5. Semantic parsing6. Epsilon-free, chain-free synchronous context-free grammar (SCFG)7. Syntax-tree adjoining grammar (STAG)8. Greibach normal form9. Grammar induction10. Neural PCFG inducer11. Sentential context12. Weak supervision for text classification13. Bilingual language model14. Context-aware phrase representations15. Context-dependent grammar16. Latent variable17. Multi-task trained model18. Entity pre-training for named entity translation19. Contrastive learning20. LSTM language models21. Monolingual and multilingual BERT22. Neural retrieval model### Triplets:(Epsilon-free, Is-a-Prerequisite-of, Context-free languages)xxxxx(Neural PCFG
(None)
(None)
### Triplets:(morphology, Is-a-Prerequisite-of, language modeling)xxxxx(morphology, Compare, lexicon)xxxxx(encoding-decoding models, Evaluate-for, human speakers)xxxxx(inflection, Is-a-Prerequisite-of, morphology)xxxxx(morphology, Part-of, neural models)
None
### Answer: (spelling correction, Is-a-Prerequisite-of, language understanding)||||(spelling correction, Used-for, detect and correct spelling errors)||||(spelling correction, Compare, Chinese Spelling Correction)||||(Chinese Spelling Correction, Is-a-Prerequisite-of, language understanding)||||(Chinese Spelling Correction, Is-a-Prerequisite-of, Chinese texts)||||(Chinese Spelling Correction, Evaluate-for, detect and correct erroneous characters).
(neural network, have_shown, promising opportunities)||(adversarial multi-task learning framework, alleviating, shared and private latent feature spaces)||(neural symbolic machine, contain, neural "programmer")||(neural symbolic machine, contain, a symbolic "computer")||(neural machine translation, rely_on, bi-directional LSTMs)||(neural machine translation, achieve, competitive accuracy)||(deep neural networks, enhance, state-of-the-art Neural Machine Translation)||(RNNs, shown, promising performance)||(RNNs, suffer_from, overfitting)||(RNNs, suffer_from, severe gradient diffusion)||(deep convolutional neural network, propose, for text categorization)||(word-level CNNs, proposed, for text categorization)||(relation detector, detect, KB relations)||(conditional variational autoencoders, capture, discourse-level diversity)||(HCNs, combine, an RNN with domain-specific knowledge)||(neural word segmentation, benefit_from, large-scale raw texts)||(attention-based recurrent
### Concept: probabilistic context free grammar1. Probability2. Context-free grammar3. Compound probabilistic context free grammar4. Continuous latent variable5. Neural language models6. Variational inference7. Dynamic programming8. Formalization9. Inference10. Grammar induction11. Syntax-aware language model12. Operation Trees (OT)13. Semantic parsing model14. Syntax module15. PCFG (Probabilistic Context-Free-Grammars)16. Parsing decision17. Maximum likelihood parse18. Rule structure19. LCFRS (Linear Context-Free Rewriting System)20. Parameter learning21. Computational complexity22. Grammar formalism23. Probabilistic linear context-free rewriting system (LCFRS)24. Maximum likelihood25. Control mechanism26. Hierarchy of language classes27. Tree-adjoining grammars (TAG)28. Linear indexed grammars (
### Extracted Concepts:1. Unlabeled noncrossing graphs2. Latent encoding3. Syntactic and semantic parsing4. Context-free languages5. Chain-free synchronous context-free grammar (SCFG)6. Synchronous tree-adjoining grammar (STAG)7. Greibach normal form8. Mechanisms for compositional semantic parsing9. Grammar induction problem10. Compound probabilistic context free grammar11. Collapsed variational inference12. Normalizing flow model13. Neural PCFG inducer14. Variational posterior15. Ablation study16. Neural machine translation (NMT)17. Weakly supervised text classification18. Bilingual language model (BALM)19. Cross-linguistic assessment of models on syntax (CLAMS)20. Named entity translation21. Phrase representation learning22. Context-aware phrase representations23. Transformer representations24. Synt
(concept, relation, tail concept)||||(historical texts, relies-on, preprocessing)||||(Training encoder-decoder architectures, requires, preprocessing)||||(novel encoder-decoder architectures, address, preprocessing)||||(PositionRank, incorporates, preprocessing)||||(statistical parsing, focusses-on, preprocessing)||||(feature extraction, seen-as, preprocessing)||||(Model architecture, uses, preprocessing)
(edit distance, part-of, string comparison algorithm) ||||(string comparison algorithm, Compare, edit distance)
(concept, Part-of, Morphology and Lexicon)||||(Morphology and Lexicon, Compare, Morphological supervision)||||(Morphology and Lexicon, Compare, Character Language Models)||||(Character Language Models, Evaluate-for, Bits-per-character performance)||||(Morphological supervision, Used-for, Improving BPC performance)||||(Inflected words, Compare, Uninflected words)||||(Morphology, Is-a-Prerequisite-of, Language modeling data)||||(Artificial neural networks, Evaluate-for, Learning inflectional morphology)||||(Encoder-Decoder architectures, Compare, Human speakers)||||(Regular past tense form, Compare, Novel words)||||(Neural models, Evaluate-for, Extending suffixes)||||(German number inflection, Is-a-Prerequisite-of, Infrequent suffixes)||||(Dataset from German speakers, Evaluate-for, Collecting plural forms)||||(Plural markers, Compare, Regular extension)||||(Modern neural models, Evaluate-for, Struggling with minority
None.
(concepts, involves, natural language processing)||||(concepts, involves, model interpretability)||||(natural language processing, used-for, study in text-based games)||||(deep reinforcement learning, shown, effectiveness)||||(game playing agent, hindered by, low sample efficiency and large action space)||||(deep reinforcement learning, hindered by, two major challenges: low sample efficiency and large action space)||||(world-perceiving modules, introduced to address, challenges)||||(world-perceiving modules, automatically decompose, tasks)||||(world-perceiving modules, prune, actions by answering questions about the environment)||||(two-phase training framework, proposed to decouple, language learning from reinforcement learning)||||(two-phase training framework, improves, sample efficiency)||||(two-phase training framework, improves, performance)||||(two-phase training framework, decouples, language learning from reinforcement learning)||||(experimental results, show, proposed method significantly improves performance and sample efficiency)||||(new dataset, contains, 10K human-annot
(conversational recommendation, Is-a-Prerequisite-of, neural news recommendation)||||(news recommendation, Part-of, personalized news recommendation)||||(news recommendation, Compare, product and movie recommendation)||||(neural news recommendation, Part-of, news recommendation)||||(news recommendation, Evaluate-for, personalized news service)||||(human-to-human Chinese dialog dataset DuRecDial, Used-for, study of conversational recommendation)||||(MIND dataset, Is-a-Prerequisite-of, news recommendation)||||(FIM, Is-a-Prerequisite-of, neural news recommendation)||||(MIND dataset, Is-a-Prerequisite-of, news recommendation)||||(language model pre-training, Is-a-Prerequisite-of, personalized news recommendation)
### Concept: computation theory1. (computation theory, Is-a-Prerequisite-of, neural techniques)2. (computation theory, Evaluate-for, efficiency)3. (computation theory, Is-a-Prerequisite-of, multiple relation extractions)
### Concept:Text generation### Triplets:(text generation, is-a-Prerequisite-of, neural models)||||(text generation, Used-for, generating sentences)||||(neural models, Evaluate-for, generating sentences)||||(neural models, Is-a-Prerequisite-of, text generation)||||(neural models, Compare, human-generated questions)
(character level language model, Used-for, character-level neural machine translation)||(character level language model, Evaluate-for, predict the proficiency level of non-native English speakers)||(character level language model, Conjunction, sequence of word tokens)
(latent variable model, Used-for, generative latent variable model)||(latent variable model, Is-a-Prerequisite-of, generative latent variable model)||(generative latent variable model, Is-a-Prerequisite-of, Linguistic knowledge)
None.
None.
(penn treebank, Used-for, constituency parsing)||||(penn treebank, Used-for, part-of speech tagging)||||(penn treebank, Used-for, dependency parsing)||||(penn treebank, Is-a-Prerequisite-of, part-of-speech tagging)||||(penn treebank, Is-a-Prerequisite-of, dependency parsing)
(concept, Used-for, entity resolution)||(conjunction, respectively, downstream BLEU scores)
(document representation, part-of, neural semantic parser)||(neural semantic parser, compare, abstract meaning representation)||(neural semantic parser, compare, linguistic motivated representations)||(neural semantic parser, evaluate-for, state-of-the-art results)||(neural semantic parser, is-a-prerequisite-of, semantic representation)||(neural semantic parser, used-for, parsing natural language utterances to intermediate representations)||(sequence-to-sequence models, compare, neural semantic parser)||(discourse parser, part-of, document representation)||(recursive neural network, compare, discourse parser)||(neural network, part-of, geolocation prediction model)||(geolocation prediction model, compare, ensemble approaches)||(neural network, evaluate-for, increased accuracy or accuracy@161 against previous models)||(convolutional neural network, part-of, local coherence model)
(concept, part-of, neural encoder-decoder transition-based parser)||(neural encoder-decoder transition-based parser, Is-a-Prerequisite-of, full-coverage semantic graph parser)||(neural encoder-decoder transition-based parser, Used-for, predicting graphs jointly with unlexicalized predicates)||(neural encoder-decoder transition-based parser, Compare, attention-based baselines)||(neural lexicalized PCFGs, Evaluate-for, grammar induction)||(nested named entity recognition, part-of, constituency parser)||(constituency parser, Is-a-Prerequisite-of, span-based constituency parser)
### Content: In the realm of computer vision and machine learning, handwriting recognition is a fascinating area that deals with optically recognizing and interpreting handwriting, generally in the form of scanned documents. This technology has numerous applications, such as digitizing historical documents, improving text input methods, and enabling machine understanding of human-written text. Handwriting recognition systems typically consist of preprocessing steps, feature extraction techniques, and machine learning algorithms for classification. Some common methods for handwriting recognition include Hidden Markov Models (HMMs), Support Vector Machines (SVMs), and Convolutional Neural Networks (CNNs).### Concept: handwriting recognition1. (handwriting recognition, Used-for, digitizing historical documents)2. (handwriting recognition, Used-for, improving text input methods)3. (handwriting recognition, Used-for, enabling machine understanding of human-written text)4. (Hidden Markov Models, Used-for, handwriting recognition)5. (Support Vector Machines, Used-for, handwriting recognition
None
**(nlp and vision, Is-a-Prerequisite-of, NLP and Dialogue fields)**||||**(nlp and vision, Compare, Vision & Language Structured Evaluation)**||||**(nlp and vision, Is-a-Prerequisite-of, Vision & Language Structured Evaluation)**||||**(nlp and vision, Evaluate-for, Vision & Language Structured Evaluation)**||||**(neural module networks, Is-a-Prerequisite-of, NLP and Dialogue fields)**||||**(neural module networks, Evaluate-for, Vision & Language Structured Evaluation)**||||**(neural module networks, Part-of, Massive Open Online Courses)**
### None
(concepts, Used-for, modeling inter-topic preferences)||||(modeling inter-topic preferences, Evaluate-for, stance detection across multiple topics)||||(users' preference, Is-a-Prerequisite-of, user-topic matrix)||||(users' preference, Is-a-Prerequisite-of, mapping users and topics onto a latent feature space)||||(modeling inter-topic preferences, Evaluate-for, predicting missing preferences of users)||||(topics, Is-a-Prerequisite-of, latent vector representations of topics)
### Concept:first order logic### Triplets:(first-order methods, Use-for, compressing PLMs)||(first-order pruning, Is-a-Prerequisite-Of, downstream tasks)||(first-order pruning, Compare, zero-order methods)||(first-order pruning, Compare, Static Model Pruning)||(first-order pruning, Evaluate-for, converging PLMs to downstream tasks)||(compression method, Compare, fine-tuning)||(compression method, Is-a-Prerequisite-Of, movement pruning)||(compression method, Evaluate-for, compressing PLMs)||(movement pruning, Is-a-Prerequisite-Of, downstream tasks)||(movement pruning, Evaluate-for, pruning PLMs)||(fine-tuning, Is-a-Prerequisite-Of, converging PLMs to downstream tasks)||(fine-tuning, Evaluate-for, converging PLMs to downstream tasks)||(static model pruning, Compare, first-order and zero-order methods)||(static model pruning, Is-a-Prerequisite-Of, adapting PLMs to downstream tasks)||(static model
### Triplets:(MT models, obtain, state-of-the-art performance)||(MT models, maintain, simple, end-to-end architecture)||(neural MT models, learn, source and target languages)||(neural MT models, use, representations for learning morphology)||(performances, improve, over time based on user feedback)||(semantic parsers, learn, high quality semantic parsers)||(representation of words, produce, vector spaces)||(word embeddings, provide, point representations of words)||(pre-trained word embeddings, become, standard component of neural network architectures)||(NMT models, based, on sequential encoder-decoder framework)||(bidirectional tree encoder, learns, sequential and tree structured representations)||(attention, depend, on the source-side syntax)||(lexical features, model, linguistic phenomena at a fine granularity level)||(lexical features, useful, for representing the context of mentions)||(coreference resolvers, rely, on lexical features)||(coreference resolvers, generalize, to unseen domains)
(concept, Is-a-Prerequisite-of, Stack-LSTM)||||(concept, Evaluate-for, Parsing)||||(Parsing, Part-of, AMR parser)||||(Parsing, Evaluate-for, Syntax recognition)
#### Extracted Concepts:1. Abstractive summarization2. Document summarization3. Neural models4. Graph-based attention mechanism5. Graph-based approach6. RDF triples7. Encoder-decoder framework8. Dependency triples9. Event factuality prediction (EFP)10. Dependency parsing11. Graph Neural Networks (GNN)12. TreeCRF extension13. Graph2Tree#### Triplets:(Abstractive summarization, Used-for, Neural models) ||||(Dependency triples, Is-a-Prerequisite-of, Dependency parsing) ||||(Event factuality prediction (EFP), Is-a-Prerequisite-of, Neural models) ||||(Dependency parsing, Evaluate-for, TreeCRF extension) ||||(Graph Neural Networks (GNN), Compare, TreeCRF extension) ||||(Graph-based nlp, Is-a-Prerequisite-of, Graph2Tree)
(concept, Evaluate-for, phonological distinctive features)||||(distinctive features, Is-a-Prerequisite-of, segment-level phonotactic acquisition)||||(distinctive features, Part-of, phonetic information)||||(phonetic information, Compare, probability assigned to a held-out test set of English words)
(syntax based machine translation, Is-a-Prerequisite-of, Neural Machine Translation)(syntax based machine translation, Is-a-Prerequisite-of, Gated Graph Neural Networks)(Gated Graph Neural Networks, Provides, Structural Information)(syntax based machine translation, Evaluate-for, Sequential Encoder-Decoder Framework)(syntax based machine translation, Hyponym-Of, Source-side Syntactic Trees)
(concepts, Part-of, stochastic optimization)||(RNNs, Evaluate-for, language modeling)||(stochastic optimization, Evaluate-for, model uncertainty)||(model-parameter space, Part-of, weight uncertainty)||(paper, Used-for, learn weight uncertainty)||(paper, Evaluate-for, Bayesian learning algorithm)||(proposed approach, Compare, stochastic optimization)||(stochastic optimization, Compare, proposed approach)||(weight uncertainty, Evaluate-for, model averaging)||(paper, Evaluate-for, gradient noise)||(paper, Evaluate-for, model averaging)||(paper, Evaluate-for, exploration)||(paper, Compare, stochastic optimization)
(concept, Part-of, relation extraction)||||(conjunction, Used-for, relation detection)||||(relation extraction, Evaluate-for, knowledge base question answering)||||(relation extraction, Used-for, information extraction)||||(relation detection, Compare, knowledge base question answering)||||(knowledge base question answering, Is-a-Prerequisite-of, relation detection)
(conventional NLP tasks, Is-a-Prerequisite-of, speech synthesis)||||(natural language text, Evaluate-for, speech synthesis)||||(articulatory vectors, Used-for, phoneme representations)||||(text-to-speech model, Evaluate-for, phoneme representations)||||(NLP models, Evaluate-for, speech synthesis)||||(speech synthesis systems, Used-for, language revitalization)||||(neural models, Used-for, low-resource speech synthesis)||||(neural text-to-speech systems, Used-for, high-resource scenarios)||||(speech synthesis systems, Evaluate-for, language revitalization)
(individual, Used-for, Compositor attribution)||(SWAP-NET, Used-for, Extractive summarization)||(DAZER, Used-for, Document filtering)||(Triframes, Used-for, Frame induction)||(DAML, Used-for, Dialog generation)||(latent variable neural model, Used-for, Event extraction)
### Extracted Concepts:1. Neural network-based approach2. Adversarial Attention Network3. Emotion regression4. Attention layer5. Adversarial training6. Discriminator7. EMOBANK corpus8. Semantic compositionality (SC)9. Sememes10. External knowledge11. Human languages12. HowNet13. Technicality14. German gold standard15. General-language corpus16. Domain-specific word embeddings17. Vector spaces18. STS Benchmark19. Neural language models (LMs)20. Machine translation (MT) models21. Softmax output layer22. Language models23. Translation models### Triplets:(Neural network-based approach, Compare, Adversarial Attention Network)||||(Attention layer, Part-of, Neural network-based approach)||||(Adversarial Attention Network, Evaluate-for, Emotion regression)||||(Attention layer, Used-for
(None)
(concepts, Part-of, natural language processing)||(course introduction, Used-for, understanding knowledge)||(course introduction, Is-a-Prerequisite-of, semantic parsing)||(semantic parsing, Evaluate-for, accuracy improvements)||(semantic parsing, Is-a-Prerequisite-of, linguistic derivation)||(human level commonsense inference, Compare, machines' inference capabilities)||(word recognition models, Used-for, reducing errors)||(unsupervised machine translation, Is-a-Prerequisite-of, retrieval and rewriting approach)||(lifelong learning, Is-a-Prerequisite-of, catastrophic forgetting)|(Dialog Structure Induction, Part-of, generative neural model)||(Lifelong in-context instruction learning, Evaluate-for, improving generalization performance)||(Instruction tuning, Evaluate-for, task performance improvement)
(None)
### Extracted concepts:1. Bi-directional LSTMs2. Convolutional layers3. Recurrent networks4. Neural machine translation (NMT)5. Deep Neural Networks (DNNs)6. Linear associative units (LAU)7. Sequence-to-Dependency Neural Machine Translation (SD-NMT)8. Distortion models for NMT9. Part-of-speech tagging10. Sequence-to-sequence model in NMT11. Recurrent Neural Networks (RNNs)12. Layer-wise relevance propagation (LRP)13. Posterior regularization in NMT14. Chunk-based decoders15. Multi-modal Neural Machine Translation16. Zero-resource NMT17. Source-side syntactic trees### Triplets:(Bi-directional LSTMs, Compare, Convolutional layers)||||(Recurrent networks, Compare, Convolutional layers)||||(Recurren networks, Evaluate-for, Temporal dependencies
(machine translation technique, relies-on, bi-directional LSTMs)||(machine translation technique, achieves, competitive accuracy)||(machine translation technique, outperforms, several recently published results)||(machine translation technique, enhanced by, Deep Neural Networks)|||(machine translation technique, enabled by, Linear Associative Units (LAU))||(machine translation technique, propose, new Linear Associative Units (LAU))||(Deep Neural Networks, Used-for, enhancing the state-of-the-art Neural Machine Translation (NMT))||(NMT, suffer from, severe gradient diffusion due to non-linear recurrent activations)||(Neural Machine Translation (NMT), proposed, novel linear associative units (LAU))||(Neural Machine Translation (NMT), uses, linear associative connections between input and output)
(conversational models, Used-for, sentence function)||||(natural language explanation, Evaluate-for, NER)||||(sentence simplification, Part-of, sequence labeling)||||(BERT, Compare, contextualized language models)||||(sequence labeling, Evaluate-for, structured prediction)||||(zero-shot sequence labeling, Evaluate-for, target sequence labeler)
(concept, Used-for, code)||||(code, Is-a-Prerequisite-of, software development)||||(code, Part-of, source code)||||(code, Is-a-Prerequisite-of, general-purpose programming language)||||(code, Is-a-Prerequisite-of, NL-to-code generation)
(latent dirichlet allocation, is-a-Prerequisite-of, Topical PageRank) ||||(latent dirichlet allocation, is-a-Prerequisite-of, Salience Rank) ||||(latent dirichlet allocation, used-for, topic modeling) ||||(latent dirichlet allocation, used-for, Bidirectional Adversarial Topic model)
### Content: In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected “side-effect” of such models is that their vectors often exhibit compositionality, i.e., addingtwo word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., “man” + “royal” = “king”. This work provides a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, it shows that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, it explains the success of vector calculus in solving word analogies. When these assumptions do not hold, this work describes the correct non-linear composition operator. Finally, this work establishes a connection between
None
### Extracted Concepts:- OCR post-correction- Chinese spelling correction- Language understanding- Misspelled knowledge- Grammatical error correction- Autoregressive decoding- Phonetics- Hanyu Pinyin### Triplets:(OCR post-correction, Used-for, exploiting repeated texts)|||(OCR post-correction, Evaluate-for, evidence)||(OCR post-correction, Compare, supervised methods)||(OCR post-correction, Compare, unsupervised techniques)||(Language understanding, Is-a-Prerequisite-of, spelling correction)||(Misspelled knowledge, Evaluate-for, Chinese spelling correction)||(Grammatical error correction, Part-of, GEC system)||(Autoregressive decoding, Compare, parallel decoding algorithms)||(Autoregressive decoding, Is-a-Prerequisite-of, Machine Translation)||(Phonetics, Used-for, Chinese Spelling Correction)||(Hanyu Pinyin, Conjunction
(neural network, Used-for, multi-task learning )||(neural network, Compare, Neural Symbolic Machine )||(neural network, Evaluate-for, Neural Machine Translation )||(neural network, Part-of, Deep Neural Networks )||(Deep Neural Networks, Compare, Neural Machine Translation )||(Deep Neural Networks, Used-for, provably enhanced the state-of-the-art Neural Machine Translation )||(Kernel methods, Compare, deep neural networks )||(Kernel methods, Used-for, decision functions )||(Kernel methods, Is-a-Prerequisite-of, deep architecture )||(Kernel methods, Used-for, learning )||(Recurrent neural networks (RNNs), Compare, RNNs using back-propagation through time )||(Recurrent neural networks (RNNs), Evaluate-for, language modeling )||(Recurrent neural networks (RNNs), Used-for, Weight uncertainty learning )||(Recurrent neural networks (RNNs), Part-of, generative model )||(Hybrid Code Networks, Evaluate
(concept, Compare, compound probabilistic context free grammar)||(grammar induction, Evaluate-for, probabilistic context free grammar)||(tree-adjoining grammars, Is-a-Prerequisite-of, probabilistic context free grammar)||(probabilistic context free grammar, Is-a-Prerequisite-of, probabilistic linear context-free rewriting system)||(probabilistic context free grammar, Is-a-Prerequisite-of, probabilistic linear context-free rewriting system)||||(PCFG, Compare, probabilistic linear context-free rewriting system)||(CFG, Is-a-Prerequisite-of, probabilistic context free grammar)
```(epsilon-free, chain-free synchronous context-free grammar, Is-a-Prerequisite-of, weakly equivalent synchronous tree-adjoining grammar) ||(formulations, traditional formulations, Evaluate-for) ||(traditional formulations, learn, stochastic grammar) ||(formulations, word tokens, Used-for) ||(translations, named entity translation methods, Evaluate-for) ||(splitting, Jelinek and Lafferty’s algorithm, Compare)||(splitting, size of the grammar, Compare)||(grammars, natural language processing, Is-a-Prerequisite-of)||(alternatives, neural models, Compare)||(necessity, contextualized weak supervision, Evaluate-for)||(methods, neural methods, Is-a-Prerequisite-of)||(computational cost, large grammars, Compare)||(hierarchy, language classes, Is-a-Prerequisite-of)||(language classes, L2, Evaluate-for)||(morphology, sparse vocabulary, Compare)||(rules probabilities, context
(automated processing, relies on, pre-normalization)||(training encoder-decoder architectures, requires, a lot of training data)||(encoder-decoder architectures, requires, training data)||(MTL architecture, uses, a grapheme-to-phoneme dictionary)||(grapheme-to-phoneme dictionary, is, auxiliary data)||(ITransF, uses, sparse attention mechanism)||(sparse attention mechanism, discovers, hidden concepts)||(ITransF, performs, knowledge base completion)||(ITransF, transfers, statistical strength)||(ITransF, represents, associations between relations and concepts)||(embedding model, proposes, ITransF)||(PositionRank, is, an unsupervised model for keyphrase extraction)||(unsupervised model, incorporates, information from all positions)||(position extraction, can describe, the document's content)||(neural encoder-decoder transition-based parser, is, a full-coverage semantic graph parser)||(parser, predicts, graphs jointly with unlexicalized predicates)||(feature extraction, is seen as, a
(edit distance, Evaluate-for, string similarity)||(string similarity, Compare, edit distance)
(conjunction, Presents, neural seq2seq models)||(morphology, Part-of, language models)||(morphology, Is-a-Prerequisite-of, lexicon)||(neural seq2seq models, Evaluate-for, inflection)||(morphology, Used-for, inflection)||(lexicon, Used-for, inflection)||(morphology, Is-a-Prerequisite-of, inflection)
None
(concept, Contains, game playing agent)|||(game playing agent, Used-for, developing agent)|||(deep reinforcement learning, Used-for, developing agent)|||(deep reinforcement learning, Presents, challenges)|||(world-perceiving modules, Used-for, prune actions)|||(world-perceiving modules, Used-for, decompose tasks)|||(two-phase training framework, Used-for, decouple language learning from reinforcement learning)|||(experimental results, Show, proposed method)|||(proposed method, Improves, performance and sample efficiency)|||(natural language annotations, Used-for, model interpretability)|||(approach, Uses, linear probing)|||(approach, Uses, domain-specific terms)|||(game concepts, Encoded-in, policy networks)|||(policy networks, Trained-via, imitation learning)|||(policy networks, Trained-via, reinforcement learning)|||(layer of models, Encoded, high-level abstractions)|||(high-level abstractions, Used-in, natural language annotations).
(recommendation system, Is-a-Prerequisite-of, neural news recommendation approach)||(recommendation system, Is-a-Prerequisite-of, personalized news recommendation)||(user representation, Is-a-Prerequisite-of, accurate news recommendation methods)||(news content, Is-a-Prerequisite-of, accurate news and user representations)||(semantic features, Is-a-Prerequisite-of, fine-grained information for recommendation)||(news content, Part-of, news recommendation methods)||(user representations, Part-of, news recommendation methods)||(news encoder, Part-of, neural news recommendation approach)||(user encoder, Part-of, neural news recommendation approach)||(user interest, Evaluate-for, news recommendation methods)||(user interest, Evaluate-for, personalized news recommendation)
### Triplets:(computation theory, Is-a-Prerequisite-of, neural techniques)|||(computation theory, Is-a-Prerequisite-of, end-to-end computational argumentation mining)|||(neural techniques, Used-for, end-to-end computational argumentation mining)|||(end-to-end computational argumentation mining, Compare, dependency parsing)|||(dependency parsing, Part-of, token-based dependency parsing)|||(dependency parsing, Evaluate-for, performance results)|||(dependency parsing, Compare, subpar performance results)|||(token-based dependency parsing, Evaluate-for, performance results)|||(token-based sequence tagging, Part-of, end-to-end computational argumentation mining)|||(token-based sequence tagging, Evaluate-for, robust performance results)|||(token-based sequence tagging, Compare, performance results)|||(BiLSTMs, Used-for, performance results)|||(multi-task learning, Is-a-Prerequisite-of, performance)|||(A* CCG parsing model, Is-a-Prerequisite-of, English and Japanese CCG
### Concept:text generation### Triplets:(text generation, Compare, language generation task)(text generation, Used-for, generating complex programs)(text generation, Used-for, generation of conversational text)(text generation, Is-a-Prerequisite-of, generating automatic Pyramid scores)(text generation, Is-a-Prerequisite-of, AMR-to-text generation)(text generation, Used-for, Automatic Retrieval)
(character level language model, Used-for, character-level neural machine translation)||(character level language model, Evaluate-for, bits-per-character performance)||(character level language model, Part-of, hierarchical LSTM language model)||(hierarchical LSTM language model, Evaluate-for, language model prediction)||(hierarchical LSTM language model, Is-a-Prerequisite-of, generation of conversational text)||(character level language model, Is-a-Prerequisite-of, language modeling)
(latent variable model, used-for, generative latent-variable model)||(latent variable model, part-of, response generation) ||(latent variable model, Is-a-Prerequisite-of, entity-linking model)||(latent variable model, Evaluate-for, multilingual word representations) ||(latent variable model, Hyponym-Of, Gaussian Mixture LVeGs) ||(latent variable model, Conjunction, generative latent-variable model) ||(generative latent-variable model, Evaluate-for, text generation) ||(entity-linking model, Is-a-Prerequisite-of, neural entity-linking model)||(neural entity-linking model, Evaluate-for, entity linking) ||(generative latent-variable model, Is-a-Prerequisite-of, semi-supervised learning) ||(generative latent-variable model, Evaluate-for, generative latent-variable model)
None.
(None)
(conjuction, part-of, Wall Street Journal)||||(part-of, Is-a-Prerequisite-of, adverbial presupposition triggers)||||(part-of, part-of, full Wall Street Journal portion)||||(Used-for, extracted from, English Gigaword dataset)||||(part-of, of, author's gender)||||(annotate, annotation, Penn Treebank)||||(dependency, represents, syntactic structures)||||(determine, of, structure into single tree-based parsing)
(bio text mining, Part-of, entity resolution)||(bio text mining, Is-a-Prerequisite-of, entity normalization)||(entity normalization, Is-a-Prerequisite-of, biomedical named entities)||(biomedical named entities, Evaluate-for, many biomedical text mining tools)||(bio text mining, Evaluate-for, text mining tools)||(biomedical named entities, Used-for, entity normalization)||(entity resolution, Used-for, knowledge base creation)
(document representation, Used-for, semantic parsing)(document representation, Evaluate-for, translation)(neural networks, Used-for, geolocation prediction model)(neural networks, Compare, kernel methods)(neural networks, Compare, discourse structure)(neural networks, Compare, neural machine translation)(neural machine translation, Evaluate-for, morphology learning)(NER, Evaluate-for, local detection approach)(NER, Is-a-Prerequisite-of, geolocation prediction model)
(concept, Used-for, neural encoder-decoder transition-based parser)||||(semantic graph parser, Is-a-Prerequisite-of, lexicalized parsing)||||(lexicalized parsing, Evaluate-for, natural language input)||||(lexicalized parsing, Part-of, neural machine translation system)||||(lexicalized parsing, Is-a-Prerequisite-of, generative grammar)||||(lexicalized parsing, Compare, syntax-agnostic NMT baseline)
(concept, Compare, Named entity recognition)||||(NER, Part-of, NLP)||||(NER, Evaluate-for, FFNN)||||(NER, Evaluate-for, FOFE method)||||(NER, Part-of, MD)||||(NER, Used-for, reject or predict entity label)||||(NER, Evaluate-for, entity label prediction performance)||||(NER system, Part-of, supervised machine learning models)||||(NER data, Is-a-Prerequisite-of, machine learning models training)||||(NER, Evaluate-for, performance improvement)||||(NER, Compare, linear-chain CRFs)||||(NER, Compare, BiLSTM)||||(NER, Is-a-Prerequisite-of, transfer learning for BiLSTM)||||(NER, Evaluate-for, BiLSTM performance improvement)||||(NER, Evaluate-for, model performance improvement)||||(NER systems, Used-for, NER tasks)||||(NER data, Part-of, NER tasks)||||(NER, Compare, Transformer-based models)
(None)
(None)
### Triplets:(feature selection, part-of, feature extraction)||||(feature selection, Is-a-Prerequisite-of, data quality)||||(feature selection, Evaluate-for, model performance)||||(feature selection, Compare, training data selection)||||(feature selection, Used-for, task-specific model training)
(matrix factorization, Represents, approach) ||| (users, represented-as, user-topic matrix) ||| (topics, represented-as, latent feature space) ||| (matrix factorization, Useful-for, predicting missing preferences) ||| (topics, encoded-as, latent vector representations)
(first-order models, compress to high sparsity, PLMs)|||(PLMs, SMP, adapt to downstream tasks)|||(PLMs, first-order pruning, converge to downstream tasks)|||(PLMs, pruning, compression method)|||(chain reasoning paradigm, generate logic rules, reasoning)|||(chain reasoning paradigm, capture long-range interdependence, compositional nature)|||(chain reasoning paradigm, improve interpretability, explicitly modeling)|||(chain reasoning paradigm, long-range dependencies, significant challenge)|||(chain reasoning paradigm, model decomposable logic rules, reasoning)|||(SMP, more parameter efficient, does not require fine-tuning)|||(SMP, applicable to low sparsity, outperforms zero-order methods)|||(SMP, outperforms first-order methods, at various sparsity levels)|||(long-range dependencies, challenging, beyond sentence level)|||(long-range dependencies, model, significant challenge)|||(document-level event argument extraction, identify event arguments, beyond sentence level)|||(document-level event argument extraction
### Extracted Concepts:- neural semantic parser- domain-general natural language representations- predicate-argument structures- Morphologically rich languages- distributional vector space models- morph-fitting procedure- morphological constraints- downstream task of dialogue state tracking- neural machine translation (MT) models- layer-wise relevance propagation (LRP)- word embeddings- recurrent neural network (RNN)- bidirectional language models- encoder-decoder framework### Triplets:(neural semantic parser, Used-for, converting natural language utterances to intermediate, domain-general natural language representations)||(domain-general natural language representations, Is-a-Prerequisite-of, induced with a transition system)||(predicates-arguments structures, Evaluate-for, end-to-end training using annotated logical forms)||(distributional vector space models, Compare, to morph-fitting procedure )||(morph-fitting procedure, Is-a-Prerequisite-of, dealing with morphologically rich languages)||(morph-fitting procedure, Evaluate
(concept, enriches, Stack-LSTM transition-based AMR parser)|||(Stack-LSTM transition-based AMR parser, includes, Policy Learning)|||(Stack-LSTM transition-based AMR parser, combined with, attention mechanism)|||(Stack-LSTM transition-based AMR parser, augmented with, pre-processed concept identification)
(concept, Is-a-Prerequisite-of, Natural Language Processing Models) ||| (Graph-based approach, Used-for, Abstractive Document Summarization) ||| (Graph-based neural network model, Used-for, Relation extraction) ||| (Graph-based method, Is-a-Prerequisite-of, Dependency tree linearization task) ||| (Graph encoding framework, Used-for, Graph-to-sequence modeling) ||| (TextING, Compare, Text classification methods) ||| (Graph neural networks, Compare, Multi-layer BiLSTMs)
(concepts, Part-Of, phonetic information)||(phonetic information, Part-of, phonological distinctive features)||(recurrent neural network models, Compare, phonotactic learning)||(distintive features, Evaluate-for, phonotactic learning)||(phonetic information, Is-a-Prerequisite-of, segment-level phonotactic acquisition)||(phonetically grounded language generation, Used-for, lyrics and poetry)||(tongue twisters, Is-a-Prerequisite-of, phonetically conditioned language)||(TwistList, Part-of, dataset of tongue twisters)||(TwisterMisters, Evaluate-for, tongue twister generation)||(mainstream pre-trained models, Compare, tongue twister generation)||(TableQA models, Is-a-Prerequisite-of, retriever-reader pipelines)||(retriever component, Part-of, TableQA models)||(interactions, Part-of, retriever component)||(joint training scheme, Evaluate-for, retriever and reader)||(binary relevance token, Used-for, filtering table usage)||(strategies
(None)
(concept, Compare, stochastic optimization)||||(stochastic optimization, Used-for, training of RNNs)||||(training of RNNs, Is-a-Prerequisite-of, language modeling)||||(stochastic optimization, Evaluate-for, model uncertainty)||||(stochastic optimization, Hyponym-Of, stochastic gradient Markov Chain Monte Carlo)
(concept, Used-for, relation extraction)||(relation extraction, Is-a-Prerequisite-of, information extraction)||(relation extraction, Compare, distant supervision)||(relation extraction, Compare, joint extraction of entities and relations)||(relation extraction, Compare, AMR-to-text generation)||(relation extraction, Evaluate-for, model performance)||(relation extraction, Is-a-Prerequisite-of, relation detection)||(relation extraction, Compare, Knowledge Base Question Answering (KBQA))||(relation extraction, Compare, event extraction)
### Output:(speech synthesis, Is-a-Prerequisite-of, speech translation) || (speech synthesis, Part-of, text-to-speech model) || (speech synthesis, Evaluate-for, model training) || (text-to-speech model, Is-a-Prerequisite-of, speech synthesis) || (text-to-speech model, Used-for, generating spoken audio captions) || (model training, Evaluate-for, speech synthesis) || (image captioning module, Part-of, speech synthesis) || (image captioning module, Used-for, generating captions) || (sub-word speech units, Part-of, speech synthesis) || (image captioning module, Evaluate-for, speech synthesis)
(None)
(concepts, include, Compare)||(dimensionality reduction, is-a-Prerequisite-of, sentence meta-embeddings)||(sentence meta-embeddings, Evaluated-for, STS Benchmark)||(dimensionality reduction, Is-a-Prerequisite-of, STS Benchmark)||(STS Benchmark, Used-for, evaluating semantic textual similarity)||(dimensionality reduction, Used-for, generating sentence meta-embeddings)||(sentence meta-embeddings, Part-of, Meta-embedding methods)
(None)
(concepts, Part-of, course introduction)||(Lifelong learning, Is-a-Prerequisite-of, language model)||(DynaInst, Evaluate-for, generalization performance)||(pretrained language models, Used-for, new tasks inference)||(instruction tuning, Evaluate-for, pretrained language models)||(Neural Probabilistic Soft Logic Dialogue Structure Induction, Compare, DSI)||(Rational LAMOL, Compare, LAMOL)||(WiC task, Used-for, creation of reliable word embeddings)||||(DynaInst, Evaluate-for, instance-level generalization)||(language model, Is-a-Prerequisite-of, course introduction)||(XL-LEXEME, Evaluate-for, Lexical Semantic Change Detection model)
(None)
(neural machine translation, relies-on, bi-directional LSTMs)||(neural machine translation, achieves, competitive accuracy)||(neural machine translation, outperforms, recently published results)||(neural machine translation, enhances, state-of-the-art NMT)||(neural machine translation, with, deep architecture)||(neural machine translation, suffers-from, severe gradient diffusion)||(neural machine translation, mitigates, gradient diffusion)||(neural machine translation, proposes, linear associative units approach)||(neural machine translation, shows, effectiveness)||(neural machine translation, improves, by 11.7 BLEU points)||(neural machine translation, achieves, comparable results with state-of-the-art)||(neural machine translation, obtains, state-of-the-art performance)||(neural machine translation, interprets, internal workings)||(neural machine translation, uses, layer-wise relevance propagation)||(neural machine translation, integrates, prior knowledge sources)||(ne
(machine translation technique, Based-on, neural machine translation)|||(neural machine translation, Use, bi-directional LSTMs)|||(neural machine translation, Use, convolutional layers)|||(neural machine translation, Achieve, competitive accuracy)|||(neural machine translation, Achieve, outperform several results)|||(neural machine translation, Benefit-from, Deep Neural Networks (DNNs))||(neural machine translation, Use, Linear Associative Units (LAU))||(neural machine translation, Benefit-from, Linear Associative Units (LAU))||(neural machine translation, Provide, improvements with proper configuration)
(None)
(concept, Used-for, general-purpose programming language)||||(concept, Compare, NL-to-code generation)||||(external knowledge, Evaluate-for, NL-to-code generation)||||(programming language API documentation, Part-of, external knowledge)||||(NL-to-code generation, Is-a-Prerequisite-of, code generation testbed CoNaLa)||||(visual models, Used-for, generated Python programs)
(concept, Part-of, Latent Dirichlet Allocation)||(concept, Evaluate-for, Topic Modeling Approach)||||(Topic Modeling Approach, Is-a-Prerequisite-of, Latent Dirichlet Allocation)||(Latent Dirichlet Allocation, Compare, Bidirectional Adversarial Topic (BAT) model)||(Bidirectional Adversarial Topic (BAT) model, Evaluate-for, Text Clustering)||(concept, Used-for, Bidirectional Adversarial Topic model)
(word embedding, gained popularity on several tasks, word analogy questions and caption generation) || (word embedding, exhibits compositionality, i.e., combining two word-vectors results in a vector of a word representing the semantic composite) || (word embedding, learns using the Skip-Gram model) || (word embedding, establishes a connection with the Sufficient Dimensionality Reduction (SDR) framework) || (word embedding, exploited in aspect extraction tasks) || (word embedding, utilized for aspect coherence improvement) || (word embedding, used in bilingual word embeddings learning) || (word embedding, leveraged in neural word segmentation research) || (word embedding, applied in dependency parsing tasks) || (word embedding, plays a key role in dependency parsing) || (word embedding, provides point representations of words)
(None)
(concepts, Is-a-Prerequisite-of, natural language understanding)||(comparison paragraphs, Is-a-Prerequisite-of, reading comprehension)||(neural reading comprehension model, Evaluate-for, answering questions)||(recurrent neural networks, Compare, reading comprehension)||(multiple passages, Is-a-Prerequisite-of, answering questions)||(reading comprehension, Part-of, machine reading comprehension)||(RC datasets, Evaluate-for, natural-language understanding systems)||(document and query, Compare, mining relationship)
### Triplets extracted:(newton method, Compare, gradient descent)||||(newton method, Evaluate-for, finding roots of equations)||||(newton method, Is-a-Prerequisite-of, linear algebra)||||(newton method, Used-for, optimization)||||(newton method, Compare, bisection method)||||(newton method, Is-a-Prerequisite-of, calculus)||||(newton method, Compare, secant method)
(constraints, Evaluate-for, neural models)||(constraints, Evaluate-for, neural networks)||(constraints, Evaluate-for, expressive input representations)||(constraints, Evaluate-for, structured tuning framework)
None.
(spelling correction, Is-a-Prerequisite-of, language understanding)(spelling correction, Evaluate-for, Chinese Spelling Correction (CSC))(spelling correction, Compare, Grammatical Error Correction systems)(spelling correction, Used-for, improving written communication)(spelling correction, Part-of, Chinese Spelling Correction (CSC))(spelling correction, Evaluate-for, autoregressive decoding)(spelling correction, Compare, joint decision making)
(neural network, shown-for, multi-task learning)(neural network, is-a part-of, Neural Symbolic Machine)(neural network, used-for, information extraction)(neural network, evaluate-for, model performance)(neural network, used-for, natural language processing)(neural network, is-a part-of, Deep Neural Networks)(neural network, is-a part-of, Recurrent Neural Networks)(neural network, compare-to, Kernel methods)(neural network, used-for, text categorization)(neural network, evaluate-for, training efficiency)(neural network, used-for, language understanding)(neural network, is-a part-of, Neural Machine Translation)
(conquer, Used-for, grammar induction)(conquer, Used-for, parsing decision)(conquer, Used-for, representation)(conquer, Used-for, speed-up)(grammar induction, Is-a-Prerequisite-of, parsing decision)(grammar induction, Is-a-Prerequisite-of, representation)(grammar induction, Is-a-Prerequisite-of, speed-up)(parsing decision, Is-a-Prerequisite-of, representation)(parsing decision, Is-a-Prerequisite-of, speed-up)(representation, Used-for, speed-up)(representation, Used-for, parsing decision)(representation, Used-for, grammar induction)(representation, Used-for, speed-up)
(conjuction, and, syntactic parsing)(epsilon-free chain-free synchronous context-free grammar (SCFG), Is-a-Prerequisite-of, synchronous tree-adjoining grammar (STAG))(Greibach normal form, Used-for, SCFG)(formal properties, Is-a-Prerequisite-of, SCFG)(grammar induction problem, Evaluate-for, compound probabilistic context-free grammar)(sentence, Is-a-Prerequisite-of, compound probabilistic context-free grammar)(practice, Conjunction, size increase)(grammar rank, Compare, rank)(grammar induction models, Evaluate-for, semantics and morphology)(semantic parsing, Compare, grammar-based systems)(grammar model, Compare, neural language models)(state-of-the-art methods, Compare, normalizing flow model)(normalizing flow, Evaluate-for, grammar induction)(grammar induction model, Evaluate-for, state-of-the-art accuracy)(phonetic transliteration, Is-a-Prerequisite-of, DEEP)(context embeddings, Evaluate-for, neural
(preprocessing, is-a-Prerequisite-of, Automated processing)(preprocessing, part-of, Training encoder-decoder architectures)(preprocessing, Compare, Multi-task learning)(preprocessing, part-of, analysis)(preprocessing, is-a-Prerequisite-of, Performance improvement)
(None)
(morphology, Part-of, character language models)(morphology, Evaluate-for, bits-per-character performance)(morphology, Compare, inflected words)(morphology, Compare, uninflected words)(morphology, Is-a-Prerequisite-of, inflectional morphology)(morphology, Is-a-Prerequisite-of, lexicon)(inflectional morphology, Used-for, inflected words)(lexicon, Is-a-Prerequisite-of, inflectional morphology)
(None)
(Text-based games, provide, interactive way)(deep reinforcement learning, shown effectiveness in, developing game playing agent)(deep reinforcement learning, face, major challenges)(world-perceiving modules, introduced to address, challenges)(world-perceiving modules, automatically decompose tasks and prune actions, answering questions about environment)(two-phase training framework, proposed to decouple, language learning from reinforcement learning)(two-phase training framework, further improves, sample efficiency)(experimental results, show, proposed method significantly improves performance and sample efficiency)(new dataset, containing, 10K human-annotated games of Go)(natural language annotations, used as, tool for model interpretability)(our approach, uses, linear probing to predict mentions of domain-specific terms)(game concepts, encoded in, two distinct policy networks)(mentions of domain-specific terms, predicted most easily from, later layers of both models)
(recommendation system, Used-for, document retrieval)(recommendation system, Used-for, news recommendation)(recommendation system, Part-of, conversational recommendation)(news recommendation, Part-of, personalized news recommendation)(news recommendation, Evaluate-for, user representation learning)(news recommendation, Evaluate-for, interest matching)(news recommendation, Evaluate-for, click prediction)(news recommendation, Is-a-Prerequisite-of, user interest modeling)(news recommendation, Is-a-Prerequisite-of, accurate news selection)(conversational recommendation, Part-of, conversational goal imposition)(conversational recommendation, Is-a-Prerequisite-of, natural dialog transition)(conversational recommendation, Evaluate-for, recommendation strategy)(conversational recommendation, Evaluate-for, dialog evaluation)(dialog evaluation, Evaluate-for, dialog comparison)(dialog evaluation, Is-a-Prerequisite-of, automatic dialog evaluation)
(None)
(text generation, is-a-prerequisite-of, neural language model)(neural language model, evaluate-for, generated sentences)(text generation, used-for, abstractive document summarization)(text generation, used-for, paraphrase generation)(neural language model, is-a-prerequisite-of, affective text generation)(hard attention mechanism, evaluate-for, morphological inflection generation)(deep latent variable models, evaluate-for, response generation)(is-a-prerequisite-of, automatic metric, human evaluation)
(character level language model, Used-for, character-level neural machine translation)(character level language model, Compare, word level language model)(character level language model, Compare, open-vocabulary language model)(character level language model, Is-a-Prerequisite-of, generative neural language model)(character level language model, Is-a-Prerequisite-of, hierarchical LSTM language model)
(latent variable model, used-for, response generation)(latent variable model, used-for, entity linking)(latent variable model, used-for, generative modeling)(latent variable model, Conjunction, variational autoencoder)(latent variable model, Evaluate-for, text generation)(latent variable model, used-for, discourse-level diversity capture)(latent variable model, Is-a-Prerequisite-of, conditional response generation)(latent variable model, Is-a-Prerequisite-of, multilingual word representations)
None.
(None)
(conversion, Is-a-Prerequisite-of, treebank)(annotation, Used-for, discourse relations)(conversion, Evaluate-for, parsing performance)(model, Compare, LSTM)(Wall Street Journal, Part-of, Penn Treebank)(NER tools, Evaluate-for, nested entity mentions)(Penn Treebank, Evaluate-for, taggers and parsers)(MWE-Aware English Dependency Corpus, Conjunction, non-English dependency treebanks)
(bio text mining, Used-for, knowledge base creation)(bio text mining, Used-for, text mining tools)(entity resolution, Is-a-Prerequisite-of, knowledge base creation)(entity resolution, Evaluate-for, text mining)(entity resolution, Used-for, deep learning methods)(text infilling, Evaluate-for, text generation)(text infilling, Used-for, neural sequence generative models)(named entity normalization, Part-of, biomedical text mining tools)(named entity normalization, Evaluate-for, text mining)(named entity normalization, Used-for, learning representations)(named entity normalization, Used-for, entity normalization)(named entity normalization, Is-a-Prerequisite-of, normalization of biomedical entities)(bilingual lexicons, Used-for, word alignment)(bilingual lexicons, Evaluate-for, lexicon quality)(bilingual lexicons, Part-of, bilingual text mining)(bilingual lexicons, Is-a-Prerequisite-of, multilingual word alignment)(bitext mining, Used-for, training machine translation
(document representation, used-for, semantic parser)(document representation, compared-to, fixed-size ordinally forgetting encoding)(document representation, used-for, neural machine translation)(document representation, used-for, named entity recognition)(document representation, used-for, geolocation prediction)(document representation, is-a-prerequisite-of, neural semantic parser)(document representation, is-a-prerequisite-of, discourse parser)
(lexicalized parsing, Used-for, syntactic analyzer)(syntactic analyzer, Part-of, neural machine translation system)(neural machine translation system, Evaluate-for, syntactic information)(lexicalized parsing, Evaluate-for, logical forms)
(handwriting recognition, Is-a-Prerequisite-of, named entity recognition)(handwriting recognition, Used-for, implicit discourse relation recognition)(handwriting recognition, Compare, image entity recognition)(handwriting recognition, Compare, text-based entity span detection)(handwriting recognition, Compare, Multimodal Named Entity Recognition)(handwriting recognition, Is-a-Prerequisite-of, Named Entity Recognition)(handwriting recognition, Used-for, Multimodal Named Entity Recognition)
(None)
(None)
(None)
(matrix factorization, used-for, modeling inter-topic preferences)(matrix factorization, Is-a-Prerequisite-of, latent feature space)(matrix factorization, Evaluate-for, predicting missing preferences)(matrix factorization, Is-a-Prerequisite-of, tensor factorization)(matrix factorization, Is-a-Prerequisite-of, TFBA)(matrix factorization, Is-a-Prerequisite-of, deep relevance model)(matrix factorization, Is-a-Prerequisite-of, TreeCRF extension)
(first order logic, Conjunction, logical reasoning)(first order logic, Evaluate-for, Document-level event argument extraction)(first order logic, Evaluate-for, chain reasoning paradigm)
(sparse vectors, used-for, word embeddings)(aggregate vectors, Part-of, sentence representation)(dense vectors, Part-of, word embeddings)(word2vec skip-grams, Is-a-Prerequisite-of, Gaussian mixtures)(word representations, Conjunction, orthographic features)(character-based representations, Compare, word-based representations)
(None)
```(graph-based nlp, Used-for, document summarization)(graph-based nlp, Used-for, relation extraction)(graph-based nlp, Is-a-Prerequisite-of, dependency parsing)(graph-based nlp, Is-a-Prerequisite-of, machine translation)(graph-based nlp, Compare, tree-based neural models)(graph-based nlp, Compare, LSTM)(graph-based nlp, Is-a-Prerequisite-of, semantic parsing)(graph-based nlp, Is-a-Prerequisite-of, structured output modeling)(document summarization, Compare, extractive methods)(document summarization, Compare, abstractive methods)(document summarization, Part-of, summarization)(relation extraction, Part-of, NLP tasks)(dependency parsing, Is-a-Prerequisite-of, semantic dependency parsing)(dependency parsing, Is-a-Prerequisite-of, Neural dependency parsing)(semantic dependency parsing, Compare, tree-based models)(semantic dependency parsing, Compare, transition-based parsing
(phonetics, Compare, phonological distinctive features)(phonetics, Part-of, phonotactic acquisition)(phonetics, Compare, tongue twisters)(phonetics, Evaluate-for, phonotactic patterns)(phonetics, Is-a-Prerequisite-of, phonotactic learning)(phonetics, Compare, phoneticians)(phonology, Part-of, phonetics)(phonological distinctive features, Is-a-Prerequisite-of, phonotactic acquisition)(phonetic knowledge, Evaluate-for, tongue twister generation)
(SD-NMT, Used-for, target word sequence and dependency structure)(source-side syntactic trees, Part-of, improved model)(seq2seq models, Part-of, HMM with direct lexicon and alignment models)(Arabic NLP application, Is-a-Prerequisite-of, morphological segmentation)(neural architectures on graph-to-sequence, Compare, standard recurrent networks)(ParaNMT-50M, Evaluate-for, paraphrastic sentence embeddings)(cross-cultural differences and similarities, Used-for, evaluate framework)(fully unsupervised machine translation model, Compare, supervised machine translation model)
(markov chain monte carlo, used-for, learn weight uncertainty in RNNs)\(markov chain monte carlo, Evaluate-for, stochastic gradient)\(markov chain monte carlo, Evaluate-for, model averaging)\(markov chain monte carlo, Compare, stochastic optimization)\(markov chain monte carlo, Compare, traditional training of RNNs)\(stochastic optimization, Is-a-Prerequisite-of, model uncertainty)\(stochastic optimization, Evaluate-for, large training sets)
(relation extraction, Is-a-Prerequisite-of, distant supervision)(relation extraction, Compare, knowledge base population)(relation extraction, Evaluate-for, extraction results)(relation extraction, Evaluate-for, model performance)(relation extraction, Evaluate-for, state-of-the-art accuracy)(relation extraction, Part-of, NLP applications)(relation extraction, Part-of, information extraction)
(speech synthesis, used-for, language revitalization)(speech synthesis, used-for, hate speech detection)(speech synthesis, Evaluate-for, improving automatic detection and active avoidance mechanisms for hate speech)(speech synthesis, Evaluate-for, language revitalization)(speech synthesis, Evaluate-for, capturing diverse visual semantics from images)(speech synthesis, Is-a-Prerequisite-of, hate speech detection)(speech synthesis, Is-a-Prerequisite-of, automating speech synthesis systems)(hate speech detection, Is-a-Prerequisite-of, speech synthesis)(hate speech detection, used-for, preventing harm to society)
(Vector space representations, capture many aspects of word similarity)(clustering, Used-for, spectral clustering)(spectral clustering, Is-a-Prerequisite-of, Compositor attribution)(clustering, Used-for, Compositor attribution)(clustering, Used-for, document filtering)(clustering, Compare, dependency triples)(clustering, Used-for, frame induction)(clustering, Used-for, event extraction)(clustering, Is-a-Prerequisite-of, text summarization)(clustering, Is-a-Prerequisite-of, user intent classification)(clustering, Is-a-Prerequisite-of, unknown intent detection)
(concept, Is-a-Prerequisite-of, unsupervised State of The Art)(concept, Is-a-Prerequisite-of, generalized Canonical Correlation Analysis)(concept, Is-a-Prerequisite-of, cross-view auto-encoders)
(gated recurrent unit, Compare, LSTM)  (gated recurrent unit, Compare, simple RNN)  (gated recurrent unit, Part-of, recurrent neural network)  (gated recurrent unit, Is-a-Prerequisite-of, vanishing gradients)  (gated recurrent unit, Evaluate-for, computational efficiency)  
(course introduction, relates-to, semantic parsing)(course introduction, covers, common sense reasoning)(course introduction, includes, NLP)(course introduction, introduces, neural networks)(course introduction, explains, lifelong learning)(course introduction, highlights, dialog systems)(course introduction, introduces, deep learning models)
(None)
(neural machine translation, based-on, bi-directional LSTMs)(neural machine translation, achieves-comparable-results-with, state-of-the-art)(neural machine translation, propels, performance)(neural machine translation, involves, deep learning)(neural machine translation, incorporates, syntactic knowledge)(neural machine translation, uses, linear associative units)(neural machine translation, utilizes, convolutional layers)
(machine translation technique, relies-on, bi-directional LSTMs)(machine translation technique, presents, architecture based on a succession of convolutional layers)(machine translation technique, achieves, competitive accuracy to state-of-the-art on WMT’16 English-Romanian translation)(machine translation technique, outperform, several recently published results on WMT’15 English-German)(machine translation technique, achieves, almost the same accuracy as a very deep LSTM setup on WMT’14 English-French translation)(machine translation technique, uses, Deep Neural Networks (DNNs) to enhance the state-of-the-art Neural Machine Translation (NMT))(machine translation technique, addresses, the problem of severe gradient diffusion in NMT with deep architecture)(machine translation technique, proposes, linear associative units (LAU) to reduce the gradient propagation path inside the recurrent unit)(machine translation technique, improves by, 11.7 BLEU upon Groundhog in Chinese-English
(bilinear form, used-for, link predictions)(DihEdral, Compare, existing bilinear form)(kernel function, Is-a-Prerequisite-of, objective function)
(Python, is-a-Prerequisite-of, code generation)(visual models, used-for, Python programs)(Python programs, Evaluate-for, accuracy improvement)
(latent dirichlet allocation, used-for, topical pagerank)(latent dirichlet allocation, Compare, neural topic modeling)(neural topic modeling, Compare, traditional topic models)(traditional topic models, Is-a-Prerequisite-of, latent dirichlet allocation)(latent dirichlet allocation, Is-a-Prerequisite-of, topic modeling research)(latent dirichlet allocation, Evaluate-for, topic descriptors)(latent dirichlet allocation, Evaluate-for, document-topic distribution)(document-topic distribution, Part-of, neural topic modeling)(neural topic modeling, Evaluate-for, text clustering)
(word embedding, used-for, word analogy questions)(word embedding, used-for, caption generation)(word embedding, part-of, word-to-word semantic relationship)(word embedding, part-of, distribution of word co-occurrences)(word embedding, evaluate-for, word similarity)(word embedding, evaluate-for, entailment)(word embedding, evaluate-for, sentence parsing)(word embedding, evaluate-for, named entity recognition)(word embedding, evaluate-for, chunking)(word embedding, used-for, network analysis)
(None)
```(reading comprehension, Is-a-Prerequisite-of, question answering)(reading comprehension, Conjunction, document)(reading comprehension, Conjunction, text)(reading comprehension, Is-a-Prerequisite-of, neural understanding)(reading comprehension, Conjunction, natural language)(reading comprehension, Evaluate-for, machine learning)(reading comprehension, Evaluate-for, language understanding)(reading comprehension, Is-a-Prerequisite-of, knowledge integration)(reading comprehension, Is-a-Prerequisite-of, document understanding)(document, Is-a-Prerequisite-of, reading comprehension)(text, Is-a-Prerequisite-of, reading comprehension)(machine learning, Is-a-Prerequisite-of, reading comprehension)(language understanding, Is-a-Prerequisite-of, reading comprehension)(neural understanding, Is-a-Prerequisite-of, reading comprehension)(natural language, Is-a-Prerequisite-of, reading comprehension)(knowledge integration, Is-a-Prerequisite-of, reading comprehension)(document understanding
(None)
(log-linear model, represents, features)(log-linear model, guides, learning processing)(log-linear model, used-for, integrating prior knowledge)(neural machine translation, features, log-linear model)(neural machine translation, integrates, prior knowledge)(expressive input representations, are, orthogonal to knowledge-rich constrained decoding mechanisms)(structured tuning framework, improves, models)(structured tuning framework, provides, supervision)(RoBERTa, is-a-Prerequisite-of, framework)(framework, outperforms, baseline)(framework, achieves, improvements)(linear guardedness, is-a-Prerequisite-of, downstream log-linear model)(downstream log-linear model, leverages, linear decomposition)(downstream classifiers, are trained on, modified representations)(ReLU-activated Transformer, can be considered as, linear model)(linear decomposition, generates, local explanations)
(None)
(highway network, Used-for, modeling relationships among the utterances)(highway network, Is-a-Prerequisite-of, multi-turn conversation)(highway network, Compare, SMN)(highway network, Compare, GCN)(highway network, Is-a-Prerequisite-of, Hierarchical text classification)(highway network, Compare, Generative Feature Matching Network)(highway network, Compare, Neural Graph Matching Networks)
(caption generation, Is-a-Prerequisite-of, word-embedding models)(word-embedding models, Evaluate-for, caption generation)(caption generation, Used-for, question generation)(caption generation, Used-for, entailment generation)(caption generation, Compare, semantic summarization)(caption generation, Hyponym-Of, abstractive summarization)(caption generation, Hyponym-Of, question generation)(caption generation, Hyponym-Of, entailment generation)
(construction grammarians, contrast with classic approaches to content moderation, heuristic-based rules)(Rule By Example, compare, rule-based heuristic approach)(heuristic search, used-for, learning architectures of a recurrent or convolutional cell)(heuristic search, compare, deep learning-based approaches)
(context-sensitive grammar, Compare, context-free grammar)  (context-sensitive grammar, Compare, probabilistic context-free grammar)  (context-sensitive grammar, Conjunction, linear indexed grammars)  (context-sensitive grammar, Conjunction, tree-adjoining grammars)  (context-sensitive grammar, Compare, mildly context-sensitive grammars)
(machine translation, relies-on, bi-directional LSTMs)(machine translation, based-on, convolutional layers)(machine translation, is-used-for, Neural Machine Translation)(machine translation, outperforms, recently published results)(machine translation, addresses, severe gradient diffusion)(machine translation, achieves, competitive accuracy)(machine translation, benefits-from, syntactic encoders)(machine translation, learns, representations)(machine translation, interprets, internal workings)(machine translation, integrates, prior knowledge)(machine translation, improves, translation performance)(machine translation, learns, translational correspondences)(machine translation, is-based-on, sequential encoder-decoder framework)(machine translation, incorporates, source-side syntactic trees)
(concept, part-of, deep learning architectures)(concept, used-for, modeling compositionality in text sequences)(concept, part-of, neural models of language)(concept, Compare, representation function)(concept, Evaluate-for, better understanding of neural models)(concept, part-of, semantic models)(concept, Compare, decoding fMRI patterns)
(None)
(None)
(Generative Domain-Adaptive Nets, Evaluate-for, Question Answering Models)(Generative Domain-Adaptive Nets, Used-for, Boosting Performance)(Hybrid Code Networks, Is-a-Prerequisite-of, Dialog Systems)(Hybrid Code Networks, Evaluate-for, Reducing Training Data)(Model, Used-for, Task of Labeled Sequence Transduction)(Generative Domain-Adaptive Nets, Evaluate-for, Training Framework)(Generative Domain-Adaptive Nets, Evaluate-for, Unlabeled Text)
(neural techniques, Used-for, parsing)(dependency parsing, Is-a-Prerequisite-of, AM)(dependency parsing, Compare, sequence tagging)(dependency parsing, Is-a-Prerequisite-of, sequence tagging)
(convolutional neural network, Is-a-Prerequisite-of, deep pyramid CNN)(convolutional neural network, Is-a-Prerequisite-of, Picturebook embeddings)(convolutional neural network, Used-for, text categorization)(convolutional neural network, Used-for, generating word embeddings)(convolutional neural network, Compare, SoPa model)(convolutional neural network, Part-of, disconnected recurrent neural network)(convolutional neural network, Compare, recurrent neural network)(convolutional neural network, Compare, neural semantic parsing)(convolutional neural network, Compare, character-level CNN)
(sentence boundary recognition, part-of, neural language model)(sentence boundary recognition, evaluate-for, predictive distributions)
(Word embedding models, Used-for, Language modeling)(ACL 2014, Evaluate-for, Neural Machine Translation)(Geolocation prediction model, Used-for, Social media)(Anchor methods, Compare, Interactive topic models)(Anchor methods, Evaluate-for, Interactive topic modeling)(GloVe, Evaluate-for, Relation vectors)(Hierarchical document encoder, Evaluate-for, Attention-based extractor)(Interactive topic models, Used-for, Anchor methods)
(monte carlo method, Used-for, numerical simulations)(monte carlo method, Compare, statistical methods)(monte carlo method, Evaluate-for, stochastic processes)
(None)
(weakly-supervised learning, utilized-for, natural language processing tasks)(weakly-supervised learning, utilized-for, entity linking without labeled data)(weakly-supervised learning, utilized-for, automated cognitive task analysis transcript parsing)(Natural language processing tasks, Compare, weakly-supervised learning)(entity linking without labeled data, Compare, weakly-supervised learning)(automated cognitive task analysis transcript parsing, Compare, weakly-supervised learning)
(None)
(None)
(None)
(unlexicalized parsing, a) Compare, lexicalized parsing)(unlexicalized parsing, Is-a-Prerequisite-of, semantic graph parser)(unlexicalized parsing, Is-a-Prerequisite-of, neural encoder-decoder transition-based parser)(neural encoder-decoder transition-based parser, Part-of, model architecture)(neural encoder-decoder transition-based parser, Used-for, predicting graphs jointly with unlexicalized predicates and their token alignments)(neural encoder-decoder transition-based parser, Compare, attention-based baselines on MRS)(neural encoder-decoder transition-based parser, Is-a-Prerequisite-of, GPU batch processing)(neural encoder-decoder transition-based parser, Compare, high-precision grammar-based parser)(ONLG, Part-of, data-driven architecture)(ONLG, Is-a-Prerequisite-of, generating subjective responses triggered by users' agendas)(ONLG, Is-a-Prerequisite-of, wide-coverage automatically-acquired generative grammars)(
None.
(logistic regression, method, probing task)(logistic regression, used-for, quantifying harmful content)(logistic regression, Evaluate-for, assessing toxicity)(logistic regression, Compare, logistic regression classifiers)(logistic regression, Compare, continuous-time deconvolutional regressive neural network)(logistic regression, Compare, linguistic representations)(logistic regression, Compare, transformer-based architecture)
(sequence classification, Conjunction, span extraction)(span extraction, Is-a-Prerequisite-of, sequence classification)
(concept, Part-of, scientific articles)(auxiliary tasks, Used-for, scientific article summarization)(semantic super-sense tagging, Used-for, scientific article summarization)(multi-word expressions, Used-for, scientific article summarization)(task, Compare, scientific article summarization)(deep recurrent neural networks, Used-for, scientific article summarization)(models, Compare, previous state of the art approaches)(scientific KBC datasets, Part-of, scientific article summarization)(research paper, Is-a-Prerequisite-of, scientific article summarization)(novel method, Used-for, scientific article summarization)(videos of talks at scientific conferences, Used-for, scientific article summarization)(talks, Is-a-Prerequisite-of, summaries for scientific papers)(model, Compare, models trained on a dataset of summaries created manually)(Copy module, Used-for, abstractive summarization models)(encoder-decoder attention, Compare, copy distribution)(self-attention layer, Is-a-Prerequisite-of, copy
`(deep learning introduction, Compare, data augmentation method)(deep learning introduction, Evaluate-for, training a deep learning, abstractive summarization model)(deep learning introduction, Part-of, artificial titles)(deep learning introduction, Evaluate-for, sequential training)(deep learning introduction, Is-a-Prerequisite-of, boosting performance for unsupervised adaptation)(deep learning introduction, Used-for, fine-tuning with limited data)(data augmentation method, Compare, traditional augmentation methods)(data augmentation method, Evaluate-for, novel data augmentation method)(data augmentation method, Compare, performance over strong baselines)(traditional augmentation methods, Evaluate-for, boosting accuracy)(novel data augmentation method, Evaluate-for, boosting performance)(artificial titles, Compare, randomly drop, swap or replace words)(sequential training, Evaluate-for, capturing grammatical style)(training a deep learning, abstractive summarization model, Evaluate-for, lack of a large set of training summaries)(adapting
(concept, Is-a-Prerequisite-of, regex tasks)(natural language processing, Evaluate-for, regular expression)(concept, Part-of, structured regex dataset)(natural language processing, Is-a-Prerequisite-of, regex synthesis dataset)(regex tasks, Part-of, StackOverflow posts)(structured regex dataset, Used-for, linguistically diverse natural language descriptions)(regex synthesis dataset, Compare, StructuredRegex dataset)(concept, Part-of, regular expression generation)(StackOverflow posts, Conjunction, regex tasks)
(morphology, Part-of, syntax)(semantics, Part-of, syntax)(machine translation, Evaluate-for, morphology)(machine translation, Evaluate-for, semantics)
(Vector space representations of words, Capture, aspects of word similarity)(Vector space representations of words, Produce, vector spaces)(vector spaces, Contain, antonyms, synonyms)(word embeddings, Link, words)(words, points, vector space)(spectral clustering, Use, word embeddings)
(language modeling, Is-a-Prerequisite-of, recurrent neural networks)(expressive kernels, Used-for, NLP)(neural language models, Compare, fixed-vocabulary language models)(language modeling, Is-a-Prerequisite-of, natural language processing)(language models, Evaluate-for, perplexity experiments)(language modeling, Is-a-Prerequisite-of, named entity recognition)(neural encoder-decoder transition-based parser, Evaluate-for, Full-coverage semantic graph parser)(language models, Is-a-Prerequisite-of, document context modeling)(language modeling, Is-a-Prerequisite-of, word-level language detection)
(sentence representation, Used-for, semantic parsing)(sentence representation, Evaluate-for, sentence-level sentiment classification)(sentence representation, Part-of, Natural Language Processing)(sentence representation, Compare, word embeddings)(sentence representation, Compare, word representations)(sentence representation, Is-a-Prerequisite-of, language understanding systems)
(None)
(semantic similarity, Compare, stylistic similarity)(semantic similarity, Compare, lexical similarity)(semantic similarity, Used-for, text similarity measures)(semantic similarity, Used-for, document similarity measures)(document similarity measures, Part-of, text similarity measures)(text similarity measures, Is-a-Prerequisite-of, text summarization models)(text summarization models, Is-a-Prerequisite-of, social media text summarization models)(text summarization models, Part-of, document matching approach)(text matching, Evaluate-for, use of domain knowledge)(text matching, Evaluate-for, thematic similarity)(text summarization, Is-a-Prerequisite-of, text matching)(text similarity measures, Compare, automatic methods)(text similarity measures, Compare, sentence mover's similarity)(text matching, Is-a-Prerequisite-of, sentence clustering benchmark)(document similarity measures, Is-a-Prerequisite-of, semantic similarity)(document similarity measures, Compare, stylistic similarity)(text classification, Used-for, adversarial attacks
(kernel graphical models, Compare, word-occurance semantic information)(kernel graphical models, Evaluate-for, short text clustering)(word-occurance semantic information, Part-of, OSDM)(OSDM, Is-a-Prerequisite-of, Online Semantic-enhanced Dirichlet Model)(CompareNet, Is-a-Prerequisite-of, end-to-end graph neural model)
None.
(dirichlet processes, Used-for, modeling words)(dirichlet processes, Is-a-Prerequisite-of, topic-sensitive representations)(topic-sensitive representations, Compare, single word representations)(dirichlet processes, Used-for, learning multiple topic-sensitive representations)(dirichlet processes, Is-a-Prerequisite-of, Hierarchical Dirichlet Process)(Hierarchical Dirichlet Process, Compare, Dirichlet Process)(dirichlet processes, Is-a-Prerequisite-of, modeling topics)(dirichlet processes, Is-a-Prerequisite-of, integrating topic distributions)(Hierarchical Dirichlet Process, Used-for, distinguishing between different meanings of a given word)
(mean field approximation, Used-for, second-order parsing)(second-order parsing, Is-a-Prerequisite-of, second-order semantic dependency parser)(second-order parsing, Used-for, mean field (MF) variational inference)(mean field (MF) variational inference, Evaluate-for, approximate second-order parsing)(mean field approximation, Used-for, recurrent layers)(recurrent layers, Compare, loopy belief propagation (LBP))(mean field approximation, Is-a-Prerequisite-of, approximated second-order parsing)(mean field approximation, Compare, loopy belief approximation)(mean field (MF) variational inference, Is-a-Prerequisite-of, recurrent layers)
(belief propagation, is-a-prerequisite-of, second-order semantic dependency parsing)(belief propagation, used-for, document-level AMRs)(belief propagation, evaluate-for, detection of rumors on social media)(belief propagation, part-of, end-to-end AMR coreference resolution model)
(None)
(None)
(None)
(None)
```(Markov random fields, Used-for, conditional random field)(Markov random fields, Is-a-Prerequisite-of, neural sequence labeling)(Markov random fields, Is-a-Prerequisite-of, Conditional Random Fields)(Markov random fields, Compare, Weighted finite-state machines)(Markov random fields, Is-a-Prerequisite-of, SeqVAT)(Markov random fields, Is-a-Prerequisite-of, Supervised aspect extraction)(Markov random fields, Compare, hybrid semi-Markov conditional random fields)(Markov random fields, Compare, Masked language models)(Markov random fields, Compare, AfricaPOS dataset)(Markov random fields, Is-a-Prerequisite-of, Named entity recognition)```  
(singular value decomposition, Evaluate-for, feature decomposition)(feature decomposition, Compare, ConFEDE)(ConFEDE, Used-for, enhancing representation)(multimodal sentiment analysis, Is-a-Prerequisite-of, predicting sentiment)(multimodal sentiment analysis, Compare, unimodal sentiment analysis)(text, Part-of, modalities)(video frames, Part-of, modalities)(audio, Part-of, modalities)(multimodal sentiment analysis, Compare, multimodal information)(representation, Compare, information)(bias-variance tradeoff, Evaluate-for, learning methods)(variance, Evaluate-for, model complexity)(learning methods, Is-a-Prerequisite-of, balance)(over-fitting, Evaluate-for, under-fitting)(neural networks, Hyponym-Of, learning methods)(neural models, Used-for, computational psycholinguistics)(language models, Used-for, providing conditional probability distributions)(language models, Evaluate-for, psychometric quality)(Transformer-based language models, Compare
(Evaluation of dependency parsing, Used-for, evaluating new approaches)(Evaluation of dependency parsing, Compare, measuring performance across different models)(Evaluation of dependency parsing, Compare, comparing results with state-of-the-art)(Dependency parsing, Is-a-Prerequisite-of, semantic dependency parsing)(Dependency parsing, Is-a-Prerequisite-of, second-order dependency parsing)(Dependency parsing, Is-a-Prerequisite-of, transition-based dependency parsing)(Dependency parsing, Part-of, syntactic dependency analysis)(Dependency parsing, Part-of, graph-based models)(Chinese word segmentation, Evaluate-for, dependency parsing)(Chinese word segmentation, Compare, performance with and without joint learning)(Word embeddings, Is-a-Prerequisite-of, applying to dependency parsing)(Structured prediction, Is-a-Prerequisite-of, search-based dependency parsing)(Structured prediction, Is-a-Prerequisite-of, neural machine translation)(Structured prediction, Evaluate-for, improving model performance)(SPIGOT, Used-for, backpropagating through structured predictions)(Semantic dependency
(variational autoencoder, utilizes, kullback-leibler divergence)(variational autoencoder, can result in, posterior collapse)(kullback-leibler divergence, computed individually for each, datapoint)(kullback-leibler divergence, proposed to follow a distribution across, dataset)(kullback-leibler divergence, needs its expectation to be kept, positive)(batch normalized-vae, proposed as, an approach to prevent posterior collapse)(batch normalized-vae, achieved by, regularizing the distribution of approximate posterior's parameters)(batch normalized-vae, can be extended to, conditional VAE)(our approach, surpasses, autoregressive baselines on language modeling)(our approach, rivals, more complex approaches in efficiency)
### Extracted Concepts:1. Text similarity measures2. Plagiarism detection3. Information ranking4. Paraphrases5. Natural language generation6. Sequential models7. Deep learning8. N-grams9. Skip-grams10. TextFlow11. DNA sequence alignment algorithms12. Sentiment analysis13. Multimodal sentiment analysis14. LSTM-based model15. Contextual information16. Recurrent neural network17. Entity mentions18. Relations19. Word embeddings20. Semantic information21. Sense-level information22. NLP systems23. Sense representation24. Fine-grained classes25. Sentiment classification26. RNN with attention27. ASC (Aspect sentiment classification)28. Memory networks (MNs)29. Attention mechanism30. Target-sensitive sentiment31. Sentiment "translation"32. Textual entailment (
(None)
(None)
(None)
(None)
None.
(concept, Evaluate-for, labeled sequence transduction)(labeled sequence transduction, Evaluate-for, neural networks)(neural networks, Used-for, handling both discrete and continuous latent variables)(unsupervised learning, Is-a-Prerequisite-of, labeled sequence transduction)(unsupervised learning, Is-a-Prerequisite-of, machine translation)(word embeddings, Is-a-Prerequisite-of, unsupervised learning)
(word embedding variation, Compare, Probabilistic FastText)(word embedding variation, Evaluate-for, word sense induction)(word embedding variation, Evaluate-for, Gaussian mixtures)(word embedding variation, Evaluate-for, uncertainty information)(word sense induction, Is-a-Prerequisite-of, word embedding variation)(Gaussian mixtures, Is-a-Prerequisite-of, word embedding variation)(uncertainty information, Is-a-Prerequisite-of, word embedding variation)
(neive bayes, Evaluate-for, text classification) (neive bayes, Compare, Support Vector Machine) (neive bayes, Compare, Logistic Regression)
(None)
(sequence-to-sequence, utilized-for, abstract meaning representation)(sequence-to-sequence, employed-for, parsing)(sequence-to-sequence, employed-for, generating text)(sequence-to-sequence, has, limitations)(sequence-to-sequence, has, shortcomings)(sequence-to-sequence, has, potential)
(Visual Question Answering, Used-for, Image Captioning)(Visual Question Answering, Compare, Question-Answering)(Visual Question Answering, Used-for, Captions Generation)(Visual Question Answering, Is-a-Prerequisite-of, Image Understanding)(Visual Question Answering, Compare, Image Captioning Systems)
(concept, Is-a-Prerequisite-of, word embeddings)(concept, Is-a-Prerequisite-of, novel model)(word embeddings, Used-for, sentiment classification tasks)(word embeddings, Is-a-Prerequisite-of, Domain Adapted (DA) word embeddings)(word embeddings, Is-a-Prerequisite-of, Generic word embeddings)(sentiment classification tasks, Evaluate-for, DA embeddings)(sentiment classification tasks, Evaluate-for, generic embeddings)(novel model, Is-a-Prerequisite-of, PCCA)(novel model, Is-a-Prerequisite-of, DPCCA)(PCCA, Evaluate-for, multilingual word similarity)(PCCA, Evaluate-for, cross-lingual image description retrieval)(DPCCA, Evaluate-for, multilingual word similarity)(DPCCA, Evaluate-for, cross-lingual image description retrieval)
(sampling, is-a-Prerequisite-of, training)(sampling, used-for, SGNS)(sampling, Evaluate-for, system)(sampling, Evaluate-for, training)(sampling, is-a-Prerequisite-of, Skip-Gram Negative Sampling)(Skip-Gram Negative Sampling, is-a-Prerequisite-of, SGNS)(Skip-Gram Negative Sampling, used-for, word representation learning)
(None)
(principal component analysis, Compare, Visual language grounding)(principal component analysis, Used-for, machine vision)(machine vision, Is-a-Prerequisite-of, neural image captioning systems)(neural image captioning systems, Evaluate-for, Show-and-Fool)(Show-and-Fool, Evaluate-for, robustness analysis)(neural image captioning systems, Used-for, adversarial examples generation)
### Concept: autoencoders(autoencoders, Combine-with, amortized variational inference)(autoencoders, Utilize, latent variables)(autoencoders, Used-for, text modeling)(autoencoders, Evaluate-for, probabilistic modeling)(autoencoders, Is-a-Prerequisite-of, understanding of encoder-decoder incompatibility)(autoencoders, Part-of, deep neural networks)(autoencoders, Part-of, generative modeling)
(Deep learning models, Used-for, automatic readability assessment)(Deep learning models, Evaluate-for, readability assessment)(automatic readability assessment, part-of, machine learning models)(linguistic features, part-of, linguistic feature traditionally used in machine learning models)(linguistic features, Used-for, automatic readability assessment)(linguistic features, Evaluate-for, readability assessment)(neural network models, Is-a-Prerequisite-of, learning syntactic dense embeddings)(features, Is-a-Prerequisite-of, embeddings)(features, Used-for, learning their embeddings)(neural network models, part-of, proposed methodology)(embedding, Is-a-Prerequisite-of, represented features)(linguistic features, part-of, features)(proposed methodology, Evaluate-for, readability assessment)(BERT-only model, Compare, proposed methodology)(neural network models, Compare, BERT-only model)(BERT-only model, Compare, proposed methodology).
(None)
(mixture models, capture, multiple word senses)(mixture models, capture, sub-word structure)(mixture models, capture, uncertainty information)(mixture models, represent, word with Gaussian mixture density)(word, represent, Gaussian mixture density)(mixture models, achieve, state-of-art performance)(mixture models, outperform, FastText embeddings)(mixture models, outperform, dictionary-level probabilistic embeddings)
(Multi-task learning, Used-for, Extracting common and task-invariant features)(Multi-task learning, Evaluate-for, Extensive experiments on text classification tasks)(Multi-task learning, Compare, Jointly learning 'natural' subtasks)(Multi-task learning, Used-for, Improving keyphrase boundary classification)(Multi-task learning, Is-a-Prerequisite-of, Holistic extendable framework)(Multi-task learning, Compare, Improvement in sequence prediction algorithms)(Multi-task learning, Hyponym-Of, Credit assignment problem)(Multi-task learning, Is-a-Prerequisite-of, Improving automatic taxonomy induction)(Multi-task learning, Compare, Adaptive collaboration with routing network)(Multi-task learning, Is-a-Prerequisite-of, Enhanced performance through dual learning algorithm)
(None)
(LSTM Noisy Channel Model, Evaluate-for, disfluency detection)(Noisy Channel Model, Used-for, disfluency detection)(LSTM Noisy Channel Model, Used-for, MaxEnt reranker)(LSTM Noisy Channel Model, Hyponym-Of, disfluency detection)(Span-ConveRT, Used-for, dialog slot-filling)(Span-ConveRT, Hyponym-Of, dialog slot-filling)
None.
(support vector machine, superior-to, other classifiers)(support vector machine, used-for, identifying MCI in transcripts)(support vector machine, compared-with, LSTM-based recurrent neural network)
(communication, Used-for, multi-agent system)(language model, Used-for, multi-agent system)(multi-agent system, Is-a-Prerequisite-of, communication)(multi-agent system, Is-a-Prerequisite-of, language model)(multi-agent system, Is-a-Prerequisite-of, training)(multi-agent system, Is-a-Prerequisite-of, teaching)(multi-agent system, Is-a-Prerequisite-of, data-driven approaches)
(TECHS, Contains, first-order logic)(TECHS, Contains, propositional reasoning)(TECHS, Integrates, first-order reasoning)(chain reasoning paradigm, Aims-to-model, long-range dependencies)(chain reasoning paradigm, Generates, decomposable first-order logic rules)
(cross entropy, is-a-Prerequisite-of, Autoregressive language models)(cross entropy, is-a-Prerequisite-of, Existing models for extractive summarization)(Autoregressive language models, Evaluate-for, minimizing the forward cross-entropy)(cross entropy, Evaluate-for, maximizing inter-class variance)(cross entropy, Evaluate-for, minimizing intra-class variance)(cross entropy, Used-for, training language models)(cross entropy, Compare, Models trained with MixCE outperform models trained with traditional cross-entropy)
None.
(dual-purpose relationship, implies, entity)(evaluate, related-to, deep learning model)(part-of, denotes, BiLSTM architecture)(compare, is-a-attribute-of, Chinese SRL)(is-a-Prequisite-of, prerequisite, syntactic information)(used-for, improves, SRL performance)
(Evaluation of text classification, Used-for, sentiment polarity and sarcasm detection)(Evaluation of text classification, Evaluate-for, automatic speech recognition)(Evaluation of text classification, Evaluate-for, natural language processing)(Evaluation of text classification, Part-of, state-of-the-art neural models)(Evaluation of text classification, Compare, adversarial attacks on text classification)(Evaluation of text classification, Compare, distant supervision in relation classification)
(EWISE, outperforms, previous systems)(EWISE, achieves, new state-of-the-art WSD performance)(Extended WSD Incorporating Sense Embeddings, utilizes, sense definitions)(Extended WSD Incorporating Sense Embeddings, learns, sentence encoder)(Extended WSD Incorporating Sense Embeddings, achieves, generalized zero-shot learning)(EWISER, breaks through, 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks)(EWISER, enables, the network to predict synsets)(Enhanced WSD Integrating Synset Embeddings and Relations, taps into, relational information encoded in Lexical Knowledge Bases)(Enhanced WSD Integrating Synset Embeddings and Relations, exploits, pretrained synset embeddings)(Contextual embeddings, used to achieve, unprecedented gains in Word Sense Disambiguation tasks)(EWISE, Nearest Neighbors, surpass, performance of previous systems)(Dense vector representations,
(neural network-based semantic parsers, Evaluate-for, database queries)(neural network-based semantic parsers, Part-of, NLP tasks)(NLP tasks, Is-a-Prerequisite-of, database queries)(neural network-based semantic parsers, Is-a-Prerequisite-of, NLP tasks)(neural network-based semantic parsers, Evaluate-for, answering queries from natural language text)(database queries, Evaluate-for, reasoning over sets of relevant facts)(neural network-based semantic parsers, Compare, transformer models)(transformer models, Evaluate-for, processing noisy data)
`(autonomous car, Is-a-Prerequisite-of, learning to follow instructions)(learning to follow instructions, Evaluate-for, autonomous car)(autonomous car, Used-for, vision-and-language navigation)(autonomous car, Part-of, agent)(learning to follow instructions, Compare, autonomous car)`
**(finite state machine, Used-for, NLP systems)  (finite state machine, Part-of, Weighted finite-state machines)  (finite state machine, Part-of, Weighted finite-state machines)  (finite state machine, Compare, Transformer in machine translation)  (finite state machine, Compare, neural paraphrasing model)  (finite state machine, Compare, multi-layer recurrent neural network model)  (finite state machine, Evaluate-for, Computation of higher-order derivatives)  (finite state machine, Evaluate-for, Automatic postediting (APE))**
(neural parsing, used-for, end-to-end computational argumentation mining)(neural parsing, part-of, dependency parsing)(neural parsing, part-of, sequence tagging)(neural parsing, Compare, dependency parsing)(neural parsing, Evaluate-for, robust performance)(dependency parsing, Is-a-Prerequisite-of, argumentation mining)(dependency parsing, Evaluate-for, subpar performance results)(sequence tagging, Used-for, local tagging)(sequence tagging, Compare, dependency parsing)(BiLSTMs, Compare, dependency parsing)(multi-task learning, Is-a-Prerequisite-of, performance improvement)(neural machine translation, Compare, neural parsing)(source sentence, Evaluate-for, encoding)(convolutional layers, part-of, architecture)(convolutional layers, Compare, recurrent networks)(architecture, Evaluate-for, simultaneous encoding)(Chinese-to-English translation, Evaluate-for, proposed syntactic encoders)(Mixed RNN encoder, Compare, Parallel RNN encoder)
### Extracted Concepts:1. Recurrent neural networks2. Part-of-speech tagging3. Rich initial word encodings4. Context sensitive representations5. Weighted finite state transducers (FSTs)6. GPU implementation7. Non-contextual subword embeddings8. FastText9. BPEmb10. Contextual representation (BERT)11. Named entity recognition12. OSCAR corpus13. ELMo embeddings14. Common Crawl15. Emoji processing in NLP16. Cross-lingual transfer17. Multi-lingual language model pretraining18. Morphological typology19. Sentiment analysis### Triplets:1. (statistical part of speech tagging, Evaluate-for, recurrent neural networks)2. (statistical part of speech tagging, Evaluate-for, part-of-speech tagging)3. (statistical part of speech tagging, Evaluate-for, rich initial
(semantic dependency parsing, Is-a-Prerequisite-of, iterative predicate selection)(neural machine translation, Is-a-Prerequisite-of, the ibm model)
(dynamic programming, Used-for, optimization)(dynamic programming, Part-of, spanning)(dynamic programming, Evaluate-for, training)(dynamic programming, Compare, greedy algorithm)(greedy algorithm, Is-a-Prerequisite-of, top-down inference)(dynamic programming, Compare, exact inference)(dynamic programming, Compare, state-of-the-art results)(dynamic programming, Part-of, sentence compression)(dynamic programming, Is-a-Prerequisite-of, event coreference resolution)(dynamic programming, Is-a-Prerequisite-of, segmentation algorithm)
(None)
(word segmentation, Used-for, pretraining character and word embeddings)(word segmentation, Compare, statistical segmentation research)(word segmentation, Is-a-Prerequisite-of, neural word segmentation)(neural word segmentation, Used-for, building a modular segmentation model)(word segmentation, Evaluate-for, model accuracy improvement)(word segmentation, Is-a-Prerequisite-of, different linguistic perspectives)(word segmentation, Compare, adversarial multi-criteria learning for CWS)(word segmentation, Part-of, Chinese word segmentation)(word segmentation, Evaluate-for, performance improvement)(word segmentation, Is-a-Prerequisite-of, Chinese analysis)(word segmentation, Part-of, character embeddings)(Chinese word segmentation, Is-a-Prerequisite-of, dependency parsing)(Chinese word segmentation, Evaluate-for, model accuracies)(word segmentation, Evaluate-for, representation comparison)(word segmentation, Evaluate-for, dependency parsing accuracy)(word segmentation, Conjunction, speech register and prosody)(word segmentation, Is-a-Prerequisite-of, discourse segmentation
(concept, Is-a-Prerequisite-of, query expansion)(concept, Used-for, AVE)(AVE, Evaluate-for, performance improvement)(concept, Is-a-Prerequisite-of, knowledge-driven query expansion)(AVE, Evaluate-for, product data enrichment)(concept, Evaluate-for, rare and ambiguous queries handling)(concept, Used-for, knowledge dropout)(concept, Used-for, knowledge token mixing)
(noisy channel model, used-for, disfluency detection)(noisy channel model, used-for, language model prompting)(noisy channel model, compare, direct models)(noisy channel model, compare, channel models)
(linear algebra, Used-for, Quaternion algebra)(linear algebra, Used-for, Quaternion-inspired models)(linear algebra, Used-for, Quaternion attention Model)(linear algebra, Is-a-Prerequisite-of, Quaternion-inspired models)(linear algebra, Is-a-Prerequisite-of, Quaternion Transformer)
(None)
(None)
(dependsyntac, Part-of, computational argumentation mining)(dependency syntax, Compare, part-of)(dependency parsing, Used-for, syntactic knowledge)(dependency syntax, Conjunction, word embeddings)(dependency syntax, Is-a-Prerequisite-of, dependency parsing)(Neural Machine Translation, Evaluate-for, dependency syntax)(dependency syntax, Compare, joint extraction of entity mentions and relations)(dependency syntax, Used-for, deep neural architecture)(dependency parsing, Is-a-Prerequisite-of, dependency syntax)(dependency syntax, Compare, transition-based dependency parser)(dependency parsing, Compare, dependency syntax)(dependency syntax, Part-of, Abstract Meaning Representations)
(shift-reduce parsing, Requires, Topology of grammar tree)(shift-reduce parsing, Is-a-Prerequisite-of, Deterministic attention mechanisms)(shift-reduce parsing, Compare, Sequence-to-sequence constituent parsing)(sequence-to-sequence constituent parsing, Is-a-Prerequisite-of, Top-down tree linearizations)(shift-reduce parsing, Compare, Traditional shift-reduce parsing schemes)(shift-reduce parsing, Compare, In-order linearization)(sequence-to-sequence constituent parsing, Is-a-Prerequisite-of, Linearization)(sequence-to-sequence constituent parsing, Is-a-Prerequisite-of, Topology of grammar tree)(sequence-to-sequence constituent parsing, Compare, Transition-based parsers)
(latent semantic indexing, Used-for, capture latent semantics)(latent semantic indexing, Compare, unsupervised representation)(latent semantic indexing, Evaluate-for, semantic textual similarity tasks)(latent semantic indexing, Is-a-Prerequisite-of, semantic textual similarity tasks)(latent semantic indexing, Compare, neural network models)(latent semantic indexing, Compare, skip-thought vectors)
(None)
(policy gradient method, utilized-for, summarization model)(policy gradient method, enables, bridge non-differentiable computation)(summarization model, Used-for, rewriting sentences)(policy gradient method, trained using, reinforcement learning)(policy gradient method, is-a-prerequisite-of, end-to-end reinforcement learning model)(policy gradient method, leads to, more accurate normalizations)(coreference models, usually trained with, heuristic loss functions)(policy gradient training, enables, direct optimization for exact matches)(policy gradient training, has the potential to, reduce exposure bias)(coreference models, adapted by, introduced an end-to-end reinforcement learning based model)(policy gradient training, leads to, significant improvements)
(CARI, Evaluate-for, FST)(FST, Is-a-Prerequisite-of, sentiment classification)(neo-pronouns, Part-of, non-binary gender identities)(neo-pronouns, Evaluate-for, language models)(gender-neutral pronouns, Part-of, gender-neutral pronoun)(gender-neutral pronouns, Evaluate-for, language models)(non-binary gender identities, Evaluate-for, language models)
(Kernel methods, Enable, direct usage of structured representations)(Kernel methods, Achieve, excellent performance)(Deep neural networks, demonstrated to be effective in learning feature representations)(Kernel methods, combined with, deep neural networks)(Nystrom low-rank approximation, applied to, input layer of deep architecture)(Word embeddings, trained on large-scale corpora)(Domain Specific (DS) word embeddings, trained only on data from a domain of interest)(Domain Adapted (DA) word embeddings, formed by aligning corresponding word vectors)(String kernels, used for automatic essay scoring)(String kernels, capture similarity among strings based on common n-grams)(String kernels, combined with, bag-of-super-word-embeddings)(Representational Similarity Analysis (RSA), allows to quantify neural activation patterns)
(loss function, limits expressivity of, structured prediction)(loss function, used in, loss-augmented setting)(loss function, enables the use of, more complex loss functions)(loss function, used in, learning algorithms)(loss function, used in, extractive and abstractive models)(loss function, introduced to penalize, inconsistency between attention levels)(loss function, used in, end-to-end training)(loss function, introduced to evaluate, resources required for performance targets)(loss function, employed in, supervisory learning)(loss function, used in, knowledge graph embedding)(loss function, compared with, negative sampling loss)(loss function, adjusted by, assigning different training weights)(loss function, measures learning difficulty of, each target token)(loss function, used in, token-level adaptive training)
### Concept: image retrieval(bigrams, Describe, images)(images, Conjunction, text features)(image pairs, annotated, relationship sentences)(image editing dataset, Include, image pairs)(image editing dataset, Contains, editing instructions)(document retrieval, For, image retrieval systems)(image captioning, Focus, single image captioning)
### Extracted Concepts:1. Evaluation dataset2. Compositional distributional semantics models3. Semantic parsing4. Multilingual model5. Text extraction6. Convolutional neural network7. Sequence-to-sequence model8. Cardinalities in relations9. Dependency parser10. Sequence-to-tree model11. Transition-based dependency parser12. Distance supervision method13. Neural abstractive method14. Compound splitters15. RNN/CNN models16. Point-by-point comparative study17. Simple Word-Embedding-based Models (SWEMs)18. Extractive and abstractive summarization19. Grammatical Error Correction (GEC) evaluation20. Automatic metrics21. Cross-lingual natural language understanding22. Specificity control variable23. Statistical significance testing### Triplets:1. (Evaluation dataset, Is-a-Prerequisite-of, Compositional
(None)
**(Semantic relations, related to, propositional logic)(Source domain, Is-a-Prerequisite-of, Target domain)(Attention mechanism, Used-for, Natural language processing)(Attention module, Has, Weight sum operation)**
(None)
(None)
(information extraction, used-for, relation extraction)(relation extraction, Evaluate-for, extraction results)(relation extraction, Compare, distant supervision)(relation extraction, Compare, baselines)(relation extraction, Is-a-Prerequisite-of, model performance)(relation extraction, Is-a-Prerequisite-of, state-of-the-art)(relation extraction, Used-for, training data)(relation extraction, Evaluate-for, human efforts)(relation extraction, Is-a-Prerequisite-of, building training data)(relation extraction, Compare, baselines)(relation extraction, Compare, noise)(relation extraction, Evaluate-for, dynamic transition matrix)(distant supervision, Is-a-Prerequisite-of, relation extraction)(distant supervision, Compare, experimental results)(distant supervision, Compare, technique)(distant supervision, Is-a-Prerequisite-of, model performance)(distant supervision, Compare, noise)(distant supervision, Evaluate-for, dynamic transition matrix)(distant supervision, Is-a-Prerequisite-of, training data)(distant
None.
(training neural network, Used-for, language understanding)(training neural network, Evaluate-for, task reward)(training neural network, Used-for, machine reading)(training neural network, Is-a-Prerequisite-of, symbol reasoning)(training neural network, Evaluate-for, model optimization)(training neural network, Compare, state-of-the-art models)(training neural network, Is-a-Prerequisite-of, question answering)(training neural network, Used-for, labelled sequence transduction)(training neural network, Used-for, question answering models)(training neural network, Is-a-Prerequisite-of, generative domain adaptive nets)(training neural network, Evaluate-for, model improvement)(training neural network, Is-a-Prerequisite-of, text categorization)(training neural network, Is-a-Prerequisite-of, named entity recognition)
(speech signal analysis, involves, discovering word-like acoustic units)(speech signal analysis, involves, grounding to image regions)(speech signal analysis, involves, spoken language acquisition)(speech signal analysis, used-for, spoken instances detection)(speech signal analysis, evaluates-for, semantic enrichment)(speech signal analysis, part-of, linguistic aspects)(speech signal analysis, part-of, prosodic and phonemic variation)
(None)
(Hierarchical LSTM language model, Evaluate-for, Statistical parsing)(Hierarchical LSTM language model, Used-for, Language modeling)(Statistical parsing, Is-a-Prerequisite-of, Neural encoder-decoder transition-based parser)(Statistical parsing, Compare, Bilexical dependencies)(Statistical parsing, Compare, Domain-specific logical forms)(Neural encoder-decoder transition-based parser, Is-a-Prerequisite-of, Full-coverage semantic graph parser)(Full-coverage semantic graph parser, Evaluate-for, Accuracy)(Neural encoder-decoder transition-based parser, Is-a-Prerequisite-of, Attention-based baselines)(Attention-based baselines, Used-for, Evaluation)(Hierarchical LSTM language model, Compare, Character-level language models)
(pagerank, uses, TPR)(pagerank, ranking, noun phrases)(pagerank, modified-into, Salience Rank)(pagerank, part-of, Salience Rank)(pagerank, proposed-in, paper)(pagerank, running, K times)(pagerank, number-of, topics)(pagerank, extracted-by, Latent Dirichlet Allocation)(pagerank, comparing-to, keyphrases)(pagerank, extracted-by, Salience Rank)(pagerank, varying, topic specificity)(pagerank, varying, corpus specificity)(pagerank, conducted-on, papers)(pagerank, comparing-to, competitors)(pagerank, comparing-to, hyperdoc2vec)(pagerank, suggested-in, comparison)(pagerank, named-as, hyperdoc2vec)(pagerank, experimented-on, paper classification)(pagerank, experimented-on, citation recommendation)(pagerank, validating-towards, criteria)(pagerank, preferring-over, other models)(pagerank, concerned-with, dis
(None)
(neural techniques, Used-for, bagging)(models, Compare, results)(approach, Used-for, bagging)(tagging models, part-of, BiLSTMs)
- (prosody, is-a-Prerequisite-of, segmentation)- (prosody, Used-for, parsing)- (prosody, Evaluate-for, detecting concealed information)- (prosody, Used-for, natural and expressive speech synthesis)- (linguistics, Conjunction, prosody)- (parsing, Evaluate-for, incorporating prosody for disfluent speech)- (speech, Part-of, dialogue)- (supervised machine learning models, Evaluate-for, detecting parody tweets)- (TTS systems, Evaluate-for, synthesizing natural and expressive speech)- (ProtoTEx, Compare, BART-large)- (natural language definitions, Is-a-Prerequisite-of, interpretable word representations)
(dependency parsing, Is-a-Prerequisite-of, neural techniques)(dependency parsing, Evaluate-for, computational argumentation mining)(dependency parsing, Part-of, token-based dependency parsing)(dependency parsing, Part-of, token-based sequence tagging)(dependency parsing, Evaluate-for, multi-task learning)(dependency parsing, Evaluate-for, joint extraction of entity mentions and relations)(dependency parsing, Used-for, semantic dependency parsing)(dependency parsing, Is-a-Prerequisite-of, Maximum Subgraph algorithms)(dependency parsing, Is-a-Prerequisite-of, dynamic oracle)(dependency parsing, Compare, non-monotonic dynamic oracle)(dependency parsing, Compare, arc-swift transition system)(dependency parsing, Part-of, dependency graph)(dependency parsing, Hyponym-Of, deep dependency parsing)(dependency parsing, Used-for, convolutional neural network)(dependency parsing, Part-of, structured prediction)(dependency parsing, Is-a-Prerequisite-of, stack
(None)
(deep neural networks, used-for, discourse analysis)(LSTM, part-of, neural model)(predicates, Evaluate-for, entity identification)(segmenter, Evaluate-for, discourse coherence)(NER, Is-a-Prerequisite-of, coreference resolution)(graph-based meaning representation, Compare, traditional underspecification approach)
(speech processing, is-a-prerequisite-of, natural language processing)(speech processing, used-for, speech perception)(speech processing, used-for, word recognition)(speech processing, compare, visual processing)(speech processing, conjunction, linguistic knowledge)(speech processing, hyponym-of, auditory processing)(speech processing, compare, image processing)
```(Natural Language Processing, Concept-of, Chinese)(Chinese Word Segmentation, Used-for, Adversarial multi-criteria learning)(Adversarial multi-criteria learning, Evaluate-for, Performance improvement)(Adversarial multi-criteria learning, Used-for, Chinese Word Segmentation)(Adversarial multi-criteria learning, Is-a-Prerequisite-of, Neural Network-based models)(Chinese Word Segmentation, Compare, POS Tagging)(Chinese Word Segmentation, Compare, Dependency Parsing)(Word Embeddings, Is-a-Prerequisite-of, Dependency Parsing)(Dependency Parsing, Evaluate-for, Model's performance)(Bi-LSTM models, Used-for, Chinese Poem Generation)(Chinese Poem Generation, Evaluate-for, Creative process)(Neural Model, Evaluate-for, Chinese Poem Generation)(Memory Augmented Neural Model, Is-a-Prerequisite-of, Chinese Poem Generation)(Memory Augmented Neural Model, Used-for, Rule
(domain adaptation, used-for, transfer learning)(domain adaptation, Is-a-Prerequisite-of, domain dependence problem)(domain adaptation, Compare, unsupervised domain adaptation)(domain adaptation, Evaluate-for, sentiment analysis)(domain adaptation, Is-a-Prerequisite-of, neural machine translation)(domain adaptation, Evaluate-for, named-entity recognition)(domain adaptation, Is-a-Prerequisite-of, dialog system building)(domain adaptation, Part-of, open information extraction)(domain adaptation, used-for, recognizing domain shift)(domain adaptation, Is-a-Prerequisite-of, knowledge transfer)
(deep learning tool,  Used-for,  semantic role labeling)(deep learning tool,  Compare,  language modeling)(deep learning tool,  Is-a-Prerequisite-of,  automatic fake news detection)(deep learning tool,  Compare,  sentence scoring)(deep learning tool,  Part-of,  slot filling)
(None)
(None)
(None)
(reading comprehension, involves, question answering)(knowledge bases, are used for, question answering)(neural network-based methods, have achieved, impressive results)(COREQA, is used for, question answering)(KBQA, overlaps with, NN-based KB-QA)(automatic question generation, aids in, question answering)(TriviaQA, contains, question-answer-evidence triples)(dynamic neural semantic parsing framework, is trained using, reward-guided search)(Wikipedia, is used as, knowledge source for question answering)(Generative Domain-Adaptive Nets, are used for, boosting question answering models)(TrivaQA, poses a challenge for, existing QA algorithms)(EviNets, is a neural network architecture for, factoid question answering)(Open Information Extraction, is used for, reasoning with knowledge)(universal schema, is exploited for, question answering)(transfer learning, can improve, QA performance)(SQuAD, is used for, transfer learning for QA datasets)
(None)
**(morphological disambiguation, Used-for, language modeling)  (morphological disambiguation, Evaluate-for, improving NLP applications)  (morphological disambiguation, Conjunction, surface context)  (morphological disambiguation, Evaluate-for, deep learning-based approach)  (morphological disambiguation, Is-a-Prerequisite-of, morphological segmentation)  (morphological disambiguation, Compare, semantics disambiguation)**
(None)
(gradient descent, Used-for, learn word representations)(gradient descent, Part-of, Stochastic Gradient Descent)(gradient descent, Evaluate-for, word representations)(gradient descent, Is-a-Prerequisite-of, generative learning)(gradient descent, Part-of, AllVec)(AllVec, Compare, sampling-based SGD methods)(AllVec, Used-for, generate word representations)(AllVec, Is-a-Prerequisite-of, batch gradient learning)(AllVec, Evaluate-for, benchmark tasks)(Adversarial training, Compare, best-performing methods)(Adversarial training, Used-for, improving robustness)(Adversarial training, Evaluate-for, time consumption)(DSRM, Evaluate-for, adversarial loss estimation)(DSRM, Is-a-Prerequisite-of, robust model)(DSRM, Evaluate-for, global loss minimization)(Gradient Ascent Post-training (GAP), Compare, fine-tuning)(Gradient
(transliteration, Used-for, neural machine translation)(transliteration, Conjunction, Romanization)(transliteration, Is-a-Prerequisite-of, machine translation)(transliteration, Compare, translation)
(Expressive kernels, Used-for, achieve excellent performance in NLP)(Neural Network, Used-for, automatically learning feature representations during training)(Multimodal sentiment analysis, Is-a-Prerequisite-of, multi-modal learning)(Multimodal fusion, Evaluate-for, improve efficiency)(Multilingual learning, Is-a-Prerequisite-of, multi-modal learning)
(None)
(paraphrasing, part-of, natural language processing)(paraphrasing, Used-for, sentence simplification)(paraphrasing, Used-for, paraphrase generation)(paraphrasing, Evaluate-for, text similarity measures)(paraphrasing, Is-a-Prerequisite-of, sentence simplification)(paraphrasing, Compare, summarization)(paraphrasing, Evaluate-for, automatic evaluation)
(None)
(diacritics, Part-of, abugida)(consonant letters, Part-of, abugida)(diacritics, Part-of, abugida)(abugida, Compare, syllable)(supervised learning, Is-a-Prerequisite-of, semi-supervised learning)(supervised learning, Evaluate-for, model)(feed-forward networks, Used-for, bridge modalities)(feed-forward networks, Compare, BiLSTM models)(negative sampling, Is-a-Prerequisite-of, distributed word representation learning)(PCFG induction, Compare, neural PCFG inducer)(encoder-decoder models, Compare, Transformer)(depicted images, Part-of, MMT)
(None)
(concept, Evaluate-for, distributional word embeddings)(distributional word embeddings, Compare, multilingual word embedding)(cross-lingual word embeddings, Compare, multilingual word embedding)(bilingual word embeddings, Compare, multilingual word embedding)
(None)
(None)
(evaluation of information retrieval, Used-for, cope with abundant information)(evaluation of information retrieval, Compare, previous techniques)(evaluation of information retrieval, Compare, Semantic hashing)(evaluation of information retrieval, Compare, information pollution)(evaluation of information retrieval, Evaluate-for, determine what we should actually believe)(evaluation of information retrieval, Conjunction, Information retrieval and extraction)(evaluation of information retrieval, Used-for, developing a general framework)
(None)
(combinatory categorial grammar, is-a-Prerequisite-of, parsing)(combinatory categorial grammar, Evaluate-for, domain adaptation method)(combinatory categorial grammar, Used-for, statistical parsing)(statistical parsing, Evaluate-for, parser evaluations)(statistical parsing, Used-for, predicate-argument structure)(domain adaptation method, Part-of, parsing)(parser evaluations, Compare, decomposed scoring)
(particle filter, Evaluate-for, multimedia event extraction)(particle filter, Evaluate-for, named entity recognition)(particle filter, Evaluate-for, melody-to-lyric generation)
(weighted self-matching networks, Compare, gated self-matching networks)( passage, Compare, Question)(pointer networks, Evaluate-for, answer locations)(COREQA, Used-for, encoder-decoder framework)(COREQA, Evaluate-for, answering system)(semantic parsing, Part-of, question answering)(Generative Domain-Adaptive Nets, Compare, novel training framework)(Generative Domain-Adaptive Nets, Compare, generative model)(Generative Domain-Adaptive Nets, Evaluate-for, question answering models)(EviNets, Part-of, neural network architecture)(EviNets, Designed-for, factoid question answering)(Universal schema, Compare, structured KB methods)(Universal schema, Compare, web text methods)(control variates, Evaluate-for, generation systems)(Hierarchical attention network, Compare, reading comprehension method)(Hierarchical attention network, Part-of, hierarchical attention network)(Hierarchical attention network, Compare, attention networks)(Hierarchical attention network, Evaluate-for, reading comprehension)(Hier
(concept, Evaluate-for, NER systems)(NER systems, Part-of, Machine learning models)(NER systems, Compare, Facial recognition system)
(programming language, is-a-Prerequisite-of, natural language processing)(programming language, Used-for, code generation)(programming language, Compare, natural language)
None.
(None)
(mathematical model, Part-of, deep learning techniques)(mathematical model, Is-a-Prerequisite-of, learning)(deep learning techniques, Evaluate-for, enhancing abstractive text summarization)(deep learning techniques, Compare, traditional attention mechanisms)(rap generation, Compare, rhyming lyrics)(news articles, Used-for, creating news content structures)(Quantity Tagger, Is-a-Prerequisite-of, automatic discovery of hidden relations)(news articles, Is-a-Prerequisite-of, event coreference resolution)(dialogue response selection, Is-a-Prerequisite-of, matching model training)(neural topic models, Part-of, automatic topic extraction from text)
### Extracted Concepts:- Variance- Bias- Model complexity- Data size- Learning methods- Neural networks- Over-fitting- Under-fitting- Optimization- Ensemble methods### Triplets:- (bias-variance, Is-a-Prerequisite-of, model complexity)- (bias-variance, Is-a-Prerequisite-of, data size)- (learning methods, Used-for, balancing bias and variance)- (neural networks, Evaluate-for, variance decomposition)- (neural networks, Evaluate-for, ensemble methods)- (ensemble methods, Is-a-Prerequisite-of, variance reduction due to optimization)
(None)
(dialog system, comprising, dialog state tracking)(dialog system, used-for, end-to-end learning)(dialog system, part-of, task-oriented dialogue systems)(dialog system, used-for, response selection)(task-oriented dialogue systems, is-a-prerequisite-of, dialog system)(dialog system, is-a-prerequisite-of, dialog state tracking)(dialog state tracking, used-for, response selection)(dialog state tracking, part-of, task-oriented dialogue systems)(response selection, evaluate-for, response selection)(response selection, evaluate-for, task-oriented dialogue systems)
(semi-supervised learning, used-for, labeled sequence transduction)(semi-supervised learning, used-for, generative model)(semi-supervised learning, Is-a-Prerequisite-of, labeled sequence transduction)(semi-supervised learning, Is-a-Prerequisite-of, generative model)(labeled sequence transduction, Compare, Hybrid Code Networks)(Hybrid Code Networks, Is-a-Prerequisite-of, labeled sequence transduction)(semi-supervised learning, used-for, language modeling)
(None)
(None)
(shallow parsing, is-a-prerequisite-of, constituency trees)(shallow parsing, compare, shallow decoder)(shallow parsing, evaluate-for, online inference efficiency)
None
(None)
(planning, Is-a-Prerequisite-of, multi-task learning)(planning, Is-a-Prerequisite-of, adversarial learning)(planning, Used-for, framework)(planning, Is-a-Prerequisite-of, decentralized policies)(planning, Evaluate-for, interpret agents' messages)(planning, Is-a-Prerequisite-of, translation model)(planning, Is-a-Prerequisite-of, sequence transduction)(planning, Used-for, labeled data generation)(planning, Is-a-Prerequisite-of, event extraction)(planning, Is-a-Prerequisite-of, semantic role labeling)(planning, Is-a-Prerequisite-of, extractive summarization)(planning, Is-a-Prerequisite-of, language resource creation)(planning, Is-a-Prerequisite-of, question generation)(planning, Is-a-Prerequisite-of, taxonomic learning)(planning, Is-a-Prerequisite-of, network analysis)
( MemSum, Contains, Markov decision process)( Parameter-Efficient Tuning, Compare, Markov decision process)( DiffusionNER, Utilizes, Markov decision process)( Multi-step Episodic Markov decision process extractive SUMmarizer, Used-for, Extractive summarization)( Multi-step Episodic Markov decision process extractive SUMmarizer, Evaluate-for, Performance improvement)( Co-UCB, Is-a-Prerequisite-of, Multi-armed dueling bandits)( Co-UCB, Is-a-Prerequisite-of, Multi-armed bandit learning)
(syntax, Is-a-Prerequisite-of, semantic parsing)(syntax, Conjunction, semantic role labeling)(syntax, Evaluate-for, code generation)(syntax, Is-a-Prerequisite-of, deep learning frameworks)(syntax, Is-a-Prerequisite-of, NMT models)
(concept, Used-for, document dating)(document dating, Conjunction, Graph Convolutional Network)(document dating, Part-of, Graph Convolutional Network)(document dating, Is-a-Prerequisite-of, relation extraction)(document dating, Compare, dependency trees)
(spectral method, used-for, language modeling) (spectral method, used-for, learning weighted non-deterministic automata)
(conjunction, Is-a-Prerequisite-of, zero pronoun resolution)(reinforcement learning, Evaluate-for, sentence selection)(LSTM, Part-of, deep neural networks)(neural network-based (NN-based) methods, Compare, traditional methods)(entity linking, Is-a-Prerequisite-of, relation detector)(attention mechanism with LSTM network, Conjunction, extract semantic relations)(feature-based joint model, Compare, proposed attention-based recurrent neural network)(Agent-Artifact relations, Is-a-Prerequisite-of, improvement on relations detection)(NER as a sequence labeling problem, Is-a-Prerequisite-of, local detection approach)(explicit syntactic features, Part-of, deletion-based Long Short-Term Memory (LSTM) neural network model)(continuous representations of KBs, Used-for, enhance the learning of recurrent neural networks)(grid-type recurrent neural networks, Is-a-Prerequisite-of, improvement on PAS analysis)(TriviaQA dataset, Evaluate-for, pointer network)
(Structured information, Is-a-Prerequisite-of, Structured prediction)(Encoding text, Type-of, Structured prediction)(Graph Neural Network, Used-for, incorporating structured information)(Structured projection, Used-for, backpropagating through neural networks)(Feedback, Evaluate-for, predicted output structure)(Learning algorithms, Used-for, incorporating control variates)(Sequence prediction algorithm, Is-a-Prerequisite-of, structured prediction)(Structured information about entities, Used-for, semantic parsing tasks)(Structured projection, Used-for, training with SPIGOT)(Structured projection, Evaluate-for, downstream task improvement)(Sequence labeling, Evaluate-for, accurate sequence labeling)(Meta-word, Evaluate-for, generating structured predictions)(Sparse data structures, Used-for, natural language models)(Few-shot natural language generation, Is-a-Prerequisite-of, structured prediction).
(conjunction, connect, search engine)(retrieved passages, help, verify answer candidates)(Bilingual Lexicon Induction(BLI), match, between two languages)(Memory-augmented generative model, assist, response generation)(generate query completions, using, recurrent neural network language model)(a search engine feature, suggest, completed queries)(machine reading comprehension, require, analyzing multiple passages)(findings, reveal, importance of using both human and automatic detectors)(answer candidates, verify, each other)(knowledge base, derived from, textual contexts)
(sentiment analysis, used-for, aspect-based sentiment analysis)(sentiment analysis, compare, aspect sentiment classification)(sentiment analysis, evaluate-for, polarity orientation)(sentiment analysis, evaluate-for, significance of a word in expressing opinion)(sentiment analysis, evaluate-for, polarity-preserving significant words)(sentiment analysis, part-of, multimodal sentiment analysis)(sentiment analysis, part-of, cross-domain sentiment analysis)(supervised learning, is-a-prerequisite-of, sentiment classifier for target domain)(deep learning, is-a-prerequisite-of, LSTM-based model for sentiment analysis)(deep learning, is-a-prerequisite-of, Memory networks (MNs) for aspect sentiment classification)
(None)
(constraint, Evaluate-for, availability)(automated essay scoring, Is-a-Prerequisite-of, training data)(automated essay scoring, Evaluate-for, essay grading)(automated essay scoring, Compare, automated writing evaluation)(automated essay scoring, Used-for, formative feedback)(automated essay scoring, Part-of, AES models)(automated essay scoring, Is-a-Prerequisite-of, domain generalization)(AES models, Evaluate-for, scoring essays)(AES models, Is-a-Prerequisite-of, data access)(AES models, Evaluate-for, generalization ability)(AES models, Used-for, representation learning)
(concepts, involves, GPU)(GPUs, are-used-for, training and evaluating neural networks)(matrix multiplication, involves, GPU operations)(matrix multiplication, is-part-of, new GPU algorithms)(matrix multiplication, generalizes, multiplication by a one-hot vector)
(None)
(collaborative filtering, used-for, movie recommendation system)(collaborative filtering, used-for, personalized content recommendation)(collaborative filtering, Compare, content-based filtering)(collaborative filtering, Part-of, recommendation system approach)(collaborative filtering, Is-a-Prerequisite-of, personalized recommendation)(cross-lingual named entity recognition, Conjunction, federated learning)(CDSs, Evaluate-for, dialogues)(NER system, Hyponym-Of, named entity recognition)(NER system, a) Compare, previous methods)(NER system, a) Compare, label noise)(NER system, a) Compare, generalizing to distant languages)
(lexical semantics, part-of, semantics)(lexical semantics, Evaluate-for, interpretation)(coreference resolvers, part-of, information)(lexical semantics, part-of, types)(lexical semantics, Used-for, semantic disambiguation)(lexical semantics, part-of, natural language)(neural search systems, Evaluate-for, effectiveness)(lexical resources, part-of, tasks)(semantic representations, part-of, representations)(lexical semantics, part-of, linguistic phenomena)
(None)
(neural network models, have-shown, transfer learning)(transfer learning, is-used-for, text classification tasks)(transfer learning, is-a-Prerequisite-of, domain adaptation)(transfer learning, is-a-Prerequisite-of, inductive transfer learning)(domain adaptation, involves, neural networks)(neural networks, include, adversarial training)(financial evaluation, is-proposed-for, automatic Pyramid scores)(domain adaptation, involves, adversarial training)(adversarial training, can-enhance, neural domain adaptation)(neural networks, can-be-enchanced-by, Universal Language Model Fine-tuning)(inductive transfer learning, has-impacted, computer vision)(neural networks, can-be-improved-by, multi-task learning)
(None)
(memory network, Used-for, question answering)(memory network, Is-a-Prerequisite-of, reading comprehension)(memory network, Compare, question answering)(memory network, Is-a-Prerequisite-of, cloze-style questions)(memory network, Is-a-Prerequisite-of, multi-hop architecture)(memory network, Is-a-Prerequisite-of, attention mechanism)(memory network, Is-a-Prerequisite-of, recurrent neural network)
(concept sensitive grammar, Used-for, language independent context sensitive lemmatization)(context sensitive grammar, Compare, traditional formulations)(context sensitive grammar, Compare, probabilistic context free grammar)(context sensitive grammar, Compare, mildly context-sensitive grammars)
(part of speech, used-for, part-of-speech tagging)(part of speech, Evaluate-for, lexical features)(part of speech, Evaluate-for, part-of-speech tagging)(part of speech, Is-a-Prerequisite-of, syntax)(syntax, Is-a-Prerequisite-of, part of speech)(part of speech, Evaluate-for, syntactic tasks)(part of speech tagging, Compare, dependency parsing)(part of speech tagging, Is-a-Prerequisite-of, models)(models, Is-a-Prerequisite-of, part of speech tagging)(part of speech tagging, Evaluate-for, text infilling)(part of speech tagging, Is-a-Prerequisite-of, parsers)(parsers, Is-a-Prerequisite-of, part of speech tagging)
(None)
(statistical machine translation, improves, neural machine translation)(statistical machine translation, used-for, sentence alignment)(phrase-based systems, outperform, neural machine translation)
(thesaurus-based similarity, part-of, word similarities)(word similarities, Compare, discrepancies)(discrepancies, part-of, analysis)(analysis, part-of, applying procedure)(applying procedure, Evaluate-for, usefulness)(usefulness, part-of, discussion)(word sense description, Evaluate-for, inaccurate relationships)(inaccurate relationships, part-of, errors)(errors, part-of, word sense description)(word sense description, part-of, serious errors)(missing senses, part-of, word sense description)(ambiguous words, part-of, missing senses)
(conjunction, indication, semantic parser)(parser, used-for, parse)(model, combines, strength)(model, guards-against, spurious programs)(algorithm, presents, learning)(algorithm, combines, systematic search)(algorithm, combines, randomized exploration)(search space, controlled, parser input)
(graph theory, is-a-Prerequisite-of, inter-sentence relation extraction)(graph theory, Used-for, knowledge graph completion)(graph theory, is-a-Prerequisite-of, graph convolutional neural network)(graph theory, Used-for, dependency parse structures)
(neural turing machine, Compare, Global Context Layer)(neural turing machine, Is-a-Prerequisite-of, context-aware neural network model)(neural turing machine, Used-for, storing processed temporal relations)(neural turing machine, Is-a-Prerequisite-of, Neural Turing Machine)(neural turing machine, Is-a-Prerequisite-of, model with long-term memory and attention mechanisms)(neural turing machine, Compare, regular RNNs such as LSTM)
(None)
```(Uncertainty, Evaluation-for, Model predictions)(Uncertainty, Evaluation-for, Named entity recognition)(Uncertainty, Is-a-Prerequisite-of, Confidence modeling)(Uncertainty, Evaluate-for, End-to-end dialog tasks)(Uncertainty, Evaluate-for, Goal-oriented visual dialogue)(Uncertainty, Evaluate-for, Uncertain Natural Language Inference)(Uncertainty, Evaluate-for, Clinical trial outcome prediction)(Uncertainty, Evaluate-for, Event detection models)(Uncertainty, Evaluate-for, Negative sampling in NER)(Uncertainty, Evaluate-for, Uncertainty estimation in NLP)(Uncertainty, Part-of, Natural language processing)```
(neural summarization, Is-a-Prerequisite-of, abstractive summarization)(neural summarization, Evaluate-for, generate summaries)(neural summarization, Compare, abstractive summarization)(neural summarization, Evaluate-for, document summarization)(neural summarization, Evaluate-for, extractive summarization)(neural summarization, Compare, traditional summarization approaches)(neural summarization, Evaluate-for, sentence scoring)(neural summarization, Evaluate-for, sentence selection)(neural summarization, Part-of, neural sequence-to-sequence model)(neural sequence-to-sequence model, Is-a-Prerequisite-of, neural summarization)(neural summarization, Evaluate-for, multi-task learning)(neural summarization, Evaluate-for, entailment generation)(neural summarization, Evaluate-for, question generation)(neural summarization, Evaluate-for, multi-task architectures)
(generative adversarial network, Used-for, generating adversarial examples)(generative adversarial network, Compare, generative neural network)(generative adversarial network, Compare, recurrent network)(generative adversarial network, Compare, generative neural network model)(generative adversarial network, Compare, generative adversarial networks)(generative adversarial network, Compare, Generative Adversarial Networks)(generative adversarial network, Is-a-Prerequisite-of, adversarial training)(generative adversarial network, Is-a-Prerequisite-of, adversarial learning)(generative adversarial network, Evaluate-for, performance improvement)(generative adversarial network, Evaluate-for, robustness improvement)
(Text similarity measures, Used-for, Plagiarism detection)(deep learning, Used-for, Natural language generation)(sequential models, Compare, text similarity measures)(sequential models, Compare, deep learning)(common representation, Is-a-Prerequisite-of, DNA sequence alignment algorithms)(TextFlow, Is-a-Prerequisite-of, text similarity measures)(Gaussian mixtures, Is-a-Prerequisite-of, multimodal word distributions)(Gaussian mixtures, Is-a-Prerequisite-of, entailment recognition)(word2vec skip-grams, Hyponym-Of, TextFlow)(word2vec skip-grams, Hyponym-Of, Gaussian embeddings)
(attention model, used-for, text categorization)(attention model, used-for, geolocation model)(attention model, used-for, conversation modeling)(attention model, used-for, summarization)(attention model, used-for, metaphor identification)(attention model, used-for, controlled response generation)(attention model, is-a-Prerequisite-of, neural machine translation)(attention model, is-a-Prerequisite-of, sequence-to-sequence models)
(tweets, Mention, probabilistic grammar)(tweets, Mention, political framing)(NLP, Study, probabilistic grammar)(probabilistic grammar, Used-for, NLP)(political framing, Evaluate-for, Twitter)(NLU tasks, Compare, probabilistic grammar)
None.
(stemming, Is-a-Prerequisite-of, vector space representations)(stemming, Used-for, spectral clustering)(stemming, Compare, clustering)(stemming, Is-a-Prerequisite-of, word embeddings)(stemming, Used-for, machine translation)(stemming, Is-a-Prerequisite-of, natural language processing)
**(computer vision, used-for, image captioning)**  **(computer vision, Compare, natural language processing)**  **(computer vision, used-for, Brain-Computer Interface)**  **(image captioning, Part-of, image representation)**  **(image captioning, used-for, caption generation)**  **(image captioning, Compare, natural language processing)**  **(Brain-Computer Interface, used-for, decoding text stimuli)** 
(finite state transducer, Used-for, handle tasks such as part-of-speech tagging and speech recognition)(finite state transducer, Compare, parallel graphics processing unit (GPU) implementations)(finite state transducer, Used-for, Accelerate finite state algorithms)(finite state transducer, Used-for, Achieve the best performance on GPU architecture)(finite state transducer, Compare, OpenFST)(finite state transducer, Part-of, encoder-decoder framework)(finite state transducer, Is-a-Prerequisite-of, end-to-end model for semantic parsing)(finite state transducer, Part-of, neural semantic parsers)(finite state transducer, Used-for, develop an approach to morph-based auto-completion)(finite state transducer, Used-for, map prefixes in a language to a set of possible completions)(finite state transducer, Compare, neural models)
(None)
(concepts, Evaluate-for, end-to-end models)(concepts, Evaluate-for, tagging scheme)(tagging scheme, Is-a-Prerequisite-of, joint extraction task)(end-to-end models, Compare, existing pipelined and joint learning methods)(end-to-end models, Compare, tagging based methods)(end-to-end models, Used-for, extract entities and relations directly)(tagging based methods, Compare, existing pipelined and joint learning methods)(MGbank, Part-of, corpus of Minimalist Grammar derivation trees)(AMR graph, Is-a-Prerequisite-of, Abstract Meaning Representations)(supertagging, Part-of, parsing algorithm)(supertagging, Used-for, achieve state-of-the-art parsing performance)(CCG parser, Hyponym-Of, C&C parser)(holographic composition, Evaluate-for, state-of-the-art parsing performance)(supertagging, Used-for, improve parsing accuracy)
(None)
(tree-adjoining grammar, is-a-Prerequisite-of, natural language processing)(tree-adjoining grammar, Hyponym-Of, synchronous tree-adjoining grammar)(tree-adjoining grammar, is-a-Prerequisite-of, TAG)(TAG, is-a-Prerequisite-of, linear indexed grammars)(TAG, is-a-Prerequisite-of, combinatory categorial grammars)(TAG, is-a-Prerequisite-of, head grammars)(TAG, is-a-Prerequisite-of, PDAs)(TAG, is-a-Prerequisite-of, CFGs)(TAG, is-a-Prerequisite-of, PGAs)(tree-adjoining grammar, is-a-Prerequisite-of, Pushdown Adjoining Automaton)
(AMR parser, Used-for, parsing sentences into Abstract Meaning Representations)(AMR parser, Part-of, system)(SMN, Used-for, response selection for multi-turn conversation in retrieval based chatbots)(RNN, Used-for, inferring AMR graphs)(hierarchical recurrent neural network, Used-for, relation detection in KBQA)(KBLSTM, Used-for, enhancing the learning of recurrent neural networks for machine reading)(Bandit structured prediction, Compare, lifting linear bandit learning to neural sequence-to-sequence learning)(grid-type recurrent neural networks, Used-for, PAS analysis)(NMT+RNNG, Compare, incorporating linguistic prior to neural machine translation)(Disconected recurrent neural network, Compare, achieves the best performance on several benchmark datasets for text categorization)(RNN, Part-of, the rise of neural networks for part-of-speech tagging accuracy)
(None)
(conjunction, Indicates, algorithm)(algorithm, Used-for, beam search)(beam search, Used-for, neural machine translation)(neural machine translation, Evaluate-for, translation quality)(beam search, Compare, hypotheses)
(WordNet, Is-a-Prerequisite-of, Sememes)(Sememes, Used-for, Word Representation Learning)(WordNet, Is-a-Prerequisite-of, Lexical Ambiguity)(Sememes, Compare, Word Senses)(Word Senses, Evaluate-for, Word Similarity)(Word Senses, Compare, Word Analogy)(WordNet, Is-a-Prerequisite-of, Word Sense Disambiguation)
(None)
(None)
(None)
(syntaxnet, Is-a-Prerequisite-of, abstract syntax networks)(syntaxnet, Used-for, semantic parsing)(syntaxnet, Is-a-Prerequisite-of, syntactic structure learning)(syntaxnet, Compare, Visually Grounded Neural Syntax Learner)(syntaxnet, Compare, syntax-infused variational autoencoder)(syntaxnet, Evaluate-for, word generation)(syntaxnet, Is-a-Prerequisite-of, treebank creation)(syntaxnet, Evaluate-for, syntactic structure improvement)
(concept, Part-of, machine translation)(machine translation, Evaluate-for, translation accuracy)(token, Part-of, target token)(translation, Evaluate-for, prediction errors)(model, Used-for, k-nearest-neighbor retrieval)(neighbor, Part-of, nearest neighbor)(k-nearest-neighbor retrieval, Evaluate-for, translation accuracy)(token, Part-of, target token)
(part of speech tagging, task, Penn Treebank)(part of speech tagging, compare, Stanford Parser)(part of speech tagging, part-of, neural networks)(part of speech tagging, evaluated-for, sentiment analysis)(part of speech tagging, evaluate-for, syntactic tasks)(part of speech tagging, used-for, part-of-speech analysis)(neural networks, part-of, part of speech tagging)(neural networks, compare, recurrent neural networks)(neural networks, part-of, state-of-the-art models)(neural networks, compare, classic approaches)(recurrent neural networks, part-of, neural networks)(recurrent neural networks, conjunction, part-of-speech accuracy)(recurrent neural networks, compare, initial word encodings)(recurrent neural networks, evaluate-for, language processing)(recurrent neural networks, part-of, part-of-speech accuracy)(recurrent neural networks, used-for, part-of-speech tagging)(recurrent neural networks, part-of, specialized word encodings
(chatbot, Used-for, response selection)(chatbot, Is-a-Prerequisite-of, multi-turn conversation)(chatbot, Used-for, generating responses)(chatbot, Compare, chit-chat model)(chatbot, Compare, sequence-to-sequence model)(chatbot, Evaluate-for, personalized news recommendation)(chatbot, Is-a-Prerequisite-of, context-aware neural machine translation)
(search engine indexing, involves, representation)(search engine indexing, uses, query representation)(representation, described by, Verbatim queries)(search engine indexing, utilizes, top-ranked documents)(representation, modified by, Pseudo-relevance feedback techniques)(top-ranked documents, retrieved by, lexical methods)(top-ranked documents, retrieved by, dense methods)(Pseudo-relevance feedback techniques, have, been shown)(Pseudo-relevance feedback techniques, improve, retrieval effectiveness)(ColBERT-PRF approach, chooses, new embeddings)(new embeddings, added to, query representation)(ColBERT-PRF approach, uses, inverse document frequency)(embeddings, potentially ignore, valuable context)(CWPRF, contrastive weighting model-based, learn)(CWPRF, trained, discriminate)(CWPRF, learns, select useful expansion embeddings)(CWPRF, outperforms, various baselines)
(logic and reasoning, requires, human cognition)(logic and reasoning, applied to, benchmark tasks)(logic and reasoning, fundamental capacity of, human cognition)(logic and reasoning, struggle to model, logical structures)(logic and reasoning, limits ability to generalize, within the same structure)
(None)
(backpropagation, Used-for, training)(backpropagation, Evaluate-for, optimization)(backpropagation, Evaluate-for, training)(backpropagation, Is-a-Prerequisite-of, training)(backpropagation, Is-a-Prerequisite-of, optimization)(backpropagation, Used-for, parsing)(backpropagation, Part-of, neural networks)
(Vector space, Representations of)(Words, Capture many aspects of)(Antonyms)(Similar methods, Produce vector spaces)(Synonym)(Vector space, Representations of)(Text, Similarity measures, Used in multiple tasks)(Text similarity measures, Do not fully exploit, Sequential nature of language)(Text similarity measure, Inspired from)(Representation, DNA sequence alignment algorithms)(TextFlow, Computes, Similarity value)(Vector space representations, Tend to produce, Vector spaces)(Antonyms, Are close to each other)
(capsule network, outperforms, recurrent networks)(capsule network, introduces, agreement score)(capsule network, introduces, adaptive optimizer)(capsule network, introduces, capsule compression)(capsule network, introduces, partial routing)(capsule network, improves, scalability of capsule networks)(capsule network, is used in, multi-label text classification)(capsule network, is used in, question answering)
(None)
(bootstrapping, uses, BONIE)(bootstrapping, improves, performance)(bootstrapping, involves, seed selection)(bootstrapping, related to, noise reduction)(bootstrapping, applies, clustering)(bootstrapping, iteratively refines, labels)
(concept, Compare, generation of repeated phrases)(text summarization, Evaluate-for, generate a shorter version)(documents, Part-of, summarize)(query-based summarization, Part-of, abstractive summarization)(neural sequence-to-sequence models, Used-for, abstractive text summarization)(source text, Compare, summaries)(seq2seq framework, Evaluate-for, informativeness)(document summarization, Is-a-Prerequisite-of, text summarization)(document summarization, Used-for, abstractive sentence summarization)(abstractive summarization, Is-a-Prerequisite-of, document summarization)
(recurrent neural network, Used-for, language modeling)(recurrent neural network, Used-for, speech perception)(recurrent neural network, Hyponym-Of, deep recurrent neural network)(recurrent neural network, Compare, convolutional neural network)(recurrent neural network, Compare, deep neural networks)(recurrent neural network, Compare, neural encoder-decoder models)(recurrent neural network, Evaluate-for, language modeling)(recurrent neural network, Evaluate-for, speech perception)(recurrent neural network, Evaluate-for, dialog systems)(recurrent neural network, Evaluate-for, joint extraction of entity mentions and relations)(recurrent neural network, Is-a-Prerequisite-of, Hybrid Code Networks)(recurrent neural network, Is-a-Prerequisite-of, RNNs with domain-specific knowledge)(recurrent neural network, Is-a-Prerequisite-of, joint extraction of entity mentions and relations)
(None)
(EviNets, Evaluate-for, question answering)(DEISTE, Compare, EviNets)(Masque, Compare, EviNets)(SAN, Compare, EviNets)(Masque, Compare, Masque)(question answering, Is-a-Prerequisite-of, semantic parsing)(SAN, Compare, REasoNet)(EviNets, Compare, ReasoNet)(EviNets, Evaluate-for, factoid question answering)(SAN, Evaluate-for, factoid question answering)
(concept, Created-by, semi-automatically creating linguistically challenging micro-planning data-to-text corpora)(concept, Used-for, exploring relations between entities and generating text)(concept, Made-available, dataset of 21,855 data/text pairs)(concept, Can-be-applied-to, any large scale knowledge base)(concept, Used-for, training and learning KB verbalisers)(concept, Used-for, inferencing relative physical knowledge of actions and objects)(concept, Used-for, visual grounding of speech perception model)(concept, Used-for, Knowledge Base Question Answering)(concept, Used-for, improving recurrent neural networks for machine reading)(concept, Used-for, entity extraction and event extraction)(concept, Tackles, open-domain question answering using Wikipedia)(concept, Present-in, supervised aspect extraction)(concept, Represents, dense embeddings vector in EviNets)(concept, Considers, multiple overlapping, arbitrary prior knowledge sources in
(Stancetaking, Is-a-Prerequisite-of, discourse parsing)(Discourse parsing, Evaluate-for, discourse coherence)(Rhetorical Structure Theory (RST) Discourse Treebank, Used-for, discourse parsing)(SDP, Is-a-Prerequisite-of, discourse parsing)(sentence-level discourse analysis, Used-for, discourse parsing)(open-domain neural semantic parser, Used-for, discourse parsing)(deep learning model, Evaluate-for, discourse parsing)(SciDTB, Used-for, discourse parsing)(SPARK, Evaluate-for, discourse parsing)(transition-based discourse parser, Is-a-Prerequisite-of, discourse parsing)(text coherence model, Evaluate-for, discourse parsing)(long-span dependencies, Part-of, discourse parsing)(Discourse Representation Theory (DRT), Part-of, discourse parsing)(implicit discourse relations, Is-a-Prerequisite-of, discourse parsing)(neural framework, Used-for, discourse parsing)(document-level coherence score, Compare, discourse parsing)(Neural Machine Translation (N
(Conditional VAE, Improve, Machine translation models)(Variational Autoencoder, Combine, Latent variables)(Batch Normalized-VAE, Prevent, Posterior collapse)(Variational Autoencoder, Model, Text generation)(Variational Autoencoder, Capture, Coherent long-term structure)(Variational Autoencoder, Regularize, Latent space)(Unsupervised paraphrasing, Enable, Novel applications)(SIVAE, Integrate, Syntactic trees)(Coupled-VAE, Improve, Probability estimation)(BHWR, Facilitate, Variational Bayes word representation learning)
(conversational question simplification, address, maximum likelihood estimation)(train, focuses on, maximum likelihood estimation)(models, trained via, maximum likelihood estimation)(models, are commonly trained using, maximum likelihood estimation)(method, achieves a new state-of-the-art result on, maximum likelihood estimation)(method, achieves, new state-of-the-art result on)(models, trained using, maximum likelihood estimation)(rules, fixed in advance and focuses on, maximum likelihood)
(None)
(random walk, inferred-from, neural language model)(random walk, used-for, autoregressive models)(random walk, part-of, training autoregressive models)(random walk, Compare, autoregressive models)(random walk, Compare, biased random walkers)
(object detection, task of weakly-supervised spatio-temporally grounding sentences, spatio-temporal tube)(object detection, part-of, weakly-supervised spatio-temporally grounding sentences)(object detection, used-for, exploit fine-grained relationships)(object detection, evaluated-for, ranking loss)(object detection, part-of, attentive interactor)(object detection, evaluated-for, diversity loss)(object detection, compare, baseline approaches)(object detection, part-of, VID-sentence)(object detection, conjunction, detection tasks)(object detection, part-of, neural event detection approaches)(object detection, part-of, event detection systems)(object detection, compare, trigger-centric representations)
(DT-Solver, Used-for, guiding the search procedure)(search procedure, Used-for, guiding)(dynamic-tree Monte-Carlo search algorithm, Is-a-Prerequisite-of, DT-Solver)
(Proposed multi-space variational encoder-decoders, Used-for, labeled sequence transduction)(Proposed multi-space variational encoder-decoders, Compare, single-model state-of-art results)(Encoder-decoder dialog model, Compare, traditional systems)(Bayesian Hierarchical Words Representation, Evaluate-for, Variational Bayes word representation learning)(Variational autoencoders, Part-of, end-to-end architecture)
(None)
(recursive neural network, utilized-in, modeling inference)(recursive neural network, employed-in, parsing Singlish)(recursive neural network, improved-by, incorporation of syntactic parsing information)(recursive neural network, used-in, dependency parsing)
(simple sentence, Evaluate-for, sentence simplification)(clinical letters, Used-for, sentence simplification)(complex medical terminology, Part-of, clinical letters)(sentence simplification, Compare, sentence-alignment)(sentence simplification, Is-a-Prerequisite-of, automated TS systems)(MSR abstractive sentence summarization datasets, Evaluate-for, sentence simplification)(sentence simplification, Part-of, sentence-alignment)(context-aware neural machine translation model, Used-for, sentence simplification)(sentence splitting, Is-a-Prerequisite-of, fine-tuned simplification operations)(neural text simplification systems, Is-a-Prerequisite-of, automated TS systems)(sentence simplification, Is-a-Prerequisite-of, lexical simplification)(NTS systems, Compare, automated TS systems)(sentence simplification, Used-for, sentence-alignment)(neural text simplification software, Used-for, sentence simplification)(sentence simplification, Compare, sentence-alignment)(review texts, Is-a-Pr
**(discourse model, Evaluate-for, identification of discourse modes)  (discourse model, Evaluate-for, automatic identification of narrative modes)  (identification of discourse modes, Is-a-Prerequisite-of, automatic identification of narrative modes)  (identification of discourse modes, Used-for, study characteristics of discourse modes)  (discourse modes, Used-for, improving automatic essay scoring)  (discourse modes, Evaluate-for, automatic essay scoring)  (joint modeling approach, Evaluate-for, identifying salient discussion points)  (discourse relations, Is-a-Prerequisite-of, identification of salient discussion points)  (discourse relations, Used-for, labeling speaker turns)  (discourse segmentation, Is-a-Prerequisite-of, building end-to-end discourse parsers)  (lattice-structured LSTM model, Used-for, Chinese NER)  (NeuralREG, Is-a-Prerequisite-of, improving over strong baselines)  (topic tensor network, Is-a
**(social medium analysis, Evaluate-for, user’s socio-economic profile)**  **(social medium analysis, Part-of, language use in social media)**  **(user cognitive structure, Used-for, to build a predictive model of income)**  **(predictive model of income, Is-a-Prerequisite-of, classifier)**  **(classifier, Used-for, to automatically time-tag tweets as past, present, or future)**  **(user cognitive structure, Is-a-Prerequisite-of, quantify a user’s overall temporal orientation)**  **(quantify a user’s overall temporal orientation, Is-a-Prerequisite-of, predictive model of income)**  **(quantify a user’s overall temporal orientation, Evaluate-for, predictive model of income)**  **(future temporal orientation, Compare, income)**  **(Twitter, Part-of, social media)**  **(social media English, Conjunction, social media African-American English (AAE))**  **(Twitter, Part-of, English dependency parsing)**  **(social media
(learning, involves, neural network models)(learning, applied-to, text classification tasks)(learning, contributes-to, machine translation)(learning, important-for, referential expression generation)(learning, necessary-for, commonsense knowledge acquisition)(learning, key-for, event extraction)(learning, employs, neural word embeddings)(learning, uses, document-aligned corpora)(learning, improves, coherence in aspect-based sentiment analysis)(learning, significant-to, semantic role labeling)(learning, utilized-in, end-to-end automatic speech recognition)(learning, aids-in, second language acquisition)(learning, used-for, multilingualism studies)(learning, contributes-to, visually grounded speech perception)(learning, essential-in, dialog system development)(learning, integrated-with, neural machine translation)(learning, enhances, abstractive text summarization)(learning, utilized-in, extractive multi-document summarization)
(Deep Dyna-Q, Is-a-Prerequisite-of, planning)(reinforcement learning, Evaluate-for, bandit neural machine translation)(evaluative relationship, Evaluate-for, neural network)(reinforcement learning, Is-a-Prerequisite-of, task-oriented dialogue systems)(task-oriented dialogue systems, Evaluate-for, user simulator)(reinforcement learning, Is-a-Prerequisite-of, dialogue policy learning)(reinforcement learning, Evaluate-for, conversational game)(reinforcement learning, Evaluate-for, end-to-end reinforcement learning)(reinforcement learning, Evaluate-for, coreference resolution)(reinforcement learning, Is-a-Prerequisite-of, policy gradient training)(reinforcement learning, Evaluate-for, historical text normalization)(reinforcement learning, Is-a-Prerequisite-of, unsupervised parsers)(reinforcement learning, Is-a-Prerequisite-of, incremental learning framework)(reinforcement learning, Is-a-Prerequisite-of, Generation-Evaluation framework)
(robotics, Used-for, human-robot communication)(human-robot communication, Is-a-Prerequisite-of, collaboration)(robotics, Is-a-Prerequisite-of, grounded verb semantics)(robotics, Used-for, acquiring optimal policy)(robotics, Evaluate-for, long-term reward)(robotics, Is-a-Prerequisite-of, interactive learning approach)
(long short term memory network, Compare, Recurrent Neural Networks)(long short term memory network, Evaluate-for, Sentence Compression)(long short term memory network, Evaluate-for, Aspect-Level Sentiment Classification)(long short term memory network, Is-a-Prerequisite-of, Relation Extraction)(long short term memory network, Is-a-Prerequisite-of, Named-Entity Recognition)(long short term memory network, Is-a-Prerequisite-of, Information Extraction)(long short term memory network, Part-of, Affect-LM)(long short term memory network, Part-of, LSTM Noisy Channel Model)
(inference, models, neural-network-based)(inference, formulated as, Recognizing Textual Entailment)(inference, utilized for, coreference resolution)(inference, challenges, in natural language understanding)(inference, trained on, large annotated data)(inference, enriched with, external knowledge)
(None)
(transition-based dependency parsing, Used-for, produce certain attachments)(transition-based dependency parsing, Is-a-Prerequisite-of, correct individual decisions)(transition-based dependency parsing, Compare, reduce error)(transition-based dependency parsing, Conjunction, need sequences of local shift and reduce operations)(transition-based dependency parsing, Used-for, leverage lexical information more directly)(transition-based dependency parsing, Hyponym-Of, existing transition systems)(transition-based dependency parsing, Evaluate-for, reduce error by 3.7–7.6% relative to those using existing transition systems)
(neural network models, have shown, promising opportunities for multi-task learning)(multi-task learning, focus on, learning the shared layers)(multi-task learning, extract, common and task-invariant features)(existing approaches, prone to, contaminating shared features)(multi-task learning framework, alleviate, shared and private latent feature spaces)(demonstrate, benefits of, multi-task learning approach)(multi-task learning model, can be regarded as, off-the-shelf knowledge)(neural techniques, used for, end-to-end computational argumentation mining)(end-to-end computational argumentation mining, frame as, dependency parsing problem)(dependency parsing, leads to, subpar performance results)(local tagging models, based on, BiLSTMs)(jointly learning, ‘natural’ subtasks, improves, performance)(encoder-decoder architectures, require, a lot of training data)(multi-task learning architecture, uses, grapheme-to-phoneme dictionary as auxiliary data)(multi
(social network extraction, Used-for, continual learning)(social network extraction, Evaluate-for, occupational class prediction)(social network extraction, Is-a-Prerequisite-of, homophily exploitation)(social network extraction, Evaluate-for, classification)(social network extraction, Conjunction, data set)(social network extraction, Used-for, graph convolutional network)
(logical patterns, captured by, hyperbolic KG embedding models)(logical patterns, preserved in, embedding space)(logical patterns, combined with, attention)(logical patterns, preserved in, hierarchical and logical patterns)(logical patterns, improved by, previous Euclidean- and hyperbolic-based efforts)(logical patterns, modeled as, separate languages)(logical patterns, detected by, sentence-level classifier)(logical patterns, tagged for, NMT model)(logical patterns, biased to, produce more natural outputs)(logical patterns, analyzed using, metrics)(logical patterns, help in, reasoning)(logical patterns, help in, structural understanding)
(None)
(generative models, Compare, discriminative models)(generative models, Part-of, joint distributions)(generative models, Conjunction, parse trees and sentences)(generative models, Conjunction, joint distributions)(generative models, Used-for, parsing and language modeling)(framework, Is-a-Prerequisite-of, parsing and language modeling)(framework, Used-for, parsing and language modeling)(framework, Evaluate-for, generative model)(framework, Evaluate-for, discriminative recognition model)(framework, Is-a-Prerequisite-of, generative model)(framework, Is-a-Prerequisite-of, discriminative recognition model)(framework, Conjunction, encoder-decoder setting)
(None)
(word distribution, descriptor-of, Morphologically rich languages)(word distribution, descriptor-of, distributional vector space models)(word distribution, is-a-Prerequisite-of, word embeddings)(word distribution, is-a-Prerequisite-of, word representations)(word distribution, Used-for, sentiment analysis)(word distribution, Used-for, word generation)(word distribution, is-a-Prerequisite-of, word types)(word types, Used-for, language models)(word types, Part-of, word representations)(word representations, is-a-Prerequisite-of, word embeddings)(word embeddings, Evaluate-for, semantic information)(word embeddings, Evaluate-for, text classification)(partner information, Compare, conversation success)(partner information, Hyponym-Of, conversation partners)(partner information, is-a-Prerequisite-of, interlocutor frequency domain representations)(data shift, Used-for, domain adaptation)(data shift, Used-for, model training)(data shift, Compare, training and evaluation data)(model, Compare
(neural language modeling, Used-for, generation of rhythmic poetry)(neural language modeling, Compare, statistical parsing)(neural language modeling, Is-a-Prerequisite-of, natural language processing)(neural language modeling, Part-of, Recurrent Neural Networks)(neural language modeling, Evaluate-for, perception studies)(neural language modeling, Is-a-Prerequisite-of, sequence labeling)(neural language modeling, Evaluate-for, text similarity measures)(neural language modeling, Is-a-Prerequisite-of, named entity recognition)(neural language modeling, Evaluate-for, document context incorporation)
(speech recognition, Used-for, automatic speech recognition)(speech recognition, Is-a-Prerequisite-of, language processing)(speech recognition, Is-a-Prerequisite-of, natural language processing)(speech recognition, Is-a-Prerequisite-of, multi-speaker speech recognition)(speech recognition, Used-for, end-to-end automatic speech recognition)(speech recognition, Used-for, joint decoding algorithm)(speech recognition, Is-a-Prerequisite-of, named entity recognition)(speech recognition, Is-a-Prerequisite-of, language and vision tasks)(speech recognition, Is-a-Prerequisite-of, multimodal language processing)(speech recognition, Used-for, speech translation)(speech recognition, Used-for, action recognition)(speech recognition, Evaluate-for, word error rate estimation)(speech recognition, Evaluate-for, humor recognition)
(crawling the web, Used-for, create the largest publicly available parallel corpora)(crawling the web, Compare, publishing benchmark datasets)(crawling the web, Used-for, sentence alignment)(crawling the web, Is-a-Prerequisite-of, sentence pair filtering)(crawling the web, Is-a-Prerequisite-of, creating machine translation systems)
(tokenization, part-of, text categorization)(tokenization, part-of, speech recognition)(tokenization, evaluate-for, NLP systems)(tokenization, evaluate-for, semantic analysis)(tokenization, used-for, NLP preprocessing)(tokenization, is-a-prerequisite-of, NLP processing)(tokenization, is-a-prerequisite-of, natural language understanding)(tokenization, compare, subword segmentation)
(deep learning model, used-for, optimization)(optimization, Used-for, Deep neural networks)(optimization, Is-a-Prerequisite-of, Stochastic optimization)(stochastic optimization, Used-for, Large training sets)(optimizer, Is-a-Prerequisite-of, Better model performance)(Riemannian optimization, Is-a-Prerequisite-of, Optimize SGNS objective)(entity linking, Evaluate-for, optimization)
### Extracted Concepts:- Predicate Argument Structure Analysis- Dependency Trees- RDF Triples- Entailment Graph- Open Information Extraction (OpenIE)- Logical Reasoning### Triplets:- (Predicate Argument Structure Analysis, Is-a-Prerequisite-of, Dependency Trees)- (RDF Triples, Part-of, Predicate Logic)- (Entailment Graph, Part-of, Predicate Logic)- (Open Information Extraction, Used-for, RDF Triples)- (Logical Reasoning, Evaluate-for, Logical Reasoning) - (Predicate Argument Structure Analysis, Is-a-Prerequisite-of, Logical Reasoning)
(hidden markov model, is-a-Prerequisite-of, neural approach)(neural approach, Evaluate-for, hidden markov model)(neural approach, Used-for, predict sequences)(hidden markov model, is-a-Prerequisite-of, unsupervised learning)(unsupervised learning, Evaluate-for, noisy labels)(noisy labels, is-a-Prerequisite-of, weakly supervised NER models)
(None)
(bayesian network, used-for, interpretable relationships)(bayesian network, used-for, diagnosis system)
(None)
(feature learning, Used-for, text classification)(feature learning, Used-for, opinion analysis)(feature learning, Is-a-Prerequisite-of, fine-grained opinion analysis)(text classification, Is-a-Prerequisite-of, feature learning)(opinion analysis, Compare, fine-grained opinion analysis)
(linguistics basic, Capture, linguistic regularities)(linguistics basic, Incorporate-Into, NMT)(linguistics basic, Underlie, stancetaking)(linguistics basic, Involve, word semantics)(linguistics basic, Improve, language identification)(linguistics basic, Utilize, sememes)(linguistics basic, Analyze, cultural fit)(linguistics basic, Extend, word embeddings)
(seq2seq, used-for, machine translation)(seq2seq, Is-a-Prerequisite-of, end-to-end computational argumentation mining)(seq2seq, Is-a-Prerequisite-of, conversational agents)(seq2seq, used-for, abstractive text summarization)(seq2seq, Is-a-Prerequisite-of, neural abstractive summarization)(seq2seq, Is-a-Prerequisite-of, response generation)(seq2seq, Conjunction, E2E NLG Challenge)(seq2seq, Is-a-Prerequisite-of, task-oriented dialogue systems)(seq2seq, Is-a-Prerequisite-of, grammatical error correction)(seq2seq, Conjunction, RNN architecture)(seq2seq, Is-a-Prerequisite-of, Generative Neural Network for Slot Filling)
(informed search, Used-for, neural search systems)(informed search, Compare, entity-oriented search)(BERT, Part-of, contextualized language models)(LayerNorm, Compare, PreNorm)(LayerNorm, Compare, PostNorm)
(problem solving, used-for, math problem solving system)(math problem solving system, used-for, end-to-end math problem solving)(math problem solving system, used-for, accept natural language input)(math problem solving system, evaluate-for, producing logical forms)(math problem solving system, part-of, hybrid system)(math problem solving system, compare, NumS2T)(NumS2T, evaluate-for, enhancing math word problem solving performance)(math problem solving system, evaluate-for, incorporating numerical values)(math problem solving system, compare, NumS2T)(math problem solving system, evaluate-for, receiving considerable attention)(math problem solving system, evaluate-for, achieving promising results)(problem solving, is-a-prerequisite-of, math word problem solving)(math word problem solving, compare, Geometry problem solving)(Geometry problem solving, part-of, Interpretable Geometry Problem Solver)(Interpretable Geometry Problem Solver, used-for, geometry problem solving)(Interpretable Geometry Problem
#### Triplets extracted from the context:(neural machine translation, relies-on, bi-directional LSTMs)  (neural machine translation, presents, a faster and simpler architecture based on a succession of convolutional layers)  (neural machine translation, achieves, competitive accuracy to the state-of-the-art on WMT’16 English-Romanian translation)  (neural machine translation, outperforms, several recently published results on WMT’15 English-German)  (neural machine translation, obtains, almost the same accuracy as a very deep LSTM setup on WMT’14 English-French translation)  (neural machine translation, enhanced by, Deep Neural Networks (DNNs))  (neural machine translation, suffers-from, severe gradient diffusion in deep architectures of encoder or decoder RNNs)  (neural machine translation, proposes, linear associative units (LAU) to reduce gradient propagation path inside the recurrent unit)  (neural machine translation, improves by, 11.
(phonological feature, part-of, Chinese Spelling Check)(linguistic phenomena, Used-for, training data for the generative model)(phonological distinctive features, Evaluate-for, segment-level phonotactic acquisition)(phonological distinctive features, Evaluate-for, phonotactic learning)(phonological distinctive features, Compare, human judgments of non-words)(phonological distinctive features, Is-a-Prerequisite-of, predictive accuracy of a model)(phonological distinctive features, Conjunction, phonological distinctive features and phonotactic patterns)
(None)
(semantic representation, is-a-Prerequisite-of, NLP)(semantic representation, part-of, AMR)(NLP, part-of, parsing)(AMR, part-of, linguistic properties)(semantic representation, compare, syntactic schemes)(NLP, compare, sequence-to-sequence models)(semantics, used-for, parsing)(AMR, used-for, parsing)
(None)
(semantic parsing, Is-a-Prerequisite-of, code generation)(neural semantic parser, Evaluate-for, semantic parsing)(neural semantic parser, Used-for, code generation)(Sequence-to-Action, Is-a-Prerequisite-of, code generation)(code generation, Is-a-Prerequisite-of, Seq2Tree models)(Seq2Tree models, Evaluate-for, code generation)(semantic parsing, Compare, code generation)(abstract syntax networks, Part-of, semantic parsing)
(linguistic knowledge identification, Evaluate-for, event extraction)(linguistic knowledge identification, Evaluate-for, named entity recognition)(linguistic knowledge identification, Evaluate-for, coreference resolution)(linguistic knowledge identification, Evaluate-for, relation extraction)(linguistic knowledge identification, Evaluate-for, sentiment classification)(linguistic knowledge identification, Evaluate-for, personality detection)(linguistic knowledge identification, Evaluate-for, dialogue response generation)(linguistic knowledge identification, Part-of, document-grounded dialogue)(linguistic knowledge identification, Part-of, psycholinguistic knowledge)(document-grounded dialogue, Is-a-Prerequisite-of, dialogue response generation)
(constrained text generation, Compare, controlled text generation)(S2S models, Evaluate-for, controlled text generation)(Pre-trained S2S models, Conjunction, Copy Mechanism)(Conditional Masked Language Modeling (C-MLM), Evaluate-for, BERT)(Seq2Seq models, Evaluate-for, text generation performance)
(concept, Evaluate-for, multilingual neural machine translation model)(multilingual neural machine translation model, Is-a-Prerequisite-of, multilingual learning)(multilingual neural machine translation model, Is-a-Prerequisite-of, multilingual NMT with one encoder-decoder model)(multilingual NMT with one encoder-decoder model, Compare, multilingual translation paradigm)(multilingual NMT with one encoder-decoder model, Conjunction, language commonality and parameter sharing between encoder and decoder)(multilingual translation paradigm, Evaluate-for, language commonality and parameter sharing between encoder and decoder)(multilingual NMT with one encoder-decoder model, Compare, individual models trained on bilingual corpus)(multilingual NMT with one encoder-decoder model, Evaluate-for, model performance)(multilingual translation paradigm, Is-a-Prerequisite-of, universal representor)
(learn event, Is-a-Prerequisite-of, event detection)(event detection, Compare, Nugget Proposal Networks)(GloVe, Used-for, word embedding)(GloVe, Compare, Latent Meaning Models)(Latent Meaning Models, Evaluate-for, word similarity)(Nugget Proposal Networks, Compare, deep recurrent neural networks)(Graph Neural Network, Used-for, semantic parsing)(Graph Neural Network, Compare, BERT pre-training)(deep recurrent neural networks, Is-a-Prerequisite-of, syntax prediction)(BERT, Is-a-Prerequisite-of, neural network based models)
(None)
(commonsense evaluation, Used-for, coreference resolution)(coreference resolution, Evaluate-for, neural network models)(commonsense evaluation, Compare, neural network models)(coreference resolution, Is-a-Prerequisite-of, coreference linking actions)(coreference resolution, Evaluate-for, coreference linking actions)(neural network models, Is-a-Prerequisite-of, heuristic loss functions)(commonsense evaluation, Used-for, evaluation benchmark)(evaluation benchmark, Is-a-Prerequisite-of, retrievers)(retrievers, Evaluate-for, entity disambiguation capabilities)
(sequence labeling model, used-for, text simplification)(sequence labeling model, used-for, named entity recognition)(sequence labeling model, used-for, sentiment analysis)(sentence, part-of, text)(sentence, part-of, passage)(sentence, part-of, narrative essays)(sequence labeling model, Evaluate-for, natural language processing)(sequence, part-of, text)(sequence labeling model, Evaluate-for, neural network architectures)(sequence labeling model, Evaluate-for, NLP tasks)(sequence labeling model, used-for, machine translation)(sequence labeling model, Evaluate-for, machine translation)(sequence labeling model, Compare, machine translation)(sequence labeling model, Compare, neural network architectures)(sequence labeling model, Compare, sequence-to-sequence learning)(sequence labeling model, used-for, entity recognition)(sequence labeling model, Evaluate-for, entity recognition)(sequence labeling model, Is-a-Prerequisite-of, entity recognition)(sequence labeling model, Is-a-Prerequisite-of, sentiment analysis)(sequence labeling model, Is-a-
(argument invention, Evaluate-for, argument detection)(argument invention, Evaluate-for, argument generation)(argument invention, Evaluate-for, argument mining)(argument invention, Evaluate-for, implicit event argument extraction)(argument mining, Used-for, argument invention)(implicit event argument extraction, Evaluate-for, argument invention)
(None)
(conversation model, utilizes, examples)(retrieval model, retrieves, conversation examples)(context, has-similarity-with, retrieved response)(model, learns, assign higher similarity scores)(recurrent neural network language models, suffers-from, limitations)(model, builds upon, reward augmented maximum likelihood approach)(recent reward augmented maximum likelihood approach, encourages, model)(model, assigns, similarity scores)(reward augmented maximum likelihood approach, encourages, predict sentences)(limitation, overcome-by, reward augmented maximum likelihood approach)(model, predicts, tokens)(training, requires, matching)(model, addresses, issues)(model, achieves, improvements)(train time, predicts, ground truth words)(inference, has-to-generate, entire sequence)(fed context, leads-to, error accumulation)(training, requires, strict matching)(context words, sampled-from, ground truth sequence)(predicted sequence, selected-with, sentence-level optimum)(approach, achieves, improvements)(BERTScore, can-ach
(Functional Distributional Semantics, Provides, Interpretability)(BERT, Outperforming, Pixie Autoencoder)(Attention, Successful, Natural Language Processing)(Functional Distributional Semantics, Introduces, Pixie Autoencoder)
(generated explanation, explain, natural language processing)(generated explanation, is-a-Prerequisite-of, natural language processing)
(topic model, Used-for, language understanding)(topic model, Compare, LSTM)(topic model, Is-a-Prerequisite-of, multi-task learning)(topic model, Compare, topic models)(topic model, Evaluate-for, perplexity)
(semantic role labeling model, Compare, deep learning model)(deep learning model, Is-a-Prerequisite-of, semantic role labeling model)(deep learning model, Used-for, improving state of the art)(deep learning model, Used-for, natural language processing)(deep learning model, Used-for, semantic role labeling)(deep learning model, Used-for, sequence learning model)(deep learning model, Used-for, neural network)
`(relation classification, Used-for, feature extraction)`  `(relation classification, Is-a-Prerequisite-of, sentiment analysis)`  `(relation classification, Is-a-Prerequisite-of, sarcasm detection)`  `(CNN, Used-for, feature learning)`  `(sentiment analysis, Compare, sarcasm detection)`  `(relation classification, Is-a-Prerequisite-of, gaze information)`  `(CNN, Used-for, classification)`  `(sentiment analysis, Is-a-Prerequisite-of, text features)`  `(sarcasm detection, Is-a-Prerequisite-of, text features)`  `(relation classification, Is-a-Prerequisite-of, learning system)`
(None)
(None)
(social bias, Evaluate-for, text style transfer)(social media, Used-for, propagation of fake news)(social media, Part-of, modern communication)(social media, Is-a-Prerequisite-of, societal information dissemination)(fake news, Compare, real stories)(fake information, Evaluate-for, identification)(user groups, Compare, political ideology)(online social systems, Is-a-Prerequisite-of, combating antisocial behavior)(language variation, Conjunction, factors influencing)(language variation, Used-for, machine translation)(Machine Translation, Is-a-Prerequisite-of, speaker traits reflection)(social media posts, Evaluate-for, understanding through images)(conversations, Evaluate-for, mental health counseling)(social media, Used-for, expressing opinions)(social media posts, Part-of, multimodal communication)(social structures, Compare, internal cognitive processes)(social structures, Evaluate-for, language propagation)(language change, Is-a-Prerequisite-of, social structure)(s
(distributed word representations, used-for, word modeling)(distributed word representations, part-of, NLP tasks)(distributed word representations, Evaluate-for, word similarity and word analogy)(word representation learning, Is-a-Prerequisite-of, distributed word representations)(NLP tasks, Is-a-Prerequisite-of, distributed word representations)
(Domain aspect, Is-a-Prerequisite-of, Cross-domain sentiment analysis)(Cross-domain sentiment analysis, Compare, Aspect-based Sentiment Analysis)(Cross-domain sentiment analysis, Evaluate-for, Sentiment classifier)(Polarity orientation, Compare, Significance of a word)(Cross-domain sentiment classification, Evaluate-for, Challenging task)(Transferable information, Compare, Usable information)(Labeled source domain, Evaluate-for, Sentiment classifier)(Sentiment classifier, Is-a-Prerequisite-of, Aspect-based Sentiment Analysis)(Sentiment classification, Evaluate-for, Sentiment classifier)(Aspect term extraction, Is-a-Prerequisite-of, Aspect sentiment classification)(Aspect-based Sentiment Analysis, Compare, Opinion mining system)(Aspect-opinion pairs, Part-of, Aspect-based Sentiment Analysis)(Aspect-term polarity pairs, Is-a-Prerequisite-of, Fine-grained Aspect Based Sentiment Analysis)(Aspect-opinion pairs, Evaluate-for, Opinion mining systems)(OpinionDigest, Is-a-Prerequisite-of
(concept, Evaluate-for, semantic parsers)(semantic parsers, Compare, task performance)(semantic parsing, Is-a-Prerequisite-of, learning)(learning algorithm, Used-for, guard against spurious programs)(learning algorithm, Compare, existing state-of-the-art results)(semantic parsing, Is-a-Prerequisite-of, transducing natural language utterances)(semantic parsing, Evaluate-for, performance improvement)(semantic parsing, Is-a-Prerequisite-of, transducing natural language utterances)(semantic parsing, Is-a-Prerequisite-of, formal meaning representations)(semantic parsing, Compare, end-to-end semantic graph generation)(end-to-end semantic graph generation, Is-a-Prerequisite-of, semantic parsing)(semantic parsing, Evaluate-for, performance improvement)(entities, Conjunction, forbidden patterns)(structure, Part-of, family of directed and undirected graphs)(RNN model, Used-for, map sentences to action sequences)(natural language feedback, Evaluate-for, semantic parsing improvement)(HSP, Is-a
(concepts, compare, representation learning) (models, generate, representations) (weights, sharing, representations) (techniques, evaluate, representation learning) (models, obtain, embeddings) (sentences, represent, low-rank subspace)
1. (Pseudofit, specializes, word embeddings)2. (Pseudofit, utilzes, semantic similarity)3. (first study, aimed at, capturing stylistic similarity)4. (embedding model, extended to, learn style-sensitive word vectors)5. (style-sensitive word vectors, learn through, wider context window)6. (continuous bag of words, model for, learn style-sensitive word vectors)7. (contextualized word embeddings, leverage, power)8. (contextualized word embeddings, used, experiment)
(annotated training, requires, labeled data)(labeled data, is-a-Prerequisite-of, annotated training)(labeled data, Used-for, cross-lingual transfer learning)(annotated training, evaluates, neural network models)
(Cross-lingual model transfer, Used-for, predicting annotations)(Cross-lingual model transfer, Compare, unsupervised bilingual word embeddings)(Cross-lingual model transfer, Evaluate-for, method development)(unsupervised bilingual word embeddings, Compare, machine translation)(unsupervised bilingual word embeddings, Compare, back-translation)(unsupervised bilingual word embeddings, Used-for, unsupervised machine translation)(unsupervised bilingual word embeddings, Evaluate-for, method development)
(None)
(None)
(conpositional distributional semantics model, Is-a-Prerequisite-of, evaluation dataset)(conpositional distributional semantics model, Compare, count-based distributional semantic models)(conpositional distributional semantics model, Is-a-Prerequisite-of, functional distributional semantics)(conpositional distributional semantics model, Is-a-Prerequisite-of, pixie autoencoder)(compositional distributional semantics model, Evaluate-for, semantic relatedness and entailment)(compositional distributional semantics model, Evaluate-for, compositionality degree of multiword expressions)(compositional distributional semantics model, Evaluate-for, prediction of compositionality)(compositional distributional semantics model, Used-for, decoding fMRI patterns)(compositional distributional semantics model, Evaluation-for, sentiment composition)(compositional distributional semantics model, Compare, BERT)
(concepts, compare, sentiment)(sentiment, part-of, sentiment analysis)(concepts, used-for, aspect-based sentiment analysis)(aspect-based sentiment analysis, Is-a-Prerequisite-of, aspect-term sentiment analysis)(sentiment, Evaluate-for, sentiment polarity)(aspect-based sentiment analysis, Evaluate-for, sentiment polarity)(aspect-based sentiment analysis, Compare, general sentiment analysis)(sentiment analysis, Evaluate-for, polarity orientation)(sentiment classification, Compare, sentiment analysis)(aspect-based sentiment analysis, Compare, sentiment analysis)(supervised sentiment classification, Used-for, sentiment analysis)(word embeddings, part-of, word embeddings models)(word embeddings models, Evaluate-for, sentiment information)(word embeddings, part-of, multimodal word distributions)(word distributions, Evaluate-for, sentiment classification)(domain-sensitive word embeddings, Compare, word embeddings)(general sentiment analysis, Compare, aspect based sentiment analysis)(aspect based sentiment analysis, Evaluate-for, aspect extraction)(sentiment classification, Compare,
(existing word embedding, used-for, word similarity)(existing word embedding, evaluate-for, text representation)(existing word embedding, Compare, distributional word embeddings)(distributional word embeddings, Is-a-Prerequisite-of, object naming)(word embeddings, conjunction, distributional word embeddings)(word embeddings, Part-of, deep learning architectures)(word embeddings, used-for, neural network architectures)(word embeddings, compare, unsupervised learning techniques)
(sarcasm detection, Compare, text similarity measures)(sarcasm detection, Evaluate-for, tweet datasets)(sarcasm detection, Used-for, improve personal health mention detection)(sarcasm detection, Used-for, multimodal sarcasm detection)(sarcasm detection, Compare, multimodal sarcasm detection)(sarcasm detection, Evaluate-for, iSarcasm dataset)(sarcasm detection, Evaluate-for, Stance detection dataset)(sarcasm detection, Compare, Semi-supervised GeNerative Active Learning)(sarcasm detection, Conjunction, social-media conversations)(sarcasm detection, Compare, author context)(sarcasm detection, Evaluate-for, Multimodal Sarcasm Detection Dataset)(sarcasm detection, Evaluate-for, TREE LSTM models)
(recurrent neural networks, Compare, recurrent neural tensor)(recurrent neural networks, Part-of, machine learning)(recurrent neural tensor, Is-a-Prerequisite-of, neural tensor)(recurrent neural tensor, Used-for, machine reading)(neural tensor, Compare, neural networks)(neural tensor, Part-of, tensor)(machine reading, Used-for, question answering)(tensor, Is-a-Prerequisite-of, machine learning)
(neural parser, Is-a-Prerequisite-of, AMR parsing)(AMR parsing, Used-for, achieving state-of-the-art results)(neural parser, Used-for, parsing documents in specific domains)(neural parser, Compare, sequence-to-sequence model)(neural parser, Used-for, parsing for constituency parsing)(neural parser, Compare, neural and non-neural parsers)(neural parser, Is-a-Prerequisite-of, structured output prediction)
(fact checking article, Used-for, detecting misinformation)(fact checking article, Evaluate-for, improving factual correctness)(fact checking article, Evaluate-for, claim verification)(fact checking article, Part-of, automated fact checking)(fact checking article, Evaluate-for, claim veracity prediction)(fact checking article, Is-a-Prerequisite-of, fact-checking system)(fact checking article, Compare, misinformation stories)(fact checking article, Evaluate-for, generating explanations)(fact checking article, Evaluate-for, verifying the truthfulness of a claim)(fact checking article, Hyponym-Of, DialFact dataset)
(generative neural models, Used-for, constituency parsing)(generative neural models, Compare, base parsers)(generative conversational systems, Compare, context information)(generative neural network architecture, Is-a-Prerequisite-of, dialogue act classification)(unsupervised neural machine translation, Is-a-Prerequisite-of, machine translation)(generative adversarial networks, Used-for, cross-language translation)(generative models, Compare, existing approaches)(generative neural network model, Used-for, slot filling)(adversarial attacks, Used-for, text classification)(unsupervised relation extraction, Conjunction, generative and discriminative approaches)
(Abstractive summarization, compare, query-based summarization)(query-based summarization, evaluate-for, Abstractive summarization)(encode-attend-decode paradigm, used-for, machine translation)(encode-attend-decode paradigm, used-for, extractive summarization)(encode-attend-decode paradigm, used-for, dialog systems)(encode-attend-decode paradigm, evaluate-for, generation of repeated phrases)(query attention model, evaluate-for, focus on different portions of the query)(query attention model, evaluate-for, different time steps)(query attention model, evaluate-for, static representation for the query)(diversity based attention model, evaluate-for, alleviate the problem of repeating phrases)(doable attention model, evaluate-for, repeating phrases)(query-based summarization dataset, used-for, testing model)(sequence-to-sequence models, used-for, abstractive text summarization)(sequence-to-sequence models, evaluate-for, reproduce factual details inaccurately)(sequence-to
(machine translation, based-on, bi-directional LSTMs)(machine translation, relies-on, convolutional layers)(machine translation, achieves, competitive accuracy on WMT'16 English-Romanian translation)(machine translation, outperforms, several recently published results on WMT’15 English-German)(machine translation, obtains, almost the same accuracy as a very deep LSTM setup on WMT’14 English-French translation)(machine translation, enhanced-by, Deep Neural Networks (DNNs))(machine translation, suffers-from, severe gradient diffusion due to non-linear recurrent activations)(machine translation, proposed, linear associative units (LAU) to reduce the gradient propagation path)(machine translation, effective-for, Chinese-English translation)(machine translation, improves-by, 11.7 BLEU upon Groundhog)(machine translation, equivalent-to, neural machine translation (NMT))(machine translation, shows, source syntax can be explicitly incorporated into NMT)(machine translation, proposed, Parallel RNN encoder
(dialogue state tracking, essential part of task-oriented dialogue systems, Dialogue State Tracking)(dialogue state tracking, responsible for inferring user intentions through dialogue history, DST)(dialogue state tracking, Natural Language Understanding (NLU) component, Dialog State Tracker)
(None)
(conductor, Used-for, electric current)(decoder, Is-a-Prerequisite-of, generated sentence)(source sequence, Is-a-Prerequisite-of, generated sentence)(model, Compare, generated sentence)
(visual semantic pretraining, Evaluate-for, text complexity)(visual semantic pretraining, Compare, distributional semantic representations)(visual semantic pretraining, Is-a-Prerequisite-of, improving language models)(visual semantic pretraining, Compare, contrastive visual semantic pretraining)(contrastive visual semantic pretraining, Is-a-Prerequisite-of, mitigating anisotropy)
(None)
(None)
(morphological family, Conjunction, morphological inflection)(morphological family, Conjunction, morphological analysis)(morphological family, Conjunction, morphological relation)(morphological family, Part-of, morphological inflection)(morphological family, Part-of, morphological analysis)(morphological family, Part-of, morphological relation)(morphological family, Is-a-Prerequisite-of, morphological inflection)(morphological family, Is-a-Prerequisite-of, morphological analysis)(morphological family, Is-a-Prerequisite-of, morphological relation)(morphological inflection, Used-for, morphological analysis)(morphological analysis, Evaluate-for, morphological relation)(morphological relation, Evaluate-for, morphological inflection)
(vision language pre training, Used-for, VLP)(vision language pre training, Evaluate-for, downstream tasks)(vision language pre training, Compare, language model pre-training)(vision language pre training, Compare, multilingual vision-language pre-training)(vision language pre training, Compare, weakly supervised vision-and-language pre-training)(VLP, Is-a-Prerequisite-of, Multilingual Vision-Language Pre-training)(weakly supervised vision-and-language pre-training, Is-a-Prerequisite-of, RELIT)(RELIT, Evaluate-for, downstream tasks)
(sparse retrieval, Used-for, phrase embeddings)(sparse retrieval, Evaluate-for, phrase retrieval models)(sparse retrieval, Evaluate-for, question-passage matching)(sparse retrieval, Is-a-Prerequisite-of, contrastive learning)(sparse retrieval, Is-a-Prerequisite-of, dual-encoder model)(sparse retrieval, Is-a-Prerequisite-of, vector representations)
(Dialog state tracking, Is-a-Prerequisite-of, Task-oriented dialogue systems)(Encoder-decoder dialog model, Is-a-Prerequisite-of, Task-oriented dialogue systems)(Dialogue system, Evaluate-for, Dialogue state tracking)(Task success prediction models, Compare, Global-Locally Self-Attentive Dialogue State Tracker)(Dialogue systems, Part-of, Task success prediction models)(Dialogue belief tracking, Evaluate-for, Dialogue systems)
```python(neural semantic parser, uses, neural network models)(neural semantic parser, generates, natural language representations)(neural semantic parser, trained, end-to-end)(neural semantic parser, converts, natural language utterances)(neural semantic parser, interpretable, Yes)(neural semantic parser, scalable, Yes)(neural semantic parser, state-of-the-art, Yes)(neural semantic parser, connects, reinforcement learning (RL) and maximum marginal likelihood (MML)(neural semantic parser, trained, using annotated logical forms or their denotations)(neural semantic parser, maps, natural language utterances into executable programs)(neural semantic parser, search the space of programs, for those that output the correct result)(neural semantic parser, combines, strengths of reinforcement learning (RL) and maximum marginal likelihood (MML))(neural semantic parser, guards against, spurious programs)```
(automatic evaluation metrics, biased, human judgements)(ADEM, learns to predict, human-like scores)(ADEM model, predictions correlate significantly, human judgements)(ADEM, generalize, dialogue models unseen during training)(LSTM neural network model, deletion-based, domain adaptability)(ILP, syntactic constraints)(text-to-SQL systems, evaluations, limitations)(image caption annotations, Conceptual Captions dataset, Inception-ResNetv2 based model)(embedding learning by concept induction, multilingual embedding learning)(ReCoSa model, finds relevant contexts)(neural dialogue generation, neural knowledge diffusion model)(definition modeling technique, distributed vector representations of words)(Gender Bias in Machine Translation, analysis)(Multilingual BERT, surprisingly good, zero-shot cross-lingual model transfer)(Meta-words, open domain dialogue generation)(VIFIDEL metric, estimates faithfulness of generated captions)(dialogue generation pre-training framework, supports various kinds of conversations
(data text generation, Uses, neural language model)(data text generation, Considers, poetry generation)(data text generation, Informed by, semantic parsing)(data text generation, Uses, text similarity measures)(data text generation, Uses, genetic algorithm)(data text generation, Uses, distributed neural models)(data text generation, Employs, sequence-to-sequence models)(data text generation, Improves, video captioning)(data text generation, Investigates, sentence-level information)(data text generation, Employs, unsupervised text segmentation)(data text generation, Uses, graph-based attention mechanism)(data text generation, Employs, neural machine translation)(data text generation, Employs, summarization model)(data text generation, Improves, summarization model)(data text generation, Improves, dialogue generation)(data text generation, Employs, LSTM for encoding AMR structure)(data text generation, Addresses, generic, repetitive, and self-contradictory text)(data text generation, Improves
(di scourse mode, Identifies, Narration)(di scourse mode, Identifies, Exposition)(di scourse mode, Identifies, Description)(di scourse mode, Identifies, Argument)(di scourse mode, Identifies, Emotion)(di scourse mode, Used-for, Automatic identification)(di scourse mode, Evaluate-for, Essay scoring)(di scourse mode, Improves, Essay scoring)(di scourse mode, Used-for, Features)(di scourse mode, Discusses, Impacts)(di scourse mode, Part-of, Sociolinguistic construct of stancetaking)(di scourse mode, Part-of, Interactional phenomena)(di scourse mode, Part-of, Discourse structure)(di scourse mode, Identified-as, Features)(di scourse mode, Used-for, Stancetaking)(di scourse mode, Used-for, Automatic revision purpose prediction)(di scourse mode, Used-for, Recognition)(di scourse
(hate speech detection, Used-for, automatic detection)(hate speech detection, Evaluate-for, performance improvement)(hate speech detection, Compare, methods)(hate speech detection, Is-a-Prerequisite-of, hate speech)(hate speech detection, Evaluate-for, sentiment knowledge sharing)(hate speech detection, Evaluate-for, sentiment features)(hate speech detection, Evaluate-for, affective features)
(ABSA, Is-a-Prerequisite-of, aspect-category sentiment analysis)(ABSA, Is-a-Prerequisite-of, aspect-term sentiment analysis)(ACSA, Is-a-Prerequisite-of, Aspect-Category-Opinion-Sentiment quadruple extraction)(Aspect-term sentiment analysis, Is-a-Prerequisite-of, Aspect-Category-Opinion-Sentiment quadruple extraction)(ACSA, Compare, aspect-term sentiment analysis)(ACSA, Part-of, Restaurant-ACOS dataset)(ACSA, Part-of, Laptop-ACOS dataset)(Aspect-Category-Opinion-Sentiment quadruple extraction, Evaluate-for, aspect-based sentiment analysis)
(conducting experiments, Used-for, achieving accurate results)(proposed model, Is-a-Prerequisite-of, read and comprehend text passages)(state-of-the-art model, Compare, proposed model)(single model, Compare, ensemble model)(reading comprehension task, Is-a-Prerequisite-of, understanding natural texts and answering questions)(reading comprehension, Conjunction, question answering)(natural-language understanding systems, Is-a-Prerequisite-of, quality of reading comprehension datasets)(answer span, Part-of, question and passage representation)(proposed model, Evaluate-for, improving performance)(ReasoNet, Compare, SAN)(attention-based sequence learning model, Evaluate-for, generating questions from text passages)(hierarchical attention network, Used-for, answering questions from narrative paragraphs)
(concept: neural news recommendation approach, relation: Is-a-Prerequisite-of, concept: user encoder)(concept: neural news recommendation approach, relation: Is-a-Prerequisite-of, concept: news encoder)(concept: neural news recommendation approach, relation: Used-for, concept: alleviate information overload)(concept: neural news recommendation approach, relation: Evaluate-for, concept: accurate news)
`(reasoning ability, is-a-prerequisite-of, geometric reasoning)``(reasoning ability, compare, pragmatic reasoning)``(reasoning ability, evaluate-for, understand narrative)``(reasoning ability, evaluate-for, machine comprehension)``(reasoning ability, evaluate-for, dialogue belief tracking)``(reasoning ability, compare, conversational reasoning)``(reasoning ability, evaluate-for, adversarial attacker for NLP tasks)`
(argument extraction, Used-for, relation extraction)(argument extraction, Is-a-Prerequisite-of, relation extraction)(relation extraction, Compare, relation extraction)(argument extraction, Evaluation-for, sentiment polarity and sarcasm detection)(argument extraction, Used-for, sentiment polarity and sarcasm detection)(with a novel attention-based recurrent neural network, Evaluate-for, argument extraction)(existing KB population methods, Evaluate-for, argument extraction)(existing KB population methods, Used-for, relation extraction)(novel open information extraction methods, Is-a-Prerequisite-of, argument extraction)(Open IE knowledge, Compare, traditional QA methods)(Open IE knowledge, Is-a-Prerequisite-of, multi-lingual multi-task architecture)(multi-lingual multi-task architecture, Is-a-Prerequisite-of, supervised models)(Bidirectional LSTM, Compare, Aspect Exaction from Bi-LSTM)(Aspect Exaction from Bi-LSTM, Used-for, supervised aspect extraction)(Sentence Extraction, Compare, Visual Language Grounding)
```(transition-based parser, Is-a-Prerequisite-of, Natural Language Processing)(transition-based parser, Evaluate-for, generating noncrossing graphs)(transition-based parser, Used-for, Semantic Dependency Parsing)(transition-based parser, Used-for, Dependency Parsing)(transition-based parser, Used-for, parsing rhetorical structures)```
(None)
(syntactic generalization performance, Is-a-Prerequisite-of, neural language models)(syntactic generalization performance, Is-a-Prerequisite-of, syntactic knowledge)(neural language models, Used-for, syntactic generalization performance)(sequential models, Compare, other architectures)(sequential models, Perform-poorly-on, compositional generalization)(neural decoders, Part-of, neural network models)(transduction, Is-a-Prerequisite-of, semantic parsing)(transduction, Is-a-Prerequisite-of, machine translation)(transduction, Is-a-Prerequisite-of, instruction following)(neural decoders, Used-for, systematic generalization)(neural decoders, Used-for, sequence modeling tasks)(sequence-to-sequence transduction, Is-a-Prerequisite-of, neural network models)(sequence-to-sequence transduction, Is-a-Prerequisite-of, semantic parsing)(sequence-to-sequence transduction, Is-a-Prerequisite-of,
(None)
```(level distant relation extraction, Evaluate-for, noise reduction)(level distant relation extraction, Is-a-Prerequisite-of, relation extraction)(noise reduction, Evaluate-for, performance improvement)(feature extraction, Is-a-Prerequisite-of, statistical NLP)(relation extraction, Is-a-Prerequisite-of, Knowledge Base Population)(relation extraction, Is-a-Prerequisite-of, KB enrichment)(Bi-LSTM, Used-for, relation extraction)(Named Entity Disambiguation, Is-a-Prerequisite-of, entity extraction)(Attention Guided Graph Convolutional Networks, Used-for, relation extraction)(GP-GNNs, Used-for, relation extraction)(GPT, Used-for, distant supervised relation extraction)(GCN, Used-for, entity and relation extraction)(GCN, Used-for, relation extraction)```
(Document summarization, Is-a-Prerequisite-of, Single document summarization)(Document summarization, Used-for, Abstractive summarization)(Abstractive summarization, Compare, Extractive summarization methods)(Document summarization, Compare, Multi-document summarization)(Document summarization, Part-of, Abstractive sentence summarization)(Multi-document summarization, Part-of, Document summarization)(Multi-document summarization, Compare, Single document summarization)(Multi-document summarization, Evaluate-for, Document summarization)(Multi-document summarization, evaluate-for, Transformer-based encoder-decoder framework)(Multi-document summarization, Evaluate-for, Selective Encoding with Template (BiSET) model)(Sentence scoring, Used-for, Extractive document summarization)(Sentence selection, Used-for, Extractive document summarization)(Transformer-based encoder-decoder framework, Evaluate-for, Abstractive document summarization)(Bi-directional Selective Encoding with Template (BiSET
(structured knowledge, is-a-Prerequisite-of, factoid question answering)(structured knowledge, Used-for, reasoning)
(entity linking, Used-for, disambiguating entity mentions)(entity linking, Compare, entity alignment)(entity linking, Compare, entity typing)(entity linking, Used-for, enriching texts with semantics)(entity linking, Conjunction, named entities)(entity linking, Compare, neural entity linking models)
(None)
1. (fine-grained entity typing, is-a-prerequisite-of, taxonomy learning)2. (fine-grained entity typing, is-a-prerequisite-of, entity categorization)3. (fine-grained entity typing, is-a-prerequisite-of, query understanding)4. (paper, focuses-on, supervised aspect extraction)5. (supervised aspect extraction, used-for, aspect extraction)6. (supervised aspect extraction, used-for, sentiment analysis)7. (neural entity typing models, represent, fine-grained entity types)8. (fine-grained entity typing, is-a-prerequisite-of, few-shot named entity recognition)9. (few-shot named entity recognition, is-a-prerequisite-of, fine-grained entity typing)10. (few-shot named entity recognition, is-a-prerequisite-of, fine-grained entity types)
(multilingual pre-trained, used-for, multilingual word representations)(multilingual pre-trained, Compare, monolingual embeddings)(pretrained embedding, Evaluate-for, NMT transfer)(Code-switching, Is-a-Prerequisite-of, multilingual processing)
(Stanford Question Answering Dataset, Compare, SQuAD)(SQuAD, Is-a-Prerequisite-of, neural architecture)(neural architecture, Part-of, constituent-centric neural architecture)(candidate answers, Is-a-Prerequisite-of, representation learning)(candidate answers, Evaluate-for, correct answers)(correct answers, Compare, syntactic, hierarchical and compositional structure)(semantic parsing, Used-for, question answering)(question answering, Is-a-Prerequisite-of, final answer selection)(final answer selection, Evaluate-for, multiple signals)(EviNets, Used-for, factoid question answering)(EviNets, Evaluate-for, candidate answer entities)(EviNets, Compare, structured knowledge bases and unstructured text documents)(universal schema, Is-a-Prerequisite-of, question answering)(Memory networks, Used-for, attending to a large body of facts)(Question Answering, Is-a-Prerequisite-of, machine comprehension)(CNN model, Used-for, text-based multiple
(Markables, Part-of, Named Entity Recognition)(Gazetteers, Used-for, Named Entity Recognition)(Named Entity Recognition, Evaluate-for, Performance)(Named Entity Recognition, Compare, Sequence Labeling)(Multi-Grained Named Entity Recognition, Is-a-Prerequisite-of, Named Entity Recognition)(Named Entity Recognition, Is-a-Prerequisite-of, Natural Language Processing)(Named Entity Recognition, Used-for, Text Segmentation)(Named Entity Recognition, Used-for, Information Extraction)(Named Entity, Part-of, Mention)
(neural symbolic machine, Used-for, language understanding)(neural symbolic machine, Part-of, Neural Symbolic Machine)(Neural Symbolic Machine, Part-of, a neural “programmer”)(Neural Symbolic Machine, Part-of, a Lisp interpreter)(Neural Symbolic Machine, Evaluate-for, REINFORCE)(COREQA, Used-for, natural answers)(COREQA, Is-a-Prerequisite-of, question answering system)(neural language model, Used-for, generation of conversational text)(LSTM, Used-for, Affect-LM)(Affect classification, Is-a-Prerequisite-of, Affect-LM)
(Chinese named entity, Part-of, Named Entity Recognition)(Chinese named entity, Evaluate-for, Named Entity Recognition)(Chinese named entity, Used-for, Named Entity Recognition)
`(political debate, Is-a-Prerequisite-of, predicting human activities)``(political debate, Is-a-Prerequisite-of, argument mining)``(political debate, Is-a-Prerequisite-of, computational construction of discourse networks)``(political debate, Is-a-Prerequisite-of, understanding political decision making)``(political debate, Hyponym-Of, argument mining)``(political debate, Compare, candidates' positions)``(argument mining, Is-a-Prerequisite-of, predicting the political bias)``(argument mining, Is-a-Prerequisite-of, factuality of reporting)``(candidates' positions, Is-a-Prerequisite-of, understanding political decision making)``(political bias, Is-a-Prerequisite-of, factuality of reporting)`
(None)
(concept, has-aspect, linguistic aspect)(concept, has-aspect, social context)(concept, is-impacted-by, what was written)(concept, is-impacted-by, who reads it)(concept, is-studied-in, media profiling)(concept, is-relevant-in, fake news detection)(concept, is-mentioned-in, evaluation results)
(Information, comprise, CMU-MOSEI)(Dialogue systems, based-on, unimodal sources)(Multimodal dialogue systems, have, opened new frontiers)
(None)
(None)
(translation, Evaluate-for, human translator)(source sentence, segue-to, neural machine translation)(Chinese-to-English translation, type-of, neural machine translation)(Japanese-English translation, type-of, neural machine translation)(human translator, Compare, neural machine translation)(linear associative units, Is-a-Prerequisite-of, recurrent unit)(Mixed RNN encoder, Is-a-Prerequisite-of, improvement)(NMT+RNNG, Is-a-Prerequisite-of, hybrid model)(translation, Used-for, source syntax benefits)(translation, Used-for, generating translations)(layer-wise relevance propagation, Evaluate-for, neural machine translation)(source-side syntactic trees, Is-a-Prerequisite-of, outperforming)(translation, Used-for, integrating multiple overlapping, arbitrary prior knowledge sources)(posterior regularization, Is-a-Prerequisite-of, integrating prior knowledge into neural machine translation)(distortion models, Is-a-Prerequisite-of, improving translation performance)(chunk-based decoders, Is-a-Pr
(concept, Used-for, reading comprehension style question answering)(conjunction, Contains, neural network)(neural network, Used-for, generating responses)(semantic dependency parsing, Is-a-Prerequisite-of, semantic graph parsing)(semantic graph parsing, Is-a-Prerequisite-of, semantic parsing)(semantic parsing, Is-a-Prerequisite-of, neural machine translation systems)(neural machine translation systems, Evaluate-for, calculating the output layer)(LSTM network, Used-for, extract semantic relations)(semantic relations, Compare, feature-based joint model)(neural encoder-decoder transition-based parser, Compare, grammar-based parser)
(argument mining, used-for, argumentative relation prediction)(argument mining, Is-a-Prerequisite-of, stance detection)(argument mining, Is-a-Prerequisite-of, entity recognition)(argument mining, Is-a-Prerequisite-of, sentiment analysis)(argument mining, Is-a-Prerequisite-of, discourse analysis)(argument mining, Compare, MeSH indexing)(argument mining, Compare, debate preparation)(argument mining, Compare, discourse parsing)(argument mining, Compare, event argument extraction)(argument mining, Conjunction, neural model)(argument mining, Conjunction, stance polarity and intensity prediction)(argument mining, Conjunction, discourse relation recognition)(argument mining, Part-of, task-specific parameterization)(stance detection, Is-a-Prerequisite-of, stance polarity and intensity prediction)(stance detection, Compare, event argument extraction)(stance detection, Evaluate-for, stance detection methods)(stance detection, Evaluate-for, stance intensity prediction)(stance detection, Evaluate-for, level of intensity prediction)(stance detection, Evaluate-for
1. (nlg model, Used-for, question answering)2. (nlg model, Used-for, document-level attention)3. (nlg model, Compare, sequence-to-sequence model)4. (document-level attention, Part-of, nlg model)5. (sequence-to-sequence model, Compare, memory augmented neural model)
(task learning, Used-for, multi-task learning)(task learning, Evaluate-for, performance results)(task learning, Compare, dependency parsing)(task learning, Evaluate-for, state-of-the-art results)(task learning, Is-a-Prerequisite-of, knowledge base population)(task learning, Is-a-Prerequisite-of, error detection)(task learning, Evaluate-for, improvement)(task learning, Compare, event extraction)(task learning, Evaluate-for, labeled data)(task learning, Evaluate-for, accuracy)(task learning, Evaluate-for, QA datasets)(task learning, Evaluate-for, entailment task)(task learning, Compare, multi-task learning)(task learning, Used-for, transfer learning)(task learning, Conjunction, unsupervised learning)(task learning, Evaluate-for, human evaluations)
(relational knowledge, Induce, textual entailment)(NLP, Is-a-Prerequisite-of, semantic parsing)(coreference resolution, Evaluate-for, natural language question)(natural language question, Evaluate-for, answer generation)(info retrieval, Is-a-Prerequisite-of, natural language question)
### Extracted Concepts:1. Response selection2. Multi-turn conversation3. Sequential matching network (SMN)4. Recurrent neural network (RNN)5. Dialogue6. Collaborative task7. Natural language processing (NLP)8. Context information9. Generative models for conversational systems10. Transition-based dependency parser11. Context-aware neural machine translation model12. Exemplar Encoder-Decoder network (EED)13. Sentence function### Triplets:1. (conversation model, Used-for, response selection)2. (conversation model, Used-for, multi-turn conversation)3. (conversation model, Used-for, dialogue)4. (context information, Is-a-Prerequisite-of, dialogue)5. (conversation model, Compare, generative models for conversational systems)6. (context information, Is-a-Prerequisite-of, generative models for conversational systems)7. (RNN
(None)
(None)
(None)
(machine translation model, is-a-prerequisite-of, Neural Machine Translation)(Neural Machine Translation, used-for, machine translation)(machine translation model, used-for, natural language processing)(Neural Machine Translation, part-of, machine translation system)(machine translation model, evaluate-for, translation performance)(machine translation model, is-a-prerequisite-of, deep learning)
(document date, essential-for, document retrieval)(knowledge bases, Used-for, task-oriented dialog systems)(information pollution, Related-to, information retrieval)(NeuralDater, Is-a-Prerequisite-of, document dating)(documents, Is-a-Prerequisite-of, NeuralDater)(NeuralDater, Used-for, document dating)(Entity-Duet Neural Ranking Model, Is-a-Prerequisite-of, EDRM)(queries, Used-for, EDRM)(documents, Used-for, EDRM)(sense-aware neural model, Is-a-Prerequisite-of, pun location)(SemEval 2017 dataset, Used-for, sense-aware neural model)(trustworthiness of information sources, Evaluate-for, information retrieval)(probabilistic models, Is-a-Prerequisite-of, trustworthiness estimation)(retrieved datapoints, Evaluate-for, semantic parsing)(context, Is-a-Prerequisite-of, retrieved datapoints)(lightweight and memory-efficient neural architectures, Is-a-
(None)
(encode-attend-decode paradigm, Compare, selective encoding model)(selective encoding model, Is-a-Prerequisite-of, abstractive summarization)(compressive summarization, Is-a-Prerequisite-of, oracle summary)(normalization, Used-for, parser adaptation)(SWAP-NET, Is-a-Prerequisite-of, extractive summarization)(semantic relevance based neural model, Is-a-Prerequisite-of, summarization)(graph-based attention mechanism, Is-a-Prerequisite-of, neural abstractive models)(template-aware summary generation, Used-for, template Reranking)(end-to-end neural network framework, Is-a-Prerequisite-of, extractive summarization)(graph-based framework, Is-a-Prerequisite-of, abstractive summarization)(sentence-level policy gradient method, Used-for, summarization model)(multi-task learning, Is-a-Prerequisite-of, accurate abstractive summarization)(cycled reinforcement learning method, Used-for, sentiment
(bias pretrained language model, Used-for, relation extraction)(bias pretrained language model, Compare, fact verification)(relation extraction, Evaluate-for, fact verification)
(automatic labeling, Used-for, event extraction)(training data, Is-a-Prerequisite-of, event extraction)(trigger words, Is-a-Prerequisite-of, event extraction)(DICE model, Evaluate-for, clinical event extraction)(argument prediction, Evaluate-for, DICE model)(not effective, Compare, traditional approaches)(entity interactions, Conjunction, modeling)(global view, Part-of, event extraction)(data-efficient generative model, Evaluate-for, clinical event extraction)(contrastive learning objective, Used-for, DICE model)
(None)
(Word embeddings, Used-for, Cross lingual word embeddings)(Word embeddings, Capture, linguistic regularities)(Word embeddings, Encourage, words with similar contexts)(Word embeddings, Be-linkable-to, Signed spectral normalized graph cut algorithm)(Word embeddings, Incorporate, external semantic information)(Word embeddings, Improve, representation of short texts)
(Semantic parsing datasets, Used-for, Semantic parsing)(Semantic parsing datasets, Part-of, Data-driven methods)(Semantic parsing, Evaluative, State-of-the-art results)(Data-driven methods, Compare, Language generation task)(Semantic parsing, Is-a-Prerequisite-of, Code generation)(Semantic parsing, Used-for, Mapping natural language utterances)
(concept, used-for, parsing) (language model, used-for, fine-tuning) (language model, used-for, text classification) (language model, Evaluate-for, performance improvement)
(Human language, Contains, Primordial home)(Interactional data, Harbors, Insights)(Conversational corpora, Provide, Empirical foundations)(Human language technology, Implications for, Language technology)(Linguistically diverse conversational corpora, Harbours, Insights)(Diverse conversational corpora, Increasingly becoming available, Through worldwide language documentation movement)
(MULTI-HOP QA, Focuses-on, EXPLAINABLE QA)(EXPLAINABLE MULTI-HOP QA, Requires, SYSTEM)(EVIDENCE EXTRACTION, Extracts, EVIDENCE SENTENCES)(MULTI-HOP QA, Involves, REASONING)(MULTI-HOP QA, Contains, DISJOINT PIECES OF REFERENCE TEXTS)
(None)
(neural machine translation, relies-on, bi-directional LSTMs)(neural machine translation, relies-on, convolutional layers)(neural machine translation, achieves, competitive accuracy)(neural machine translation, outperforms, multiple recently published results)(neural machine translation, enhanced-by, Deep Neural Networks (DNNs))(neural machine translation, suffers-from, severe gradient diffusion)(neural machine translation, addressed-by, linear associative units (LAU))(neural machine translation, can-improve, by incorporating source syntax)(neural machine translation, benefits-from, Mixed RNN encoder)(neural machine translation, integrates, Sequence-to-Dependency Neural Machine Translation (SD-NMT) method)
(word embeddings, gained popularity on tasks, word-embedding models)(word vectors, exhibit compositionality, semantic composite)(Skip-Gram model, explains success of vector calculus, additive compositionality)(Skip-Gram model, connection to Sufficient Dimensionality Reduction framework)(Skip-Gram model, parameters obtained from SDR models)(word embeddings, encourage words in similar contexts to be close, distribution of word co-occurrences)(neural word embeddings model, improves coherence)(neural word embeddings model, uses attention mechanism)(wheel, part of car)(topic models, applied on aspect extraction task)(topic models, do not produce highly coherent aspects)(neural approach, discovers coherent aspects)(bilingual word embeddings, rely on large parallel corpora)(bilingual word embeddings, motivated research to relax requirement using bilingual dictionaries)(character strings embeddings, proposed for Chinese word segmentation)(neural encoder-decoder parser, goal is parsing sentences to semantic representations)(Mild Cognitive Impairment, difficult to
(task-oriented dialogue system, Used-for, task success prediction models)(task-oriented dialogue system, Is-a-Prerequisite-of, structured knowledge)(task-oriented dialogue system, Is-a-Prerequisite-of, unstructured language)(task-oriented dialogue system, Is-a-Prerequisite-of, Global-Locally Self-Attentive Dialogue State Tracker)(task-oriented dialogue system, Is-a-Prerequisite-of, mem2Seq)(task-oriented dialogue system, Compare, Global-Locally Self-Attentive Dialogue State Tracker)(task-oriented dialogue system, Compare, two-stage CopyNet)(task-oriented dialogue system, Compare, EmoDS)(task-oriented dialogue system, Is-a-Prerequisite-of, incremental learning framework)
### Content: We present an algorithm for generating corrective REs that use contrastive focus (“no, the BLUE button”) to emphasize the information the hearer most likely misunderstood. We address semantic parsing in a multilingual context. We train one multilingual model that is capable of parsing natural language sentences from multiple different languages into their corresponding formal semantic representations. Natural language processing has increasingly moved from modeling documents and words toward studying the people behind the language.### Concept: natural language generation task1. (algorithm, Used-for, generating corrective REs)2. (algorithm, Evaluate-for, contrastive focus)3. (semantic parsing, Is-a-Prerequisite-of, natural language generation task)4. (multilingual model, Used-for, parsing natural language sentences)5. (multilingual model, Is-a-Prerequisite-of, natural language generation task)6. (natural language processing, Part-of, natural language generation task)
(conversational Question Answering, Is-a-Prerequisite-of, information retrieval)(old book Facts, Is-a-Prerequisite-of, information retrieval)(language models, Is-a-Prerequisite-of, information retrieval)(OpenBookQA, Hyponym-Of, Natural Language Processing)(information retrieval, Used-for, text understanding)(information retrieval, Compare, QA systems)(information retrieval, Is-a-Prerequisite-of, datasets)(information retrieval, Evaluate-for, machine reading)(information retrieval systems, Used-for, semantic hashing)(information retrieval, Evaluate-for, similarity search)
(large language model, Used-for, language understanding systems)(large language model, Evaluate-for, semantic quality improvement)(large language model, Is-a-Prerequisite-of, dialogue state tracking)(large language model, Used-for, generation of rhythmic poetry)(large language model, Compare, recurrent neural networks)(large language model, Is-a-Prerequisite-of, LSTM language model)(large language model, Compare, neural machine translation models)(large language model, Evaluate-for, learning morphology)(large language model, Is-a-Prerequisite-of, GuessTwo task)(large language model, Part-of, NLP systems)(large language model, Used-for, adding pretrained context embeddings)(large language model, Evaluate-for, language model perplexity improvement)(large language model, Used-for, sequence labeling tasks)(large language model, Part-of, generative models)(large language model, Is-a-Prerequisite-of, neural machine translation system)
(sentiment classifier, involves, Multimodal sentiment analysis)(Multimodal sentiment analysis, involves, identification of sentiments in videos)(sentiment classifier, used-for, classification process)(sentiment classifier, Used-for, sentence-level sentiment classification)(sentiment classifier, Used-for, aspect sentiment classification)(sentiment classifier, evaluates-for, effectiveness)(sentiment classifier, Evaluate-for, performance improvement)(simple models, used-for, sentence-level sentiment annotation)(Performance, evaluated-for, robustness)(uttarancess, juxtaposition, independent entities)(uttarancess, ignores, interdependencies)(uttarancess, ignores, relations)(uttarancess, involves, research)(uttarancess, part-of, video)(uttarancess, evaluate-for, classification process)(implicit discourse relation classification, challenging, lack of connectives)(implicit discourse relation classification, involves, classification)(implicit discourse relation classification, evaluate-for, recognization)(connectives
(Unsupervised machine translation, relies-on, unsupervised cross-lingual word embedding technique)(Unsupervised machine translation, examines, adversarial technique)(Unsupervised machine translation, involves, bilingual dictionary induction)(Unsupervised machine translation, relies-on, bilingual tasks)(Unsupervised machine translation, is-a-Prerequisite-of, bilingual lexicon induction)(Unsupervised machine translation, performs, unsupervised lexical induction)(Bilingual lexicon induction, involves, morphological variation)(Bilingual lexicon induction, requires, word embedding spaces)(Bilingual lexicon induction, aims to induce, word translations)(Bilingual lexicon induction, outperforms, previous methods)(Bilingual lexicon induction, improves, cross-lingual embeddings)(Bilingual lexicon induction, can, induce bilingual lexicons)(Bilingual lexicon induction, uses, nearest neighbor methods)(Bilingual lexicon induction, benefits-from, morphology-aware alignment model)
( relation learning, Is-a-Prerequisite-of, multi-task learning)( relation learning, Evaluate-for, framework)( relation learning, Used-for, multi-lingual attention)( relation learning, Compare, dependency parsing)( relation learning, Hyponym-Of, error detection)( relation learning, Part-of, feature imitation framework)( relation learning, Evaluate-for, feature imitation)
```(relation extraction task, used-for, finding unknown relational facts from plain text)(relation extraction task, evaluate-for, multi-lingual neural relation extraction framework)(relation extraction task, evaluate-for, cross-lingual attention)(relation extraction task, evaluate-for, model performance)(relation extraction task, is-a-prerequisite-of, building training data)(relation extraction task, is-a-prerequisite-of, distant supervision)(relation extraction task, is-a-prerequisite-of, relation detection)```
(event ontology, zero-shot event extraction, Textual Entailment)(event ontology, concept linking, entity linking)(event ontology, long-form answers, long-form answer functional roles)
(None)
(concept, Evaluate-for, Neural Symbolic Machine)(Neural Symbolic Machine, Is-a-Prerequisite-of, language understanding task)(language understanding systems, Evaluate-for, low-frequency word forms)(low-frequency word forms, Is-a-Prerequisite-of, language understanding systems)(language understanding systems, Evaluate-for, distributional vector space models)(distributional vector space models, Is-a-Prerequisite-of, language understanding systems)(language understanding systems, Evaluate-for, morph-fitting procedure)(morph-fitting procedure, Is-a-Prerequisite-of, language understanding systems)
(langauges, share, multilingual translation)(translation, is-a-prerequisite-of, multilingual translation)(code-switching, used-for, multilingual translation)(CLWE, used-for, multilingual translation)
(bias, pretrained language model, concept)(pretrained language model, used-for, bias)
(neural summarization model, is-based-on, encoder-decoder framework)(neural summarization model, aims-to-improve, semantic relevance)(neural summarization model, uses, gated attention encoder)(neural summarization model, produces, summary representation)(neural summarization model, maximizes, similarity score)(neural summarization model, outperforms, baseline systems)(encoder-decoder framework, is-a-Prerequisite-of, neural summarization model)(encoder-decoder framework, is-based-on, deep learning)(encoder-decoder framework, is-used-for, summarization)(gated attention encoder, is-a-Prerequisite-of, summary representation)(summary representation, is-used-for, decoding)(document encoding, is-represented-by, hierarchical encoder)(hierarchical encoder, learns, latent dependencies)(hierarchical encoder, takes-advantage-of, explicit graph representations)(cross-document relationships, are-represented-via, attention mechanism)
(unsupervised bilingual lexicon induction, Used-for, unsupervised machine translation)(unsupervised bilingual lexicon induction, Compare, bilingual lexicon induction)(unsupervised bilingual lexicon induction, Is-a-Prerequisite-of, bilingual word embeddings)(unsupervised bilingual lexicon induction, Part-of, unsupervised methods)(unsupervised bilingual lexicon induction, Evaluate-for, graph similarity metric)(unsupervised bilingual lexicon induction, Evaluate-for, task of unsupervised bilingual lexicon induction)(unsupervised bilingual word embedding, Used-for, unsupervised neural machine translation)(unsupervised bilingual word embedding, Is-a-Prerequisite-of, unsupervised neural machine translation)(unsupervised neural machine translation, Compare, conventional UNMT)(unsupervised neural machine translation, Evaluate-for, performance of UBWE)
```(cross lingual semantic parsing, Used-for, neural translation)(cross lingual semantic parsing, Compare, multilingual semantic parsing)(cross lingual semantic parsing, Evaluate-for, performance improvement)```
(relations, Compare, representations)(semantic parser, Used-for, semantic parsing)(parser, Hyponym-Of, semantic parser)(model, Used-for, semantic role labeling)(parser, Part-of, neural architecture)
(None)
**(multimodal machine translation, Is-a-Prerequisite-of, visual features)  (multimodal machine translation, Compare, neural machine translation)  (multimodal machine translation, Compare, traditional machine translation)  (neural machine translation, Is-a-Prerequisite-of, machine translation)  (neural machine translation, Compare, traditional machine translation)  (neural machine translation, Compare, statistical machine translation)  (machine translation, Is-a-Prerequisite-of, automatic question answering)  (statistical machine translation, Is-a-Prerequisite-of, automatic question answering)**
(Document, Mention, Summarization dataset)(Document, Introduce, Query-based summarization)(Document, Mention, Abstractive summarization)(Document, Build-on, Document summarization research)(Document summarization research, Is-a-Prerequisite-of, Abstractive summarization)(Document, Evaluate-for, Proposed model)(Document, Evaluate-for, Proposed model)(Extractive summarization, Compare, Abstractive summarization)(Extractive summarization, Compare, Query-based summarization)(Sequence-to-sequence models, Used-for, Abstractive text summarization)(Sequence-to-sequence models, Compare, Previous summarization systems)(Neural models, Is-a-Prerequisite-of, Abstractive summarization)(Extractive methods, Compare, Abstractive methods)(Document summarization research, Used-for, Extractive summarization)(Document summarization research, Evaluate-for, Proposed selective encoding model)(Query-based summarization dataset, Build-on, Debatepedia)(Query-based summarization dataset, Used
(dependency parse, Is-a-Prerequisite-of, token-based dependency parsing)(dependency parse, Is-a-Prerequisite-of, token-based sequence tagging)(dependency parse, Compare, token-based sequence tagging)(dependency parse, Used-for, neural techniques for end-to-end computational argumentation mining)(dependency parse, Compare, dependency graph)(dependency parse, Part-of, natural language processing)(dependency parse, Part-of, semantic parsing)
(image text retrieval, Used-for, Natural Language Processing)(Natural Language Processing, Is-a-Prerequisite-of, Image Text Retrieval)(image text retrieval, Compare, Dense Retrieval)(Neural Networks from Natural Language, Evaluate-for, Image Text Retrieval)(Open-domain question answering, Is-a-Prerequisite-of, Image Text Retrieval)
(None)
**(political debate, Used-for, citizens)  (political debate, Is-a-Prerequisite-of, understanding democratic political decision making)  (Argument Mining, Compare, existing research)  (Argument Mining, Evaluate-for, understanding the typology of argument components in political debates)  (US presidential campaigns, Hyponym-Of, political debates)  (tweet, Part-of, microblogging platform)  (network-based behavioral features, Used-for, predict frames used in political discourse on Twitter)  (premises, Part-of, argumentative components in political debates)**
(None)
(semantic representation, Evaluate-for, sentence representation learning)(word representation learning, Evaluate-for, sentence representation learning)(AMR parsing, part-of, sentence representation learning)(abstract meaning representations, Hyponym-Of, sentence representation learning)(SoPa, Used-for, sentence representation learning)(Tree Transformer, Used-for, sentence representation learning)(latent representations, Part-of, sentence representation learning)(sentence embeddings, Is-a-Prerequisite-of, sentence representation learning)(lexicon relations, Is-a-Prerequisite-of, sentence representation learning)(neural MT models, Compare, sentence representation learning)(encoder-decoder dialog model, Compare, sentence representation learning)
(None)
(named entity relation, Extracted-from, Multimodal Named Entity Disambiguation (MNED))(Multimodal Named Entity Disambiguation (MNED), Used-for, Social media posts disambiguation)(named entity relation, Is-a-Prerequisite-of, Named Entity Disambiguation)(named entity relation, Used-for, Named Entity Disambiguation)(named entity relation, Compare, Named Entity Disambiguation)(named entity relation, Is-a-Prerequisite-of, Cross-lingual Named Entity Recognition)(Multimodal Named Entity Disambiguation (MNED), Evaluate-for, State-of-the-art text-only NED models)
(concept, Is-a-Prerequisite-of, zero pronoun resolution)(concept, Evaluate-for, document and query relationship mining)(concept, Evaluate-for, question answering)(document and query relationship mining, Is-a-Prerequisite-of, automatic question generation)(document and query relationship mining, Evaluate-for, question answering)(question answering, Used-for, reading comprehension)(question answering, Evaluate-for, reading comprehension)(reading comprehension, Is-a-Prerequisite-of, question answering)(reading comprehension, Evaluate-for, machine reading comprehension (MRC))(reading comprehension, Used-for, question answering)(machine reading comprehension (MRC), Evaluate-for, answering questions)
(concept, Part-of, machine reading comprehension style question answering)(concept, Compare, recurrent neural networks)(concept, Is-a-Prerequisite-of, natural-language understanding systems)(concept, Hyponym-Of, TRIVIAQA)(concept, Evaluate-for, identifying relevant parts)(concept, Is-a-Prerequisite-of, document comprehension)(concept, Evaluate-for, extracting answers)(machine reading comprehension style question answering, Part-of, reading comprehension)(machine reading comprehension style question answering, Compare, natural language understanding systems)(machine reading comprehension style question answering, Part-of, document comprehension)(machine reading comprehension style question answering, Compare, TRIVIAQA)(recurrent neural networks, Part-of, reading comprehension)(recurrent neural networks, Is-a-Prerequisite-of, machine reading comprehension style question answering)(natural language understanding systems, Is-a-Prerequisite-of, reading comprehension)(TRIVIAQA, Compare, machine reading comprehension style question answering)(identifying relevant
(Complex Word Identification, Part-of, Named Entity Recognition)(Gazetteers, Used-for, Named Entity Recognition)(Named Entity Recognition, Evaluate-for, External Knowledge)(Named Entity Recognition, Evaluate-for, Sequence Labeling)(Named Entity Recognition, Is-a-Prerequisite-of, Named Entity Disambiguation)(Named Entity Recognition, Compare, Named Entity Recognition in German)(Named Entity Recognition, Compare, Multilingual Learning)(Named Entity Recognition, Hyponym-Of, Named Entity Disambiguation)(Named Entity Recognition, Part-of, Named Entity Recognition Model)(Named Entity Recognition, Part-of, Language Modeling)(Named Entity Recognition, Part-of, Slot Filling)(Named Entity Recognition, Part-of, Transformer-based Models)(Named Entity Recognition, Part-of, Noisy Sequence Labeling)
(recognition, Used-for, relation extraction)(relation extraction, Compare, relation facts)(relation extraction, Is-a-Prerequisite-of, multi-lingual neural relation extraction framework)(relation extraction, Is-a-Prerequisite-of, distant supervision)(recognition, Evaluate-for, relation extraction)(relation extraction, Evaluate-for, entity tuple)(document-level RE, Is-a-Prerequisite-of, relation extraction)(relation extraction, Compare, relation instances)(relation extraction, Evaluate-for, model training)(relation extraction, Is-a-Prerequisite-of, graph neural network)(relation extraction, Is-a-Prerequisite-of, Open Information Extraction)(relation extraction, Hyponym-Of, relation facts)(relation extraction, Is-a-Prerequisite-of, factoid question-answering)(relation extraction, Evaluate-for, model performance)
(claim verification, used-for, fact checking)(fact checking, Used-for, verifying claim)(fact checking, is-a-prerequisite-of, automated fact checking)(automated fact checking, used-for, generating justifications)(fact checking, Evaluate-for, verifying veracity)(automated fact checking, is-a-prerequisite-of, claim verification)(fact checking, is-a-prerequisite-of, misinformation detection)
(network, part-of, machine learning)(neural networks, compare, symbolic reasoning)(recurrent neural networks, part-of, deep learning)(neural networks, compare, LSTMs)(CNN, used-for, text categorization)(hierarchical of layers, compare, encoding of semantic aspects)
(context word vector, is-a-Prerequisite-of, word embedding models)(context word vector, part-of, distributional vector space)(distributional vector space, Used-for, GLEN model)(GLEN model, Evaluate-for, LE)(word embedding models, Compare, context word vector)
(word embeddings, Learn-from-approaches, bilingual word embeddings)(word embeddings, Utilized-in, multilingual model)(word embeddings, Used-for, semantic parsing)(bilingual word embeddings, Used-for, neural machine translation)(cross-lingual word embeddings, Encode, word meaning)(cross-lingual word embeddings, Independent-of, language)(cross-lingual word embeddings, Measure-by, modularity)(cross-lingual word embeddings, Could-be-improved-by, intrinsic validation metric)(cross-lingual word embeddings, Jointly-learn-with, skip-gram)(cross-lingual word embeddings, Contribute-for, bilingual lexicon induction)
(dialog generation, used-for, neural models)(dialog generation, used-for, knowledge diffusion)(dialog generation, Compare, story generation)(dialog generation, Is-a-Prerequisite-of, interpretable response generation)(dialog generation, Is-a-Prerequisite-of, encoder-decoder model)(dialog generation, Is-a-Prerequisite-of, variational autoencoders)(neural models, Evaluate-for, sentence function)(neural models, Compare, variational autoencoders)(neural models, Compare, sequence-to-sequence models)(neural models, Compare, hard attention mechanism)(sequence-to-sequence models, Part-of, state-of-the-art models)
(conditional text generation, Pertains-to, Neural Network Models)(conditional text generation, Utilizes, Pre-train and Plug-in Variational Auto-Encoder)(conditional text generation, Involves, Conditional Generation Models)(Conditional Generation Models, Requires, Full Retraining)(Neural Network Models, Employed-for, Text Generation)(Pre-train and Plug-in Variational Auto-Encoder, Represents, New Framework)(Pre-train and Plug-in Variational Auto-Encoder, Employs, VAE Architecture)
(unsupervised cross-lingual, Compare, unsupervised multilingual models)(unsupervised multilingual models, Is-a-Prerequisite-of, unsupervised cross-lingual)(unsupervised cross-lingual, Used-for, unsupervised machine translation)(unsupervised bilingual word embeddings, Compare, unsupervised cross-lingual)(unsupervised cross-lingual, Is-a-Prerequisite-of, unsupervised bilingual word embeddings)(unsupervised cross-lingual, Evaluate-for, machine translation)(unsupervised cross-lingual, Is-a-Prerequisite-of, machine translation)
### Extracted Concepts:1. Pretrained language models2. NLP applications3. Gender biases4. Word embeddings5. Social biases6. Sentence-level representations7. Contextualized representations8. StructuralLM9. Large pre-trained models10. Domain-specific data11. BERT12. Racial biases13. Embedded biases14. ACL anthology### Triplets:1. (Pretrained language models, Used-for, NLP applications)(NLP applications, Evaluate-for, Pretrained language models)2. (Gender biases, Evaluate-for, Pretrained language models)3. (Word embeddings, Used-for, Gender biases)(Word embeddings, Compare, Sentence-level representations)4. (Social biases, Compare, Gender biases)5. (Sentence-level representations, Evaluate-for, Social biases)6. (Contextualized representations, Compare, Sentence-level representations)(Contextualized representations, Used-for, Downstream N
(summarization datasets, Used-for, query-based summarization)(summarization datasets, Used-for, abstractive sentence summarization)(summarization datasets, Part-of, English Gigaword)(summarization datasets, Part-of, DUC 2004)(summarization datasets, Part-of, MSR abstractive sentence summarization)(summarization datasets, Used-for, extractive multi-document summarization)(summarization datasets, Used-for, compressive summarization)(summarization datasets, Part-of, CNN/Daily Mail dataset)(summarization datasets, Part-of, DUC-2002)(summarization datasets, Evaluate-for, evaluating summarization systems)
(adversarial training, Used-for, improving robustness of models)(adversarial training, Utilized-in, neural machine translation)(adversarial training, Evaluate-for, training adversarial examples)(adversarial training, Is-a-Prerequisite-of, crafting adversarial examples)(adversarial training, Evaluate-for, self-attentive neural networks)(adversarial training, Evaluate-for, text adversarial attack)(adversarial training, Evaluate-for, improving performance of neural models)
(CMU-MOSEI dataset, Evaluate-for, multimodal language)(Multimodal Named Entity Disambiguation task, Used-for, multimodal language)(Multimodal dialogue systems, Is-a-Prerequisite-of, multimodal language)(multimodal machine translation, Is-a-Prerequisite-of, multimodal language)
(morphological analyzer, Used-for, inflection generation)(inflection generation, Compare, morphological analysis)(morphological analyzer, Evaluate-for, accuracy improvement)(inflection generation, Is-a-Prerequisite-of, morphological analysis)(morphological analysis, Part-of, morphological analyzer)(inflection generation, Part-of, morphological analyzer)(morphological analysis, Evaluate-for, language understanding systems)
1. (long form question answering, Used-for, answering multiple choice questions)2. (long form question answering, Evaluate-for, facilitating geometric reasoning tasks)3. (long form question answering, Compare, distant supervised open-domain question answering)4. (open-domain question answering, Evaluate-for, supporting reasoning tasks with semi-structured knowledge)5. (dynamic neural semantic parsing, Is-a-Prerequisite-of, long form question answering)6. (reading comprehension, Is-a-Prerequisite-of, long form question answering)7. (question answering models, Is-a-Prerequisite-of, long form question answering)
(document modeling, Utilized-for, semantic understanding)(response generation, Utilized-for, conversation systems)(Codec connection, Is-a-Prerequisite-of, sentence generation)(sentence generation, Part-of, abstractive summarization)(Sentence generation, Compare, Question generation)(sentence generation, Evaluate-for, response generation)(sentence generation, Part-of, dialog systems)(response generation, Hyponym-Of, machine translation)(sentence generation, Evaluate-for, document modeling)(document modeling, Evaluate-for, summarization)
(dialogue generation model, Used-for, generating responses)(dialogue generation model, Evaluate-for, response relevance)(dialogue generation model, Evaluate-for, response diversity)(dialogue generation model, Evaluate-for, accuracy of response generation)(dialogue generation model, Compare, hierarchical recurrent encoder-decoder models)(dialogue generation model, Compare, neural dialogue models)
(Discourse coherence, Is-a-Prerequisite-of, Document-level coherence score)(Discourse coherence, Part-of, Text quality)(Discourse coherence, Conjunction, Discourse parsing)(Discourse coherence, Evaluate-for, Summarization)(Discourse coherence, Compare, Coherence assessment)(Discourse coherence, Part-of, Dialogue coherence evaluation)(Discourse coherence, Evaluate-for, Text-level parsing)
(None)
(neural retrieval model, Used-for, conversation generation)(neural retrieval model, Is-a-Prerequisite-of,  coreference resolution)(neural retrieval model, Compare, sequence-to-sequence model)(coreference resolution, Used-for, context-dependent semantic parsing)(sequence-to-sequence model, Is-a-Prerequisite-of, neural language model)
(None)
(entity recognition ner aim, Used-for, auxiliary data)(entity recognition ner aim, Evaluate-for, named entity recognition)(entity recognition ner aim, Evaluate-for, natural language processing)(named entity recognition, Is-a-Prerequisite-of, entity recognition ner aim)(natural language processing, Is-a-Prerequisite-of, entity recognition ner aim)
1. (relation extraction model, Used-for, finding unknown relational facts from plain text)2. (relation extraction model, Compare, state-of-the-art relation extraction approaches)3. (relation extraction model, Evaluate-for, significant improvements in relation extraction)4. (relation extraction model, Is-a-Prerequisite-of, considering information consistency and complementarity among cross-lingual texts)5. (baseline, Compare, our model in relation extraction)6. (baseline, Evaluate-for, comparison with our model in relation extraction)
(relations, Is-a-Prerequisite-of, pre-trained model)(pre-trained model, Used-for, NLP tasks)
(morphological inflection, Used-for, morpheme)(morphological inflection, Evaluate-for, word embedding)(morphological inflection, Evaluate-for, language understanding systems)(morphological inflection, Is-a-Prerequisite-of, semantic quality)(morphological inflection, Is-a-Prerequisite-of, dialogue state tracking)(morpheme, Compare, word)(morphological inflection, Compare, word embeddings)(morphological inflection, Evaluate-for, morphological analysis)(morphological inflection, Is-a-Prerequisite-of, word representations)(morphological inflection, Is-a-Prerequisite-of, words)
(semantic information, part-of, content-related posts)(structural information, part-of, reply relationship)(language, Is-a-Prerequisite-of, social bias frame)(societal biases, Evaluate-for, amplifying inequality)(word embeddings, Used-for, measurement of social meaning)(word embeddings, Used-for, beliefs assessment)(word embeddings, Used-for, reflecting survey data)(word embeddings, Is-a-Prerequisite-of, societal bias)(semantic formalisms, Is-a-Prerequisite-of, pragmatic implications)(statement, Conjunction, pragmatic implications)(Social Bias Frames, Evaluate-for, modelling pragmatic frames)(Social Bias Frames, Used-for, projecting social biases)(Social Bias Frames, Is-a-Prerequisite-of, inference corpus)(model, Is-a-Prerequisite-of, pragmatic inference)(model biases, Evaluate-for, societal bias)(models, Evaluate-for, amplifying inequality)(bias, Used-for, measurement)(bias, Used-for, mitigation)(bias, part-of, pretrained language models)(bias
(None - No concepts related to "translation model" extracted from the provided content.)
(Adversarial robustness, is-a-prerequisite-of, Adversarial examples)(Adversarial robustness, used-for, Improving NLP systems' robustness)(Adversarial robustness, evaluated-for, Performance metrics)(Adversarial robustness, part-of, Defense mechanisms)(Evaluative relationship, used-for, Adversarial training)(Adversarial examples, part-of, Adversarial attacks)(Adversarial robustness, used-for, Adversarial training)(Adversarial examples, is-a-prerequisite-of, Adversarial attacks)
(word embeddings, gained popularity, word-embedding models)(word-embedding models, exhibit compositionality, vectors)(word embeddings, used for, word analogy questions)(word embeddings, used for, caption generation)(word embeddings, used to, discover coherent aspects)(word embeddings, capture, linguistic regularities)(word embeddings, produce vector spaces, where antonyms are close)(word embeddings, used in, dependency parsing)(word embeddings, learn bilingual embeddings)(word embeddings, provide, point representations)(word embeddings, provide semantic information)(word embeddings, introduce, multimodal word distributions)(word embeddings, used in, neural network architectures)(word embeddings, capture, word similarity)(word embeddings, used for, document clustering)(word embeddings, evaluated using, Wasserstein distance)(word embeddings, proposed in, financial markets)(word embeddings, used in, temporal word analogies)
(Recurrent Neural Networks, Used-for, Reading comprehension)(Recurrent Neural Networks, Used-for, Question answering)(Reading comprehension, Is-a-Prerequisite-of, Question answering)(COREQA, Evaluate-for, Natural language sentence generation)(Neural models, Compare, Traditional QA models)(TextWorldsQA, Part-of, Text-based QA datasets)(TextWorldsQA, Used-for, Testing QA models)(SciDTB, Used-for, Evaluating discourse dependency parsers)(DEISTE, Is-a-Prerequisite-of, SciTail entailment task)(SAN, Evaluate-for, Machine reading comprehension)(HEAD-QA, Is-a-Prerequisite-of, Advanced reasoning techniques)
(answer, contains, dataset)(answer, retrieved-from, Wikipedia)(dataset, compared-to, TriviaQA)(dataset, used-in, evaluation)(dataset, contains, question-answer pairs)(dataset, part-of, TextWorldsQA)
(representation, Utilized-for, task)(representation, Improve, performance)(representation, Part-of, language models)(representation, Used-for, semantic parsing)(representation, Incorporate, cross-lingual distributed logical representations)(representation, Incorporate, morphological disambiguation)
(multi-task learning, Compare, single-task learning)(multi-task learning, Evaluate-for, performance improvement)(multi-task learning, Is-a-Prerequisite-of, transfer learning)(multi-task learning, Used-for, sharing parameters)(multi-task learning, Conjunction, multiple tasks)(multi-task learning, Evaluate-for, state-of-the-art performance)(multi-task learning, Evaluate-for, performance ablations)
(concept: automatic hate speech detection models, relation: used-for, concept: detecting online hate)(concept: hate speech detection models, relation: part-of, concept: automatic hate speech detection models)(concept: hate speech detection models, relation: evaluate-for, concept: performance)(concept: detecting hate speech, relation: Is-a-Prerequisite-of, concept: hate speech detection)(concept: affective tasks, relation: part-of, concept: sentiment analysis)(concept: sentiment analysis, relation: used-for, concept: hate speech detection)(concept: effective language models, relation: used-for, concept: detecting hate speech)
(None)
(unsupervised selective rationalization, Part-of, deep learning models)(unsupervised selective rationalization, Evaluate-for, explanation for output)(unsupervised selective rationalization, Compare, unsupervised neural machine translation)(unsupervised selective rationalization, Compare, rationale generator)(unsupervised selective rationalization, Compare, predictor)(unsupervised selective rationalization, Evaluate-for, training technique)(unsupervised selective rationalization, Evaluate-for, rationale plausibility)
(Zero pronoun resolution, Used-for, Annotated data)(Zero pronoun resolution, Compare, Lack of annotated data)(Zero pronoun resolution, Evaluate-for, Automatic generation of pseudo training data)(Zero pronoun resolution, Is-a-Prerequisite-of, Cloze-style reading comprehension neural network model)(Cloze-style reading comprehension neural network model, Used-for, Zero pronoun resolution)(Cloze-style reading comprehension neural network model, Is-a-Prerequisite-of, Two-step training mechanism)(Reading comprehension datasets, Part-of, Reading comprehension)(Reading comprehension datasets, Evaluate-for, Development of natural-language understanding systems)(Reading comprehension datasets, Is-a-Prerequisite-of, Knowing the quality of RC datasets)(Reading comprehension, Compare, Multiple passages reading comprehension)(Reading comprehension, Compare, Co-matching approach to multi-choice reading comprehension)(Multi-passage reading comprehension, Is-a-Prerequisite-of, Combining cross-passage information)(Multi-passage reading comprehension, Part-of, Verification of answer candidates)(
(conversational agents, interact with, external database)(conversational agents, issue, symbolic query)(conversational agents, break, differentiability of the system)(conversational agents, prevent, end-to-end training of neural dialogue agents)(KB-InfoBot, is, multi-turn dialogue agent)(KB-InfoBot, helps users search, Knowledge Bases)(KB-InfoBot, replaces, symbolic queries)(KB-InfoBot, integrates, soft retrieval process)(KB-InfoBot, leads to, higher task success rate)(KB-InfoBot, leads to, higher reward)(neural dialogue agents, trained, entirely from user feedback)(neural dialogue agents, application towards, personalized dialogue agents)(dialogue agents, task-oriented, spoken)(dialogue agents, novel hybrids, systems)(dialogue systems, investigate, chat detection)(dialogue systems, construct, dataset)(dialogue systems, construct, new dataset)(dialogue systems, investigate,
(chinese spelling correction, Is-a-Prerequisite-of, spelling error correction)(chinese spelling correction, Compare, spelling correction)(chinese spelling correction, Evaluate-for, Spelling Check)(chinese spelling correction, Is-a-Prerequisite-of, language understanding ability)(chinese spelling correction, Used-for, Chinese texts)(chinese spelling correction, Compare, Language model)(chinese spelling correction, Is-a-Prerequisite-of, error model)(chinese spelling correction, Compare, Textual information)(chinese spelling correction, Is-a-Prerequisite-of, phonetic information)(chinese spelling correction, Is-a-Prerequisite-of, semantic information)
(None)
(consecutive sentence, Is-a-Prerequisite-of, Language Understanding)(sequential models, Compare, Existing similarity measures)(relational-realizational grammar, Is-a-Prerequisite-of, Opinionated Natural Language Generation)(generative grammars, Used-for, Opinionated Natural Language Generation)(automatic translation, Evaluate-for, Event extraction)
(social bias encoded, Is-a-Prerequisite-of, word embeddings)(social bias encoded, Compare, social biases in word embeddings)(social bias encoded, Evaluate-for, studies on society)(word embeddings, Used-for, downstream applications)(word embeddings, Compare, biases in embeddings)(word embeddings, Evaluate-for, recover Social Bias Frames)(studies on society, Is-a-Prerequisite-of, social bias measurements)(studies on society, Evaluate-for, bias measurements)(Social Bias Frames, Conjunction, social biases and stereotypes)(sentence-level representations, Used-for, detect social biases)(NLP models, Compare, NLP tasks)(NLP models, Evaluate-for, understand implicit biases)(BERT, Is-a-Prerequisite-of, FinBERT)(BERT, Evaluate-for, assess the implicit stock market preferences)(social biases and stereotypes, Part-of, culture)(social biases and stereotypes, Conjunction, biases in children stories)(story analysis, Used-for, analyze
(S2ST, Used-for, direct speech speech translation)(AV-S2ST, Conjunction, audio-visual speech translation)(AV-S2ST, Used-for, AV-TranSpeech)(AV-S2ST, Hyponym-Of, S2ST)(AV-S2ST, Compare, S2ST)(AV-S2ST, Is-a-Prerequisite-of, AV-TranSpeech)(AV-TranSpeech, Compare, S2ST)(AV-TranSpeech, Is-a-Prerequisite-of, AV-S2ST)(UnitY, Is-a-Prerequisite-of, direct speech speech translation)(UnitY, Used-for, direct speech speech translation)(UnitY, Compare, S2ST)(UnitY, Is-a-Prerequisite-of, S2ST)(UnitY, Used-for, S2ST)(SpeechMatrix, Is-a-Prerequisite-of, S2ST)(SpeechMatrix, Is-a-Prerequisite-of, AV-S2ST)(Text machine
(contextualized embeddings, used-for, aspect-based sentiment analysis)(contextualized embeddings, part-of, knowledge graph builder)(contextualized embeddings, Compare, traditional word embeddings)(contextualized embeddings, Evaluate-for, temporal changes)(contextualized embeddings, part-of, NLP tasks)(contextualized embeddings, used-for, Named Entity Recognition)(contextualized embeddings, Is-a-Prerequisite-of, Word Sense Disambiguation)(contextualized embeddings, Evaluate-for, machine translation evaluation)(contextualized embeddings, Is-a-Prerequisite-of, Social-oriented tasks)(contextualized embeddings, Is-a-Prerequisite-of, argument clustering)(contextualized embeddings, Evaluate-for, lexical substitution)(contextualized embeddings, Is-a-Prerequisite-of, VAMPIRE pretraining framework)
(oriented spoken dialogue system, is-a-Prerequisite-of, task-oriented dialogue system)(task-oriented dialogue system, Used-for, dialogue policy optimization)(oriented spoken dialogue system, Is-a-Prerequisite-of, end-to-end neural architecture for dialogue systems)(task-oriented dialogue system, Is-a-Prerequisite-of, slot filling)(oriented spoken dialogue system, Part-of, GLAD)(slot filling, Is-a-Prerequisite-of, belief tracker)(dialogue policy optimization, Evaluate-for, training intermediate dialogue turns)(end-to-end neural architecture for dialogue systems, Evaluate-for, goal-oriented dialog systems)
(emotion cause pair extraction, Is-a-Prerequisite-of, emotion cause extraction)(emotion cause pair extraction, Is-a-Prerequisite-of, emotion-cause pair extraction)(emotion cause pair extraction, Compare, emotion-cause pair extraction)(emotion cause pair extraction, Compare, ECE)(emotion cause pair extraction, Part-of, Emotion-Cause Pair Extraction)(emotion cause pair extraction, Is-a-Prerequisite-of, document emotion analysis)(emotion cause pair extraction, Part-of, emotion-cause pair)(emotion cause pair extraction, Evaluate-for, emotion clause identification)
(contextual embeddings, Part-of, word embeddings)(contextual embeddings, Evaluate-for, coherence)(word embeddings, Compare, topic models)(word embeddings, Used-for, coherence improvement)(embeddings, Is-a-Prerequisite-of, coherence)(embeddings, Compare, topic models)(coherence, Part-of, aspects)(coherence, Evaluate-for, relevance)(coherence, Evaluate-for, results)(coherence, Used-for, improving aspects)(topic models, Is-a-Prerequisite-of, aspect-based sentiment analysis)(aspect-based sentiment analysis, Compare, sentiment analysis)(aspect-based sentiment analysis, Is-a-Prerequisite-of, aspect extraction)(aspect-based sentiment analysis, Evaluate-for, effectiveness)(sentence embedding, Compare, word embedding)(sentence embedding, Evaluate-for, similarity)(sentence embedding, Is-a-Prerequisite-of, sentence-level tasks)(sentence embedding, Used-for, sentence similarity)(word embeddings, Compare, sentence embeddings)(word embeddings, Evaluate-for, multilingual tasks)(word
(annotated training data, is expensive to produce, hand-labeled data)(hand-labeled training data, is expensive to produce, in low coverage of event types)(annotated training data, is competitive with elaborately human-labeled data, the quality of)(annotated training data, can incorporate with human-labeled data, improve the performance of models learned from these data)(annotated training data, can be effectively trained using a novel curriculum learning based method, the transition matrix)(annotated training data, can incorporate with human-labeled data, performance of models learned from these data)(annotated training data, reduces human efforts in building training data, many classification tasks)
(relation linking, Used-for, Knowledge Base Question Answering systems)(relation linking, Is-a-Prerequisite-of, Neural models)(relation linking, Evaluate-for, AMR semantic parse)(relation linking, Compare, Heuristics)(relation linking, Used-for, Knowledge Graphs)(relation linking, Evaluate-for, DBpedia)(relation linking, Is-a-Prerequisite-of, Transformer-based neural model)
(comprehension datasets, Is-a-Prerequisite-of, natural-language understanding systems)(comprehension datasets, Evaluate-for, development of natural-language understanding systems)(comprehension datasets, Is-a-Prerequisite-of, document and query relationship)(comprehension datasets, Evaluate-for, knowing the quality of reading comprehension datasets)(comprehension datasets, Evaluate-for, deep language understanding)(comprehension datasets, Evaluate-for, creation of an RC dataset)(comprehension datasets, Is-a-Prerequisite-of, understanding of common entities and their attributes)(comprehension datasets, Is-a-Prerequisite-of, GuessTwo task)(comprehension datasets, Part-of, TriviaQA dataset)(comprehension datasets, Part-of, DuoRC dataset)(comprehension datasets, Evaluate-for, question generation for reading comprehension)(comprehension datasets, Is-a-Prerequisite-of, neural approaches in language understanding)(comprehension datasets, Evaluate-for, evaluation of story comprehension and script learning)
(None)
(professional fact checker, used-for, fact verification)(professional fact checker, outperforming, models trained on FEVER dataset)(professional fact checker, limited in scale, manually verified by professional fact checkers)
(Document summarization, Is-a-Prerequisite-of, Natural language representations)(Document summarization, Evaluate-for, Salient information extraction)(Document summarization, Evaluate-for, Logical entailment)(Document summarization, Evaluate-for, Rewrite of a summary)(Document summarization, Compare, Extractive summarization)(Document summarization, Compare, Abstractive summarization)(Document summarization, Compare, Text generation techniques)
(Adversarial purification, Adversarial detection, Defense mechanisms)  (Adversarial purification, Textual adversarial attacks, Word-substitution attacks)  (Adversarial purification, Language models, Noise injection)  
1. (self-attentive parser, Used-for, disfluency detection)2. (ELMo, Is-a-Prerequisite-of, self-attentive parser)3. (BERT, Is-a-Prerequisite-of, self-attentive parser)4. (self-attentive parser, Compare, recurrent neural models)5. (self-attentive parser, Compare, self-training)6. (self-attentive parser, Evaluate-for, state-of-the-art results)7. (Bert, Is-a-Prerequisite-of, BERT based ranking models)
(language model, Used-for, language modeling)(language modeling, Is-a-Prerequisite-of, language model)(language model, Evaluate-for, text generation)(sequence-to-sequence model, Used-for, text generation)(neural language model, Is-a-Prerequisite-of, language model)(language modeling, Evaluate-for, text generation)
(None)
(argument generation, Used-for, discourse modes)(argument generation, Evaluate-for, automatic essay scoring)(argument generation, Evaluate-for, machine-generated poems)(argument generation, Used-for, corrective REs generation)(argument generation, Part-of, Paraphrase-50M dataset)(argument generation, Compare, response generation)(argument generation, Evaluate-for, open-domain dialog systems)(argument generation, Part-of, summarization model)(argument generation, Part-of, neural machine translation)(argument generation, Evaluate-for, paraphrase generation)(argument generation, Is-a-Prerequisite-of, abstractive question answering)(argument generation, Evaluate-for, sentence-level supporting argument detection)(argument generation, Is-a-Prerequisite-of, dialog systems)(argument generation, Part-of, recurrent neural networks)(argument generation, Evaluate-for, text similarity measures)(argument generation, Is-a-Prerequisite-of, natural language understanding tasks)(argument generation, Used-for, identifying abnormalities in medical images)(argument generation, Compare, question generation)
### Extracted Concepts:1. Object naming2. Referring expression generation3. Distributional word embeddings4. Zero-shot learning5. Aspect-based sentiment analysis6. Neural word embeddings7. Neural word segmentation8. Chinese word segmentation9. Dependency parsing10. Taxonomy learning11. Bilingual word embeddings12. Network embedding13. Cross-lingual word embeddings14. Character-level models15. Mid-level text representations16. Word analogies17. Arabic word embeddings18. Knowledge graph completion### Triplets:- (Graph embedding, Used-for, Network embedding)- (Graph embedding, Evaluate-for, Arabic word embeddings)- (Graph embedding, Compare, Neural word embeddings)- (Graph embedding, Evaluate-for, Word analogies)- (Network embedding, Is-a-Prerequisite-of, Graph embedding)- (Neural word embeddings, Is-a-Prerequisite-of, Cross-ling
(conventional concepts, vital for, comprehending the leap accomplished by)(neural machine translation systems, showed, promising performance)(existing methods, extract, relations)(Baum-Welch algorithm, used to train, direct HMM)(direct HMM, applied to, rerank the n-best list)(existing Knowledge Base Population methods, lead to, sparse KBs)(Pocket Knowledge Base Population (PKBP), involves, dynamically constructing a KB)(novel Open Information Extraction methods, leverage, PKB)(Classifier, Learn, without access to any labeled examples)(learned classifiers, outperform, previous approaches)(DNNs, hard to understand)(explanation methods, effective for, explaining DNNs)(Human, generates responses, relying on semantic dependencies)(improved model, outperforms, state-of-the-art models)(neural model, helps, improving performance)(BabbleLabble, framework for training classifiers)(shared encoder, shared by, entities and neg
(vision, Generate-from, abstract story)(language, Used-for, communication)(NLP, Compare, vision)(translation, Evaluate-for, communication)(computer vision, Is-a-Prerequisite-of, visual captioning)
1. (dependency parser, Compare, sequence tagging)2. (dependency parser, Part-of, end-to-end computational argumentation mining)3. (dependency parser, Compare, dependency parsing)4. (dependency parser, Is-a-Prerequisite-of, multi-task learning setup)5. (dependency parser, Compare, transition-based parser)6. (dependency parser, Evaluate-for, State-of-the-art performance)7. (dependency parsing, Used-for, parsing sentences)
(None)
((neural machine translation, Used-for, machine reading))(layer-wise relevance propagation, Used-for, machine reading)((layer-wise relevance propagation, Evaluate-for, interpret NMT internal workings))
(contemporary language model, part-of, neural language model)(neural language model, Compare, character-level language models)(novel methodologies, Evaluate-for, poetry generation)(parsing, Hyponym-Of, language modeling)(neural language model, used-for, disfluency detection)(text feature sets, Conjunction, pre-trained textual language model)(self-attention mechanism, Is-a-Prerequisite-of, extending context size)(entity names, Is-a-Prerequisite-of, entity type information)
(concept, Example, Sentiment analysis)(Aspect based sentiment analysis, Is-a-Prerequisite-of, Predict sentiment)(Memory networks, Used-for, Predict sentiment)(Target-sensitive memory networks, Compare, Memory networks)(Bilingual Sentiment embeddings, Used-for, Predict sentiment)(Multi-sentiment-resource Enhanced Attention Network, Evaluate-for, Predict sentiment)(Open-domain targeted sentiment analysis, Compare, Aspect based sentiment analysis)(Transfer Capsule Network, Evaluate-for, Predict sentiment)(Dual cross-shared RNN framework, Is-a-Prerequisite-of, Predict sentiment)
(task sentiment classification, Is-a-Prerequisite-of, Natural Language Processing)(task sentiment classification, Evaluate-for, Sentence Classification)(Natural Language Processing, Compare, Computer Vision)(Natural Language Processing, Is-a-Prerequisite-of, Deep Learning)(Sentence Classification, Evaluate-for, Document Classification)
(News Summarization, Is-a-Prerequisite-of, Machine Translation)(News Summarization, Is-a-Prerequisite-of, Document Summarization)(News Summarization, Is-a-Prerequisite-of, Query-based Summarization)(Document Summarization, Evaluate-for, Abstractive Summarization)(Query-based Summarization, Used-for, Query Attention Model)(Extractive Summarization, Evaluate-for, Query-based Summarization)(Abstractive Summarization, Is-a-Prerequisite-of, Document Summarization)(Abstractive Summarization, Compare, Extractive Summarization)
(None)
(adversarial sample, Used-for, neural classifier)(neural machine translation models, Used-for, adversarial stability training)(NMT models, Used-for, adversarial perturbation)(natural language processing field, Used-for, neural machine translation models)(translation model, Evaluate-for, adversarial source examples)(translation model, Evaluate-for, adversarial target inputs)(adversarial source examples, Evaluate-for, translation model)(adversarial target inputs, Evaluate-for, translation model)
(None)
(machine translation nmt model, utilised-for, Neural Machine Translation)(Neural Machine Translation, Is-a-Prerequisite-of, machine translation nmt model)(Neural Machine Translation, Used-for, generating translations)(Neural Machine Translation, Part-of, end-to-end architecture)(end-to-end architecture, Used-for, maintaining state-of-the-art performance)(Neural Machine Translation, Compare, statistical machine translation)(Neural Machine Translation, Compare, traditional machine translation)(Neural Machine Translation, Compare, neural networks in NLP)(Neural Machine Translation, Conjunction, Sequence-to-Dependency Neural Machine Translation)(machine translation nmt model, Conjunction, layer-wise relevance propagation)
(multi-lingual, Used-for, semantic parsing)(multi-lingual, Used-for, language identification)(multi-lingual, Compare, monolingual)(multi-lingual, Is-a-Prerequisite-of, multilingual distributed representations)(multi-lingual, Is-a-Prerequisite-of, multilingual NLP)(multi-lingual, Used-for, multilingual translation)(multi-lingual, Evaluate-for, NER performance)
(Aspect-level sentiment classification, Is-a-Prerequisite-of, Sentiment classification)(Aspect-level sentiment classification, Evaluate-for, Sentence-level sentiment)(Aspect-level sentiment classification, Evaluate-for, Document-level data)(Aspect-level sentiment classification, Evaluate-for, Transfer learning)(Document-level data, Is-a-Prerequisite-of, Aspect-level sentiment classification)(Document-level data, Evaluate-for, Transfer learning)(Attention mechanism, Is-a-Prerequisite-of, Aspect-level sentiment classification)(Attention mechanism, Evaluate-for, Neural models)(Aspect sentiment classification, Is-a-Prerequisite-of, Document-level knowledge)(Aspect sentiment classification, Evaluate-for, Neural models)(Memory networks, Is-a-Prerequisite-of, ASC)(Memory networks, Evaluate-for, ASC)(Transfer Capsule Network, Is-a-Prerequisite-of, Aspect-level sentiment classification)(Transfer Capsule Network, Evaluate-for, Transfer learning)(Self-supervised learning, Used-for, Attention mechanism)(Self-supervised
(language generation nlg system, Compare, Opinionated Natural Language Generation)(language generation nlg system, Used-for, natural language understanding)(language generation nlg system, Evaluate-for, content correctness)(opinionated natural language generation, Used-for, generating subjective responses)(opinionated natural language generation, Part-of, ONLG architecture)(opinionated natural language generation, Compare, lexicalized grammars)(onlg architecture, Is-a-Prerequisite-of, generating subjective responses)(natural language understanding systems, Is-a-Prerequisite-of, language generation nlg system)(content correctness, Compare, quality of reading comprehension datasets)
(story ending generation, Is-a-Prerequisite-of, sentiment intensity control)(story ending generation, Part-of, natural language generation)(natural language generation, Conjunction, sentiment intensity control)
(dialogue generation, Used-for, neural knowledge diffusion)(neural knowledge diffusion, Compare, encoder-decoder dialog model)(dialogue generation, Is-a-Prerequisite-of, task-completion dialogue agent)(dialogue generation, Compare, end-to-end neural dialogue generation)(encoder-decoder dialog model, Is-a-Prerequisite-of, interpretable response generation)(dialogue generation, Evaluate-for, task success rate)(dialogue generation, Evaluate-for, reward)(dialogue generation, Evaluate-for, quality of dialogue responses)
(translation task, demonstrates, model)(translation task, demonstrates, proposed model)
(None)
(neural abstractive models, outperforms, strong baselines)(neural abstractive models, competitive with, state-of-the-art extractive methods)(document summarization research, less investigated in comparison with, text generation techniques)(neural abstractive models, worse results than, extractive methods)(the ability to code, prerequisite of, software development)(encoder-decoder models, known to handle, obstacles)(DPP system, outperforms, strong summarization baselines)(DPP system, performance in, benchmark datasets)(GOLC, generates, high evaluation scores)(GOLC, optimizes, neural summarization model)(SCRATCHPAD mechanism, improves, fluency of seq2seq models)(SCRATCHPAD mechanism, guide, future generation)(SUPERT, correlates better with, human ratings)(seq2seq network, well-established model for, text summarization task)(seq2seq network, learn to produce, readable content)(seq2seq network
### Triplets Extracted:(bias mention, is-a-Prerequisite-of, entity linking)(bias mention, is-a-Prerequisite-of, natural language processing)(entity linking, Used-for, aligning textual mentions)(natural language processing, hyponym-Of, natural language understanding)
(concept, Used-for, zero shot cross lingual)(concept, Evaluate-for, Multilingual BERT)(Multilingual BERT, Evaluate-for, zero shot cross lingual)(zero shot cross lingual, Compare, cross-lingual OpenQA)(zero shot cross lingual, Compare, cross-lingual syntactic variation)(zero shot cross lingual, Compare, cross-lingual knowledge graph alignment)(zero shot cross lingual, Compare, abstractive sentence summarization)(zero shot cross lingual, Compare, cross-lingual document retrieval performance)(zero shot cross lingual, Compare, cross-lingual lexical entailment)
(Cognitive NLP systems, Used-for, augmenting traditional text-based features)(Cognitive NLP systems, Used-for, extracting cognitive features from eye-movement data)(CNN, Is-a-Prerequisite-of, using automatically learned text and gaze features)(Sarcasm detection, Part-of, complex classification tasks)(CNN, Is-a-Prerequisite-of, using both gaze and text features for classification)(Sarcasm detection, Is-a-Prerequisite-of, multimodal sarcasm detection)(Multi-modal sarcasm detection, Used-for, identifying sarcasm in various platforms)(Multi-modal sarcasm detection, Conjunction, utilizing textual and visual information)(Multi-modal sarcasm detection, Conjunction, constructing a cross-modal graph for explicit ironic relations)(Sarcasm detection, Compare, multimodal sarcasm detection)(Multi-modal sarcasm detection, Conjunction, analyzing cross-modality contrast)(Multi-modal sarcasm detection, Part-of, natural language processing and multimedia computing)
(document level sentiment, aspect-level sentiment classification, Is-a-Prerequisite-of)(document level sentiment, sentiment preference information, Evaluate-for)(document level sentiment, cooperative graph attention networks, Used-for)(document level sentiment, ABSA, Is-a-Prerequisite-of)(document level sentiment, sentiment preference, Evaluate-for)(document level sentiment, sentiment composition, Is-a-Prerequisite-of)(document level sentiment, sentiment semantics, Is-a-Prerequisite-of)(document level sentiment, hierarchical neural network, Used-for)
(dialogue agent, helps users search Knowledge Bases (KBs) without composing complicated queries, KB-InfoBot)(dialogue agent, interacts with an external database to access real-world knowledge, goal-oriented dialogue agents)(dialogue agent, achieved by issuing a symbolic query to the KB to retrieve entries based on their attributes, established relationships)(dialogue agent, replaced symbolic queries with an induced "soft" posterior distribution over the KB, higher task success rate)(dialogue agent, integrating soft retrieval process with a reinforcement learner, higher reward in both simulations and against real users)(dialogue agent, present a fully neural end-to-end agent, application towards personalized dialogue agents)(dialogue agent, used to focus on relative information contributions of conversation partners as a key to successful communication, key perspective on dialogue)(dialogue agent, predict the success of collaborative task in English and Danish corpora of task-oriented dialogue, prediction analysis)(dialogue agent, proposes automatic dialogue evaluation as a learning problem, introduces an evaluation
(None)
(conversational agents, Compare, automatic dialogue evaluation)(automatic dialogue evaluation, Evaluate-for, human judgements)(automatic dialogue evaluation, Compare, word-overlap metrics)(automatic dialogue evaluation, Is-a-Prerequisite-of, dialogue research)(automatic dialogue evaluation, Compare, self-reported user rating)(dialogue research, Is-a-Prerequisite-of, automatic dialogue evaluation)(automatic dialogue evaluation, Used-for, rapid prototyping and testing)(dialogue systems, Part-of, automatic dialogue evaluation)(automatic dialogue evaluation, Used-for, evaluating dialogue models)(automatic dialogue evaluation, Compare, human evaluation)(automatic dialogue evaluation, Compare, human judgement)(evaluation model, Is-a-Prerequisite-of, automatic dialogue evaluation)
```(Aspect term extraction, Is-a-Prerequisite-of, Aspect-based sentiment analysis)(Aspect term extraction, Evaluate-for, Aspect term-polarity co-extraction)(Aspect term extraction, Part-of, Fine-grained opinion analysis)(Aspect term extraction, Evaluate-for, Supervised learning methods)(Aspect term extraction, Is-a-Prerequisite-of, GraphRel)(Aspect term extraction, Is-a-Prerequisite-of, Sequence labeling)(Aspect term extraction, Used-for, Document modeling)```
(concept, Evaluate-for, manual fact-checking initiatives)(concept, Is-a-Prerequisite-of, saving time on claims)(concept, Used-for, detecting previously fact-checked claims)(manual fact-checking initiatives, Used-for, saving time on claims)(manual fact-checking initiatives, Evaluate-for, effort)(saving time on claims, Is-a-Prerequisite-of, detecting previously fact-checked claims)(detecting previously fact-checked claims, Is-a-Prerequisite-of, mitigating spread of false claims)(detecting previously fact-checked claims, Is-a-Prerequisite-of, providing evidence for detection)(detecting previously fact-checked claims, Is-a-Prerequisite-of, reranking candidate fact-checking articles)(reranking candidate fact-checking articles, Is-a-Prerequisite-of, providing evidence for detection)(event information, Is-a-Prerequisite-of, ROUGE-guided Transformer)(pattern information, Is-a-Prerequisite-of, generating pattern vectors
(concept, Is-a-Prerequisite-of, sentiment lexicon)(sentiment lexicon, Part-of, linguistic resources)(sentiment lexicon, Evaluate-for, sentiment expression)(sentiment lexicon, Used-for, sentiment analysis)(concept, Used-for, sentiment analysis)(linguistic resources, Part-of, models)(linguistic resources, Evaluate-for, model performance)
(explainability method, Is-a-Prerequisite-of, machine learning models)(explainability method, Evaluate-for, interpretability of ML based question answering models)(explainability method, Compare, attention mechanism)(explainability method, Used-for, evaluating ML models)(explainability method, Is-a-Prerequisite-of, post hoc explanation methods)
(None)
(conjunction, joint entity relation extraction)(conjunction, conduct, Chinese relation extraction)(conduct, address, ambiguity)(conjunction, propose, multi-grained lattice framework)(address, model, multiple senses)(model, alleviate, polysemy ambiguity)
None.
(consecutive, Conjunction, (natural language generation, generating answer, COREQA))(model, Used-for, natural language generation))(generate, Is-a-Prerequisite-of, natural language generation)(semantic parser, Evaluate-for, generating natural language)(semantic parser, Is-a-Prerequisite-of, generating natural language)(COREQA, Compare, semantic parser)(COREQA, Evaluate-for, generating natural language)(neural language model, Used-for, generating natural language)(algorithm, Evaluate-for, generating natural language)(text similarity measure, Compare, natural language generation)(text similarity measure, Is-a-Prerequisite-of, generating natural language)(COREQA, Evaluate-for, natural language generation)
(conduct, Used-for, experiments)(conduct, Used-for, extensive experiments)(conduct, Used-for, empirical study)(conduct, Used-for, extensive experiments)(conduct, Used-for, reinforcement learning)(question answering system, Is-a-Prerequisite-of, generating natural answers)(question answering system, Is-a-Prerequisite-of, generating correct answers)(question answering system, Is-a-Prerequisite-of, generating coherent answers)(question answering system, Used-for, COREQA)(COREQA, Used-for, generating correct answers)(COREQA, Used-for, generating coherent answers)(COREQA, Used-for, natural answer generation)(COREQA, Used-for, integrating copying and retrieving mechanisms)(COREQA, Used-for, integrating copying and retrieving mechanisms)(COREQA, Is-a-Prerequisite-of, correct, coherent and natural answers)(COREQA, Evaluate-for, correct answers)(COREQA, Evaluate-for, coherent answers)(natural language sentence, Is-a-Prerequisite-of, generating natural answers)(natural
(conversational QA setting, Conjunction, answering sequences of simple but inter-related questions)(RC, Is-a-Prerequisite-of, reading comprehension)(question answering, Part-of, machine reading at scale)(question answering system, Is-a-Prerequisite-of, question representation)(open-domain question answering, Used-for, access substantial knowledge)(Generative Domain-Adaptive Nets, Evaluate-for, performance of question answering models)(Recurrent Neural Networks, Compared-to, LSTM)(question answering stage, Is-a-Prerequisite-of, final answer selection)(EviNets, Evaluate-for, factoid question answering)(open-domain question answering, Is-a-Prerequisite-of, machine reading at scale)(sentence selector system, Used-for, minimal set of sentences selection)(open-domain question answering model, Evaluate-for, robustness to adversarial inputs)
(None)
(None)
(pre trained language, Used-for, natural language understanding)(pre trained language, Part-of, semantic parser)(neural semantic parser, Is-a-Prerequisite-of, semantic parsing)(neural semantic parser, Used-for, converting natural language utterances to intermediate representations)(neural semantic parser, Evaluate-for, state-of-the-art performance on SPADES and GRAPHQUESTIONS)(neural semantic parser, Evaluate-for, competitive results on GEOQUERY and WEBQUESTIONS)(deep neural network architecture, Is-a-Prerequisite-of, supervised lemmatization)(deep neural network architecture, Used-for, context sensitive lemmatization)(supervised lemmatization, Evaluate-for, achieving state-of-the-art performance on various languages)(supervised lemmatization, Used-for, identifying the correct lemma of a surface word)(language embeddings, Is-a-Prerequisite-of, generating Arabic word embeddings)(language embeddings, Evaluate-for, intrinsic evaluation of different word embeddings)(language embeddings, Evaluate
(None)
(controllable text generation, Conjunction, emotion-controllable response generation)(controllable text generation, Evaluate-for, emotion expression)(controllable text generation, Is-a-Prerequisite-of, content consistency)(controllable text generation, Compare, sequence modeling)(controllable text generation, Compare, Conditional Text Generation)(controllable text generation, Is-a-Prerequisite-of, Pre-train and Plug-in Variational Auto-Encoder)(controllable text generation, Evaluate-for, diverse but less training effort)
(speech text translation, Used-for, trade)(speech text translation, Used-for, law)(speech text translation, Used-for, commerce)(speech text translation, Used-for, politics)(speech text translation, Used-for, literature)(speech text translation, Part-of, natural language processing)(speech text translation, Part-of, neural machine translation)
(language generation, used-for, natural language understanding)(language generation, Compare, style transfer)(style transfer, Is-a-Prerequisite-of, language transfer)(language transfer, Evaluate-for, cross-cultural understanding)(language generation, Evaluate-for, sentiment preservation)(language understanding, Compare, natural language understanding)(natural language understanding, g, language processing)
(None)
(dialogue data, Related-to, KB-InfoBot)(dialogue data, Related-to, Neural Belief Tracking (NBT))(dialogue data, Related-to, Dialogue State Tracking (DST))(dialogue data, Related-to, Response Selection)(KB-InfoBot, Used-for, dialogue data)(Neural Belief Tracking (NBT), Is-a-Prerequisite-of, Dialogue State Tracking (DST))
(None)
(constraint, Evaluate-for, dependency parsing)(experiment, Used-for, dependency parsing)(supertagging, Is-a-Prerequisite-of, dependency parsing)(SPIGOT, Evaluate-for, dependency parsing)(parsing, Is-a-Prerequisite-of, neural techniques)(semantic dependency parsing, Is-a-Prerequisite-of, semantic relation)(neural network model, Is-a-Prerequisite-of, dependency parsing)(parser, Evaluate-for, semantic dependency parsing)
(discourse relation, used-for, understanding of text quality)(discourse relation, used-for, NLP tasks)(discourse relation, Compare, event coreference)(discourse relation, Compare, causal relations)(discourse relation, Evaluate-for, joint modeling)(discourse relation, Evaluate-for, coherence measurement)(discourse relation, Used-for, discourse analysis)(discourse relation, used-for, coherence assessment)(discourse relation, Conjunction, temporal relations)(discourse relation, Conjunction, causal relations)
(conversational dialog systems, is-a-Prerequisite-of, end-to-end learning of recurrent neural networks)(information extraction, Evaluate-for, question-answering systems)(Word Sense Disambiguation, is-a-Prerequisite-of, contextual embeddings)(neural network training, Used-for, minimizing task loss)(named entity recognition, Used-for, cross-domain knowledge transfer)(cross-domain language modeling, Compare, supervised domain adaptation)(goal-oriented visual dialogue, Evaluate-for, generating questions to find an undisclosed object)(synonym detection, Is-a-Prerequisite-of, spelling error detection)
(None)
None.
("morphological rich languages", Compare, "distributional vector space models")("morphological paradigm", Is-a-Prerequisite-of, "morphological analysis")("morphological paradigm", Evaluate-for, "unsupervised morphological analysis")("morphological paradigm", Is-a-Prerequisite-of, "morphological disambiguation")("morphological paradigm", Part-of, "morphological tagging")("morphological paradigm", Is-a-Prerequisite-of, "morphological regularization")("morphological tagging", Is-a-Prerequisite-of, "full morphological tagging")("morphological tagging", Evaluate-for, "multitask learning")("morphological tagging", Evaluate-for, "adversarial training")("morphological tagging", Evaluate-for, "morphological richness")("morphological regularization", Evaluate-for, "PCFG induction")("morphological regularization", Evaluate-for, "neural PCFG inducer")
**(dialogue state tracking, is-a-Prerequisite-of, Global-Locally Self-Attentive Dialogue State Tracker (GLAD))****(dialogue state tracking, is-a-Prerequisite-of, Neural Belief Tracking (NBT))****(dialogue state tracking, is-a-Prerequisite-of, Transferable Dialogue State Generator (TRADE))****(dialogue state tracking, is-a-Prerequisite-of, Generation-Evaluation framework)****(GLAD, Used-for, estimate user goals and requests)****(GLAD, Used-for, shared parameters between estimators for different types (slots) of dialogue states)****(NBT, Evaluate-for, conversation quality maintenance)****(NBT, Hyponym-Of, update mechanism learning)****(TRADE, Used-for, dialogue state generation from utterances)****(TRADE, Used-for, zero-shot and few-shot dialogue state tracking)****(Generation-Evaluation framework, Hyponym-
(Some relation concepts extracted from the content)- (Cognitive NLP systems, Compare, Traditional text-based NLP systems)- (Multi-document summarization, Used-for, Content selection)- (Implicit discourse relation classification, Compare, Explicit discourse relation classification)- (Transition-based algorithms, Used-for, RST parsing)- (Connectives, Evaluate-for, Implicit relation network)- (Event detection, Is-a-Prerequisite-of, Argument information)- (Distant relation extraction, Evaluate-for, Class ties)- (Language models, Compare, Hierarchical LSTM model)(Note: The relationships are provided based on the context given and may not cover all possible relationships.)
#### Triplets:(Word embeddings, Are-improved-by, Non-contextual subword embeddings)(BERT, Is-a-Type-of, Contextualized word embeddings)(Deep contextualized embeddings, Represent, Fine-grained word senses)
(conjunction, example, sentiment classification) (sentence, include, sentiment lexicons) (model, propose, word embeddings) (model, achieves, state-of-the-art results) (task, focus, sentiment polarities) (task, perform, sentiment classification) (model, employ, CNN layer) (model, propose, component) (model, incorporates, mechanism)
(None)
(language model, Used-for, downstream task) (neural language model, Used-for, generating rhythmic poetry) (neural language model, Used-for, constraint satisfaction problem) (NLP systems, Used-for, applying pretrained context embeddings) (sentence embeddings, Used-for, sentence relatedness evaluation) (cross-lingual word embeddings, Used-for, supporting cross-lingual NLP) (LSTM language model, Used-for, generating personalized query completions) (bilingual attention language model (BALM), Used-for, bilingual lexicon induction)
(distributional vector space models, Emit relation, morphologically rich languages)(distributional vector space models, Used-for relation, improving accuracy of low-frequency word forms)(distributional vector space models, Evaluate-for relation, language understanding systems)(distributional vector space models, Hyponym-Of relation, distributional semantics models)(distributional vector space models, Compare relation, count-based distributional semantic models)(distributional vector space models, Compare relation, retrofitting models)(distributional vector space models, Part-of relation, distributional inference)(distributional vector space models, Is-a-Prerequisite-of relation, building a proper semantic representation)(distributional vector space models, Used-for relation, understanding sentence semantics)(distributional vector space models, Compare relation, stylometry)
(neural text generation, Utilized-for, document summarization)(neural text generation, Utilized-for, video captioning)(neural text generation, Utilized-for, dialog systems)(document summarization, Compare, video captioning)(document summarization, Evaluate-for, abstractive sentence summarization)(document summarization, Evaluate-for, abstractive document summarization)(document summarization, Is-a-Prerequisite-of, abstractive sentence summarization)(video captioning, Compare, dialog systems)(video captioning, Is-a-Prerequisite-of, video captioning)(dialog systems, Is-a-Prerequisite-of, dialog systems)(dialog systems, Utilized-for, knowledge bases)(dialog systems, Evaluate-for, multi-turn conversation)(dialog systems, Compare, neural text generation)(dialog systems, Is-a-Prerequisite-of, chatbot)(dialog systems, Is-a-Prerequisite-of, chatbot)(dialog systems, Evaluated-for, resolution of pronoun coreference)
(Aspect extraction, Is-a-Prerequisite-of, Aspect-based sentiment analysis)(Topic models, Compare, Neural word embeddings)(Topic models, Compare, Attention mechanism)(Attention mechanism, Used-for, De-emphasize irrelevant words during training)(Sarcasm, Is-a-Prerequisite-of, Sarcasm interpretation)(Sarcasm interpretation, Is-a-Prerequisite-of, Generation of non-sarcastic utterance)(Multilingual connotation frames, Is-a-Prerequisite-of, Multilingual sentiment analysis)(Memory networks, Compare, Target-sensitive memory networks)(Aspect sentiment classification, Is-a-Prerequisite-of, Sentiment analysis)(Aspect-based sentiment analysis, Is-a-Prerequisite-of, Aspect sentiment classification)(Graph convolutional autoencoder, Evaluate-for, Enriching semantics of a document)(Cross-domain sentiment classification, Compare, Domain-specific vs domain-general concepts from ConceptNet)
(None)
(word embeddings, Build, word-embedding models)(word-embedding models, Have, gained great popularity)(word-embedding models, Used-for, several tasks including word analogy questions and caption generation)(word-embedding models, Provide, point representations of words containing semantic information)(word-embedding models, Improve, coherence)(word-embedding models, Allow, words in similar contexts to be located close to each other in the embedding space)(word-embedding models, Exploit, distribution of word co-occurrences through neural word embeddings)(word-embedding models, Capture, linguistic regularities of the language)(word-embedding models, Used-for, document analysis)
`(context sensitive embeddings, Used-for, sequence labeling tasks)(context sensitive embeddings, Evaluate-for, NLP systems)(context sensitive embeddings, Part-of, deep neural network architecture)(context sensitive embeddings, Evaluate-for, PP attachment model)(context sensitive embeddings, Used-for, predicting prepositional phrase attachments)(context sensitive embeddings, Compare, word vectors)(context sensitive embeddings, Evaluate-for, sentiment classification)(context sensitive embeddings, Evaluate-for, sentiment-aware word embeddings)(context sensitive embeddings, Used-for, capturing stylistic similarity)(context sensitive embeddings, Compare, sentence embeddings)(context sensitive embeddings, Evaluate-for, sentence embeddings)(context sensitive embeddings, Part-of, a model)(context sensitive embeddings, Is-a-Prerequisite-of, sentence embeddings)(context sensitive embeddings, Used-for, sentiment polarity classification)(context sensitive embeddings, Used-for, Word Sense Disambiguation)(context sensitive embeddings, Is-a-Prerequisite-of, sentence embeddings)(context sensitive embeddings, Used-for, machine reading comprehension models)(context sensitive embeddings
(None)
(improves adversarial robustness, is-a-Prerequisite-of, model robustness)(model robustness, Compare, model performance)(CRF, Used-for, sequence models)(SeqVAT, is-a-Prerequisite-of, VAT)(SeqVAT, Compare, model performance)(text-to-SQL models, Evaluate-for, synonym substitution)(Spider-Syn, is-a-Prerequisite-of, Spider benchmark)(Spider-Syn, Evaluate-for, synonym substitution)(DIR, is-a-Prerequisite-of, model robustness)(ATINTER, Used-for, adversarial inputs)(ATINTER, Improve-for, adversarial robustness)
(None)
(lexical resources, Used-for, part-of-speech induction)(lexical resources, Used-for, named-entity recognition)(lexical resources, Evaluate-for, overfitting)(generative model, Is-a-Prerequisite-of, lexicon)(training data, Part-of, generative model)(sequence labeling framework, Evaluate-for, learning general-purpose patterns)(sequence labeling framework, Used-for, part-of-speech induction)(sequence labeling framework, Used-for, named entity recognition)(sequence labeling framework, Compare, linear-chain CRFs)(sequence labeling framework, Compare, BiLSTM)(neural models, Compare, data-driven models)(neural models, Compare, segmentation neural NER models)(NER data sets, Compare, biomedical NER data sets)(NER data sets, Compare, generic domains)(pipeline tasks, Part-of, in-formation extraction)(end-to-end neural model, Evaluate-for, joint extraction of entities and negations)(NER model, Is-a-Prerequisite-of
(None)
(None)
(neural machine translation model, based-on, bidirectional LSTMs)(neural machine translation model, based-on, convolutional layers)(neural machine translation model, outperform, several recently published results)(neural machine translation model, achieve, comparable results)(neural machine translation model, improved by, 11.7 BLEU points)(neural machine translation model, integrated, syntactic information)(neural machine translation model, generates, translations from left to right)(neural machine translation model, proposes, Sequence-to-Dependency NMT method)(neural machine translation model, corrects, global errors and local errors)(neural machine translation model, introduces, binary code prediction)(neural machine translation model, analyzes, representations)(neural machine translation model, interprets, internal workings)(neural machine translation model, uses, layer-wise relevance propagation)
(human attention, Used-for, keyphrase extraction)(human attention, Represents, human reading behavior)(keyphrase, Is-a-Prerequisite-of, extraction)(human reading behavior, Is-a-Prerequisite-of, human attention)(human reading behavior, Not-Capture, semantic meaning)(human attention, Conjunction, reading duration)(human attention, Evaluate-for, relevance of words)(reading duration, Represents, human attention)(human attention, Merge-with, neural network models)(human attention, Merge-with, unsupervised models)(human attention, Used-for, integration into models)(human attention, Used-for, significant improvements)
(None)
(None)
(category opinion sentiment, part-of, aspect-category-opinion-sentiment quadruples)(category opinion sentiment, used-for, aspect-based sentiment analysis)(aspect-category-opinion-sentiment quadruples, Is-a-Prerequisite-of, ACOS Quadruple Extraction)(ACOS Quadruple Extraction, Evaluate-for, aspect-based sentiment analysis)(ACOS Quadruple Extraction, part-of, Product reviews)(aspect-based sentiment analysis, Is-a-Prerequisite-of, full support)(Product reviews, part-of, existing studies)(existing studies, ignored, implicit aspects and implicit opinions)(aspect-based sentiment analysis, Evaluate-for, implicit aspects)(aspect-based sentiment analysis, Evaluate-for, implicit opinions)(implicit aspects, part-of, existing studies)(implicit opinions, part-of, existing studies)
(None)
(None)
(text similarity measures, used-in, plagiarism detection)(deep learning, highlights, relevance)(sequential models, exploited in, similarity measures)(n-grams, used-for, text overlap)(skip-grams overlap, used-for, text slices)(TextFlow, similar-to, DNA sequence alignment algorithms)(TextFlow, utilizes, actual position of words)(TextFlow, utilizes, sequence matching)(Sentences, are, important semantic units)(sentence representation, used-in, natural language processing tasks)(BERT-based language models, collapse, sentence representations)(ConSERT, achieves, 8% relative improvement)(ConSERT, comparable-to, SBERT-NLI)(ConSERT, outperforms, previous state-of-the-art)(pairwise representation pairs, constitute, contrastive learning methods)(ArcCSE, outperforms, previous state-of-the-art)(RCMD, computes, sentence distance)(CLRCMD, computes, effective sentence similarity)(Whitened
(None)
(constraint, "used-for", semantic role labeling)("model", "has", predicate-argument extraction)("paper", "discusses", semantic dependency correlation)("proposed method", "achieves", improvement)("SPRL systems", "perform", part of an information extraction pipeline)("rapid progress of recent neural SRL models", "challenges", the importance of syntactic information)
(chain thought prompting, Used-for, improving reasoning capabilities)(chain thought prompting, Used-for, transferring reasoning capabilities)(chain thought prompting, Is-a-Prerequisite-of, improving task performance)(chain thought prompting, Used-for, incorporating external tools)(chain thought prompting, Part-of, MultiTool-CoT framework)(chain thought prompting, Evaluate-for, improving state-of-the-art performance)
(syntactic structures, Part-of, graphs)(Penn Treebank, Evaluate-for, syntactic structures)(methods, Compare, accuracy)(methods, Conjunction, discrepancy in datasets)(TaBERT, Part-of, pretrained LM)(Language Models, Evaluate-for, unsupervised syntactic parsing)(Neural approaches, Compare, fully mathematical understanding)(sequence-to-sequence models, Compare, transition-based parsers)
(None)
(Cross-lingual transfer, Used-for, improving performance of natural language processing)  (Cross-lingual transfer, Evaluate-for, low-resource task language)  (Cross-lingual transfer, Is-a-Prerequisite-of, high-resource transfer language)  (Cross-lingual transfer, Compare, standard strategy for language transfer)  (Cross-lingual transfer, Compare, parallel texts availability)  
**(word embeddings, used-for, representing word meaning)(author context, helps-in, textual sarcasm detection)(semantic representations, introduced-in, neural language modelling)(Faithfulness property, defining, contextual embeddings)(PLM, based-on, dynamic contextualized word embeddings)**
(Automatic generation of image descriptions, Used-for, summary generation)(Style transfer, Evaluate-for, summary generation)(Automatic topic-to-essay generation, Is-a-Prerequisite-of, summary generation)(Automatic argument generation, Evaluate-for, summary generation)(Structured query generation, Is-a-Prerequisite-of, summary generation)(Sequence-to-sequence model, Is-a-Prerequisite-of, summary generation)
(text generation, Compare, abstractive summarization)(abstractive summarization, Part-of, document summarization)(text generation, Compare, document summarization)(text generation, Compare, sentence summarization)(text style, Conjunction, format)(text style, Conjunction, rhyme)(text style, Conjunction, sentence integrity)
( entity recognition , Compare , relation extraction )( relation extraction , Part-of , entity mentions)( gazetteers , Used-for , entity recognition)( neural relation extraction framework , Evaluate-for , relation extraction)( bidirectional long short-term memory (Bi-LSTM) , Is-a-Prerequisite-of , relation extraction)( named entity recognition (NER) , Compare , relation extraction)( named entities , Is-a-Prerequisite-of , relation extraction)( distant supervision , Evaluate-for , relation extraction)( graph convolutional network (GCN) , Used-for , entity relation bipartite graph)( DocRED , Is-a-Prerequisite-of , relation extraction)( neural network , Compare , relation extraction)
(dialog datasets, used-for, model)(dialog datasets, part-of, end-to-end task-oriented dialog systems)(dialog datasets, Evaluate-for, performance)
(visually grounded language, Used-for, image grounding)(visually grounded language, Evaluate-for, multi-hop reasoning)(visually grounded language, Is-a-Prerequisite-of, natural language navigation)(image grounding, Part-of, visually grounded language)(multi-hop reasoning, Is-a-Prerequisite-of, visually grounded language)(natural language navigation, Is-a-Prerequisite-of, visually grounded language)
(Feature attribution methods, help interpret, predictions)(unintended bias, mitigating, text classifiers)(unintended bias, formalize, unintended biases)(unintended bias, debiasing, training framework)(unintended bias, alleviate, impacts)(data-level manipulations, employed, previous studies)
**(continual relation extraction, Compare, relation extraction)**  **(continual relation extraction, Evaluate-for, noise reduction)**  **(relation extraction, Is-a-Prerequisite-of, continual relation extraction)**  **(relation extraction, Part-of, multi-lingual neural relation extraction framework)**  **(relation extraction, Conjunction, relation facts)**  **(relation extraction, Evaluate-for, significant improvements)**  **(relation extraction, Compare, GraphRel model)**  **(relation extraction, Part-of, state-of-the-art relation extraction approaches)**  
(neural semantic parser, used-for, natural language utterances)(neural semantic parser, trained end-to-end using, annotated logical forms)(neural semantic parser, obtains, state of the art on SPADES and GRAPHQUESTIONS)(neural semantic parser, achieves, competitive results on GEOQUERY and WEBQUESTIONS)(neural semantic parser, parses sentences into, intermediate representations)(neural semantic parser, maps to, target domains)(neural semantic parser, does not rely on, syntactic pre-parse)(neural semantic parser, uses, five recurrent neural networks)(neural semantic parser, induces, predicate-argument structures)(neural semantic parser, improves, state-of-the-art results for parsing sentences into Abstract Meaning Representations)(neural semantic parser, is trained end-to-end using, annotated logical forms)(neural semantic parser, sheds light on types of representations useful for semantic parsing)(neural semantic parser, enhances, state-of-the-art results for parsing sentences into Abstract Meaning Representations)
(abstractive summarization, generate, generated summary)(query-based summarization, based on, encode-attend-decode paradigm)(neural models, used for, Chinese poem generation)(neural model, balance, linguistic accordance and aesthetic innovation)(sequence-to-sequence model, used for, extractive summarization)(seq2seq model, generate, summaries)(template-based summarization approaches, guide, seq2seq model)(RNNs, generated, long-form text)(RNN generator, guided by, committee of discriminators)(sonnet modelling, capture, language, rhyme and meter)(essay generation, based on, topics)(data-to-text generation model, simulate, human-like writing process)(paper summarization, generated from, videos of talks at scientific conferences)(abstractive meeting summarizer, developed from, videos and audios of meeting recordings)(multi-sentence compression, aim to, generate, grammatical but reduced compression)
(level event argument extraction, Is-a-Prerequisite-of, document-level event extraction)(level event argument extraction, Is-a-Prerequisite-of, event argument extraction)(level event argument extraction, Is-a-Prerequisite-of, sentence-level event argument extraction)(sentence-level event argument extraction, Used-for, extracting event arguments)(sentence-level event argument extraction, Is-a-Prerequisite-of, event argument extraction)(document-level event extraction, Part-of, document-level event argument extraction)(document-level event extraction, Used-for, extracting event arguments)(Event Argument Extraction, Is-a-Prerequisite-of, sentence-level event argument extraction)
(Aspect extraction, Part-of, Aspect-based sentiment analysis)(Model, Compare, Aspect-based sentiment analysis)(Aspect-based sentiment analysis, Is-a-Prerequisite-of, Aspect extraction)(Sentiment analysis methods, Evaluate-for, Volatility prediction)(Aspect-based sentiment analysis, Compare, General sentiment analysis)(Aspect-based sentiment analysis, Compare, Aspect term sentiment analysis)(textual information, Used-for, Market data resources)(Aspect-based sentiment analysis, Evaluate-for, Aspect term extraction)(Dual crOss-sharEd RNN framework, Is-a-Prerequisite-of, Aspect term-polarity co-extraction)(Aspect-based sentiment analysis, Evaluate-for, Sentiment prediction)(Aspect-based sentiment analysis, Part-of, Aspect sentiment analysis)(Opinion targets, Part-of, Sentence)(Aspect term extraction, Is-a-Prerequisite-of, Aspect sentiment classification)
(neural dialogue generation, Used-for, knowledge diffusion)(neural knowledge diffusion, Is-a-Prerequisite-of, meaningful response generation)(neural knowledge diffusion, Evaluate-for, convergent and divergent thinking)(neural dialogue generation, Compare, end-to-end neural dialogue generation)(neural knowledge diffusion, Is-a-Prerequisite-of, fact matching and entity diffusion)
(entity relation extraction, Used-for, relation extraction)(relation extraction, Evaluate-for, multiple entities)(relation extraction, Evaluate-for, relation class)(relation extraction, Is-a-Prerequisite-of, entity pairs)(relation extraction, Compare, entity relation extraction)(relation extraction, Compare, relation classification)(relation extraction, Used-for, deep learning architecture)(relation extraction, Evaluate-for, model performance)(relation extraction, Is-a-Prerequisite-of, entity-relation bipartite graph)(model training, Is-a-Prerequisite-of, relation identification)(relation identification, Evaluate-for, cross-entropy loss)(relation identification, Compare, relation classification)(relation classification, Evaluate-for, ranking loss)(relation classification, Is-a-Prerequisite-of, entity types)(relation classification, Evaluate-for, F1-score)(relation classification, Is-a-Prerequisite-of, state-of-the-art models)(relation extraction, Is-a-Prerequisite-of, machine reading comprehension)(relation extraction, Compare, multi-turn QA)(multi-turn
(linguistically diverse conversational corpus, is-a-Prerequisite-of, computational linguistics)(linguistically diverse conversational corpus, is-a-Prerequisite-of, natural language understanding)(linguistically diverse conversational corpus, is-a-Prerequisite-of, conversational interfaces)(linguistically diverse conversational corpus, Used-for, Computational linguistics)(linguistically diverse conversational corpus, Used-for, Language technology)(linguistically diverse conversational corpus, Part-of, Interactional data)
(Political debates, offer rare opportunity for citizens to compare the candidates’ positions on the most controversial topics of the campaign)(German newspaper reports, based on which an annotated pilot corpus of migration claims is created)(Political debates, empirical task is addressed by annotating 39 political debates from the last 50 years of US presidential campaigns)(Political debates, create new corpus of 29k argument components labeled as premises and claims)(Argument Mining, proposed as an approach to political debates)(39 political debates from the last 50 years of US presidential campaigns, annotated for argumentative components)(USElecDeb60To16 corpus, released to the research community).
(None)
### Concept: text generation model1. (Generative Domain-Adaptive Nets, Is-a-Prerequisite-of, semi-supervised question answering)2. (Generative Domain-Adaptive Nets, Used-for, boosting the performance of question answering models)3. (abstract syntax networks, Is-a-Prerequisite-of, code generation and semantic parsing)4. (abstract syntax networks, Used-for, representing outputs as abstract syntax trees)5. (Attention-based sequence learning model, Is-a-Prerequisite-of, automatic question generation for sentences)6. (seq2seq learning, Is-a-Prerequisite-of, training a text generation model end-to-end)7. (AliMe Chat, Used-for, open-domain chatbot engine)8. (Variational autoencoders (VAEs), Is-a-Prerequisite-of, the unsupervised discrete sentence representation learning method)9. (Generative neural models, Is-a-Prerequisite-of, constituency parsing)10
(response generation, Used-for, dialog systems)(response generation, Compare, iterative training process)(response generation, Used-for, boosting method)(response generation, Used-for, sequence-to-sequence architecture)(response generation, Evaluate-for, diversity of responses)(response generation, Evaluate-for, relevance of responses)(response generation, Part-of, abstractive summarization)(response generation, Is-a-Prerequisite-of, interpretability of responses)(response generation, Compare, NeuralREG model)(response generation, Compare, Spatio-Temporal Matching network)(response generation, Used-for, multi-task learning)
1. (**neural language model**, Is-a-Prerequisite-of, **Affect-LM**)2. (**neural language model**, Is-a-Prerequisite-of, **Universal Language Model Fine-tuning (ULMFiT)**)3. (**neural language model**, Is-a-Prerequisite-of, **Taylor's law in natural language analysis**)4. (**neural language model**, Is-a-Prerequisite-of, **Joint architecture for sonnet modelling**)5. (**neural language model**, Is-a-Prerequisite-of, **RNN-based language model for Code-mixed language**)6. (**neural language model**, Is-a-Prerequisite-of, **Hierarchical multi-scale language model**)
(None)
(conduct, Used-for, experiments)(conduct, Used-for, performance)(COREQA, Used-for, generating natural answers)(COREQA, Used-for, end-to-end question answering system)(COREQA, Evaluate-for, generating natural answers)(COREQA, Evaluate-for, knowledge inquired questions)(COREQA, Is-a-Prerequisite-of, sequence-to-sequence learning)(COREQA, Is-a-Prerequisite-of, copying mechanisms)(COREQA, Is-a-Prerequisite-of, retrieving mechanisms)(COREQA, Is-a-Prerequisite-of, encoder-decoder framework)(COREQA, Is-a-Prerequisite-of, COREQA)(pointer networks, Used-for, locate positions)(pointer networks, Evaluate-for, locating positions)(pointer networks, Is-a-Prerequisite-of, answer location)(question, Part-of, question answering system)(question, Part-of, question answering)(knowledge, Is-a-Prerequisite-of, performance)(knowledge, Is-a-Prerequisite-of, out-of-v
(relationship, entailment, neural representation)(relationship, entailment, multilingual)(pretrained multilingual, part-of, NLP tasks)(pretrained multilingual, Used-for, enhancing language understanding)(ERNIE, Compare, BERT)(multilingual, Part-of, Bilingual Lexicon Induction)(multilingual, Evaluate-for, Neural Named Entity Recognition)(semantic parser, Is-a-Prerequisite-of, multilingual training)(BLI, Compare, BLISS)(language identification, Is-a-Prerequisite-of, processing multilingual text)
(translation quality, depends on, Neural Machine Translation system)(translation quality, substantially improved by, novel data augmentation approach)(Neural Machine Translation, focuses on, translation quality)(translation quality, depends on, sizable parallel corpora)(translation quality, improved by, adversarial stability training)
(concept, Used-for, relation extraction)(relation extraction, Evaluate-for, known relational facts)(relation extraction, Compare, KBQA)(relation extraction, Compare, relation detection)(relation extraction, Is-a-Prerequisite-of, knowledge base question answering)(relation detection, Evaluate-for, KB relations)(relation detection, Compare, question and relation names)(relation detection, Compare, relation names)(relation detection, Is-a-Prerequisite-of, input question)(relation detection, Used-for, detecting KB relations)(KBQA, Evaluate-for, outstanding relation detection performance)(KBQA, Evaluate-for, state-of-the-art accuracy)(KBQA, Used-for, entity linking)(KBQA, Used-for, relation detector)(entity, Part-of, user)(entity, Part-of, knowledge base)(relation detection, Hyponym-Of, relation classification)(relation detection, Compare, state-of-the-art systems)(relation detection, Is-a-Prerequisite-of, method)(relation detection, Is-a-Prerequisite-of
(event argument extraction, Is-a-Prerequisite-of, implicit event argument extraction)(event argument extraction, Used-for, document-level event extraction)(event argument extraction, Compare, ERE)(event argument extraction, Is-a-Prerequisite-of, document-level event extraction)(event argument extraction, Is-a-Prerequisite-of, sentence-level EAE)(event argument extraction, Is-a-Prerequisite-of, FewDocAE)(event argument extraction, Used-for, event relation extraction)(event argument extraction, Compare, sentence-level and document-level EAE)(event argument extraction, Is-a-Prerequisite-of, zero-shot cross-lingual event argument extraction)
(None)
(None)
(relation, concept): (Compare, recurrent neural tensor network, Recurrent Neural Networks)(Used-for, Recurrent Neural Networks, machine reading)(Hyponym-Of, Recurrent Neural Networks, Recurrent Models)
### Extracted Concepts:1. Neural Machine Translation (NMT)2. LSTM (Long Short-Term Memory)3. Convolutional Layers4. Deep Neural Networks (DNNs)5. Linear Associative Units (LAU)6. Layer-wise Relevance Propagation (LRP)7. Sequence-to-Dependency Neural Machine Translation (SD-NMT)8. Nested Attention Layers9. Multi-modal Neural Machine Translation10. End-to-End Neural Machine Translation11. Recurrent Neural Networks12. Chunk-based Decoders13. Zero-resource NMT### Triplets:(Neural Machine Translation (NMT), Relies-on, LSTM)(Neural Machine Translation (NMT), Based-on, Convolutional Layers)(Deep Neural Networks (DNNs), Enhances, Neural Machine Translation (NMT))(LSTM, Constraint-by, Temporal Dependencies)(LAUs, Reduces, Gradient Propagation Path)(L
(None)
(Word embeddings, Provide, Point representations)(Word embeddings, Contain, Semantic information)(Word embeddings, Learn, Distributions)(Multimodal word distributions, Formed from, Gaussian mixtures)(Multimodal word distributions, Include, Multiple word meanings)(Multimodal word distributions, Include, Rich uncertainty information)(Multimodal word distributions, Include, Entailment)(Distributions, Learn, Energy-based max-margin objective)(Approach, Capture, Semantic information)(Approach, Outperform, Alternatives)(Visual semantic pretraining, Compare, Geometry and semantic properties)(GPT-2, Encode, Image captions)(Visual semantic pretraining, Mitigate, Anisotropy)(CLIP, Outperform, GPT-2)(CLIP, Achieve, State of the art)(CLIP, Form, Fine-grained semantic representations)
(None)
(neural language model, applied-to, generating conversational text)(neural language model, used-for, language modeling)(neural language model, incorporates, document context)
(Entity linking, Evaluate-for, coreference)(Entity linking, Evaluate-for, name mentions)(biomedical entity linking, Compare, named entity recognition)(biomedical entity linking, Evaluate-for, BEL)(biomedical entity linking, Evaluate-for, domain-specific knowledge)(biomedical entity linking, Part-of, XL-BEL)(biomedical entity linking, Compare, neural entity linking models)
```(document level event extraction, Has-a-Prerequisite, event extraction)(document level event extraction, Evaluate-for, large scale data labeling)(document level event extraction, Used-for, automatically labeled data)(document level event extraction, Is-a-Prerequisite-of, event modeling)(document level event extraction, Part-of, document understanding)(document graph, Is-a-Prerequisite-of, document-level reasoning)(document graph, Used-for, capturing interactions)(document graph, Evaluate-for, document aggregation)(document graph, Is-a-Prerequisite-of, relational reasoning)(document graph, Compare, static graph construction)(document graph, Is-a-Prerequisite-of, multi-hop reasoning)(document-level relation extraction, Compare, sentence relation extraction)(document-level relation extraction, Evaluate-for, reasoning)(document-level relation extraction, Part-of, relation identification)(document-level relation extraction, Is-a-Prerequisite-of, entity pair relationship identification)(document-level relation extraction, Used-for, good performance)```
(sentiment classification, Explore-On-The, neural network models)(sentiment classification, Used-for, text classification tasks)(sentiment classification, Evaluate-for, sentiment polarity)(sentiment classification, Evaluate-for, sarcasm detection)(sentiment classification, Evaluate-for, argument mining)(sentiment classification, Evaluate-for, domain adaptation)(sentiment classification, Is-a-Prerequisite-of, aspect sentiment classification)(sentiement classification, Compare, Attention mechanism)(sentiement classification, Used-for, Aspect sentiment classification)
(latent topic, induced by, Latent Dirichlet Allocation)(latent topic, encoded in, latent vector representations)(latent topic, represented as, latent feature space)(latent topic, inferred by, Latent Dirichlet Allocation)(latent topic, utilized in, Topical PageRank)(latent topic, captured by, LVeGs)(latent topic, utilized for, aspect-based summarization)(latent topic, enforced on, latent document-topic vectors)(latent topic, improved by, Maximum Mean Discrepancy)
(None)
(semantic units, generated from, vocabulary)(natural answer, generated from, question)(natural answer, generated from, corresponding knowledge base)(dependency graph, made up of, sequence of words)(sequence of words, contained in, spine of the book)(noncrossing arcs, contained in, each page)(semantic graph, designed with, Maximum Subgraph algorithms)(noncrossing graphs, generated on, each page)(pages, combined into, book)(book, used for, semantic dependency parsing)
(Functional Distributional Semantics, Is-a-Prerequisite-of, compositional distributional semantics)(Word embeddings, Inherit, gender bias)(Part-of, distributional inference, compositional distributional semantics)(compositional distributional semantics, Evaluate-for, semantic relatedness)(compositional distributional semantics, Evaluate-for, entailment)(compositional distributional semantics, Used-for, prediction)(distributional semantic models, Used-for, decode patterns of brain activity)(distributional semantic models, Compare, neural semantic parser)(semantic parsers, Part-of, graph-based meaning representations)
(named entity, is-a-prerequisite-of, named entity recognition)(named entity, part-of, named entity recognition)(named entity recognition, is-a-prerequisite-of, entity linking)(named entity recognition, used-for, text processing)(entity linking, is-a-prerequisite-of, entity linking systems)(entity linking systems, evaluate-for, accuracy)(named entity, used-for, NLP applications)(named entity recognition, is-a-prerequisite-of, natural language understanding)
(task-oriented dialogue systems, play role of)(task-oriented dialogue systems, interact with, external database)(task-oriented dialogue systems, facilitated by, reinforcement learner)(user interest indication process, integrated with, reinforcement learner)(dialogue systems architecture, design, pipeline designs)(task success prediction models, based on, frequency domain representations)(tracking of dialogue state, improves with, Global-Locally Self-Attentive Dialogue State Tracker)(End-to-end task-oriented dialog systems, leverage, Memory-to-sequence model)(task-oriented dialogue system framework, designed for, automatic diagnosis)(WMM2Seq model, outperforms, state-of-the-art models)(Incremental Dialogue System, created for, robustness against unanticipated user actions)(Natural Language Inference dataset, used to improve, dialogue model consistency)(Budget-Conscious Scheduling, incorporated in, Deep Dyna-Q for task-oriented dialogue agents)(response selection models, improved by, pretraining on general-domain conversational corpora)
(spoken dialogue system, Is-a-Prerequisite-of, belief tracker)(belief tracker, Part-of, spoken dialogue system)(spoken dialogue system, Evaluate-for, Global-Locally Self-Attentive Dialogue State Tracker)(Global-Locally Self-Attentive Dialogue State Tracker, Is-a-Prerequisite-of, dialogue state tracking)(dialogue state tracking, Compare, belief tracker)(belief tracker, Is-a-Prerequisite-of, Neural Belief Tracking)(Neural Belief Tracking, Is-a-Prerequisite-of, belief tracker)(spoken dialogue system, Part-of, dialogue state tracking)(dialogue state tracking, Used-for, task-oriented dialogue systems)
(document modeling, Used-for, extractive summarization)(document modeling, Used-for, answer selection)(document modeling, Used-for, event detection)(document modeling, Compare, numerical modeling)(document modeling, Compare, question answering)(document modeling, Compare, text matching)(document modeling, Is-a-Prerequisite-of, abstractive summarization)(document modeling, Is-a-Prerequisite-of, document-level information)(document modeling, Is-a-Prerequisite-of, coreference resolution)(document modeling, Is-a-Prerequisite-of, roll-call prediction)
(utterance, has, token-level representation)(token-level representation, learned by, RoBERTa)(transformers, utilized for, dialogue contexts)(transformers, pre-trained on, language modeling tasks)(utterance, predicted by, BERT)(modeling tasks, help in, learning hierarchical representations)(models, trained with, proposed context encoding)(knowledge graphs, structured with, background context)(multi-turn dialogs, created for, pair of recommendation seeker and recommender)(pair of recommendation seeker and recommender, interact with, each other)(recommendations, made through, rich interaction behavior)(recommendation seeker, proactively approached by, recommender)(dialogue acts, neglected in, pipeline approaches)(semantic structures of multi-domain dialogue acts, preserved by, act generation module)(response generation module, dynamically attends to, different acts)(act generation module, jointly trained with, response generation module)(attention weights, restricted based on, connections in knowledge graphs)(connections in knowledge graphs, expressed through, restrictions on
### Extracted Concepts:- Multimodal sentiment analysis- Sentiment analysis- Sentiment association- Sentiment linguistic knowledge- User opinion- Sentiment transfer- Fine-grained text sentiment transfer- Hybrid Contextualized Sentiment Classifier (HCSC)- Humor recognition- Target-oriented sentiment classification- Aspect-based sentiment analysis- Event-based sentiment analysis- Dependency parsing- Neural network models- Attention mechanisms### Triplets:- (Multimodal sentiment analysis, Is-a-Prerequisite-of, Sentiment analysis)- (Sentiment association, Evaluate-for, Humor recognition)- (Sentiment linguistic knowledge, Part-of, Sentiment analysis)- (User opinion, Used-for, Sentiment transfer)- (Sentiment transfer, Evaluate-for, Fine-grained text sentiment transfer)- (Hybrid Contextualized Sentiment Classifier (HCSC), Compare, LSTM-based model)- (Aspect-based sentiment analysis, Is-a
('Dialogue State Tracking', 'Is-a-Prerequisite-of', 'Task-Oriented Dialogue Systems')('Dialogue State Tracking', 'Evaluate-for', 'User Goals Estimation')('Document Summarization', 'Is-a-Prerequisite-of', 'Abstractive Document Summarization')('Document Summarization', 'Part-Of', 'Summarization Research')('Global-Locally Self-Attentive Dialogue State Tracker', 'Used-for', 'Estimating User Goals and Requests')('Hierarchical Document Encoder', 'Part-Of', 'Document Modeling')('Multi-News', 'Part-Of', 'Multi-Document Summarization')('Summarization Evaluation Metrics', 'Evaluate-for', 'Automatic Summarization Systems')('Response Selection Models', 'Used-For', 'Task-Oriented Dialogue Tasks')
(recurrent neural, is-a-Prerequisite-of, document classification)(recurrent neural, is-a-Prerequisite-of, machine translation)(recurrent neural, is-a-Prerequisite-of, automatic question answering)(recurrent neural, is-a-Prerequisite-of, machine reading)(recurrent neural, part-of, hybrid code networks)(recurrent neural, part-of, hard attention mechanism)(recurrent neural, Compare, traditional models)(recurrent neural, Compare, diverse models)
(None)
(level language modeling, Used-for, modeling sentences)(level language modeling, Is-a-Prerequisite-of, sentence level)(level language modeling, Part-of, generative neural language model)(level language modeling, Part-of, LSTM model)(LSTM model, Is-a-Prerequisite-of, level language modeling)(neural language model, Used-for, generating coherent poetry)(RNN model, Evaluate-for, language modeling)(language modeling, Compare, code-switched text)(document context, Used-for, language modeling)(character-level language models, Part-of, hierarchical LSTM language model)(code-switched text, Evaluate-for, language labeling)
(Probabilistic FastText, Used-for, capture multiple word senses)(Probabilistic FastText, Compare, state-of-art performance)(Probabilistic FastText, Evaluate-for, English RareWord datasets)(Probabilistic FastText, Evaluate-for, foreign language datasets)(Probabilistic FastText, Compare, FastText)(Probabilistic FastText, Compare, dictionary-level probabilistic embeddings)(Words, Is-a-Prerequisite-of, Probabilistic FastText)(Probabilistic FastText, Compare, state-of-art performance)(character representations, Compare, true morphological analyses)(Probabilistic FastText, Evaluate-for, state-of-art performance)(character trigram representations, Compare, most others)(word representations, Evaluate-for, simple geometry of sentences)(word representations, Evaluate-for, average 10.23 words)(word representations, Evaluate-for, low-rank subspace)(word representations, Evaluate-for, rank 4)(word representations, Compare, skip-thought vectors)(word representations
(concept, Contains Relation, implicit connectives)(concept, Evaluation Relaiton, adversarial training)(textual adversarial, Is-a-Prerequisite-of, text adversarial attack)(adversarial training, Used-for, improving model robustness)(text adversarial attack, Evaluate-for, grammatical correctness)(adversarial training, Evaluate-for, model performance)
(Adversarial multi-task learning framework, Used-for, alleviating the shared and private latent feature spaces from interfering with each other)(Neural techniques, Evaluate-for, end-to-end computational argumentation mining)(Dependency parsing, Is-a-Prerequisite-of, token-based sequence tagging problem)(CNN, Used-for, sentiment polarity and sarcasm detection)(CNN, Used-for, classify the input text)(Feature imitation, Evaluate-for, accurate classification)(Joint decoding algorithm, Used-for, alignment between acoustic frames and recognized symbols)(SoPa, Compare, being particularly useful in small data settings)(Text Deconvolution Saliency, Compare, visualizing linguistic information detected by a CNN for text classification)
(neural network, Is-a-Prerequisite-of, interpretability)(Bayesian Network, Is-a-Prerequisite-of, interpretability)(neural network, Used-for, building)(Bayesian Network, Used-for, interpretability)
(conjunction, Represents a logical or semantic relationship, mono-lingual data)(relation extraction, Is-a-Prerequisite-of, distant supervision)(neural relation extraction framework, Used-for, utilize the information within mono-lingual texts)(relation extraction, Is-a-Prerequisite-of, distant supervision)(distant supervision, Evaluate-for, reducing human efforts in building training data)(distant supervision, Evaluate-for, introduces noise to the generated training data)(distant supervision, Is-a-Prerequisite-of, relation extraction)(distant supervision, Is-a-Prerequisite-of, labeling data for relation extraction)(relation extraction, Is-a-Prerequisite-of, labeling data for relation extraction)(relation extraction, Evaluate-for, modeling class ties)(relation extraction, Evaluate-for, class ties)(relation extraction, Evaluate-for, learning class ties)(relation extraction, Evaluate-for, extraction results)(relation extraction, Evaluate-for, effectiveness of the model)(relation extraction, Evaluate-for
(long short term memory, Is-a-Prerequisite-of, affective information)(long short term memory, Used-for, sentiment classification)(long short term memory, Evaluate-for, sequence prediction)(long short term memory, Used-for, machine translation)(long short term memory, Hyponym-Of, neural network model)(long short term memory, Hyponym-Of, LSTM language model)(long short term memory, Part-of, Tree Long Short-Term Memory Networks)(long short term memory, Part-of, syntax-infused variational autoencoder)(long short term memory, Evaluate-for, syntactic evaluation)(long short term memory, Evaluate-for, language model performance)
(event commonsense evaluation, empowered-by, CSKBs)(event commonsense evaluation, extracts, event-relation tuples)(event-commonsense evaluation, focuses-on, events and their relations)
(None)
(concept, Evaluate-for, cross-cultural differences)(cross-cultural differences, Part-of, research in social media)(concept, Evaluate-for, neural multimodal approaches)(neural multimodal approaches, Used-for, Visual Question Answering)(concept, Is-a-Prerequisite-of, catastrophic forgetting)(catastrophic forgetting, Compare, task difficulty)(concept, Is-a-Prerequisite-of, structured prediction problems)(structured prediction problems, Used-for, cross-lingual transfer learning)(cross-lingual transfer learning, Is-a-Prerequisite-of, cross-lingual transfer)(concept, Evaluate-for, cross-lingual transfer)(concept, Evaluate-for, cross-lingual natural language inference)(concept, Conjunction, prompt-learning based framework)(cross-lingual natural language inference, Is-a-Prerequisite-of, fine-grained entity typing)(concept, Is-a-Prerequisite-of, fine-grained entity typing)(fine-gr
(lexical sememe prediction, consists-of, linguistic common-sense knowledge bases)(lexical sememe prediction, relies-on, external context of words)(lexical sememe prediction, fails-for, low-frequency and out-of-vocabulary words)(lexical sememe prediction, proposed-by, novel framework)(novel framework, takes-advantage-of, internal character information)(novel framework, takes-advantage-of, external context information of words)(lexical sememe prediction, experimented-on, HowNet)(HowNet, is, Chinese sememe knowledge base)(novel framework, outperforms, state-of-the-art baselines)(novel framework, maintains, robust performance even for low-frequency words)
(specifically leverage, relation, model)(relation, Evaluate-for, extraction)(model, Used-for, extraction)(Evaluate-for, consider, information)(extraction, Is-a-Prerequisite-of, classification)(entity, Is-a-Prerequisite-of, extraction)(methods, Evaluate-for, performance)
(None)
(pretrained language model, part-of, language representation)(pretrained language model, compare, BERT)(pretrained language model, compare, ERNIE)(pretrained language model, compare, KT-NET)(pretrained language model, evaluate-for, machine reading comprehension)(pretrained language model, used-for, machine reading comprehension)(pretrained language model, part-of, natural language processing)(BERT, is-a-prerequisite-of, KT-NET)(ERNIE, is-a-prerequisite-of, KT-NET)
(Adversarial learning framework, Used-for, why-question answering)(adversarially trained, Used-for, neural response generation)(adversarially trained, Is-a-Prerequisite-of, robustness of neural networks against adversarial input perturbations)
(conversational QA tasks, Compare, factoid question answering)(RC problem on Stanford Question Answering Dataset, Evaluate-for, factoid question answering)(Generative Domain-Adaptive Nets, Used-for, factoid question answering)(EviNets, Evaluate-for, factoid question answering)(Universal Schema, Is-a-Prerequisite-of, factoid question answering)(transfer learning technique, Compare, factoid question answering)(end-to-end neural dialogue generation, Used-for, factoid question answering)(simple yet robust stochastic answer network (SAN), Compare, factoid question answering)(hierarchical attention network for reading comprehension-style QA, Evaluate-for, factoid question answering)
(Named Entity Disambiguation, Is-a-Prerequisite-of, Word Sense Disambiguation)(Knowledge Base Enrichment, Evaluate-for, Named Entity Disambiguation)(GraphConvolutionalNetworks, Used-for, Relation Extraction)(Unsupervised Machine Translation, Evaluate-for, Named Entity Disambiguation)(Streaming CDC, Compare, Named Entity Disambiguation)(Machine Translation, Evaluate-for, Word Sense Disambiguation)
(distributional vector space models, emphasize, properties)(distributional vector space models, Insensitivity-to, distinct lexical relations)(morph-fitting procedure, improves, low-frequency word estimates)(morph-fitting procedure, boosts, semantic quality)(morph-fitted vectors, improve, dialogue state tracking)(referential word meaning models, link, visual to lexical information)(referential word meaning models, assume, given through distributional word embeddings)(referential word meaning models, predict, success of zero-shot learning)(aspect-based sentiment analysis, apply, variants of topic models)(neural approach, improve, coherence of aspects)(neural approach, exploit, distribution of word co-occurrences)(neural approach, use, attention mechanism)(neural approach, de-emphasize, irrelevant words)(domain adaptation, treat, flawed training data as source domain)(domain adaptation, result in, poor performance on test data)(language models, fail to account for, frequent creation and reuse of new word types)(cache mechanism
(learning, Used-for, language modeling)(language modeling, Evaluate-for, neural architecture)(language modeling, Compare, semantic parsing)(language modeling, Evaluate-for, downstream tasks)
(Entity Recognition, part-of, Named Entity Recognition)(Entity Recognition, Evaluate-for, State-of-the-art performance)(Named Entity Recognition, Evaluate-for, State-of-the-art performance)(Named Entity Recognition, Used-for, Detection of entities in natural language text)(Named Entity Recognition, Compare, Sequence labeling method)(Named Entity Recognition, Used-for, Multilingual learning)(Named Entity Recognition, Evaluate-for, Emotion recognition in conversations)
1. (neural machine translation, relies on, bi-directional LSTMs)2. (neural machine translation, achieves, competitive accuracy to the state-of-the-art on WMT'16 English-Romanian translation)3. (neural machine translation, outperforms, several recently published results on WMT'15 English-German)4. (neural machine translation, obtains, almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation)5. (neural machine translation, improves, state-of-the-art with its capability in modeling complex functions and capturing complex linguistic structures)6. (neural machine translation, suffers from, severe gradient diffusion due to non-linear recurrent activations)7. (neural machine translation, incorporates, linear associative units (LAU) to reduce gradient propagation path inside the recurrent unit)8. (neural machine translation, shows, that model with proper configuration can improve by 11.7 BLE
(concept: automatic fake news, Evaluate-for, fake news detection)(concept: automatic fake news, Used-for, fact-checking research)(concept: fake news detection, Used-for, combating fake news)(concept: fake news detection, Part-of, LIAR dataset)(concept: fake news detection, Part-of, methods for automatic fake news detection)(concept: LIAR dataset, Is-a-Prerequisite-of, fact-checking research)(concept: methods for automatic fake news detection, Evaluate-for, fake news detection)(concept: social impacts, Evaluate-for, fake news detection)(concept: bias, Evaluate-for, automatic hate speech detection)(concept: dialect and race priming, Used-for, reducing racial bias in annotation)(concept: dialect and race priming, Used-for, reducing bias in automatic hate speech detection)(concept: automatic news timeline summarization, Evaluate-for, news timeline summarization)(concept: Computational Social Sciences (CSS),
(Aspect Extraction, Is-a-Prerequisite-of, Neural Network Models)(Aspect Extraction, Is-a-Prerequisite-of, Cognitive NLP Systems)(Neural Network Models, Used-for, Aspect Extraction)(Neural Network Models, Is-a-Prerequisite-of, Joint Extraction of Entities and Relations)(Neural Network Models, Is-a-Prerequisite-of, Conditional Random Fields)(Cognitive NLP Systems, Provided-by, Behavioral Data)(Cognitive NLP Systems, Evaluate-for, NLP Systems)(Joint Extraction of Entities and Relations, Compare, Feature-Based Joint Model)(Joint Extraction of Entities and Relations, Compare, Pipelined Learning Methods)(Joint Extraction of Entities and Relations, Evaluate-for, End-to-End Model)(Pipelined Learning Methods, Compare, Joint Based Methods)(Conditional Random Fields, Evaluate-for, Aspect Extraction)(Conditional Random Fields, Evaluate-for, Lifelong Learning)(Lifelong Learning, Is-a-Prerequisite-of,
(generated text, part-of, Sequence-to-sequence models)(generated text, Evaluate-for, Semantic Relevance Based neural model)(generated text, Is-a-Prerequisite-of, model generations)(generated text, Used-for, language generation)(generated text, Compare, human-generated text)
(neural language model, Used-for, generate rhythmic poetry)(neural language model, Evaluate-for, written by humans)(neural language model, Compare, Affect-LM)(neural language model, Evaluate-for, improve language model prediction)(neural language model, Evaluate-for, disfluency detection)(neural language model, Evaluate-for, detect changes in source code)(neural language model, Evaluate-for, disfluency detection)(neural language model, Evaluate-for, perplexity experiments)(neural language model, Evaluate-for, disfluency detection)(neural language model, Evaluate-for, improve state-of-the-art in disfluency detection)(neural language model, Evaluate-for, LSTM Noisy Channel Model)
(concept, Evaluate-for, sentiment analysis)(concept, Used-for, bilingual tasks)(bilingual tasks, Is-a-Prerequisite-of, domain adaptation)(concept, Compare, monolingual embeddings)
(concept, Is-a-Prerequisite-of, multi-lingual data)(concept, Evaluate-for, relation extraction)(concept, Used-for, cross-lingual attention)
(Dialogue summarization, Used-for, query-based summarization)(Dialogue summarization, Is-a-Prerequisite-of, query attention model)(Dialogue summarization, Is-a-Prerequisite-of, diversity based attention model)Abstractive summarization, Compare, Dialogue summarizationAbstractive summarization, Part-of, Extractive summarizationAbstractive summarization, Evaluate-for, document summarizationDialogue summarization, Conjunction, query-based summarizationDialogue summarization, Compare, Abstractive summarization
(pretrained multilingual model, Used-for, natural language processing)(pretrained multilingual model, Compare, multilingual model)(natural language processing, Used-for, pretrained multilingual model)(natural language processing, Compare, multilingual model)
(entity recognition multiple, Used-for, Gazetteers)(entity recognition multiple, Compare, gazetteers)(entity recognition multiple, Evaluate-for, machine learning)(entity recognition multiple, Is-a-Prerequisite-of, supervised learning)(gazetteers, Part-of, Lexical resources)(machine learning, Is-a-Prerequisite-of, named entity recognition)(machine learning, Evaluate-for, overfitting)(neural network, Used-for, multilingual learning)(neural network, Evaluate-for, named entity recognition)(Named Entity Recognition, Is-a-Prerequisite-of, Information Extraction)(Named Entity Recognition, Evaluate-for, multilingual learning)
(neural text classifier, used-for, text classification)(neural text classifier, used-for, text simplification)(neural text classifier, Evaluate-for, make improvements)(neural text classifier, used-for, prediction)(neural text classifier, Evaluate-for, model interpretability)
(coherent paragraph summary, Compare, academic documents)(coherent paragraph summary, Compare, existing datasets)(coherent paragraph summary, Evaluate-for, human-evaluation)(coherent paragraph summary, Evaluate-for, readability)(coherent paragraph summary, Evaluate-for, world-knowledge)(coherent paragraph summary, Conjunction, how-to articles)(existing datasets, Compare, our dataset)(existing datasets, Evaluate-for, human-evaluation)(existing datasets, Evaluate-for, effectiveness)(human-evaluation, Evaluate-for, findings)(Speech pre-training, Evaluate-for, classification tasks)(Speech pre-training, Evaluate-for, speech generation)(GSLM, Compare, pGSLM)(GSLM, Evaluate-for, prosody leverage)(pGSLM, Evaluate-for, prosody utilization)(pGSLM, Evaluate-for, content modeling)(pGSLM, Evaluate-for, speech generation)
(None)
(multi-modal datasets, include, CMU-MOSEI)(multi-modal datasets, used-for, sentiment analysis)(CMU-MOSEI, based-on, text and image)(CMU-MOSEI, part-of, dataset)(multi-modal datasets, enable, exploitation of modalities)(multi-modal datasets, used-for, emotion recognition)(multi-modal datasets, introduce, CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI))(multi-modal datasets, part-of, human multimodal language)(multi-modal datasets, composed-of, short captions and images)(multi-modal datasets, part-of, multimodal social media posts)(multi-modal datasets, address, challenges in natural language understanding)(multi-modal datasets, used-for, named entity disambiguation)(multi-modal datasets, contribute to, multimodal research)(multi-modal datasets, used-for, knowledge exploration)
(topic discovery, Used-for, natural language processing tasks)(topic discovery, Is-a-Prerequisite-of, document aggregation)(document aggregation, Is-a-Prerequisite-of, topic discovery)(topic discovery, Is-a-Prerequisite-of, keyphrase generation)(keyphrase generation, Compare, keyphrase overlap)(topic discovery, Used-for, uncovering supporting evidence)(topic discovery, Used-for, expanding intent classes)
(neural model, Used-for, semantic parsing)(neural model, Compare, LSTM language model)(neural model, Is-a-Prerequisite-of, Affect-LM)(neural model, Evaluate-for, market comments generation)(neural model, Hyponym-Of, memory augmented neural model)
(deep syntactic, Used-for, SRL performance)(deep syntactic, Part-of, abstract meaning representation)(Sequence-to-Dependency Neural Machine Translation (SD-NMT), Compare, typical Neural Machine Translation (NMT) model)(SRL performance, Evaluate-for, syntactic information)(Syntactic information, Is-a-Prerequisite-of, SRL performance)(SRL performance, Evaluate-for, Syntax-aware System)(Syntax-aware System, Is-a-Prerequisite-of, Human Evaluation)
(concept, Compare, traditional word representations)(word embeddings, Evaluate-for, semantic information)(word embeddings, Part-of, Neural network architectures)(baselines, Compare, word embeddings)(word mappings, Used-for, word embeddings)
(implicit opinions, Evaluate-for, aspect implicit opinion)(Aspect-Category-Opinion-Sentiment Quadruple Extraction, Conjunction, implicit aspects)(Aspect-Category-Opinion-Sentiment Quadruple Extraction, Evaluate-for, Aspect-Category-Opinion-Sentiment Quadruple Extraction)(Aspect-Category-Opinion-Sentiment Quadruple Extraction, Part-of, opinion)(implicit aspects, Is-a-Prerequisite-of, Aspect-Category-Opinion-Sentiment Quadruple Extraction)
(Dialogue Pre-training, Used-for, Neural Model with dynamic knowledge graph embeddings)(Structured knowledge and unstructured language, Is-a-Prerequisite-of, Neural Model with dynamic knowledge graph embeddings)(Dynamic knowledge graph embeddings, Evaluate-for, Achieving the goal)(Global-Locally Self-Attentive Dialogue State Tracker (GLAD), Is-a-Prerequisite-of, Detecting user goals and requests)(Dialogue state tracking, Is-a-Prerequisite-of, Task-oriented dialogue systems)(Zero-shot transfer learning for multi-domain dialogue state tracking, Compare, New zero-shot transfer learning technique)(Multi-domain dialogue state tracking, Evaluate-for, Predicting dialogue states)(DST-SC model, Used-for, Explicitly considering slot correlations)(Dual Slot Selector, Is-a-Prerequisite-of, Devising Two-stage DSS-DST for improved dialogue state tracking)(Knowledge Enhanced Multimodal BART (KM-BART), Is-a-Prerequisite-of, Reasoning
(self-attention mechanisms, make progress in various sequence learning tasks)(make progress in various sequence learning tasks, utilize self-attention mechanisms)(utilize self-attention mechanisms, aim to improve the efficiency of training an NMT)(efficiently train NMT, introduce a novel norm-based curriculum learning method)(introduce a novel norm-based curriculum learning method, use the norm of a word embedding as a measure)(increase training efficiency, depend on LSTM-styled gating units to replenish internal semantic importance)(depend on LSTM-styled gating units, lead to a clear margin of convergence speed)(linguistic code-switching, be an understudied phenomenon in NLP)(understudied phenomenon in NLP, receive little attention)(little attention, focus mostly on monolingual and multilingual scenarios)
(language model like bert, Compare, recurrent neural networks)(language model like bert, Used-for, natural language processing)(language model like bert, Used-for, text generation)(language model like bert, Used-for, transfer learning)(language model like bert, Evaluate-for, perplexity measurement)(language model like bert, Evaluate-for, accuracy improvement)(language model like bert, Evaluate-for, disfluency detection)(language model like bert, Is-a-Prerequisite-of, sentiment classification model)(language model like bert, Is-a-Prerequisite-of, named entity recognition)(language model like bert, Is-a-Prerequisite-of, sequence labeling)(recurrent neural networks, Hyponym-Of, language model like bert)(transfer learning, Conjunction, fine-tuning)(transfer learning, Compare, supervised learning)
(automatic fake news detection, is-a-Prerequisite-of, statistical approaches)(automatic fake news detection, is-a-Prerequisite-of, LIAR)(automatic fake news detection, Used-for, fact-checking research)(automatic fake news detection, Evaluate-for, surface-level linguistic patterns)(automatic fake news detection, Compare, conventional neural network)(automatic fake news detection, Used-for, style analysis)(automatic fake news detection, is-a-Prerequisite-of, multi-modal fake news detection)(linguistic patterns, is-a-Prerequisite-of, surface-level linguistic patterns)(LIAR, Conjunction, dataset)(conventional neural network, Compare, text-only deep learning model)(style analysis, Compare, hyperpartisan news)(style analysis, Compare, fake news)(style analysis, Compare, stylometry)(hyperpartisan news, is-a-Prerequisite-of, fake news)(mainstream news, Compare, satire)(fact-checking models, is-a
(concepts, Evaluate-for, named entity recognition ner)(approach, Evaluate-for, named entity recognition ner)(NER models, Is-a-Prerequisite-of, named entity recognition ner)(NER, Used-for, multiple entities)(NER, Part-of, NLP)(NER, Is-a-Prerequisite-of, training data)(NER, Compare, sequence labeling)(NER, Compare, part-of-speech induction)(NER, Is-a-Prerequisite-of, supervised machine learning models)(NER, Is-a-Prerequisite-of, named entity dictionaries)(NER, Used-for, neural network)(NER models, Compare, machine learning-based systems)
(concept: parsing idea treebank embedding, relation: Hyponym-Of, concept: parsing idea)  (concept: treebank embedding, relation: Is-a-Prerequisite-of, concept: parsing idea)  (concept: treebank embedding, relation: Evaluate-for, concept: training data)  (concept: fine-tuning, relation: Compare, concept: treebank embeddings)  (concept: treebank embeddings, relation: Used-for, concept: training a monolingual dependency parser)  
(supporting evidence, Is-a-Prerequisite-of, fact checking model)(fact checking model, Evaluate-for, claim veracity)(claim veracity, Is-a-Prerequisite-of, fact checking system)(veracity prediction, Is-a-Prerequisite-of, fact checking system)(state-of-the-art system, Is-a-Prerequisite-of, fact checking system)
(None)
(language technology, Used-for, promoting multilingualism and linguistic diversity)(language technology, Evaluate-for, develop and improve NLP systems)(language technology, Conjunction, natural language processing)(language technology, Is-a-Prerequisite-of, building language models)(language technology, Part-of, computational linguistics)(language technology, Conjunction, machine translation)(language technology, Evaluate-for, expanding language diversity in NLP systems)(language technology, Used-for, developing automated syntactic parsing tools)(language technology, Is-a-Prerequisite-of, accurate language recognition)
(None)
(visual information, Used-for, text generation)(text, Evaluate-for, emotional content)(neural language model, Compare, traditional training methods)(neural language model, Is-a-Prerequisite-of, generative poetry)(language model, Used-for, natural language generation)(neural language model, Is-a-Prerequisite-of, Affect-LM)(LSTM, Compare, hierarchical LSTM)(neural language model, Compare, Universal Language Model Fine-tuning)(neural language model, Compare, DuoRC dataset)
(neural network, Compare, kernel methods)(neural network, Compare, deep neural networks)(neural network, Conjunction, RNNs)(neural network, Used-for, multi-task learning)(RNNs, Is-a-Prerequisite-of, dialog systems)
(concept, Part-of, Cross-domain sentiment analysis)(concept, Is-a-Prerequisite-of, Domain adaptability)(concept, Is-a-Prerequisite-of, Sentiment classification)(concept, Evaluate-for, Domain gap)(concept, Evaluate-for, Domain awareness)(concept, Evaluate-for, Domain discrepancy)(concept, Evaluate-for, Domain-invariant representation)(concept, Compare, External commonsense knowledge)(concept, Compare, Syntactic information)(concept, Compare, Pre-training language model BERT)(concept, Compare, ConceptNet knowledge graph)(concept, Compare, Prompt tuning)(concept, Compare, Adversarial Soft Prompt Tuning method)(concept, Used-for, Sentiment prediction)(concept, Used-for, Model enhancement)(concept, Used-for, Self-supervised learning)(concept, Hyponym-Of, Transferable information)(concept, Hyponym-Of, External knowledge)(concept, Hy
(identifiability, Is-a-Prerequisite-of, attention)(attention, Evaluate-for, interpretability)(identifiability, Explain-by, theoretical justifications)(attention, Used-for, predictions)(attention, Is-a-Prerequisite-of, model performance)(attention, Compare, interpretability)(attention, Evaluate-for, trustworthiness)(attention, Evaluate-for, model's predictions)(attention, Evaluate-for, explanations)(attention, Used-for, Transformer’s predictions)(attention, Is-a-Prerequisite-of, trustworthiness)
(dialogue model, part-of, neural model)(neural model, used-for, response generation)(dialogue model, Compare, generative models)(neural model, Evaluate-for, language model perplexity)(dialogue model, Evaluate-for, response generation)(dialogue model, Compare, recurrent models)(dialogue model, Is-a-Prerequisite-of, generative responses)(dialogue model, Used-for, interpretable response generation)
(conversational text, Used-for, affective information)(Affect-LM, Is-a-Prerequisite-of, conversational text)(Affect-LM, Evaluate-for, generation of emotionally colored sentences)(LSTM, Is-a-Prerequisite-of, Affect-LM)(ULMFiT, Is-a-Prerequisite-of, language model fine-tuning)(language model fine-tuning, Compare, training from scratch)(language model, Evaluate-for, syntactic and structural collocation learning)(LSTM LM, Is-a-Prerequisite-of, disfluency detection)(Noisy Channel Model, Evaluate-for, disfluency detection)(Noisy Channel Model, Is-a-Prerequisite-of, LSTM LM)
(Word-embedding models, Have-gained-popularity-on, Several tasks)(Word-embedding models, Exhibit, Compositionality)(Additive compositionality, Holds-in, Strict sense)(Additive compositionality, Explains, Success-of-vector-calculus)(Skip-Gram model, Related-to, Theoretical justification)(Skip-Gram model, Connected-to, Sufficient Dimensionality Reduction framework)(Learn bilingual word embeddings methods, Rely-on, Large parallel corpora)(Bilingual resources, Needed-for, Most methods-to-learn bilingual word embeddings)(Word embeddings, Used-in, Neural word segmentation research)(Word embeddings, Capture, Linguistic regularities)(Energy-based max-margin objective, Used-for, Learning multimodal word distributions)(Bag-of-words approaches, Compared-with, Word embeddings)(Network embedding models, Important-for, Network analysis)(Context-Aware Network Embedding (CANE), Expected-to-model, Semantic relationships)(Types of embeddings, Used
(language understanding, involves, neural networks)(language understanding, involves, symbolic reasoning)(language understanding, is-a-Prerequisite-of, semantic parsing)(language understanding, is-a-Prerequisite-of, reading comprehension)(language understanding, Used-for, understanding spoken dialogue systems)(language understanding, Used-for, sentiment analysis)(language understanding, Used-for, action recognition)(language understanding, Used-for, document modeling)
(baseline models, Compare, REAT method)(ReCoSa model, Compare, baseline models)(ReCoSa model, Evaluate-for, metric-based and human evaluations)(different ways, Compare, the vanilla Seq2Seq model)(parallel Seq2Seq network, Evaluate-for, overlapping phrase problem)(parallel Seq2Seq network, Evaluate-for, linguistic constraints of keyphrase)(parallel Seq2Seq network, Evaluate-for, the basic Seq2Seq network)(source side, Part-of, linguistic constraints of keyphrase)
(Unsupervised machine translation, Used-for, Unsupervised bilingual word embeddings)(Unsupervised machine translation, Evaluate-for, Unsupervised bilingual word embeddings)(Unsupervised machine translation, Evaluate-for, Bilingual lexicon induction)(Unsupervised machine translation, Evaluate-for, Unsupervised neural machine translation)(Unsupervised machine translation, Is-a-Prerequisite-of, Unsupervised bilingual lexicon induction)(Unsupervised machine translation, Is-a-Prerequisite-of, Cross-lingual word embeddings)
(Aspect-based sentiment analysis, Is-a-Prerequisite-of, Sentiment analysis)(Semantic axes, Is-a-Prerequisite-of, Capture nuanced semantic representations)(Attention mechanisms, Evaluate-for, Detecting sentiment context)(Transfer knowledge, Is-a-Prerequisite-of, Improve performance)(Aspect term extraction, Part-of, Aspect-based sentiment analysis)(Aspect sentiment classifier, Part-of, Aspect-based sentiment analysis)(Self-attention mechanism, Used-for, Syntactical learning)(Syntax information, Is-a-Prerequisite-of, Aspect extraction)(Aspect-oriented dependency tree structure, Is-a-Prerequisite-of, Sentiment prediction)
(Bias, affects, predictions)(Bias, influences, analysis)(Bias, introduces, techniques)(Bias, reduces, quality)(Bias, involves, gender bias)(NLP, addresses, effect)(NLP, focuses, bias symptoms)(NLP, summarizes, literature)(NLP, proposes, framework)(NLP, analyzes, systems)(Data, exhibits, gender bias)(Data, augmentation, protocol)
(None)
(embedding learning by concept induction, part-of, multilingual model)(multilingual model, Is-a-Prerequisite-of, multilingual learning)(multilingual model, Used-for, multilingual distributed representations)(multi-lingual embeddings, part-of, multilingual model)(multilingual model, Compare, neural machine translation models)(representor, Is-a-Prerequisite-of, multilingual model)(multilingual translation, Evaluate-for, low-resource settings)
```(generation semantic parsing, Used-for, code generation)(code generation, Is-a-Prerequisite-of, generation semantic parsing)(generation semantic parsing, Evaluate-for, structured logical forms)(structured logical forms, Evaluate-for, generation semantic parsing)```  
(None)
(chinese word segmentation, Used-for, improve the performance)(chinese word segmentation, Part-of, natural language processing)(chinese word segmentation, Is-a-Prerequisite-of, Chinese Natural Language Processing)(chinese word segmentation, Evaluate-for, neural models)(chinese word segmentation, Is-a-Prerequisite-of, deep learning-based Chinese Natural Language Processing)(chinese word segmentation, Compare, char-based models)(chinese word segmentation, Compare, word-based models)
(None)
(semantic representation, Related-to, linguistic representation)(semantic representation, Used-for, understanding language)(document context, Contains, linguistic representation)(neural models, Used-in, learning semantic representations)(AMR, Is-a-Prerequisite-of, linguistic representation)
(argument extraction, Used-for, relation extraction)(relation extraction, Compare, natural language understanding)(relation extraction, Is-a-Prerequisite-of, cognitive NLP systems)(argument extraction, Compare, semantic role labeling)(relation extraction, Is-a-Prerequisite-of, aspect-based sentiment analysis)(argument extraction, Evaluate-for, sentiment analysis)(argument extraction, Evaluate-for, sarcasm detection)
(orthography, part-of, Semitic languages)(orthography, part-of, dialectal content)(orthography, part-of, noise)(morphological feature, lexicalized, Semitic languages)(lexicallyzed, joint modeling, morphological patterns)(morphological feature, identify, patterns)(lexicalized, disambiguate, lexical choices)(lexicalized, make difficult, joint modeling)
```(Unsupervised semantic parsing, Is-a-Prerequisite-of, Semantic parsing)(Unsupervised semantic parsing, Evaluate-for, Learning representations of logical forms from annotated data in different languages)(Unsupervised semantic parsing, Evaluate-for, Improving performance of monolingual semantic parser using distributed logical representations)(Unsupervised semantic parsing, Part-of, StructVAE)(Unsupervised semantic parsing, Used-for, Semi-supervised semantic parsing)(Unsupervised semantic parsing, Compare, Unsupervised dependency parsers)(Unsupervised semantic parsing, Compare, Probabilistic generative models in unsupervised dependency parsing)(Unsupervised semantic parsing, Hyponym-Of, Triframes)(Unsupervised semantic parsing, Hyponym-Of, Encoder-decoder models for unsupervised sentence representation learning)(Unsupervised semantic parsing, Is-a-Prerequisite-of, Discourse Representation Theory)(Unsupervised semantic parsing, Part-of, Hierarchical attention mechanism)```
(rationale, Generates, text)(rationalization, Enhances, performances)(rationalization, Requires, LLMs)(rationale, Can be useful for, humans)(rationale, Associated with, explanation)(rationalization, Dramatically enhances, performance)(rationalization, Correlated with, human utility)(rationalization, Measured by, Gen-U)
(None)
(language model trained, Used-for, automatic generation of rhythmic poetry)(language model trained, Evaluate-for, effectiveness of generated poems)(language model trained, Compare, traditional training of RNNs using back-propagation through time)(language model trained, Is-a-Prerequisite-of, generative neural language model)(language model trained, Conjunction, Affect-LM)(language model trained, Evaluate-for, language model prediction)(language model trained, Part-of, improving accuracy on different sequence labeling tasks)
```(statistical NLP, involves, feature extraction)(algebraic perspective, can lead to, computationally efficient feature extraction)(message passing algorithm, can restructure, feature templates)(Practical yet rarely discussed problem in dialogue state tracking (DST), involves, handling unknown slot values)(pointer network (PtrNet), can extract, unknown slot values)(Stochastic Gradient Descent (SGD), uses, negative sampling)(AllVec, uses, batch gradient learning)(negative sampling, known to be biased when, sampling distribution deviates from true data distribution)(negative sampling, causes, dramatic fluctuation due to one-sample learning scheme)(AllVec, outperforms, sampling-based SGD methods)(Plagiarism, is a major issue in, science and education)(plagdet, is the main measure for, plagiarism detection)(plagdet, shows fallibility under certain conditions when applied to, manually paraphrased datasets like PAN Summary)(new evaluation
(contextualized word embeddings, Compare, word embeddings)(contextualized word embeddings, Used-for, argument search)(contextualized word embeddings, Compare, non-contextual subword embeddings)(non-contextual subword embeddings, Compare, contextualized word embeddings)(argument search, Evaluate-for, contextualized word embeddings)
`(lingual transfer, Used-for, improve the accuracy of low-resource task language)``(lingual transfer, Is-a-Prerequisite-of, availability of parallel texts)``(transfer or share of knowledge between languages, Is-a-Prerequisite-of, solution to resource scarcity in NLP)``(cross-lingual transfer, Evaluate-for, effectiveness)``(lingual transfer, Evaluate-for, improved semantic parsing results)``(learning distributed representations, Used-for, improving the performance of a monolingual semantic parser)``(NLP models, Evaluate-for, crosslingual transfer)``(Language Transfer, Is-a-Prerequisite-of, improving performance of NLP on low-resource languages)``(variety of unsupervised methods, Compare, unsupervised CLE models)``(unsupervised methods, Evaluate-for, map pre-trained word embeddings of different languages into the same space)``(word alignment tasks, Is-a-Prerequisite-of, unsupervised methods)``(Multilingual Neural Language Models,
(congressional speeches, fatalityandnoun, frame usage)(simple models, depend-on, expensive phrase-level annotation)(neural network models, use, linguistic resources)(simple models, attempt-to-model, linguistic resources)(domain adaptation methods, rely-on, sentiment classifiers)(domain adaptation methods,decline, if distributions have difference)(recurrent neural models, requires, read whole text)(recurrent neural models, has-shortcomings, reading whole text)(neural network model, is-based-on, LSTM)(neural network models, showing promise, in many sub-areas)
(chinese spelling correction, Used-for, neural networks)(chinese spelling correction, Evaluate-for, spell checking)(neural networks, Compare, graph convolutional network)(graph convolutional network, Used-for, spelling correction)(spell checking, Evaluate-for, performance assessment)(spell checking, Is-a-Prerequisite-of, error correction)(spell checking, Is-a-Prerequisite-of, error detection)(error correction, Conjunction, error detection)(error detection, Is-a-Prerequisite-of, error correction)
(task of word segmentation, role of speech register, Explore)(task of word segmentation, role of prosody, Explore)(speech register, role in early language acquisition, Compare)(prosody, role in early language acquisition, Compare)(Japanese corpus, containing infant- and adult-directed speech, Part-of)(word segmentation models, applied to Japanese corpus, Used-for)(prosodic boundaries, knowledge helps adult-directed speech, Evaluate-for)(Neural models, achieved competitive performance, Compare)(Neural models, computationally inefficient, Evaluate-for)(greedy neural word segmenter, proposed in the paper, Evaluate-for)(subword units, effective for NMT, Compare)(subword segmentation, ambiguous, Evaluate-for)(subword regularization, proposed regularization method, Evaluate-for)(subword segmentation algorithm, proposal, Used-for)(NMT models, trained with fixed-size vocabularies, Evaluate-for)(segmenting words into sub-word units, solution to accuracy bottleneck, Explore)(statistical word segmentation methods,
(language, Conjunction, vision)  (sentence, Has, language)  (image, Conjunction, vision)  (language, Is-a-Prerequisite-of, processing)  (language, Evaluate-for, LID)  (vision, Evaluate-for, action recognition)  (processing, Part-of, natural language processing)  (vision, Used-for, image understanding)  
("generative language model", "Used-for", "poetry generation")("generative language model", "Evaluate-for", "machine-generated poems")("generative language model", "Used-for", "language modeling")("generative language model", "Evaluate-for", "code generation")("generative language model", "Used-for", "automated essay scoring")
(Word embeddings, Capture, Linguistic regularities)(Word embeddings, Used-for, Training)(Word embeddings, Part-of, Monolingual embeddings)(Word embeddings, Compare, Multilingual embeddings)(Monolingual embeddings, Used-for, Multilingual model)(Monolingual embeddings, Part-of, Language-agnostic encoder)
`(NCE, Evaluate-for, word embeddings)(word embeddings, Part-of, NLP)(word embeddings, Evaluate-for, knowledge graphs)(translating embeddings, Is-a-Prerequisite-of, knowledge graphs)(translating embeddings, Evaluate-for, domain specific embeddings)(generic word embeddings, Used-for, training)(Domain Specific (DS) word embeddings, Part-of, data)(Generic word embeddings, Part-of, large-scale generic corpora)(Domain Adapted (DA) word embeddings, Is-a-Prerequisite-of, sentiment classification)(DA embeddings, Used-for, outperforming)(DA embeddings, Is-a-Prerequisite-of, input features)(domain-specific embeddings, Part-of, aspect extraction)(general-purpose embeddings, Part-of, pre-trained embeddings)(domain-specific embeddings, Used-for, aspect extraction)(domain-specific embeddings, Is-a-Prerequisite-of, extraction)(domain-specific embeddings, Evaluate-for, comparative embeddings)`
(None)
(semantic units, dynamically predicted, COREQA)(question answering system, incorporated in sequence-to-sequence learning, COREQA)(Knowledge Base Question Answering (KBQA), domain application, Relation detection)(relation detector, integrated, KBQA system)(Questions, generated, Generative Domain-Adaptive Nets)(unlabeled text, utilized, boost the performance of question answering models)(question representation, more emphasis needed, Neural Network-based KB-QA)(fixed vector, representation of the question, Neural Network-based KB-QA)(model-generated questions, combined with human-generated questions, training question answering models)(Domain-Adaptive Nets, based on reinforcement learning, alleviate the discrepancy)(sequence learning model, used for automatic question generation for sentences from text passages)(correct answers, centered around constituents in the parse tree, hierarchical neural architecture)(neural architecture, EviNets, for factoid question answering)(Open Information Extraction (Open IE), used for reasoning with knowledge,
(learning morphological, Used-for, low-resource languages)(learning morphological, Evaluate-for, morphological analysis)(morphological analysis, Is-a-Prerequisite-of, morphological tagging)(morphological tagging, Evaluate-for, low-resource languages)(morphological tagging, Is-a-Prerequisite-of, cross-lingual morphological tagging)(cross-lingual morphological tagging, Evaluate-for, morphological richness)(morphological richness, Compare, dialectal variations)(morphological richness, Compare, polysynthetic languages)
(abstract syntax networks, framework for code generation)(outputs, represented as, abstract syntax trees)(neural parsers, generalize comparably to new domains)(neural parsers, benefit from, structured output prediction of output trees)(neural and non-neural parsers, generalize comparably to new domains)(pre-trained encoder representations, improve performance, neural parsers)(neural parsers, benefit from, structured output prediction of output trees)(context-dependent and cross-domain setup, present, significant challenges)(semantic parsing, method, Hierarchical Semantic Parsing)(Hierarchical Semantic Parsing, utilizes, decomposition-integration)(model, designed within, three-stage parsing architecture)(datasets, experiment with, two state-of-the-art text-to-SQL models)(semantic parsing, focus on, complex question)(HSP model, achieves, significant improvement)(semantic parsing, proposed, SParC dataset)(SParC dataset, consists of, coherent question sequences)(expansions, semantic hashing, technique for information retrieval)(generative hashing methods
1. (summarization, Part-of, abstractive summarization)2. (summarization, Evaluate-for, effectiveness)3. (summarization, Compare, extractive methods)4. (summarization, Is-a-Prerequisite-of, text generation techniques)5. (summarization, Is-a-Prerequisite-of, state-of-the-art methods)6. (summarization, Is-a-Prerequisite-of, automatic metrics)7. (summarization, Compare, neural abstractive models)8. (summarization, Evaluate-for, user feedback)9. (summarization, Evaluate-for, factual errors)10. (summarization, Evaluate-for, faithfulness)11. (summarization, Evaluate-for, factual correctness)12. (model, Part-of, proposed model)
(cross-lingual dependency parsing, Used-for, neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing)(cross-lingual dependency parsing, Evaluate-for, training of monolingual Transformer-based language models)(neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing, Compare, training of monolingual Transformer-based language models)
```(Dialogue Act classification, outperform, strong baselines)(Novel generative neural network architecture, incorporates, novel attentional technique)(Unsupervised learning method, identifies and interprets, metaphors) (Metaphor identification models, cannot identify, metaphorical words)(Unsupervised learning method, outperforms, strong baselines)(Metaphor identification models, improven by, unsupervised learning method)(Unsupervised learning method, paraphrases, identified metaphors)(Translation systems, improved, model)(Semantic parser, learns to parse, tree representations)(Abstract Meaning Representations, parsed into, tree representations)(Semantic parser, achieves, state-of-the-art accuracy)(Decoding algorithms, achieve, state-of-the-art accuraccy)(Response generation, employs, fixed vocabulary)(Response generation, employs, one-pass decoding)(Vocabulary Pyramid Network (VPN), incorporates, multi-pass encoding and decoding)(Previous models, trained based on
(None)
(textual adversarial attack, Evaluate-for, text classification)(textual adversarial attack, Evaluate-for, neural machine translation)(textual adversarial attack, Used-for, generating adversarial examples)(textual adversarial attack, Evaluate-for, NLP systems)(textual adversarial attack, Is-a-Prerequisite-of, generating adversarial perturbations)(textual adversarial attack, Is-a-Prerequisite-of, robust NLP models)
(entity embeddings, Learn-from, word embeddings)(neural network, Utilize, entity embeddings)(entity embeddings, Applied-in, deep neural network)(Word embeddings, Component-of, entity embeddings)(entity embeddings, Utilize, PCCA model)(entity embeddings, Generated-by, dynamic memory-based network)
(dialogue system, Is-a-Prerequisite-of, Neural Belief Tracking)(dialogue system, Is-a-Prerequisite-of, End-to-end learning of recurrent neural networks (RNNs))(dialogue system, Is-a-Prerequisite-of, Query-based summarization)(dialogue system, Used-for, task-oriented spoken dialogue systems)(dialogue system, Used-for, open-domain non-task-oriented dialogue systems)(dialogue system, Is-a-Prerequisite-of, Taylor's law)(dialogue system, Hyponym-Of, task-oriented dialogue systems)(dialogue system, Evaluate-for, achieving the goal in collaborative dialogue setting)
(Generative models, Compare, Discriminative models)(Generative models, Conjunction, Language modeling)(Generative models, Part-of, Parsing)(Discriminative models, Compare, Generative models)(Discriminative models, Part-of, Parsing)(Discriminative models, Part-of, Language modeling)(Generative models, Compare, Parsing)(Generative models, Used-for, Language modeling)(Discriminative models, Used-for, Language modeling)(multilingual masked language modeling, Compare, BERT)(multilingual masked language modeling, Compare, Multilingual BERT)(multilingual masked language modeling, Is-a-Prerequisite-of, Multilingual language modeling)(multilingual masked language modeling, Compare, Transformer model)(multilingual masked language modeling, Compare, Language understanding tasks)(multilingual masked language modeling, Part-of, Multilingual models)(multilingual masked language modeling, Is-a-Prerequisite-of, Cross-lingual transfer
(None)
(semantic parsing, Is-a-Prerequisite-of, natural language processing)(interactive semantic parsing, Compare, semantic parsing)(interactive semantic parsing, Part-of, semantic parsing)
(machine reading comprehension, involves, question answering)(machine reading comprehension, is, challenging)(machine reading comprehension, aims to, fully understand given text)(machine reading comprehension, requires, ability to combine cross-passage information)(machine reading comprehension, introduces, Dynamic Self-attention Network)(machine reading comprehension, emphasizes, human comprehension)(machine reading comprehension, uses, neural networks)(machine reading comprehension, explores, integration with general knowledge)
(language model, used-for, poetry generation)(language model, is-a-Prerequisite-of, generative neural language model)(language model, is-a-Prerequisite-of, LSTM)(language model, is-a-Prerequisite-of, RNN)(LSTM, is-a-Prerequisite-of, Affect-LM)(language model, is-a-Prerequisite-of, LSTM Noisy Channel Model)(language model, is-a-Prerequisite-of, BERT)(language model, Is-a-Prerequisite-of, CAGE)(CAGE, Evaluate-for, Commonsense Auto-Generated Explanation)
(attention mechanism, Used-for, text categorization)(recursive neural network, Used-for, text representation)(Rhetorical Structure Theory, Is-a-Prerequisite-of, discourse structure)(discourse parser, Used-for, discourse structure)(UCCA, Is-a-Prerequisite-of, discourse parser)(DAG structures, Is-a-Prerequisite-of, discourse parser)(bidirectional LSTMs, Used-for, transition-based parser)(deep neural network architecture, Used-for, lemmatization)(SPIGOT, Evaluate-for, structured prediction backpropagation)(neural network, Evaluate-for, structured prediction backpropagation)(discourse attachment, Is-a-Prerequisite-of, data programming)(Snorkel framework, Evaluate-for, data programming)(latent representations, Evaluate-for, discourse understanding)(structured attention mechanism, Evaluate-for, understanding discourse structure)(latent discourse structure, Conjunction, document extraction)(Discourse Representation Theory (DRT), Is-a-Prerequisite-of, semantic parsing)(h
(natural language generation, Used-for, opinionated Natural Language Generation)  (natural language generation, Conjunction, subjective responses)  (natural language generation, Conjunction, data-driven architecture)  (natural language generation, Is-a-Prerequisite-of, semantic parsing)  (natural language generation, Evaluate-for, response generation)  (subjective responses, Compare, human-evaluation scores)  (data-driven architecture, Used-for, generation)  
(conversational machine reading, Used-for, answer user questions)(conversational machine reading, Evaluate-for, user question answering accuracy)(conversational machine reading, Is-a-Prerequisite-of, asking clarification questions)(conversational machine reading, Compare, existing approaches)(conversational machine reading, Compare, decision making strategies)(conversational machine reading, Part-of, conversational AI system)(conversational machine reading, Conjunction, user questions and clarification questions)
(None)
(coherent summary, Compare, factual headlines)(coherent summary, Is-a-Prerequisite-of, stylistic headlines)(coherent summary, Evaluate-for, salient information)(coherent summary, Evaluate-for, cross-document relations)
### Triplets Extracted:- (natural language generation, Used-for, generating human-like subjective responses)- (natural language generation, Is-a-Prerequisite-of, end-to-end question answering systems)- (Opinionated Natural Language Generation (ONLG), Compare, Text similarity measures)- (natural language generation, Evaluate-for, question answering)- (text similarity measures, Evaluate-for, plagiarism detection)- (neural semantic parser, Compare, neural architecture powered by a grammar model)- (semantic parsing, Part-of, neural semantic parser)- (semantic parsing, Compare, language generation task)- (neural semantic parser, Evaluate-for, semantic parsing)- (universal schema, Compare, web text and knowledge base methods)- (natural language task, Compare, matching relevant facts and diffusing them)- (memory networks, Is-a-Prerequisite-of, natural language question answering)- (models, Compare, state-of-the-art)- (low-rank subs
(contrastive visual semantic pretraining, Compare, contextualized English language representations)(contrastive visual semantic pretraining, Compare, GPT-2)(contrastive visual semantic pretraining, Compare, CLIP)(contrastive visual semantic pretraining, Evaluate-for, mitigates anisotropy in contextualized word embeddings from GPT-2)(contrastive visual semantic pretraining, Evaluate-for, significantly improves performance on word-level semantic intrinsic evaluation tasks)(CLIP, Is-a-Prerequisite-of, multimodal image classifier)(GPT-2, Is-a-Prerequisite-of, CLIP)(CLIP, Used-for, encoding image captions)(CLIP, outperform, GPT-2)(Doubly Aligned Multilingual Parser (DAMP), Used-for, improving mBERT transfer performance)(Doubly Aligned Multilingual Parser (DAMP), Evaluate-for, outperforming XLM-R and mT5-Large)
(None)
(None)
(concept, built-for, automatic fake news detection)(concept, used-for, emotion detection)(concept, used-for, event detection)(concept, used-for, disfluency detection)(concept, used-for, argument detection)(concept, used-for, information concealment detection)
(word embeddings, used-for, bilingual word embeddings)(bilingual word embeddings, Is-a-Prerequisite-of, large parallel corpora)(large parallel corpora, Compare, document-aligned corpora)(document-aligned corpora, Evaluate-for, bilingual dictionaries)(statistical segmentation research, Exploits, richer sources)(bilingual word embeddings, Is-a-Prerequisite-of, neural word segmentation research)(neural word segmentation research, Evaluate-for, word embeddings)(word embeddings, Provide, point representations of words)(word embeddings, Conjunction, semantic information)(word embeddings, Compare, traditional approaches)(word embeddings, Is-a-Prerequisite-of, recurrent network)
(Abstractive summarization, Compare, Extractive summarization)(Query-based summarization, Compare, Extractive summarization)(Sequence-to-sequence models, Used-for, Extractive summarization)(SWAP-NET, Compare, Extractive summarization)(Control variates, Evaluate-for, Extractive summarization)(Sentence selection, Is-a-Prerequisite-of, Extractive summarization)
(distributional vector space models, Compare, Functional Distributional Semantics)(distributional vector space models, Evaluate-for, semantic understanding systems)(distributional vector space models, Evaluate-for, low-frequency word forms)(distributional vector space models, Evaluate-for, distinct lexical relations)(distributional vector space models, Is-a-Prerequisite-of, morph-fitting procedure)(distributional semantics model, Compare, Functional Distributional Semantics)(distributional semantics model, Evaluate-for, downstream applications)(distributional semantics model, Compare, Pixie Autoencoder)(distributional semantics model, Evaluate-for, semantic similarity in context)
(satire detection, Used-for, multimodal sarcasm detection)(satire detection, Evaluate-for, detecting sarcasm)(satire detection, Compare, sarcasm detection)(satire detection, Is-a-Prerequisite-of, sarcasm detection)
(concepts, extracted_from, context)(language, trained, models)
(None)
(None)
(extractive summarization model, Used-for, summarization)(extractive summarization model, Is-a-Prerequisite-of, neural network)(extractive summarization model, Compare, abstractive summarization model)
(None)
(neural machine translation, Used-for, incorporating source-side syntactic trees)(neural machine translation, Evaluate-for, improving translation accuracy)(neural machine translation, Is-a-Prerequisite-of, explicit incorporation of source-side syntactic trees)(neural machine translation, Is-a-Prerequisite-of, multilingual neural machine translation)(neural machine translation, Is-a-Prerequisite-of, incorporating linguistic prior)(neural machine translation, Is-a-Prerequisite-of, recurrent neural network grammar)(neural machine translation, Is-a-Prerequisite-of, domain adaptation for neural machine translation)(neural machine translation, Is-a-Prerequisite-of, subword units in neural machine translation)(neural machine translation, Hyponym-Of, sequence-to-dependency neural machine translation)(neural machine translation, Hyponym-Of, mixed fine tuning)(neural machine translation, Hyponym-Of, syntactic-aware neural machine translation)(neural machine translation, Compare, phrase-based machine
(Sememes, Are-a-Prerequisite-of, Linguistic common-sense knowledge bases)(Linguistic common-sense knowledge bases, Is-a-Prerequisite-of, ConceptNet)(Linguistic common-sense knowledge bases, Used-for, Various NLP tasks)(Word embeddings, Encode, Relational information)(Commonsense knowledge, Incorporate-into, Natural language understanding systems)(Commonsense knowledge, Benefit-from, External memory)(Knowledge base construction, Use-of, Pre-trained language models)(Knowledge base construction, Involves, Automatic generation of commonsense descriptions)(WSC problems, Require, Reasoning with commonsense knowledge)(Open book question answering, Involve, Reasoning with common knowledge)(Conversation generation model, Utilizes, Commonsense knowledge graphs)(Sentiment analysis, Enriched-by, External commonsense knowledge)
(adversarial attacks, used-for, perturbing input)(adversarial attacks, Explore, generating adversarial examples)(adversarial attacks, Evaluate-for, improving robustness of models)(adversarial attacks, Is-a-Prerequisite-of, adversarial training)(adversarial attacks, Evaluate-for, reducing classification accuracy)(adversarial attacks, Is-a-Prerequisite-of, adversarial input perturbations)
(None)
(concept, Is-a-Prerequisite-of, word embeddings)(word embeddings, Used-for, sentiment analysis)(word embeddings, Compare, morphology-based models)(word embeddings, Used-for, document clustering)(word embeddings, Hyponym-Of, contextual word embeddings)(word embeddings, Evaluate-for, semantic representations of words)
(abstractive summarization, aims-to-generate, shorter version of document)(abstractive summarization, aims-to-cover, all salient points)(abstractive summarization, Augments-standard-seq2seq-model, with hybrid pointer-generator network)(abstractive summarization, Augments-standard-seq2seq-model, with coverage to discourage repetition)(abstractive summarization, May-reproduce, factual details inaccurately)(abstractive summarization, Is-the-ultimate-goal-of, document summarization research)(idea, suggests, research into summary generation techniques)
(None)
(Recurrent neural networks (RNNs), Used-for, language modeling)(Stochastic gradient Markov Chain Monte Carlo, Compare, traditional training methods)(Pre-training, Is-a-Prerequisite-of, kernelized neural network)(Topics, Part-of, document context)(Neural Language Model, Evaluate-for, language model perplexity)(Affect-LM, Evaluate-for, perceptual studies)(Affect-LM, Evaluate-for, perplexity experiments)
(vision language navigation, Is-a-Prerequisite-of, grounding instructions)(vision language navigation, Evaluate-for, goal completion)(vision language navigation, Used-for, agents)(vision language navigation, Used-for, VLN task)(vision language navigation, Evaluate-for, instruction fidelity)(vision language navigation, Is-a-Prerequisite-of, neural module networks)(vision language navigation, Evaluate-for, VLN agents)(vision language navigation, Is-a-Prerequisite-of, autonomous agents)(vision language navigation, Part-of, VLN models)(vision language navigation, Is-a-Prerequisite-of, refer360°)(vision language navigation, Evaluate-for, navigation tasks)(vision language navigation, Evaluate-for, long paths)(vision language navigation, Evaluate-for, BabyWalk)(vision language navigation, Evaluate-for, Refer360°)(vision language navigation, Used-for, artificial intelligence research)(vision language navigation, Part-of, natural language processing)(v
(Manual fact-checking, Compare, Fully automatic fact-checking)(Fact-checking datasets, Part-of, Factual error correction)(Fact-checking datasets, Part-of, Claims)(Fact-checking datasets, Compare, DialFact dataset)(Fact-checking datasets, Compare, FAVIQ dataset)(Fact-checking datasets, Compare, bgGLUE benchmark)(Fact-checking datasets, Compare, infoVerse framework)(Fact-checking datasets, Part-of, Automatic fact-checking systems)(Fact-checking datasets, Part-of, Fact categorization)(Fact-checking datasets, Compare, FACTIFY-5WQA dataset)
(grammatical error correction, part-of, GEC)(grammatical error correction, compare, neural machine translation)(grammatical error correction, evaluate-for, local errors)(grammatical error correction, evaluate-for, global errors)(grammatical error correction, compare, ERRANT)(grammatical error correction, evaluate-for, system output)(grammatical error correction, compare, seq2seq models)(grammatical error correction, evaluate-for, sentence correction)(grammatical error correction, used-for, improving fluency)(grammatical error correction, is-a-Prerequisite-of, fact verification)(grammatical error correction, compare, Chinese Grammatical Error Correction)(grammatical error correction, compare, Transformer)(grammatical error correction, used-for, generating corrections)(grammatical error correction, compare, ODE Transformer)
(Aspect extraction, Is-a-Prerequisite-of, Aspect-based sentiment analysis)(Aspect-based sentiment analysis, Part-of, Sentiment analysis)(Aspect-based sentiment analysis, Evaluate-for, Sentiment polarity prediction)(Aspect-based sentiment analysis, Is-a-Prerequisite-of, Aspect sentiment classification)(Aspect sentiment classification, Evaluate-for, Sentiment analysis)(Aspect-based sentiment analysis, Compare, General sentiment analysis)(Sentiment analysis, Evaluate-for, Aspect-based sentiment analysis)(Sentiment analysis, Compare, Fine-grained sentiment analysis)
(neural topic modeling, Used-for, text understanding)(neural topic modeling, Is-a-Prerequisite-of, topic generation)(neural topic modeling, Compare, language modeling)
```(relation extraction method, has-property, multi-lingual neural relation extraction framework)(relation extraction method, has-property, mono-lingual attention)(relation extraction method, has-property, cross-lingual attention)(relation extraction method, Evaluate-for, relation extraction)(relation extraction method, has-property, distant supervision)(relation extraction method, is-a-Prerequisite-of, knowledge base population)(relation extraction method, Used-for, extracting relations)(relation extraction method, Compare, Open Information Extraction methods)(relation extraction method, Compare, context-aware neural network model)(relation extraction method, Compare, neural Open IE approach)```
(task question answering, has-relationship, reading comprehension)(task question answering, is-a-Prerequisite-of, question representation)(question answering, Used-for, understanding natural texts)(task question answering, Evaluate-for, performance evaluation)(question answering, Is-a-Prerequisite-of, knowledge base question answering)(task question answering, Part-of, neural network-based methods)(question answering, Evaluate-for, state-of-the-art accuracy)(task question answering, Part-of, question answering system)(question answering, Is-a-Prerequisite-of, state-of-the-art performance)
(deep learning based chinese, Part-of, NLP)(deep learning based chinese, Compare, neural char-based models)(NLP, Evaluate-for, sentiment classification)(deep learning based chinese, Part-of, MLHTC)(deep learning based chinese, Used-for, deep learning tasks)
(neural network models, Used-for, word embeddings)(word embeddings, Is-a-Prerequisite-of, neural word segmentation)(neural word segmentation, Evaluate-for, entity mentions)(entity mentions, Is-a-Prerequisite-of, relations)(neural word segmentation, Used-for, external training sources)(neural network models, Compare, statistical segmentation research)(neural word segmentation, Is-a-Prerequisite-of, joint extraction of entity mentions and relations)
(relations, Used-for, Relation extraction)(extractive qa, Is-a-Prerequisite-of, Abstractive summarization)(extractive qa, Is-a-Prerequisite-of, Query-based summarization)(extractive qa, Used-for, Extractive summarization)(extractive qa, Compare, Abstractive summarization)(extractive qa, Compare, Query-based summarization)
(Entity, Evaluate-for, Gazetteers)(Entity, Evaluate-for, Gazetteers)(Gazetteers, Evaluate-for, Named entity recognition)(Machine learning, Evaluate-for, Named entity recognition)(Named entity recognition, Evaluate-for, Chinese NER)(Graph neural networks, Used-for, Named entity recognition)(Named entity recognition, Evaluate-for, Graph neural networks)(BERT, Used-for, Named entity recognition)(BERT, Evaluate-for, BPEmb)
(semantic parser, Utilized-for, natural language inference)(inference, Is-a-Prerequisite-of, natural language understanding)(neural semantic parser, Utilized-for, natural language inference)(COREQA, Used-for, natural language inference)(TextFlow, Compare, natural language inference)(ONLG, Evaluate-for, natural language inference)(semantic graph parser, Used-for, natural language inference)(NER and MD tasks, Evaluate-for, natural language inference)
(None)
(detecting, application, emotion detection)(detecting, application, contractual obligations)(detecting, application, plot twists detection)(detecting, application, de-identification)(detecting, application, spelling error correction)(detecting, application, spelling error detection)(detecting, application, sentiment detection)(detecting, application, causal relation detection)(detecting, application, grammatical error correction)(detecting, development, large labeled datasets)(detecting, development, deep learning models)(detecting, development, self-attention mechanism)(detecting, development, hierarchical BILSTM)(detecting, development, neural network architecture)(detecting, development, privacy-preserving representations)(detecting, development, joint models)(detecting, improvement, state-of-the-art)(detecting, incorporation, similarity knowledge)(detecting, incorporation, phonological similarity knowledge)(detecting, incorporation, visual similarity knowledge
(None)
(None)
(syntactic parsing, Is-a-Prerequisite-of, neural machine translation)(neural machine translation, Evaluate-for, syntactic information)(syntactic parsing, Is-a-Prerequisite-of, natural language processing)(neural machine translation, Evaluate-for, source-side syntactic trees)(syntactic parsing, Is-a-Prerequisite-of, semantic parsing)(semantic parsing, Evaluate-for, syntactic parsing)
(word embeddings trained, captured-by, semantic regularities)(word embeddings trained, encoded-in, neural network architectures)(neural network architectures, used-for, document analysis)(document analysis, combined-with, word embeddings)(WordNet, defines, semantic concepts)(semantic concepts, estimated-by, word token)(word token, represented-by, distribution)(distribution, used-in, PP attachment model)(PP attachment model, improved-by, context-sensitive embeddings)(neural network, achieves, state of the art performance)(user geolocation model, built-based-on, neural network)(document analysis, benefited-from, word embeddings)(word embeddings, used-for, mapping words)(mapping words, combined-with, clustering algorithm)(clustering algorithm, considers, word-to-word semantic relationships)
(dialogue learning, Used-for, dialogue systems)(dialogue learning, Compare, language learning)(dialogue learning, Compare, multimodal information)(dialogue learning, Compare, sequential concept learning)(dialogue learning, Is-a-Prerequisite-of, conversational agents)(dialogue systems, Part-of, dialog systems)(dialogue learning, Evaluate-for, task-completion dialogue agent)(dialogue learning, Evaluate-for, natural language processing)(dialogue learning, Evaluate-for, social power)(dialogue learning, Evaluate-for, supervised language learning)(dialogue learning, Evaluate-for, multimodal user information)(dialogue learning, Evaluate-for, semantic parsing)(dialogue learning, Evaluate-for, sequential concept learning)
(concept, Used-for, argument mining)(argument mining, Used-for, task)(argument mining, Is-a-Prerequisite-of, argument detection)(argument detection, Is-a-Prerequisite-of, argument identification)(argument identification, Evaluate-for, classification)(argument identification, Evaluate-for, prediction)(prediction, Is-a-Prerequisite-of, event detection)(event detection, Evaluate-for, categorization)
(trained language model, Is-a-Prerequisite-of, automatic generation)(trained language model, Used-for, poetry generation)(trained language model, Compare, generative neural language model)(trained language model, Evaluative, coherent poetry generation)(trained language model, Is-a-Prerequisite-of, parsing and language modeling)(trained language model, Is-a-Prerequisite-of, hierarchical attention network)(trained language model, Part-of, neural language models)(trained language model, Part-of, recurrent neural network grammars)(trained language model, Is-a-Prerequisite-of, recurrent neural networks)(trained language model, Is-a-Prerequisite-of, recurrent neural tensor networks)(trained language model, Used-for, text classification)(trained language model, Evaluative, text classification improvement)(plms, Is-a-Prerequisite-of, rhyme and meter)(plms, Part-of, sonnet modelling)(plms, Used-for, encoding a multi-scale
(None)
(current state nlp, based-on, Word Sense Disambiguation)(current state nlp, focuses-on, Lexical Knowledge Bases)(current state nlp, focuses-on, Recurrent Neural Networks)(current state nlp, relies-on, Transformer-based models)(current state nlp, applies, PLMs)(current state nlp, struggles-with, multi-input tasks)
(language unseen pretraining, Evaluate-for, neural network)(neural network, Part-of, Neural Symbolic Machine)(Neural Symbolic Machine, Evaluate-for, symbolic reasoning)(language understanding, Is-a-Prerequisite-of, symbolic reasoning)(language understanding, Is-a-Prerequisite-of, language unseen pretraining)(neural network, Compare, symbolic "computer")(pretraining, Evaluate-for, language understanding)(neural network, Evaluate-for, language understanding)
(None)
(None)
(neural network models, Used-for, multi-task learning)(neural network models, Compare, SoPa)(multi-task learning, Evaluate-for, text classification tasks)(Domain adaptation, Is-a-Prerequisite-of, sentiment analysis field)(Universal Language Model Fine-tuning (ULMFiT), Is-a-Prerequisite-of, sentiment analysis field)(Cold-Start Aware Attention (CSAA), Used-for, Hybrid Contextualized Sentiment Classifier (HCSC))(Generic word embeddings, Compare, Domain Specific (DS) word embeddings)(DA embeddings, Compare, generic embeddings)(string kernels, Compare, deep learning approaches)(attention-based LSTM, Is-a-Prerequisite-of, aspect-level sentiment classification)(attention-based LSTM, Evaluate-for, document-level data)(Multi-sentiment-resource Enhanced Attention Network (MEAN), Is-a-Prerequisite-of, deep learning approaches)(conditional language model, Is-a-Prerequisite-of, sentiment classification)(encoder-de
(None)
(speech translation, involves, automatic speech recognition)(speech translation, utilized-in, end-to-end automatic speech recognition)(speech translation, promotes, translation)(speech translation, promotes, trade)(speech translation, requires, linguistic resources)(speech translation, requires, pronunciation dictionary)(speech translation, requires, tokenization)(speech translation, has, end-to-end architectures)(speech translation, has, attention-based methods)(speech translation, has, connectionist temporal classification)(speech translation, used-in, trade)(speech translation, used-in, law)(speech translation, used-in, commerce)(speech translation, used-in, politics)(speech translation, used-in, literature)
1. (neural machine translation, relies-on, bi-directional LSTMs)2. (neural machine translation, relies-on, convolutional layers)3. (neural machine translation, compares, state-of-the-art)4. (neural machine translation, evaluates-for, competitive accuracy)5. (neural machine translation, evaluates-for, faster model)6. (neural machine translation, evaluates-for, simpler architecture)7. (neural machine translation, used-for, encoding source sentence)8. (neural machine translation, used-for, speeding up CPU decoding)9. (neural machine translation, used-for, enhancing state-of-the-art)10. (neural machine translation, is-a-prerequisite-of, ability to interpret internal workings)11. (neural machine translation, is-a-prerequisite-of, layer-wise relevance propagation)12. (neural machine translation, is-a-prerequisite-of, posterior regularization)13. (neural machine
(None)
(learning word embedding, Used-for, neural word segmentation)(learning word embedding, Is-a-Prerequisite-of, zero-shot learning)(neural word segmentation, Is-a-Prerequisite-of, word segmentation)(neural word segmentation, Evaluate-for, segmenting words)(neural word segmentation, Evaluate-for, improving accuracy)(word embedding, Is-a-Prerequisite-of, diachronic word embeddings)
(None)
1. (Recurrent neural networks, shown promising performance for, language modeling)2. (traditional training, suffers from, overfitting)3. (optimization, does not provide, good estimates of model uncertainty)4. (stochastic gradient Markov Chain Monte Carlo, used for, learning weight uncertainty in RNNs)5. (gradient noise, enhances exploration of, model-parameter space)6. (model averaging, used when testing, to learn weight uncertainty)7. (stochastic optimization, used for, large training sets)8. (knowledge bases, are important resources for, natural language processing tasks)9. (embedding model ITransF, proposed to perform, knowledge base completion)10. (ITransF, equipped with, sparse attention mechanism)11. (sparse attention mechanism, transfers statistical strength through, the sharing of concepts)12. (associations between relations and concepts, represented by, sparse attention vectors)13. (ITransF, evaluated
`(A* CCG parsing model, Achieves, state-of-the-art results)``(new A* CCG parsing model, Decomposed into, factors of CCG categories)``(new A* CCG parsing model, Defined on, bi-directional LSTMs)``(new A* CCG parsing model, Explicitly models, sentence structures via dependencies)`
(None)
(controllable summarization, **Part-of**, user-specified aspects)(controllable summarization, **Compare**, standard summarization)(controllable summarization, **Is-a-Prerequisite-of**, named entities)(neural summarization models, **Part-of**, success of neural summarization models)(source articles, **Part-of**, success of neural summarization models)(training data, **Part-of**, success of neural summarization models)(BiSET model, **Compare**, standard summarization models)(BiSET model, **Used-for**, guide its summarization process)(translation pattern, **Part-of**, cross-lingual summary)(encoder-decoder attention distribution, **Used-for**, attend to the source words)(transformation probability, **Evaluate-for**, obtain the translation candidates)(SAMSum dataset, **Part-of**, dialogue summarization systems)(feature labeling, **Used-for**, label three types of features)(MCLAS model, **Compare**, baseline models)(DISCOBERT model
(GraphRel, Used-for, inter-sentence relation extraction)(GraphRel, Used-for, document-level graph)(GraphRel, Compare, previous baselines)(multi-grained lattice framework, Compare, existing methods)(multi-grained lattice framework, Compare, Chinese relation extraction)
(None)
(entity representations, improved by, contextualized representation)(Hyperspherical Relation Embeddings, learning, lexical relations)(BERT networks, capture, structural information)(BERT, proposed for, lexical substitution)(BiLSTM, outperformed by, SoPa)(Recurrent network, operates on, word-level representations)(Dynamic Spatial Memory Network, specializing in, latent visual representations)
(document, multi-style abstractive summarization model, Masque)(document, generate, extractive summary)(proposed model, outperforms, state-of-the-art extractive summarizers)(propose email subject line generation, effectively inform recipient)(state-of-the-art result, improve extractive summarization systems)(extractive model, identify, salient sentences)(salient sentences, contain, important key words)(salient key words, combine, extractive summary)(SWAP-NET, guide, interaction of key words and salient sentences)(Hierarchical encoder, depend on, manually annotated data)(document, encode, extractive summary)(extractive summary, build, output summary)(output summary, extract, sentences)(extractive document summarization, obtain representation, of sentences)(selection strategy, predict, relative importance)(model, achieve, state-of-the-art ROUGE scores)(including, ACE event extraction, sequence-to-sequence network)(extractive summarization models, often trained, on news
(topic aware news representation, Used-for, news recommendation)(topic aware news representation, Is-a-Prerequisite-of, user representation learning)(news recommendation, Evaluate-for, accurate news and user representations)(news recommendation, Used-for, alleviating information overload)(news recommendation, Part-of, neural news recommendation approach)(neural news recommendation approach, Is-a-Prerequisite-of, topic-aware news encoder)(topic-aware news representations, Compare, word embeddings)(word embeddings, Used-for, representing words in NLP tasks)(word embeddings, Compare, word senses)(topic-aware news representations, Compare, user representations)(user representations, Is-a-Prerequisite-of, news recommendation)(topic-aware news representations, Compare, word representations)(word representations, Used-for, representing meanings of words)(word senses, Compare, meanings of words)
(None)
(entity extraction, Used-for, relation extraction)(relation extraction, Evaluate-for, multi-lingual neural relation extraction framework)(aspect extraction, Used-for, aspect-based sentiment analysis)(aspect extraction, Evaluate-for, coherence improvement)(event extraction, Is-a-Prerequisite-of, event labeling)(keyphrase prediction, Used-for, understanding, organizing and retrieving text content)(entity mention extraction, Is-a-Prerequisite-of, relation extraction)(named entity recognition (NER), Is-a-Prerequisite-of, weakly supervised cross-lingual NER)(counselor empathy, Is-a-Prerequisite-of, motivational interviewing outcomes)
(constraint satisfaction, task, poetry generation)(language model, trained on, phonetic encoding)(language model, incorporates, document context)(transformer language model, used for, generation of conversational text)(neural language model, enhanced by, knowledge graphs)(NMT model, can significantly improve, performance)(transformer language model, boosted by, ERNIE)(transformer language model, can significantly improve, performance)
#### Concept: error correction- (error correction, aims-at, global errors)- (error correction, aims-at, local errors)- (error correction, is-a-type-of, natural language processing)- (error correction, used-for, sentence improvement)- (error correction, Evaluate-for, accuracy improvement)- (error correction, Compare, human-level language understanding ability)- (error correction, Evaluate-for, grammaticality improvement)- (error correction, Part-of, GEC system)- (error correction, Compare, state-of-the-art approaches)
(Entity-level sentiment analysis, Evaluate-for, Sentiment analysis)(Aspect-based sentiment analysis, Used-for, Sentiment analysis)(Aspect-based sentiment analysis, Evaluate-for, Aspect extraction)(Aspect-based sentiment analysis, Is-a-Prerequisite-of, Aspect sentiment classification)(Aspect-based sentiment analysis, Evaluate-for, Sentiment polarity prediction)(Sentiment analysis, Compare, Aspect-based sentiment analysis)
(concept, Evaluate-for, sentiment classification)(concept, Conjunction, sentiment lexicons, negation words, intensity words)(simple models, Evaluate-for, sentence-level annotation)(RNN with attention, Compare, state-of-the-art performance)(new model, Is-a-Prerequisite-of, state-of-the-art results)(Memory networks (MNs), Used-for, aspect sentiment classification)(MEAN, Compare, strong competitors)(extrinsically induced sentiment embeddings, Used-for, generic cues)(attention-based LSTM, Is-a-Prerequisite-of, aspect-level sentiment classification)(TransCap model, Is-a-Prerequisite-of, aspect-level sentiment classification)(progressive self-supervised attention learning approach, Compare, two state-of-the-art neural ASC models)(DOER, Compare, state-of-the-art baselines)(embedding models, Evaluate-for, sentiment classification)(RBAN approach, Compare, several state-of-the-art baselines)(sentiment grammar, Compare
(goal-oriented visual dialogue, Is-a-Prerequisite-of, reinforced learner)(goal-oriented visual dialogue, Compare, end-to-end dialogue system)(goal-oriented visual dialogue, Evaluate-for, enhanced image representation)(goal-oriented visual dialogue, Is-a-Prerequisite-of, regularized information gain)(end-to-end dialogue system, Part-of, dialogue system)(end-to-end dialogue system, Evaluate-for, response appropriateness)(enhanced image representation, Part-of, attention mechanism)(reinforced learner, Evaluate-for, task success rate)(reinforced learner, Evaluate-for, reward)(dialogue system, Is-a-Prerequisite-of, goal-oriented visual dialogue)
(Aspect based sentiment analysis, Is-a-Prerequisite-of, Semantic parsing)(Structured sentiment, Evaluate-for, Aspect based sentiment analysis)(Structured sentiment, Used-for, Sentiment analysis)(Structured sentiment, Compare, General sentiment analysis)(Syntactic information, Is-a-Prerequisite-of, Enhanced argument labeling model)(Tree-LSTMs, Compare, Bidirectional-LSTMs)(CharSequence, Compare, Word-based methods)(Lattice LSTM, Compare, Character-based LSTM baselines)
(Recurrent neural network, Compare, Convolutional neural network)(RNN, Is-a-Prerequisite-of, Disconnected recurrent neural network)(DRNN, Compare, RNN)(RNN, Part-of, Model)(CNN, Evaluate-for, Text categorization)
(conversation, includes, Natural Language Processing)(agents, Integrate-with, Knowledge Bases)(model, outperforms, State-of-the-art models)(entities, mentioned-in, Social media posts)(utterance, predicted-by, Evaluation model)(response, matched-with, Context)(modalities, interact-with, Human multimodal language)(models, trained-to, Predict profile information)(language, comprises, Words)(dataset, introduced-for, Multimodal Sarcasm Detection)(information, extracted-from, Text and image)(model, significantly outperforms, Baseline models)(dataset, contains, Cooking recipes with photos)(sequence, learnt-by, Finite State Machine)(model, better-than, Baseline model)
(neural text generation, is-a-Prerequisite-of, multi-turn information-seeking conversation systems)(neural text generation, Compare, pre-trained language models)(neural text generation, Evaluate-for, performance improvement)(neural text generation, Part-of, transformers architecture)(neural text generation, Is-a-Prerequisite-of, text summarization models)
(theses and dissertations, Discuss, the usefulness of applying a checking procedure)(theses and dissertations, Based-on, analysis of discrepancies)(checking procedure, Based-on, analysis of discrepancies)(checking procedure, Compare, existing thesauri)(checking procedure, Describe, word sense description)(checking procedure, Identify, serious errors)(fact checking, Is-a-Prerequisite-of, scientific fact checking)(fact checking, Is-a-Prerequisite-of, automated fact checking)(fact checking, Evaluate-for, truthfulness of a claim)(fact checking, Evaluate-for, veracity of claims)(fact checking, Evaluate-for, reasoning about multiple retrievable evidence)(veracity prediction, Evaluate-for, existence of evidence)(claim matching, Describe, identifying pairs of textual messages)(claim matching, Uses, fact checking)(claim matching, Evaluate-for, identifying pairs of textual messages)(inference from premise articles, Used-for, prediction of claim veracity
(represent max likelihood parse, is-a-Prerequisite-of, effectiveness of recurrent neural network language models)(represent max likelihood parse, Compare, ground truth)(represent max likelihood parse, Compare, performance metric)(represent max likelihood parse, Evaluate-for, sentences that are close to the ground truth)(represent max likelihood parse, Evaluate-for, sequence-level smoothing approach)
(catastrophic forgetting, Used-for, neural networks)(catastrophic forgetting, Used-for, continual learning)(catastrophic forgetting, Evaluate-for, neural multimodal approaches)(neural networks, Used-for, continual learning)(neural networks, Evaluate-for, catastrophic forgetting)(neural multimodal approaches, Evaluate-for, catastrophic forgetting)
(None)
(None)
(machine reading comprehension, aims-to-answer, questions)(machine reading comprehension, employs, pre-trained language models)(machine reading comprehension, requires, multi-passage reading)(machine reading comprehension, introduces, dynamic self-attention network)(machine reading comprehension, explores, integration with human knowledge)(machine reading comprehension, presents, gated self-matching networks)(machine reading comprehension, outperforms, state-of-the-art models)
(social media, information extraction is-a-Prerequisite-of sentiment knowledge)  (neural network model, utilizes is-a-Prerequisite-of sentiment knowledge)  (semantics, word-vector spaces can benefit from sentiment knowledge)  (SemAxis, captures is-a-Prerequisite-of sentiment knowledge)  (inference model, relies on is-a-Prerequisite-of sentiment knowledge)  
(Product reviews, contain, implicit aspects)(Product reviews, contain, implicit opinions)(Aspect-based sentiment analysis, ignored, implicit aspects)(Aspect-based sentiment analysis, ignored, implicit opinions)(Aspect-Category-Opinion-Sentiment Quadruple Extraction, introduce, new task)(Aspect-Category-Opinion-Sentiment Quadruple Extraction, goal, extract quadruples)(Aspect-Category-Opinion-Sentiment Quadruple Extraction, goal, provide support)(Aspect-Category-Opinion-Sentiment Quadruple Extraction, goal, aspect-based sentiment analysis)(Annotations, contain, aspect)(Annotations, contain, category)(Annotations, contain, opinion)(Annotations, contain, sentiment)(Restaurant-ACOS dataset, contain, annotations)(Laptop-ACOS dataset, contain, annotations)(SemEval Restaurant dataset, is an extension of, Restaurant-ACOS dataset)(SemEval Laptop dataset, is twice the size of, Laptop-ACOS dataset)(Task, benchmark, with four baseline systems)(Experiments
(concept, Used-for, computational linguistics)(psycholinguistics, Related-to, computational linguistics)(linguistic alignment, Used-for, measuring cultural fit)(psycholinguistics, Used-for, measuring cultural fit)(computer science, Is-a-Prerequisite-of, computational psycholinguistics)(language models, Used-for, detecting linguistic anomalies)(models, Used-for, capturing human reading behavior)(language models, Used-for, processing Chinese texts)(language models, Related-to, psycholinguistic knowledge)(transformer models, Used-for, incremental processing)(BERT, Used-for, detecting speech disfluencies)(linguistic data, Used-for, personality detection)(TrigNet, Is-a-Prerequisite-of, psycholinguistic knowledge)(linguistic corpora, Used-for, natural language understanding)(linguistic corpora, Used-for, designing conversational interfaces)(NLI datasets, Used-for, scientific text)(neural language models, Used-for, computational psych
(`Cross-View Language Modeling`, Is-a-Prerequisite-of, `Cross-lingual Cross-modal Language Model`)(`Cross-View Language Modeling`, Used-for, `pre-training framework`)(`Cross-lingual Cross-modal Language Model`, Is-a-Prerequisite-of, `CCLM`)(`CCLM`, Evaluate-for, `pre-trained model`)(`CCLM`, Used-for, `maximizing mutual information`)(`CCLM`, Is-a-Prerequisite-of, `IGLUE`)(`CCLM`, Is-a-Prerequisite-of, `multi-lingual image-text retrieval datasets`)
(constrained decoding approach, evaluate-for, structured topic model)(input news, Used-for, topic interaction graph)(article, Is-a-Prerequisite-of, graph structure)(structured output modeling, Compare, global output-structured models)(Dozat and Manning, Is-a-Prerequisite-of, graph-based neural dependency parser)(sibling, Hyponym-Of, structural constraint)(pre-trained encoder, Evaluate-for, higher-order features)
(character level language modeling, Used-for, text infilling)(character level language modeling, Used-for, hybrid language modeling)(character level language modeling, Evaluate-for, Chinese word segmentation)(character level language modeling, Evaluate-for, entity mentions storing)(character level language modeling, Compare, word-based models)(character level language modeling, Compare, word segmentation)(character level language modeling, Compare, structure-aware models)(character level language modeling, Compare, sequential recurrent neural networks)(word-based models, Compare, neural char-based models)(word segmentation, Compare, neural char-based models)(structure-aware models, Compare, neural variational language model)(sequential recurrent neural networks, Compare, neural variational language model)
(Concept: Multilingual pre training, Relation: Used-for, Concept: Multilingual Machine Reading Comprehension)(Concept: Multilingual pre training, Relation: Part-of, Concept: State-of-the-art unsupervised multilingual models)(Concept: Unsupervised multilingual models, Relation: Is-a-Prerequisite-of, Concept: Multilingual pre training)(Concept: Multilingual pre training, Relation: Evaluate-for, Concept: Low-resource Neural Machine Translation)(Concept: Multilingual pre training, Relation: Compare, Concept: Multilingual machine translation)
(Word embeddings, used-for, document analysis)(Word embeddings, Is-a-Prerequisite-of, document clustering)(Word embeddings, Is-a-Prerequisite-of, models)(Word embeddings, Is-a-Prerequisite-of, clustering approach)(Word embeddings, Hyponym-Of, Probabilistic FastText)
(neural abstractive summarization, aims-to-generate, shorter version of the document)(neural abstractive summarization, applies-to, document summarization research)(neural abstractive summarization, is-utilized-in, abstractive sentence summarization)(neural abstractive summarization, is-dependent-on, neural models)(neural abstractive summarization, improves, important aspects of abstractive summarization)(document summarization research, involves, abstractive summarization)(document summarization research, aims-to-solve, difficulties of abstractive document summarization)(abstractive sentence summarization, is-an-example-of, abstractive summarization)(abstractive sentence summarization, aims-to-achieve, considerable improvement)(sequence-to-sequence model, is-applied-to, neural abstractive summarization)(sequence-to-sequence model, often, suffers from repetition and semantic irrelevance)(abstractive text summarization models, are-based-on
(gender bias, propagated by, NLP systems)(gender bias, reduced by, Hard Debias algorithm)(gender bias, reduced by, Double Hard Debias technique)(gender bias, reduced by, Semantic-agnostic corpus regularities)(gender bias, traced in, knowledge base embeddings)
(Unsupervised semantic parsing, Part-of, Semi-supervised semantic parsing)(Unsupervised semantic parsing, Evaluate-for, StructVAE)(Unsupervised semantic parsing, Evaluate-for, Neural networks)(Unsupervised semantic parsing, Compare, Supervised semantic parsing)(Unsupervised machine translation, Compare, Supervised machine translation)(Unsupervised machine translation, Part-of, Encoder-decoder dialog model)(Unsupervised machine translation, Evaluate-for, Adversarial technique)(Unsupervised machine translation, Evaluate-for, Bilingual dictionary induction)(Unsupervised machine translation, Evaluate-for, Global GAN)(Unsupervised machine translation, Evaluate-for, Local GAN)(Unsupervised machine translation, Compare, Traditional machine translation)(Unsupervised machine translation, Evaluate-for, Cross-language translation)(Unsupervised machine translation, Evaluate-for, Intrinsic evaluation)(Unsupervised machine translation, Evaluate-for, Extrinsic evaluation)(Unsupervised machine translation, Evaluate-for, Graph similarity metric)(S
(recurrent neural network, Conjunction, RNNs)(recurrent neural network, Compare, LSTM)(RNNs, Hyponym-Of, Neural Networks)(recurrent neural network, Used-for, Machine Reading)(recurrent neural network, Evaluate-for, Document Classification)(recurrent neural network, Is-a-Prerequisite-of, Machine Translation)(recurrent neural network, Compare, Stanford Parser)
(topic aware news, evaluate-for, pre-trained language models)(topic aware news, evaluate-for, parse trees)(topic aware news, evaluate-for, chart-based method)(pre-trained language models, used-for, syntactic awareness)(parse trees, part-of, chart parsing)(chart parsing, evaluate-for, produce parse tree)(parse trees, compare, constituency tests)
(entity mention relation, Compare, joint extraction of entity mentions and relations)(entity mention relation, Used-for, NLP applications)(entity mention relation, Conjunction, semantic relations)(entity mention relation, Used-for, relation extraction)(relation extraction, Is-a-Prerequisite-of, joint entity relation extraction)(relation extraction, Compare, sentence relation extraction)(relation extraction, Used-for, downstream tasks)(entity relation extraction, Evaluate-for, downstream tasks)(entity relation extraction, Evaluate-for, relation classification)(Pull, Evaluate-for, generating embeddings)
(cross-lingual embeddings, Used-for, bilingual lexicon induction)(bilingual lexicon induction, Is-a-Prerequisite-of, cross-lingual classification)(bilingual lexicon induction, Is-a-Prerequisite-of, cross-lingual sentiment classification)(cross-lingual embeddings, Compare, bilingual sentiment embeddings)(bilingual lexicon induction, Compare, cross-lingual word embeddings)(cross-lingual embeddings, Part-of, multilingual model)
```(graph neural, Enhances-performance-on, semantic parsing)(graph neural, Used-for, relation extraction)(graph neural, Is-a-Prerequisite-of, structured projection of intermediate gradients)(graph neural, Used-for, graph measures)(graph neural, Used-for, learning graph embeddings)(graph neural, Is-a-Prerequisite-of, neural conversational models)```
(None)
(neural language model, used-for, rhythm generation)(neural language model, Compare, classical LDA topic model)(neural language model, Compare, document context incorporation)(neural dialogue, Compare, task-oriented dialogue system)
(prototype mention embeddings, is-a-Prerequisite-of, Multi-Prototype Mention Embedding model)(prototype mention embeddings, Compare, word embeddings)(Multi-Prototype Mention Embedding model, Used-for, disambiguate each mention)(prototype mention embeddings, Evaluate-for, high quality of the word)
(attention-based recurrent neural network, Used-for, joint extraction of entity mentions and relations)(attention-based recurrent neural network, Compare, end-to-end tree-based LSTM model)(analogical proportions, Conjunction, eye)(analogical proportions, Conjunction, seeing)(analogical proportions, Conjunction, ear)(analogical proportions, Conjunction, hearing)(transformer-based language models, Evaluate-for, identifying analogies)(transformer-based language models, Compare, word embedding models)(hierarchical text classification, Is-a-Prerequisite-of, text-label semantics relationship)(HiMatch, Used-for, match relationship between text semantics and label semantics)(self-attention, Compare, syntactic structure)(Tree Aggregation Transformer, Is-a-Prerequisite-of, Answer Sentence Selection)(Fact verification, Evaluate-for, judging the veracity of a claim)(CLEVER, Evaluate-for, mitigating biases)
(morphological typology, studies, human language)(human language, involves, vowels)(morphological typology, investigates, linguistic regularities)(morphological typology, examines, phonological typology)(morphological typology, relates to, inflectional lexica)(morphological typology, relevant for, unsupervised morphological analysis)(morphological typology, involves, graph auto-encoder)(morphological typology, contributes to, word formation modeling)
(morphologically rich, is-a-Prerequisite-of, accurate representations for low-frequency word forms)(morphologically rich, Evaluate-for, language understanding systems)(morphologically rich, Used-for, improving distributional vector spaces)(morphologically rich, Part-of, language)(morphologically rich, Conjunction, morph-fitting procedure)(morphologically rich, Compare, insensitive to distinct lexical relations)
(specializing word embeddings according, relies-on, external knowledge)(external knowledge, is-used-for, specializing word embeddings)(specializing word embeddings, relies-on, external knowledge)(specializing word embeddings, is-used-for, making initial embeddings generic)
(word embedding model, Used-for, word analogy questions)(word embedding model, Used-for, caption generation)(word embedding model, Hyponym-Of, neural word embeddings)(neural word embeddings, Is-a-Prerequisite-of, word segmentation)(word embedding model, Compare, bag-of-words)(word embedding model, Is-a-Prerequisite-of, bidirectional language models)(word embedding model, Compare, traditional approaches)
#### Extracted Concepts:- Text similarity measures- Deep learning- Sequential models- N-grams and skip-grams- DNA sequence alignment algorithms- Multimodal sentiment analysis- LSTM-based model- Neural machine translation (NMT)- Layer-wise relevance propagation (LRP)- Context sensitive lemmatization- Bidirectional gated recurrent structures- Composite deep neural network architecture- Pre-trained word embeddings- Neural network architectures- Natural Language Inference (NLI) / Recognizing Textual Entailment (RTE)- Entity linking- Knowledge base- Feature-Rich Networks- Natural Language Processing (NLP)- Sentiment analysis- Transformer models- Open domain question answering- Multilingual NLP- Knowledge graph traversal#### Triplets:- (contextual entity, Part-of, Text similarity measures)- (contextual entity, Compare, Deep learning)- (deep learning, Evaluate-for,
(Completing, part-of, knowledge graph)(Completing, Compare, Link prediction)(Completing, Is-a-Prerequisite-of, Relation Extraction)(Knowledge Base Completion, Evaluate-for, PathNet)(Knowledge Base Completion, Hyponym-Of, Multi-hop Reading Comprehension)(Knowledge Base Completion, Used-for, Query-dependent representation)
1. (grammatical error correction, part-of, GEC systems)2. (grammatical error correction, Compare, fact verification)3. (grammatical error correction, Compare, masked language model (MLM))4. (grammatical error correction, Used-for, correction)5. (Transformer, compare, residual networks)6. (fact verification, part-of, factual error correction)7. (Transformer, compare, numerical ODE methods)
(TBO, Contains, offensive language classifier)(TBO, Bridges gap between, token-level annotation datasets)(TBO, Used-for, identifying offensive language)
(Knowledge graph embedding, Is-a-Prerequisite-of, Learning)(Knowledge graph embedding, Used-for, Link prediction)(Knowledge graph embedding, Compare, KG embeddings)(Knowledge graph embedding, Part-of, Knowledge graph)(Knowledge graph embedding, Evaluate-for, Link prediction)(Knowledge graph embedding, Is-a-Prerequisite-of, Geometric understanding)(Knowledge graph embedding, Is-a-Prerequisite-of, Knowledge-based inference)(Knowledge graph embedding, Is-a-Prerequisite-of, Task performance)(Knowledge graph embedding, Used-for, Task performance)(Knowledge graph embedding, Compare, Traditional models)(Knowledge graph, Part-of, Knowledge graph embedding)
1. (explanation attention, used-for, NLP tasks)2. (explanation attention, Compare, Scratchpad Mechanism)3. (explanation attention, Evaluate-for, interpretability of attention distributions)4. (Scratchpad Mechanism, Conjunction, Machine Translation)5. (interpretability of attention distributions, Is-a-Prerequisite-of, natural language explanations)6. (interpretability of attention distributions, Compare, word Alignment Error Rate)
(conversational text generation model, part-of, LSTMs)(LSTMs, evaluate-for, language modeling)(language modeling, evaluate-for, text generation)(text generation, Used-for, poetry generation)(rhyme and meter model, Is-a-Prerequisite-of, sonnet modelling)(poetry generation, Compare, pun generation)(LSTMs, Is-a-Prerequisite-of, Universal Language Model Fine-tuning)(LSTMs, Compare, Hierarchical Multi-Scale Language Model)
(constituency parsing, based-on, neural model)(constituency parsing, predicts, real-valued scalar)(constituency parsing, determines, topology of grammar tree)(constituency parsing, outperforms, syntax-agnostic NMT baseline)(constituency parsing, achieves, state-of-the-art single model F1 score)(constituency parsing, utilizes, efficient arc-factored inference)(constituency parsing, improved by, self-attentive architecture)(constituency parsing, benefits from, unsupervised pre-training)(constituency parsing, approximates, PTB graph-structured representations)(constituency parsing, approximated by, trees)(constituency parsing, model poses parsing problem as, pointing tasks)
(None)
(morphological compositionality, Is-a-Prerequisite-of, morphological analyzer)(morphological compositionality, Used-for, train word embeddings)(morphological compositionality, Evaluate-for, word similarity)(morphology-based models, Hyponym-Of, morphological compositionality)(Train word embeddings, Is-a-Prerequisite-of, morphological compositionality)(Neural Langage Model, Used-for, enforce explicit relational structures)
(embeddings, capture, linguistic regularities)(embeddings, capture, semantic relations)(embeddings, transfer, across languages)(embeddings, used-for, predicting prepositional phrase attachments)(embeddings, part-of, word vectors)(semantic, capture, sentiment information)(semantic, improved by, contextual embeddings)(semantic, transform into, common vector space)(semantic, specialized by, retrofitting)(semantic, affected by, syntactic structure)(semantic, affected by, semantics)(semantic, relate to, subjectivity)(semantic, trained by, concept induction)(semantic, used for, sentiment analysis)(semantic, used for, sentence classification)(semantic, combined with, specific embeddings)(semantic, trained on, generic corpora)
(vision, part-of, language)(language, Used-for, image captioning)(vision, Conjunction, language)(language, Evaluate-for, vision)(language, Evaluate-for, neural models)(language, Used-for, communication)(neural models, Is-a-Prerequisite-of, language)
(concept, Evaluate-for, continual relation extraction)(continual relation extraction, Is-a-Prerequisite-of, relation extraction)(relation extraction, Hyponym-Of, inter-sentence relation extraction)(relation extraction, Is-a-Prerequisite-of, multiple relation extractions)(relation extraction, Is-a-Prerequisite-of, named entity recognition)(relation extraction, Is-a-Prerequisite-of, relation identification)(relation extraction, Is-a-Prerequisite-of, relation classification)(relation extraction, Is-a-Prerequisite-of, feature extraction)
**(pretrained language model, described as, neural language model)  (pretrained language model, used-for, understanding and working with numbers)  (pretrained language model, used-for, reading comprehension style question answering)  (pretrained language model, part-of, Universal Language Model Fine-tuning (ULMFiT))  (pretrained language model, Compare, Generative models defining joint distributions over parse trees and sentences)  (pretrained language model, Evaluate-for, reducing perplexity metric)  (pretrained language model, part-of, recurrent neural network grammars (RNNGs))  (pretrained language model, Is-a-Prerequisite-of, learning to predict surrounding words for every word in the dataset)**
(compositional generalization semantic parsing, focused-on, SCAN dataset)(compositional generalization semantic parsing, discovered-in, work by Lake and Baroni (2018))(compositional generalization semantic parsing, compared-to, recurrent networks (RNNs))(compositional generalization semantic parsing, compared-to, sequence-to-sequence models)(compositional generalization semantic parsing, tested-on, GeoQuery, SCAN, and CLOSURE datasets)(compositional generalization semantic parsing, used-for, semantic parsing and machine translation benchmarks)
(explainable nlp, Used-for, explainable machine learning systems)(explainable nlp, Conjunction, natural language processing)(explainable nlp, Is-a-Prerequisite-of, generative explanation framework)(explainable machine learning systems, Evaluate-for, predictions)(natural language processing, Part-of, NLP models)(generative explanation framework, Compare, several strong neural network baseline systems)
(concept, Part-of, speech-to-speech translation)(speech translation, Is-a-Prerequisite-of, speech-to-speech translation)(speech recognition model, Is-a-Prerequisite-of, speech translation)(speech-to-text translation, Is-a-Prerequisite-of, speech translation)(speech translation, Evaluate-for, end-to-end modeling techniques)(speech recognition, Is-a-Prerequisite-of, speech translation)(speech encoder, Used-for, speech translation)(speech translation, Is-a-Prerequisite-of, end-to-end modeling techniques)(speech recognition, Is-a-Prerequisite-of, end-to-end speech translation)(speech encoder, Is-a-Prerequisite-of, speech translation)(speech recognition, Is-a-Prerequisite-of, end-to-end speech translation)
(Word embeddings, Compare, Multimodal embeddings)(Multimodal embeddings, Evaluate-for, Sentiment analysis)(Multimodal embeddings, Used-for, Multimodal sentiment analysis)(Multimodal embeddings, Is-a-Prerequisite-of, Multimodal Named Entity Disambiguation)(Multimodal embeddings, Evaluate-for, Multimodal Named Entity Disambiguation)(Multimodal embeddings, Compare, Domain Specific word embeddings)(Generic word embeddings, Compare, Multimodal embeddings)(Multimodal embeddings, Evaluate-for, Language transfer)(Multimodal embeddings, Evaluate-for, Encoding contextual information)
(summarization, Is-a-Prerequisite-of, abstractive summarization)(summarization, Is-a-Prerequisite-of, extractive summarization)(summarization, Compare, query-based summarization)(summarization, Compare, sentence scoring)(summarization, Part-of, document summarization)(summarization, Part-of, opinion summarization)(summarization, Part-of, abstractive document summarization)(abstractive summarization, Compare, extractive summarization)
(None)
(language, targeted-by, language models)(language, utilized-in, neural machine translation)(language, modeled-by, neural machine translation systems)(language, evaluated-using, BLEU metric)
(None)
(ASR, Is-a-Prerequisite-of, transcribed speech)(transcribed speech, Evaluate-for, downstream SLU performance)(ASR, Is-a-Prerequisite-of, ASR system)(ASR, Used-for, speech recognition)(ASR, Is-a-Prerequisite-of, automatic speech recognition)(transcribed speech, Used-for, ASR training data)(ASR, Evaluate-for, ASR performance)(Gronings, Is-a-Prerequisite-of, TTS-generated data)(Gronings, Evaluate-for, ASR performance)
(reading comprehension datasets, include, SQuAD)(reading comprehension datasets, include, WikiReading dataset)(reading comprehension datasets, include, TriviaQA)(reading comprehension datasets, include, MCTest)(reading comprehension datasets, include, GuessTwo)(reading comprehension datasets, evaluate, readability)(reading comprehension datasets, evaluate, prerequisite skills)(SQuAD, include, TriviaQA)(SQuAD, achieve state-of-the-art performance, state of the art)(WikiReading dataset, include, MCTest)(TriviaQA, challenging, RC)(TriviaQA, require cross sentence reasoning, questions and answers)(MCTest, used for, evaluating RC datasets)(GuessTwo, created for, GuessTwo task)(reading comprehension datasets, feature complex questions, TriviaQA)(TriviaQA, contain, over 650K question-answer-evidence triples)
(machine translation using, relies-on, bi-directional LSTMs)(machine translation using, relies-on, convolutional layers)(deep architecture, Used-for, encoding and decoding in NMT)(encoding and decoding, Used-for, machine translation)(NMT, Used-for, neural machine translation)(recurrent activations, Used-for, optimization in NMT)(NMT, uses, Linear Associative Units)(LAU, Is-a-Prerequisite-of, gradient flow reduction)(gradient diffusion, Evaluate-for, optimization difficulty)(NMT, Used-for, modeling complex functions in NMT)(functions modeling, Evaluate-for, enhancing NMT)(source syntax, Is-a-Prerequisite-of, incorporation in NMT)(NMT, Evaluate-for, source syntax incorporation)(encoders, Used-for, source syntax incorporation)(source structures, Is-a-Prerequisite-of, encoders)(NMT model, achieves, comparable results)(translation accuracy, Compare, improvement)(syntactic encoders
(dialogue modeling, Is-a-Prerequisite-of, language modeling)(dialogue modeling, Study, transformer model)(dialogue modeling, Evaluate-for, response generation)(dialogue modeling, Used-for, dialog systems)(dialogue modeling, Evaluate-for, multi-turn setting)(dialogue modeling, Compare, single-turn dialogue modeling)(dialogue modeling, Compare, hybrid language modeling)(dialogue modeling, Evaluate-for, response selection)(language modeling, Part-of, character level language modeling)(transformer model, Conjunction, attentn mechanism)(response generation, Is-a-Prerequisite-of, human generates responses)(response generation, Used-for, interpretingble response generation)(dialog systems, Part-of, encoder-decoder dialog model)
(cross lingual cross modal, Is-a-Prerequisite-of, pre-training)(pre-training, Used-for, CCLM)(CCLM, Is-a-Prerequisite-of, cross-view language modeling)(cross-view language modeling, Is-a-Prerequisite-of, aligning two different views)(cross lingual cross modal, Compare, multilingual)(cross lingual cross modal, Compare, cross-lingual)(cross lingual cross modal, Compare, cross-modal)
(neural topic model, used-for, text classification tasks)(neural topic model, part-of, neural language model)(neural topic model, compare, LDA topic model)(neural topic model, evaluate-for, language model perplexity)(neural topic model, compare, coherence of topics)(neural topic model, compare, traditional topic models)(neural topic model, part-of, sentence-level sentiment classification)
(word vector, uses, distributional vector space models)(word vector, uses, spectral clustering)(word vector, captures, word similarity)(word vector, specializes, distributional vector spaces)(word vector, improved by, external lexical knowledge)(word vector, captures, linguistic regularities)(word vector, combined with, domain specific embeddings)(word vector, improved by, domain adaptation technique)
(None)
(language model like, Compare, neural language model)(language model like, Evaluate-for, perplexity experiments)(neural language model, Evaluate-for, Affect-LM)(neural language model, Is-a-Prerequisite-of, LSTM (Long Short-Term Memory))(neural language model, Used-for, conditional generation of conversational text)(neural language model, Part-of, Noisy Channel Model)(neural language model, Evaluate-for, state-of-the-art in disfluency detection)(neural language model, Evaluate-for, text classification tasks)(LSTM, Evaluate-for, LSTM Noisy Channel Model)(LSTM, Is-a-Prerequisite-of, LSTM Noisy Channel Model)(LSTM, Evaluate-for, number agreement)(LSTM, Is-a-Prerequisite-of, code-mixed language model)(recurrent neural networks, Evaluate-for, promising performance for language modeling)(recurrent neural networks, Part-of, language modeling corpus)(Markov Chain Monte Carlo, Evaluate-for
(output attention weight, is-used-for, understanding model predictions)  (output attention weight, is-utilized-in, explaining model decisions)  (output attention weight, is-a-variant-of, attention weights)  (Attention weights, provide-explanation-for, model predictions)  (Attention weights, are-used-in, Transformer architecture)
(neural word segmentation, Used-for, aspect-based sentiment analysis)(neural word segmentation, Is-a-Prerequisite-of, word embeddings)(neural word segmentation, Evaluate-for, coherence of aspects)(neural word segmentation, Compare, statistical segmentation)(neural word segmentation, Part-of, segmentation model)(neural word segmentation, Hyponym-Of, Chinese word segmentation)(neural word segmentation, Used-for, Chinese NER)(neural word segmentation, Evaluate-for, NLP benchmark tasks)
(Named entity recognition, Used-for, Nested named entity recognition)  (Nested named entity recognition, Is-a-Prerequisite-of, Overlapping named entity recognition)  (Nested named entity recognition, Compare, Nested NER compared to flat NER)  (Named entity recognition, Part-of, Nested named entity recognition task)  (Nested named entity recognition, Evaluate-for, Identifying nested entities)  
(Part-of, sentiment analysis, Multimodal sentiment analysis)(Part-of, sentiment analysis, Opinionated Natural Language Generation)(Hyponym-Of, fine-grained sentiment, Aspect-based sentiment analysis)(Is-a-Prerequisite-of, aspect-based sentiment analysis, aspect category sentiment analysis)(Part-of, aspect-based sentiment analysis, Aspect term extraction)(Part-of, aspect-based sentiment analysis, Sentiment predictions)(Part-of, deep learning, Supervised aspect extraction)(Is-a-Prerequisite-of, supervised aspect extraction, Aspect term extraction)(Compare, gate mechanism, Attention mechanism)(Compare, CNN model, LSTM model)(Compare, gated Tanh-ReLU units, Attention mechanism)(Part-of, ABSA, Aspect category sentiment analysis)(Part-of, ABSA, Aspect term sentiment analysis)
(neural network models, Used-for, multi-task learning)(multi-task learning, Is-a-Prerequisite-of, shared layers)(multi-task learning, Is-a-Prerequisite-of, task-invariant features)(neural text classification tasks, Used-for, multi-task learning)(multi-task learning, Is-a-Prerequisite-of, off-the-shelf knowledge)(complicated structures models, Compare, BiLSTMs)(BiLSTMs, Is-a-Prerequisite-of, classification scenarios)(BiLSTMs, Compare, other models)(LAUs, Used-for, reducing gradient propagation path)(neural semantic parser, Is-a-Prerequisite-of, interpretable model)(neural semantic parser, Is-a-Prerequisite-of, scalable model)
(entity recognition ner, Used-for, natural language processing)(entity recognition ner, Part-of, named entity recognition)(named entity recognition, Is-a-Prerequisite-of, entity recognition ner)(entity recognition ner, Compare, emotion recognition)(named entity recognition, Used-for, named entity recognition systems)(entity recognition ner, Is-a-Prerequisite-of, named entity recognition systems)(named entity recognition systems, Compare, natural language processing)
(concept, Used-for, language model)(model, Improve, perplexity)
(None)
(None)
(None)
(concept, Is-a-Prerequisite-of, entity-oriented search)(concept, Used-for, boosting Neural Machine Translation (NMT) performance)(concept, Is-a-Prerequisite-of, Semantic hashing)(concept, Used-for, information retrieval and extraction)(entity-oriented search, Is-a-Prerequisite-of, neural search systems)(entity-oriented search, Used-for, cycle of neural search networks)(information retrieval and extraction, Compare, generative semantic hashing)(information retrieval and extraction, Part-of, information revolution)(information retrieval and extraction, Evaluate-for, determining trustworthiness of information sources)
(opinion entity extraction, Used-for, opinion opinion pairs extraction)(opinion entity extraction, Evaluate-for, sentiment classification)(opinion entity extraction, Is-a-Prerequisite-of, opinion mining)(opinion entity extraction, Used-for, downstream tasks)
(Word embeddings, Capture, Linguistic regularities)(Transformer-based language model, Extends, BERT)(Semi-supervised Generative Adversarial Networks, Enable, Semi-supervised learning)
(None)
1. (modeling morphological, involves, morphological tagging)2. (modeling morphological, involves, morphological analysis)3. (morphological analysis, involves, predicting syntactic traits of a word)4. (morphological analysis, involves, predicting POS, Case, Gender)5. (modeling morphological, involves, morphological well-formedness)6. (modeling morphological, involves, modeling morphological processes)7. (morphological analysis, involves, inducing translations from monolingual corpora)
### Extracted Concepts:1. Word-embedding models2. Skip-Gram model3. Vector calculus4. Text and knowledge integration5. Network embedding6. Pretrained word embeddings7. General-purpose paraphrastic sentence embeddings8. Type-level word embeddings9. Context-sensitive embeddings10. Temporal word analogies### Triplets:(Word-embedding models, Used-for, Word analogy questions)(Skip-Gram model, Is-a-Prerequisite-of, Additive compositionality)(Vector calculus, Used-for, Solving word analogies)(Network embedding, Used-for, Network analysis)(Pretrained word embeddings, Evaluate-for, NLP systems)(General-purpose paraphrastic sentence embeddings, Is-a-Prerequisite-of, LSTMs)(Type-level word embeddings, Is-a-Prerequisite-of, Semantic concepts)(Context-sensitive embeddings, Evaluate-for, PP attachment model)(Temporal word analogies, Compare, Traditional word analog
(neural machine translation, uses, syntactic information)(source, improves, translation accuracy)(NMT model, generates, translations)(NMT model, generates, translations)(NMT model, generates, translations)(NMT, interprets, internal workings)(NMT, integrates, multiple overlapping, arbitrary prior knowledge sources)(NMT, integrates, prior knowledge)(NMT, incorporates, word reordering knowledge)(NMT system, calculates, output layer)(NMT system, calculates, output layer)(NMT system, calculates, output layer)(NMT system, calculates, output layer)(NMT model, achieves, improved decoding speed)(NMT model, achieves, improved decoding speed)(NMT model, incorporates, source-side syntactic trees)(NMT model, improves, by explicitly incorporating source-side syntactic trees)(NMT, combines, recurrent neural network grammar into attention-based NMT)(NMT, combines, recurrent neural network grammar)(NMT+RNNG, incorporates, linguistic prior
(Discourse segmenters, Used-for, end-to-end discourse parsers)(discourse segmenters, Is-a-Prerequisite-of, statistical discourse segmenters)(discourse segmenters, Used-for, detecting intra-sentential segment boundaries)(discourse segmenters, Used-for, learning discourse segmenters)(discourse segmenters, Is-a-Prerequisite-of, fully supervised system)(discourse segmenters, Evaluate-for, F1 score)(discourse segmenters, Evaluate-for, unsupervised (cross-lingual) results)(Discourse Representation Theory (DRT; Kamp and Reyle 1993), Compare, Discourse segmenters)(DRSs, Part-of, Discourse segmenters)(DRSs, Is-a-Prerequisite-of, statistical discourse segmenters)(NLI model, Evaluate-for, discourse markers)(NeuralREG, Compare, traditional REG models)(Rhetorical Structure Theory (RST), Is-a-Prerequisite-of, segmenter)(context-aware
(constrained stem change rules, Is-a-Prerequisite-of, unsupervised morphological)(suffixation, Part-of, morphological analysis)(infixation, Part-of, morphological analysis)(prefixation, Part-of, morphological analysis)(full and partial reduplication, Part-of, morphological analysis)(unsupervised morphological, Evaluate-for, unsupervised morphological paradigm completion)(unsupervised morphological, Evaluate-for, unsupervised cross-lingual learning)
(concepts, Compare, semantic parsing)(concepts, Used-for, express emotions)(concepts, Evaluate-for, generating conversational text)(semantically similar entities, Compare, actual entities)
(advance neural text, part-of, neural text matching models)(neural text matching models, Compare, efficient for industrial applications)(neural text matching models, Evaluate-for, labeled data)(neural text matching models, part-of, transfer learning)(neural text matching models, Evaluate-for, multi-turn information seeking conversations)(neural text matching models, part-of, convolutional neural networks)(neural text matching models, Evaluate-for, performance enhancement)(neural text matching models, Used-for, deployed in an industrial chatbot)(neural encoder-decoder model, part-of, single document summarization systems)(neural encoder-decoder model, Evaluate-for, large datasets)(neural encoder-decoder model, Evaluate-for, multi-document summarization)(Multi-News dataset, part-of, multi-document summarization)(Multi-News dataset, part-of, MDS news dataset)(Multi-News dataset, Compare, first large-scale MDS news dataset)(Multi-News dataset
(None)
(concept, Used-for, response selection)(concept, Is-a-Prerequisite-of, empircal study)(concept, Is-a-Prerequisite-of, response matching)(concept, Is-a-Prerequisite-of, sequential matching network)(concept, Used-for, extracting entity mentions and relations)(response selection, Compare, traditional methods)(response selection, Compare, state-of-the-art methods)(response selection, Used-for, retrieval based chatbots)(response selection, Used-for, response matching)(response matching, Compare, concatenation of utterances)(response matching, Compare, abstract context vector)(response matching, Is-a-Prerequisite-of, sequential matching network)(sequential matching network, Is-a-Prerequisite-of, SMN)(sequential matching network, Compare, other methods)(experimental study, Compare, state-of-the-art methods)(experimental study, Compare, feature-based joint model)(experimental study, Evaluate-for, performance)(experimental study, Used-for,
(None)
(event extraction, based-on, relation extraction)(event extraction, related-to, aspect-based sentiment analysis)(event extraction, application, ACE)(event extraction, utilized-in, relation extraction)(event extraction, evaluated-for, performance improvement)(event extraction, used-in, knowledge base population)(event extraction, compared-to, information extraction)(relation extraction, based-on, neural network)(relation extraction, evaluated-for, improvement)(aspect-based sentiment analysis, related-to, aspect extraction)(supervised learning, difficulty-to, event extraction)(supervised learning, utilized-in, event extraction)(training data, utilized-in, event extraction)(distant supervision, applied-to, relation extraction)(distant supervision, utilized-in, event extraction)(data labeling problem, solved-by, automatic labeling)(automatic labeling, utilized-in, event extraction)(performance, improved-by, distant supervision)(crowdsourcing, utilized-in, self-training strategy)(crowdsourcing, utilized-for, relation extraction)(relation classification, enhanced-by, DSGAN)(DS
(entity detection, part-of, named entity recognition)(entity detection, evaluate-for, trigger detection)(entity detection, is-a-prerequisite-of, joint entity relation extraction)(entity detection, used-for, event detection)(entity detection, compare, entity linking)(entity detection, used-for, mention detection)
( rumor detection, investigate, multi-modal sarcastic detection)(rumor detection, outperforms, state-of-the-art approaches)(rumor detection, has, user credibility information)(rumor detection, apply, attention mechanism)(rumor detection, propose, a new multi-task learning approach)(rumor detection, share, layer)(rumor detection, have, task specific layers)
1. (Natural Language Generation, Used-for, Opinionated Natural Language Generation)2. (Neural Symbolic Machine, Used-for, Language Understanding)3. (Neural Symbolic Machine, Used-for, Symbolic Reasoning)4. (Neural Symbolic Machine, Is-a-Prerequisite-of, REINFORCE)5. (Opinionated Natural Language Generation, Compare, Natural Language Generation)6. (Question Answering, Evaluate-for, Natural Language Generation)
```(natural language understanding, involves, language understanding)(natural language understanding, involves, symbolic reasoning)(natural language understanding, is-a-Prerequisite-of, symbol representation)(natural language understanding, Used-for, task optimization)(natural language understanding, Used-for, symbolic reasoning)(natural language understanding, Used-for, program learning)(natural language understanding, Used-for, dialogue state tracking)(natural language understanding, Used-for, common language system)(natural language understanding, Used-for, semantic parsing)(natural language understanding, evaluates-for, effectiveness)```
(concept, Evaluate-for, natural language descriptions)(natural language descriptions, Part-of, Lingual semantic parsing)(semantic parsing, Is-a-Prerequisite-of, neural architecture)(lingual semantic parsing, Is-a-Prerequisite-of, neural architecture)(lingual semantic parsing, Is-a-Prerequisite-of, learning algorithm)(lingual semantic parsing, Is-a-Prerequisite-of, semantic parsers)(natural language descriptions, Used-for, generating source code)(neural architecture, Part-of, semantic parsing)(semantic parsing, Evaluate-for, generation of complex programs)(generation of complex programs, Compare, state-of-the-art results)(semantic parsing, Evaluate-for, transducing natural language utterances)(natural language utterances, Evaluate-for, translating to structured queries)(lingual semantic parsing, Used-for, learning classifiers)(lingual semantic parsing, Compare, semantic parsing)(natural language descriptions, Is-a-Prerequisite-of, source code)(source code, Evaluate-for, execution result)(execution result, Evaluate
(None)
(Given Concept, realm of sentiment classification, Related)  (Aspect sentiment classification, Is-a-Prerequisite-of, Sentiment analysis)  (Aspect sentiment classification, Evaluate-for, Sentiment classification)  (Aspect sentiment classification, Evaluate-for, Aspect level sentiment analysis)  (Aspect sentiment classification, Evaluate-for, Multi-sentiment-resource Enhanced Attention Network)  
```(hierarchical attention network, Used-for, reading comprehension style question answering)(hierarchical attention network, Compare, average attention network)(hierarchical attention network, Compare, structured projection of intermediate gradients)(hierarchical attention network, Part-of, neural network-based approach)(hierarchical attention network, Evaluate-for, validate the effectiveness of the proposed method)(hierarchical attention network, Is-a-Prerequisite-of, reading comprehension)(hierarchical attention network, Used-for, fully fuse information from both global and attended representations)(hierarchical attention network, Used-for, focus on the answer span progressively with multi-level soft-alignment)(hierarchical attention network, Is-a-Prerequisite-of, reading comprehension style question answering)(hierarchical attention network, Used-for, predict aspect sentiment polarities from interactive QA style reviews)(hierarchical attention network, Compare, neural news recommendation approach)(hierarchical attention network, Is-a-Prerequisite-of, learning to represent coherent evidence)```  
(spelling error, is-a-Prerequisite-of, character n-grams)(spelling error, Evaluate-for, detecting the native language of a writer)(character n-grams, Compare, spelling error)(spelling error, Conjunction, lexical features)(spelling error, Used-for, classification of texts in the TOEFL11 corpus)(spelling error, is-a-Prerequisite-of, improvement in accuracy)(spelling error, Conjunction, other lexical features)(spelling error, Evaluate-for, detecting the native language of the author)(spelling error, Conjunction, detecting and correcting spelling errors)(spelling error, Used-for, Chinese Spelling Correction)(spelling error, Hyponym-Of, Chinese Spelling Check)
(robustness, Used-for, adversarial example)(example, is-a-Prerequisite-of, adversarial example)(word recognition model, Used-for, defending against adversarial example)(MHA, Used-for, attacking adversarial example)(adversarial example, Evaluate-for, model performance)
(attention based explanation, used-for, Neural Machine Translation models)(attention based explanation, Compare, Automatic Content Extraction)(attention based explanation, Compare, MultiRC)(attention based explanation, Conjunction, machine attention maps)(attention based explanation, Conjunction, human attention maps)(attention based explanation, Conjunction, text classification)(attention based explanation, Evaluate-for, model prediction)(attention based explanation, Evaluate-for, explainability)(attention based explanation, Evaluate-for, reliability)(attention based explanation, Is-a-Prerequisite-of, generating explanations)(attention based explanation, Is-a-Prerequisite-of, black-box models)(attention based explanation, Is-a-Prerequisite-of, model faithfulness)(attention based explanation, Is-a-Prerequisite-of, interpretability)(attention based explanation, Part-of, Visualizing)(attention based explanation, Part-of, Discrimination)(attention based explanation, Used-for, extracting entity mentions and relations)(
(Word, involves predicting, syntactic traits)(Word, specifically POS: Noun, Case: Acc, Gender: Fem)(Analysis, involves predicting, syntactic traits)(Analysis, enriching, language understanding systems)(Representation, composed of, subword units)(Representation, representing, morphological regularities)(Vector, extends, previous findings)(Vector, boosts, semantic quality of word collection)(Inflection, generated using, hard attention mechanism)(Inflection, shows state-of-the-art results)(Model, provides, analysis)(Model, employs, hard attention mechanism)(Model, incorporated with, morphological supervision)(Model, improves bits-per-character performance)(Segment, has, significant impact on performance)(Model, learns, tokenization and text classification)(Model, improves performance of text classification)(Approach, reduces, gender stereotyping)(Approach, achieves F1 scores of, 82% and 73%)
(attention based, used-for, reading comprehension style question answering)(self-matching attention mechanism, part-of, reading comprehension style question answering)(attention-based recurrent network, Is-a-Prerequisite-of, joint extraction of entity mentions and relations)(attention-based encoder-decoder framework, Compare, layer-wise relevance propagation)(attention-based sequence learning model, used-for, automatic question generation)(dependency parsing, Evaluate-for, sequence-to-sequence learning)(Tree-LSTMs, Compare, attention mechanisms)(domain-invariant representation, Evaluate-for, sentiment analysis)(domain-specific representation, Evaluate-for, sentiment analysis)
(syntactic information, helps in, making models robust across domains)(model, proposed by, researchers)(syntactic features, are added to, model)(Integer Linear Programming, introduces syntactic constraints to, model)(model, works better than, traditional non-neural-network-based model)(sentence compression models, trained on, large datasets)(training dataset, created based on, English language characteristics)(created dataset, used to train, Japanese sentence compression models)(linguistic context, is studied for predicting, quantifiers)(crowdsourced data, collected from, human participants)(humans, out-perform models, in understanding the meaning of context)(language-model-based evaluator, used for, deletion-based sentence compression)(evaluator, built by learning, syntactic and structural collocation among words)(deletion operations, conducted via, reinforcement learning framework)(trial-and-error deletion operations, conducted on, source sentences)(model, generates readable compression comparable to, strong baselines)(test
(machine translation system, relies-on, bi-directional LSTMs)(machine translation system, based-on, architecture)(machine translation system, used-for, encoding source sentences)(bi-directional LSTMs, used-for, encoding source sentence)(LSTMs, constrained-by, temporal dependencies)(machine translation system, relies-on, convolutional layers)(convolutional layers, used-for, encoding source sentence)
(None)
(event language model, Used-for, neural language model)(event language model, Compare, LSTM)(LSTM, Is-a-Prerequisite-of, disfluency detection)(event language model, Compare, unsupervised neural machine translation)(event language model, Evaluate-for, emotional sentence generation)(neural language model, Part-of, generative neural language model)(neural language model, Compare, hierarchical LSTM language model)(neural language model, Compare, character-level language models)(neural language model, Compare, fixed-vocabulary language models)
(None)
(conventional Open Information Extraction systems, face, problems of error propagation)(WIST, consist, treebank)(WIST, manually annotated by, two annotators)
(None)
(neural network models, Have Shown Opportunities for, multi-task learning)(multi-task learning, Focus on, Learning Shared Layers to Extract Common and Task-invariant Features)(multi-task learning, Shows Benefits on, Text Classification Tasks)(neural topic, Is Used-for, End-to-end Computational Argumentation Mining)(multi-task learning, Involves, Jointly Learning 'Natural' Subtasks)(neural symbolic machine, Contains, Neural "Programmer")(neural symbolic machine, Contains, Symbolic "Computer")(neural symbolic machine, Outperforms, State-of-the-Art on WebQuestionsSP Dataset)(deep neural networks, Have Enhanced, Neural Machine Translation)(linear associative units, Addresses Problem of, Severe Gradient Diffusion)(linear associative units, Allows, Information Flow Through Space and Time)(pairwise ranking method, Trains, Local Coherence Model)(local coherence model, Achieves, State of the Art Results)(KBLSTM, Leverages,
(speech, Used-for, translation)(speech, Is-a-Prerequisite-of, s2st)(s2st, Compare, speech translation)
(word representation, Capture, semantic information)(word representation, Composed of, subword units)(word representation, Used for, NLP tasks)(subword units, Part-of, word representation)(semantic information, Used for, word representation)(NLP tasks, Applied in, sequence labeling tasks)(sequence labeling tasks, Part-of, NLP systems)(sequence labeling tasks, Used for, named entity recognition)(named entity recognition, Part-of, NLP systems)(NLP systems, Used for, NLP tasks)
(abstractive summarization model, based-on, query-based summarization)(abstractive summarization model, contains, attention mechanism)(abstractive summarization model, outperforms, vanilla encode-attend-decode models)(abstractive summarization model, augmented-with, hybrid pointer-generator network)(abstractive summarization model, augmented-with, coverage to prevent repetition)(abstractive summarization model, proposed-in, improving semantic relevance)(abstractive summarization model, introduced-in, abstractive sentence summarization)(abstractive summarization model, enhanced-by, multi-task learning)(abstractive summarization model, tackles, repetition and semantic irrelevance)(abstractive summarization model, controlled-by, global encoding framework)
(semantic decoding, Used-for, semantic parsing)(semantic parsing, Is-a-Prerequisite-of, semantic decoding)(semantic parsing, Compare, neural machine translation)(semantic parsing, Part-of, natural language processing)(semantic parsing, Evaluate-for, translation accuracy)
(entity representation, Used-for, morphological constraints)(entity representation, Compare, word vector collection)(entity representation, Evaluate-for, language understanding systems)(entity representation, Is-a-Prerequisite-of, semantic representation)(entity representation, Part-of, dialogue state tracking)(entity representation, Used-for, dialogue state tracking)
(None)
(fake news detection, is-related-to, deception detection)(fake news detection, is-challenging-problem-in, computer science)(fake news detection, is-related-to, political and social impacts)(fake news detection, uses, statistical approaches)(fake news detection, is-limited-by, lack of labeled benchmark datasets)(fake news detection, uses, LIAR dataset)(LIAR dataset, is-publicly-available, for fake news detection)(fake news detection, uses, surface-level linguistic patterns)(fake news detection, uses, convolutional neural network)(fake news detection, uses, meta-data)(fake news detection, uses, text)(fake news detection, involves, comparative style analysis)(comparative style analysis, distinguishes, hyperpartisan news)(comparative style analysis, distinguishes, fake news)(comparative style analysis, does-not-work-for, style-based fake news detection)(comparative style analysis, compares, hyperpartisan news with mainstream news)(comparative style
(event mention, extracted from, ACE corpora)(event mention, extracted from, entity typing task)(event mention, extracted from, dialogue state tracking)(event mention, part-of, successful multimodal, multi-encoder model)(event mention, used-for, dialogue coherence assessment)(event mention, mentioned in, CoNLL-2012 benchmark)(event mention, assess the degree to which an event has happened)
(document level relation extraction, is-a-prerequisite-of, relation extraction)(relation extraction, is-a-prerequisite-of, neural relation extraction framework)(neural relation extraction framework, used-for, relation extraction)(relation extraction, compare, distant supervision)(relations, evaluate-for, extraction results)(relation extraction, conjunction, class ties)(relation extraction, compare, relation extraction model)(relation extraction model, hyponym-of, GraphRel)(relation extraction model, compare, document-level relation extraction)(document-level relation extraction, evaluate-for, relational reasoning)
(Cross-lingual summarization, Utilized-for, Translation)(Cross-lingual summarization, Compare, Cross-lingual Information Retrieval)(Cross-lingual summarization, Part-of, Neural abstractive summarization)(Translation, Evaluate-for, Cross-lingual summarization)(Translation, Compare, Cross-lingual summarization)(Cross-lingual Information Retrieval, Part-of, Neural abstractive summarization)(Document encoding, Used-for, Cross-lingual summarization)(Supervised learning, Evaluate-for, Cross-lingual summarization)(Discourse-aware neural summarization model, Is-a-Prerequisite-of, Cross-lingual summarization)
(sentence embeddings, used-for, natural language processing)(sentence embeddings, part-of, text classification)(sentence embeddings, compare, word embeddings)(sentence embeddings, used-for, downstream tasks)(sentence embeddings, compare, word vectors)(sentence embeddings, evaluate-for, sentence classification)(sentence embeddings, compare, generic word embeddings)(sentence embeddings, part-of, neural vector representations)(sentence embeddings, evaluate-for, performance)(sentence embeddings, compare, continuous sentence embeddings)
(None)
(reading comprehension, Used-for, question answering)(reading comprehension, Is-a-Prerequisite-of, machine reading comprehension)(reading comprehension, Used-for, multi-hop reasoning)(reading comprehension, Is-a-Prerequisite-of, answer extraction)(reading comprehension, Evaluate-for, accuracy improvement)(reading comprehension, Is-a-Prerequisite-of, document understanding)(reading comprehension, Is-a-Prerequisite-of, natural language understanding)
**(human annotated explanation, Used-for, natural language explanation)(human annotated explanation, Is-a-Prerequisite-of, explanation-guided representations)(human annotated explanation, Compare, automatically generated QA pairs)(human annotated explanation, Evaluate-for, classifier performance)(natural language explanation, Part-of, human-annotated explainable CAusal REasoning dataset)(natural language explanation, Is-a-Prerequisite-of, generating QA pairs)(natural language explanation, Evaluate-for, providing plausible explanations)(natural language explanation, Compare, generated explanations)(explanation-guided representations, Is-a-Prerequisite-of, generating QA pairs)(explanation-guided representations, Evaluate-for, classifier performance)(explanation-guided representations, Compare, baseline representation)(human-annotated explainable CAusal REasoning dataset, Is-a-Prerequisite-of, causal reasoning models)(human-annotated explainable CAusal REasoning dataset, Evaluate-for, promoting the accuracy)(generating QA pairs, Is-a-Prerequisite-of,
(annotated data, used-for, zero pronoun resolution)(annotated data, used-for, error type performance)(annotated data, Compare, neural network models)
(reading comprehension style question answering, Evaluate-for, gated self-matching networks)(reading comprehension style question answering, Is-a-Prerequisite-of, question answering models)(CoreQA, Is-a-Prerequisite-of, natural answer generation)(question answering models, Evaluate-for, Generative Domain-Adaptive Nets)(KB relations, Hyponym-Of, Hierarchical recurrent neural network)(automatic question generation, Is-a-Prerequisite-of, attention-based sequence learning model)(Wikipedia as unique knowledge source, Is-a-Prerequisite-of, machine reading at scale)(recurrent neural network, Evaluate-for, factoid question answering)(final answer selection stage, Evaluate-for, EviNets neural network architecture)
