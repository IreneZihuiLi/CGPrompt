(spelling correction, Used-for, detecting misspelled characters in texts)(spelling correction, Used-for, correcting misspelled characters in texts)(spelling correction, Hyponym-Of, Chinese Spelling Correction)(Chinese Spelling Correction, Used-for, detecting misspelled characters in Chinese texts)(Chinese Spelling Correction, Used-for, correcting misspelled characters in Chinese texts)(sequence-to-sequence model with attention, Used-for, single-input correction)(PLOME, Used-for, Chinese Spelling Correction)(UMR-Spell, Used-for, Chinese Spelling Correction)(fine-tuning BERT, Part-of, Chinese Spelling Correction)(sequence-to-sequence learning with neural networks, Used-for, Chinese Spelling Correction)(sequence-to-sequence learning with neural networks, Hyponym-Of, spelling correction)(phonetic information, Used-for, Chinese Spelling Correction)(pinyin-to-character objective, Part-of, phonetic information)(self-distillation module, Part-of
(neural network, Used-for, multi-task learning)(neural network, Part-of, Neural Symbolic Machine)(REINFORCE, Used-for, neural network)(Deep Neural Networks, Hyponym-Of, neural network)(Convolutional Neural Networks (CNN), Hyponym-Of, neural network)(Recurrent Neural Networks (RNN), Hyponym-Of, neural network)(deep pyramid CNN, Hyponym-Of, Convolutional Neural Networks (CNN))(Linear Associative Units (LAU), Part-of, neural network)(AMR parser, Part-of, neural network)(attention-based recurrent neural network, Part-of, neural network)(Neural Machine Translation (NMT), Used-for, neural network)(Gated self-matching networks, Used-for, reading comprehension)(Neural Symbolic Machine, Used-for, symbolic reasoning)(neural programmer, Part-of, Neural Symbolic Machine)(symbolic computer, Part-of, Neural Symbolic Machine)(bidirectional
(probabilistic context free grammar, Part-of, context free grammar)(probabilistic context free grammar, Used-for, grammar induction)(grammar induction, Used-for, unsupervised syntactic parsing)(probabilistic context free grammar, Evaluate-for, max-likelihood parse)(max-likelihood parse, Part-of, probabilistic context free grammar)(context free grammar, Hyponym-Of, context free grammar (CFG))(context free grammar, Used-for, parsing)(grammar induction, Used-for, parameter learning)(parameter learning, Used-for, probabilistic linear context-free rewriting system (LCFRS))(LCFRS, Hyponym-Of, context free grammar)(context free grammar, Hyponym-Of, linear indexed grammar (LIG))(linear indexed grammar (LIG), Hyponym-Of, grammar)(tree-adjoining grammars (TAG), Hyponym-Of, grammar)(combinatory categorial grammars, Hyponym-Of, grammar)(head
(context free grammar, Used-for, syntactic parsing)(context free grammar, Hyponym-Of, probabilistic context free grammar)(context free grammar, Hyponym-Of, synchronous context free grammar)(context free grammar, Part-of, context dependent grammar)(synchronous context free grammar, Used-for, syntactic parsing)(synchronous context free grammar, Hyponym-Of, synchronous tree-adjoining grammar)(context free grammar, Evaluate-for, grammar induction)(grammar induction, Used-for, modeling language acquisition)(unsupervised PCFG, Hyponym-Of, context free grammar)(context-free rules, Part-of, probabilistic context free grammar)(context embeddings, Used-for, normalizing flow model)(normalizing flow model, Used-for, PCFG induction)(contextualized representations, Used-for, weak supervision)(contextualized weak supervision, Used-for, text classification)(context representation, Part-of, neural machine translation)(bilingual language model, Used-for, language modeling)(
(preprocessing, Used-for, historical texts processing)(encoder-decoder architectures, Evaluate-for, pre-normalization to modern word forms)(multi-task learning (MTL), Compare, encoder-decoder architectures)(multi-task learning (MTL), Used-for, focus attention during decoding)(knowledge bases, Suffer-from, incompleteness)(ITransF, Used-for, knowledge base completion)(ITransF, Evaluate-for, knowledge bases)(PositionRank, Used-for, keyphrase extraction from scholarly documents)(PositionRank, Compare, PageRank models)(parsing, Used-for, semantic representations)(neural encoder-decoder transition-based parser, Evaluate-for, bilexical dependencies)(Minimal Recursion Semantics (MRS), Evaluate-for, semantic graph parser)(neural encoder-decoder transition-based parser, Compare, attention-based baselines)(named entity recognition (NER), Is-a-Prerequisite-of, mention detection (MD))(Fixed-size ordinally forgetting encoding (FO
(edit distance, Compare, shortest path distance)(edit distance, Compare, dot product distance metric)(distance measures, Conjunction, shortest path distance)(distance measures, Conjunction, dot product distance metric)(syntactic distance, Compare, dot product distance metric)(syntactic distance, Part-of, constituency parsing scheme)(entity triggers, Evaluate-for, named entity recognition)(user-defined graph distance measures, Hyponym-Of, distance measures)(conventional annotated sentences, Compare, trigger-annotated sentences)(hyperbolic space, Used-for, similarity measure)(language model adaptation, Is-a-Prerequisite-of, training setup)
(morphology, Part-of, morphology and lexicon)(lexicon, Part-of, morphology and lexicon)(morphological supervision, Used-for, character language models)(character language models, Evaluate-for, bits-per-character performance)(inflected words, Compare, uninflected words)(morphological supervision, Used-for, performance improvement)(morphological supervision, Conjunction, language modeling data)(morphological supervision, Used-for, low-resource setting)(artificial neural networks, Used-for, inflectional morphology representation)(Encoder-Decoder architectures, Used-for, human-like behavior learning)(Encoder-Decoder architectures, Evaluate-for, extending regular past tense form)(neural models, Evaluate-for, extending frequent class forms)(neural models, Evaluate-for, minority-class generalization)(shared-tasks, Part-of, SIGMORPHON)(morphological inflection models, Evaluate-for, harder train-test splits)(split-by-lemma method, Evaluate-for,
(dual problem, Compare, primal problem)(subword units, Used-for, alleviate the open vocabulary problems)(optimization of SGNS objective, Part-of, Skip-Gram Negative Sampling)(deep neural networks, Used-for, solve math word problems)(sequence-to-sequence neural models, Used-for, generate Chinese poems)(NER, Part-of, natural language processing)(event extraction, Part-of, natural language processing)(adversarial learning framework, Used-for, relation extraction)(target-sensitive sentiment, Used-for, Aspect sentiment classification).
(game playing in ai, Used-for, study natural language processing)(game playing in ai, Challenges, low sample efficiency)(game playing in ai, Challenges, large action space)(deep reinforcement learning, Used-for, developing game playing agent)(world-perceiving modules, Used-for, decomposing tasks)(world-perceiving modules, Used-for, pruning actions)(two-phase training framework, Used-for, decoupling language learning from reinforcement learning)(two-phase training framework, Improves, sample efficiency)(experimental results, Shows, performance improvement)(experimental results, Shows, sample efficiency improvement)(10K human-annotated games of Go, Used-for, model interpretability)(linear probing, Used-for, predict mentions of domain-specific terms)(AlphaGo Zero, Used-for, game playing in ai)(imitation learning, Used-for, training policy networks)(reinforcement learning, Used-for, training policy networks)(domain-specific terms, Part-of, game playing in ai)
(recommendation system, Used-for, improving users’ online news reading experience)(recommendation system, Part-of, personalized news service)(personalized news recommendation, Hyponym-Of, recommendation system)(news recommendation, Hyponym-Of, recommendation system)(information overload, Evaluate-for, personalized news recommendation)(user representations, Part-of, personalized news recommendation)(long-term user representations, Part-of, personalized news recommendation)(short-term user representations, Part-of, personalized news recommendation)(attention network, Used-for, selecting important words)(news encoder, Part-of, personalized news recommendation)(user encoder, Part-of, personalized news recommendation)(CNN networks, Used-for, learning representations of news from their titles)(GRU network, Used-for, learning short-term user representations)(FIM, Hyponym-Of, recommendation system)(fine-grained matching, Part-of, FIM)(multiple recommendations, Part-of, conversational recommendation)(dialog systems, Hyponym-Of, recommendation system)(MIND
(computation theory, Is-a-Prerequisite-of, computational models)(computation theory, Conjunction, loss-augmented setting)(computational models, Evaluate-for, coreference resolution)(computational models, Evaluate-for, dependency parsing)(dependency parsing, Part-of, parsing model)(dependency parsing, Conjunction, machine translation)(dependent parsing, Conjunction, text chunking)(machine translation, Evaluate-for, translation quality)(neural machine translation, Used-for, improving decoding speed)(neural techniques, Part-of, neural machine translation)(neural techniques, Evaluate-for, bidirectional translation tasks)(neural techniques, Evaluate-for, argumentation mining)(argumentation mining, Compare, neural machine translation)(end-to-end computational argumentation mining, Evaluate-for, argumentation strategies).
(text generation, Used-for, question answering)(text generation, Used-for, AMR-to-text generation)(text generation, Used-for, paraphrase generation)(text generation, Used-for, dialog systems)(text generation, Compare, code generation)(text generation, Compare, semantic parsing)(text generation, Hyponym-Of, abstractive summarization)(text generation, Used-for, grammatical error correction)(text generation, Used-for, multi-document summarization)(text generation, Part-of, NLG system)Grammatical Error Correction, Compare, Text Simplification)(abstractive summarization, Evaluate-for, extractive summarization)(NLG system, Part-of, natural language processing)(inflection generation, Compare, morphological inflection)(corrective REs, Used-for, misunderstandings)(local coherence, Part-of, entity grid representation)(AliMe Chat, Compare, public chatbot)(auto Pyramid scores, Compare, human Pyramid)(affective information, Used-for, conversational text)
(character level language model, Part-of, neural language model)(neural language model, Used-for, text generation)(character level language model, Used-for, word generation)(character level language model, Evaluate-for, language modeling benchmark datasets)(language model, Part-of, text classifier)(tokenization, Is-a-Prerequisite-of, text classification)(syntactic structure, Evaluate-for, model performance)(neural language model, Evaluate-for, human-like syntax acquisition)(PAST method, Used-for, improving perplexity)(open-vocabulary language modeling, Used-for, handling new word types)(hybrid language modeling, Is-a-Prerequisite-of, segmenting unsegmented languages)(affective information, Used-for, conversational text generation)(affective information, Used-for, enhancing text emotional content)(Common Sense Explanations, Used-for, commonsense reasoning)(dialogue generation pre-training framework, Used-for, support various conversations)(knowledge distillation, Used-for, structural bias transfer)(neural language
(latent variable model, Used-for, response generation)(latent variable model, Part-of, variational autoencoders)(variational autoencoders, Used-for, text generation)(latent variable model, Used-for, AMR parsing)(latent variable model, Part-of, Gaussian Mixture LVeGs)(Gaussian Mixture LVeGs, Used-for, part-of-speech tagging)(Gaussian Mixture LVeGs, Used-for, constituency parsing)(latent variable model, Used-for, sentiment analysis)(latent variable model, Used-for, semi-supervised learning)(latent variable model, Used-for, negation scope extraction)(latent variable model, Part-of, deep generative model)(deep generative model, Used-for, stock movement prediction)(latent variable model, Used-for, multilingual word representations)(latent variable model, Used-for, entity linking)(latent variable model, Part-of, neural parser)(neural parser, Used
None
(citation network, Hyponym-Of, Graph Convolutional Networks)(Graph Convolutional Networks, Part-of, citation network)(Graph Convolutional Networks, Hyponym-Of, GCN)(Graph Convolutional Networks, Used-for, relation extraction)(relation extraction, Evaluate-for, citation network)(text and network context, Used-for, citation network)(state-of-the-art, Evaluate-for, citation network)(domain adaptation, Evaluate-for, citation network)(Text Deconvolution Saliency, Evaluate-for, text classification)(Text Deconvolution Saliency, Used-for, text analysis)(Relational Reasoning, Hyponym-Of, citation network)(response selection, Used-for, dialogue systems)(sentiment classification, Conjunction, sentiment prediction)
(penn treebank, Part-of, wall street journal)(penn treebank, Used-for, ablation studies)(penn treebank, Used-for, standard datasets)(penn treebank, Used-for, word representations)(penn treebank, Used-for, syntax tasks)(penn treebank, Part-of, WikiText-2)(annotated english gigaword dataset, Conjunction, penn treebank)(penn treebank, Used-for, parsing)(penn treebank, Used-for, nonlocal dependency identification)(penn treebank, Used-for, training taggers)(penn treebank, Used-for, evaluating theories on adjective order)(dependency parsing, Part-of, information extraction)(dependency parsing, Part-of, sentiment analysis)(dependency parsing, Hyponym-Of, syntax tasks)(dependency parsing, Part-of, penn treebank).
(entity resolution, Used-for, text mining)(biomedical named entities, Part-of, bio text mining)(synonyms, Part-of, biomedical named entities)(BioSyn, Used-for, biomedical entity normalization)(bitext mining, Used-for, multilingual sentence alignment)(cross-lingual sentence representation, Used-for, bitext mining)(mutual information, Used-for, quantification of systematicity of signs)(gradient search, Used-for, iterative inference in text infilling)(deep learning, Used-for, entity resolution)(transfer learning, Used-for, deep learning for low-resource ER applications)(active learning, Used-for, fine-tuning transferred models in ER)(speech translation, Used-for, cross-modality learning in speech and text)(speech recognition, Used-for, cross-modality learning in speech and text)(self-supervised learning, Part-of, cross-modality learning in speech and text).
(document representation, Part-of, neural semantic parser)(document representation, Is-a-Prerequisite-of, natural language processing)(neural semantic parser, Used-for, natural language utterances)(predicate-argument structures, Part-of, neural semantic parser)(predicate-argument structures, Compare, linguistically motivated representations)(semantic representation, Compare, syntactic schemes)(semantic representation, Part-of, natural language processing)(Abstract Meaning Representation, Hyponym-Of, semantic representation)(morpheme segmentation, Used-for, word representations)(discourse structure, Used-for, text categorization)(named entity recognition, Used-for, mention detection)(pre-trained word embeddings, Used-for, neural network architectures for NLP tasks)(sequence labeling, Part-of, named entity recognition).
(lexicalized parsing, Compare, shallow syntactic analyzer)(lexicalized parsing, Part-of, math problem solving system)(lexicalized parsing, Compare, unlexicalized predicates)(neural machine translation, Compare, lexicalized parsing)(neural machine translation, Hyponym-Of, NMT)(NMT, Used-for, machine translation)(syntax-aware system, Compare, syntax-agnostic NMT)(nested named entity recognition, Compare, span-based constituency parser)(nested named entity recognition, Part-of, lexicalized constituency tree)(NMT, Compare, pseudo-in-domain corpus)(Relational-Realizational grammar, Compare, lexicalized grammars)(Natural Language Generation, Compare, opinionated articles)(paraphrase generation, Compare, autoregressive baselines).
(neural network, Used-for, handwriting recognition)(fixed-size ordinally forgetting encoding, Used-for, handwriting recognition)(FOFE, Used-for, handwriting recognition)(named entity recognition, Compare, handwriting recognition)(feedforward neural network, Part-of, handwriting recognition)(sequence labeling, Compare, handwriting recognition)(discourse-specific word embeddings, Used-for, handwriting recognition)(DSWE, Used-for, handwriting recognition)(neural models, Used-for, handwriting recognition)(Transformer-based models, Used-for, handwriting recognition)(knowledge distillation, Evaluate-for, handwriting recognition)(contextual subword embeddings, Compare, handwriting recognition)(multilingual NLP, Used-for, handwriting recognition).
(None)
(nlp and vision, Compare, natural language understanding)(nlp and vision, Compare, natural language generation)(nlp and vision, Part-of, multi stage pipeline)(nlp and vision, Part-of, transformer-based language models)(nlp and vision, Conjunction, cross-modal grounding)(nlp and vision, Conjunction, image captioning)(nlp and vision, Used-for, video question answering)(nlp and vision, Used-for, describing images with text)(nlp and vision, Used-for, predicting gaze patterns)(nlp and vision, Is-a-Prerequisite-of, generating relational captions for two images)(nlp and vision, Hyponym-Of, vision-language research)(video question answering, Part-of, cross-modal grounding)(motion module, Part-of, MASN)(appearance module, Part-of, MASN)(nlp, Compare, vision)
(concept selection, Hyponym-Of, feature selection)(message passing algorithm, Used-for, restructure feature templates)(text chunking, Evaluate-for, feature extraction)(relation extraction, Evaluate-for, feature extraction)(joint optimization, Used-for, content selection)(extractive multi-document summarization, Used-for, content selection)(active learning, Used-for, content selection)(integer linear programming, Used-for, multi-document summarization)(multi-document summarization, Hyponym-Of, summarization)(model ablations, Evaluate-for, structural context)(structural context, Part-of, feature extraction)(BERT, Used-for, gender prediction)(BERT, Used-for, identify trafficking ads)(spatio-temporal matching, Used-for, response selection)(neural machine translation, Evaluate-for, noise)(domain-data selection, Hyponym-Of, data selection)(clean-data selection, Hyponym-Of, data selection)(co-curricular learning, Used-for, data selection
(matrix factorization, Used-for, user-topic matrix)(matrix factorization, Used-for, latent feature space)(user-topic matrix, Part-of, stance detection)(latent feature space, Part-of, stance detection)(matrix factorization, Evaluate-for, predicting missing preferences of users)(latent vector representations of topics, Evaluate-for, inter-topic preferences)(latent feature space, Evaluate-for, user preferences)(latent feature space, Is-a-Prerequisite-of, representing users’ preference)(matrix factorization, Is-a-Prerequisite-of, stance detection)(latent vector representations of topics, Part-of, matrix factorization)
(first order logic, Hyponym-Of, logic)(first order logic, Used-for, reasoning)(AdaLoGN, Hyponym-Of, adaptive logic graph network)(neural-symbolic approach, Part-of, logical reasoning)(higher-order models, Compare, first-order models)(neural encoders, Evaluate-for, dependency parsers)(uniMorph, Used-for, evaluating CaMEL)(movement pruning, Hyponym-Of, first-order methods)(SMP, Compare, first-order methods)(first-order pruning, Is-a-Prerequisite-of, Static Model Pruning)(fuzzy logic, Used-for, T-norm optimization)(document-level event argument extraction, Used-for, T-norm fuzzy logic).
(neural semantic parser, Part-of, natural language processing)(predicate-argument structures, Hyponym-Of, vector representation)(morph-fitting procedure, Used-for, improving distributional vector spaces)(vector space models, Used-for, capturing word similarity)(self-matching attention mechanism, Part-of, reading comprehension)(pointer networks, Used-for, locating positions of answers)(morpheme segmentation, Used-for, segmenting words into morphemes)(neural machine translation models, Used-for, translating text)(part-of-speech and morphological tagging tasks, Hyponym-Of, vector representation)(layer-wise relevance propagation, Used-for, interpreting neural machine translation)(Minimal Recursion Semantics, Hyponym-Of, semantic representation)(convolutional neural network, Hyponym-Of, vector representation)(question-aware passage representation, Hyponym-Of, vector representation)(self-matching attention mechanism, Hyponym-Of, vector representation)(vector space, Used-for, spectral clustering
(stack lstm, Part-of, amr parser)(amr parser, Evaluate-for, smatch score)(amr parser, Part-of, policy learning)(amr parser, Conjunction, pre-processed concept identification)(policy learning, Evaluate-for, smatch score)(pre-processed concept identification, Conjunction, named entities)(pre-processed concept identification, Conjunction, contextualized embeddings).
(graph-based nlp, Part-of, abstractive summarization)(graph-based nlp, Used-for, abstractive sentence summarization)(graph-based nlp, Part-of, neural abstractive document summarization)(graph-based nlp, Conjunction, graph degeneracy)(graph-based nlp, Used-for, relation extraction)(graph-based nlp, Used-for, graph-to-sequence modeling)(graph-based nlp, Used-for, word sense induction)(graph-based nlp, Is-a-Prerequisite-of, graph-based evidence aggregating and reasoning framework)(graph-based nlp, Used-for, document-level relation extraction)(graph-based nlp, Used-for, dependency tree linearization)(graph-based nlp, Used-for, multi-modal neural machine translation)(graph-based nlp, Used-for, event factuality prediction)(abstractive summarization, Compare, extractive methods)(abstractive summarization, Part-of, document summarization)(text generation techniques, Used-for, neural abstractive
(phonetics, Used-for, phonological distinctive features)(recurrent neural network models, Used-for, phonotactic learning)(phonological distinctive features, Evaluate-for, segment-level phonotactic acquisition)(feature-naive model, Compare, feature-aware model)(phonetically-grounded language generation, Hyponym-Of, language generation)(tongue twisters, Hyponym-Of, language)(TwistList, Used-for, tongue twisters)(tongue twister generation, Evaluate-for, automatic evaluation)(tongue twister generation, Evaluate-for, human evaluation)(TableQA models, Used-for, retriever-reader pipelines)(Dense Passage Retriever, Part-of, retriever component)(question and table embeddings, Part-of, late interaction models)(retriever and reader, Part-of, joint training scheme)(binary relevance token, Part-of, answer)
(syntax based machine translation, Part-of, machine translation)(syntax based machine translation, Used-for, improving translation quality)(machine translation, Hyponym-Of, statistical machine translation)(machine translation, Hyponym-Of, neural machine translation)(neural machine translation, Used-for, integrating syntactic information)(neural machine translation, Used-for, sentence embedding)(neural machine translation, Used-for, domain adaptation)(seq2seq models, Is-a-Prerequisite-of, neural machine translation)(graph-to-sequence learning, Compare, grammar-based approaches)(syntax based neural machine translation, Compare, graph-to-sequence learning)(graph-to-sequence learning, Used-for, NLP applications)(linguistic prior, Used-for, neural machine translation)(NMT+RNNG, Part-of, machine translation)(NMT+RNNG, Used-for, incorporating linguistic prior)(Gated Graph Neural Networks, Used-for, encoding structural information)(words, Part-of, vocabulary
(markov chain monte carlo, Used-for, learning weight uncertainty in RNNs)(RNNs, Used-for, language modeling)(traditional training of RNNs, Part-of, back-propagation through time)(stochastic optimization, Compare, stochastic gradient markov chain monte carlo)(stochastic optimization, Used-for, large training sets)(stochastic gradient markov chain monte carlo, Used-for, large training sets)(stochastic gradient markov chain monte carlo, Used-for, learning weight uncertainty in RNNs)(Bayesian learning algorithm, Part-of, stochastic gradient markov chain monte carlo)(gradient noise, Part-of, Bayesian learning algorithm)(model averaging, Part-of, Bayesian learning algorithm)(proposed approach, Compare, stochastic optimization)(RNN models, Conjunction, broad range of applications)
(relation extraction, Part-of, information extraction)(relation extraction, Used-for, finding relational facts from plain text)(multi-lingual neural relation extraction, Part-of, relation extraction)(mono-lingual attention, Part-of, multi-lingual neural relation extraction)(cross-lingual attention, Part-of, multi-lingual neural relation extraction)(cross-lingual attention, Used-for, considering consistency and complementarity)(distant supervision, Evaluate-for, relation extraction)(distant supervision, Used-for, building training data)(distant supervision, Hyponym-Of, distant supervision method)(dynamic transition matrix, Used-for, characterizing noise in training data)(curriculum learning, Used-for, training dynamic transition matrix)(relation detection, Part-of, relation extraction)(relation detection, Part-of, Knowledge Base Question Answering)(Knowledge Base Question Answering, Used-for, answering questions)(residual learning, Part-of, hierarchical recurrent neural network
(speech synthesis, Used-for, language revitalization)(speech synthesis, Used-for, image captioning)(text-to-speech, Hyponym-Of, speech synthesis)(neural models, Used-for, speech synthesis)(FastSpeech2, Hyponym-Of, speech synthesis)(Tacotron2, Hyponym-Of, speech synthesis)(training data, Evaluate-for, speech synthesis)(articulatory vectors, Used-for, phoneme representations)(phoneme representations, Part-of, speech synthesis)(language agnostic meta learning, Conjunction, speech synthesis)(speech synthesis, Compare, text-to-schema methods)(sentiment features, Conjunction, hate speech detection)(word embedding, Used-for, intermediate)(multimodal synthesis techniques, Part-of, StructuredRegex)(intermediate speech representations, Part-of, speech synthesis)(speech units, Part-of, speech synthesis)(visual semantics, Evaluate-for, image captioning).
(clustering, Used-for, grouping points in a vector space)(word embeddings, Used-for, clustering)(spectral clustering, Hyponym-Of, clustering)(words, Part-of, vector space)(synonyms, Compare, antonyms)(vector spaces, Compare, new signed spectral normalized graph cut algorithm)(vector space, Used-for, spectral clustering)(signed spectral normalized graph cut algorithm, Used-for, clustering)(signed spectral normalized graph cut algorithm, Evaluate-for, historical printing documents)(orthographic variation, Part-of, historical printing documents)(deep relevance model, Evaluate-for, document filtering)(zero-shot document filtering, Evaluate-for, relevance between a document and a category)(clustering, Part-of, triclustering)(triframes, Used-for, frame induction)
(dimensionality reduction, Used-for, sentence meta-embeddings)(dimensionality reduction, Evaluate-for, Semantic Textual Similarity)(dimensionality reduction, Hyponym-Of, meta-embedding methods)(meta-embedding methods, Part-of, sentence meta-embeddings)(generalized Canonical Correlation Analysis, Conjunction, dimensionality reduction)(cross-view auto-encoders, Conjunction, dimensionality reduction)(neural network-based approach, Compare, multi-channel model)(attention layer, Part-of, Adversarial Attention Network)(adversarial training, Used-for, learning word weights)(sememe knowledge, Part-of, SC models)(softmax output layer, Part-of, neural language models)(unargmaxable tokens, Evaluate-for, large language models)(semantic compositionality, Is-a-Prerequisite-of, learning representations of multiword expressions).
(surface word, Part-of, word-lemma pair)(bidirectional gated recurrent structures, Used-for, extracting character level dependencies)(language models, Used-for, context embeddings)(model, Evaluate-for, Japanese predicate argument structure analysis)(predicted syntactic information, Compare, grid-type recurrent neural networks)(sequence labeling tasks, Evaluate-for, pretrained context embeddings)(Gated Recurrent Averaging Network, Compare, LSTM recurrent networks)(neural machine translation, Compare, NMT+RNNG)(causality assumption, Compare, implicitly-defined neural network architecture)(Dialogue Act classification, Evaluate-for, generative neural network)(attentional technique, Part-of, generative neural network)(label to label connection, Part-of, generative neural network)(neural transition model, Evaluate-for, AMR parsing task)(deep generative model, Evaluate-for, stock movement prediction)(encoder-decoder framework, Part-of, neural image captioning systems)(CNN, Part-of, encoder
(course introduction, Part-of, lecture slide page)(course introduction, Part-of, whole lecture)(course introduction, Used-for, knowledge graph)(knowledge graph, Conjunction, neural network models)(neural network models, Compare, data-driven model)(neural network models, Used-for, semantic graphs)(personal health mention detection, Used-for, predicting health condition)(feature augmentation-based approach, Compare, pipeline-based approach)(figurative usage detection, Used-for, personal health mention detection)(commonsense natural language inference, Used-for, selecting likely event followup)(BERT, Compare, state-of-the-art models)(Adversarial Filtering, Part-of, data collection paradigm)(Adversarial Filtering, Evaluate-for, commonsense inference difficulty)(HellaSwag, Evaluate-for, deep pretrained models)(HellaSwag, Part-of, new challenge dataset)(word recognition model, Used-for, handling rare words)(RNN semi-character architecture, Is-a-Prerequisite-of, word recognition model)(R
(k mean, Compare, monolingual machine translation)(k mean, Compare, variational auto-encoding model)(k mean, Used-for, semantic dependency parsing)(k mean, Is-a-Prerequisite-of, weight sharing)(k mean, Evaluate-for, identifying semantic relationships)(k mean, Part-of, clustering algorithms)(k mean, Conjunction, n-gram based automatic measures)(semantic dependency parsing, Used-for, identifying semantic relationships)(weight sharing, Part-of, neural networks)(variational auto-encoding model, Hyponym-Of, semi-supervised semantic parsing).
(neural machine translation, Compare, statistical machine translation)(neural machine translation, Part-of, deep neural networks)(neural machine translation, Used-for, translation tasks)(neural machine translation, Compare, traditional approaches)(bi-directional LSTMs, Part-of, neural machine translation)(bi-directional LSTMs, Compare, convolutional layers)(convolutional layers, Hyponym-Of, deep learning layers)(deep neural networks, Evaluate-for, state-of-the-art neural machine translation)(state-of-the-art neural machine translation, Hyponym-Of, neural machine translation)(gradient diffusion, Evaluate-for, deep architecture improvement)(linear associative units, Used-for, reducing gradient propagation path)(linear associative units, Compare, LSTM unit)(linear associative units, Compare, GRU)(source sentence syntax, Used-for, improving NMT accuracy)(Sequence-to-Dependency NMT, Hyponym-Of, neural machine translation)(neural machine
(machine translation technique, Used-for, neural machine translation)(neural machine translation, Part-of, machine translation technique)(machine translation technique, Part-of, deep neural networks)(gradient diffusion, Compare, gradient propagation)(encoder RNN, Part-of, neural machine translation)(decoder RNN, Part-of, neural machine translation)(linear associative units, Compare, LSTM unit)(linear associative units, Compare, GRU)(linear associative units, Hyponym-Of, neural network architectures)(convolutional layers, Compare, recurrent networks)(English-Romanian translation, Evaluate-for, translation accuracy)(English-German translation, Evaluate-for, translation accuracy)(English-French translation, Evaluate-for, translation accuracy)(Deep Neural Networks, Hyponym-Of, neural architectures)(bi-directional LSTM, Part-of, neural machine translation)(Parallel RNN encoder, Used-for, neural machine translation)(Hierarchical RNN encoder, Used
(conversation models, Used-for, conversation generation)(conversation dataset, Part-of, conversation models)(latent variable, Used-for, capturing word patterns)(type controller, Used-for, controlling sentence function)(annotator, Used-for, providing explanations)(semantic parser, Used-for, converting explanations into labeling functions)(collocations, Part-of, multilingual corpus)(adjective-noun relation, Part-of, collocations)(verb-object relation, Part-of, collocations)(nominal compounds, Part-of, collocations)(sentence simplification, Used-for, translating complex sentences)(contextualized language models, Used-for, improving NLP tasks)(BERT, Used-for, intent classification)(knowledge distillation, Used-for, transferring knowledge between models)(link prediction, Used-for, improving incomplete knowledge graph)(embedding methods, Used-for, link prediction)(DihEdral, Used-for, knowledge graph embeddings)(multi-lingual corpus, Evaluate-for, collocation identification)(text infilling, Used-for, predicting missing
(python, Part-of, general-purpose programming language)(code generation, Used-for, python)(semantic parsing, Used-for, python)(NL-to-code generation, Used-for, python)(programming language API documentation, Used-for, python)(stack overflow, Used-for, python)(semantic parsing, Is-a-Prerequisite-of, code generation)(NL intents, Used-for, code generation)(pre-trained language models, Used-for, code generation)(visual models, Used-for, code generation)(data augmentation, Used-for, code generation)(retrieval-based data re-sampling, Used-for, code generation)(VQA, Is-a-Prerequisite-of, modular code generation)(BLEU score, Evaluate-for, code generation accuracy).
(latent dirichlet allocation, Part-of, Topical PageRank)(Topical PageRank, Compare, Salience Rank)(Salience Rank, Used-for, extracting keyphrases)(named entities, Used-for, improving quality of discovered topics)(named entities, Is-a-Prerequisite-of, domain-specific terms for news-centric content)(latent dirichlet allocation, Compare, neural topic models)(Bidirectional Adversarial Topic model, Used-for, neural topic modeling)(Bidirectional Adversarial Topic model, Compare, Gaussian-BAT)(Bidirectional Adversarial Topic model, Is-a-Prerequisite-of, Bidirectional Adversarial Topic model with Gaussian)(Gaussian-BAT, Used-for, text clustering)(Gaussian-BAT, Evaluate-for, more coherent topics)(Bidirectional Adversarial Topic model, Evaluate-for, semantic patterns from texts)(generator, Part-of, Bidirectional Adversarial Topic model)(encoder, Part-of
(word embedding, Used-for, caption generation)(word embedding, Used-for, word analogy questions)(word embedding, Part-of, Skip-Gram model)(Skip-Gram model, Is-a-Prerequisite-of, additive compositionality)(word embedding, Part-of, Sufficient Dimensionality Reduction)(word embedding, Used-for, aspect extraction)(word embedding, Part-of, bilingual word embeddings)(word embedding, Used-for, neural word segmentation)(word embedding, Used-for, deep neural networks)(word embedding, Part-of, dependency parsing)(word embedding, Used-for, neural network-based joint models)(word embedding, Part-of, Chinese word segmentation)(word embedding, Part-of, POS tagging)(word embedding, Part-of, Minimal Recursion Semantics)(word embedding, Used-for, semantic parsing)(word embedding, Used-for, Mild Cognitive Impairment detection)(word embedding, Part-of, hypernym prediction)(word embedding, Used-for, synset induction)(word embedding, Part-of, Multi
(multi-space variational encoder-decoders, Hyponym-Of, labeled sequence transduction)(neural networks, Used-for, handling discrete and continuous latent variables)(generative model, Part-of, multi-space variational encoder-decoders)(cold-start problem, Evaluate-for, review spam detection)(bilingual word embeddings, Evaluate-for, learning bilingual resources)(relation detection, Used-for, Knowledge Base Question Answering)(deep residual bidirectional LSTMs, Hyponym-Of, hierarchical recurrent neural network)(GuessTwo, Is-a-Prerequisite-of, learning common entities and their attributes)(attention-based sequence learning model, Used-for, automatic question generation)
(reading comprehension, Used-for, answering questions)(reading comprehension, Used-for, understanding natural texts)(gated self-matching networks, Used-for, reading comprehension)(SQuAD dataset, Evaluate-for, reading comprehension)(attention-over-attention reader, Used-for, cloze-style reading comprehension)(natural-language understanding, Is-a-Prerequisite-of, reading comprehension)(GuessTwo, Used-for, learning about common entities)(prerequisite skills, Evaluate-for, reading comprehension datasets)(readability, Evaluate-for, reading comprehension datasets)(question generation, Used-for, reading comprehension)(TriviaQA, Used-for, reading comprehension)(open-domain question answering, Used-for, reading comprehension)(stochastic answer network, Used-for, multi-step reasoning in reading comprehension)(hierarchical attention network, Used-for, reading comprehension)(multi-layer recurrent neural network, Used-for, machine reading)(paragraph selector, Used-for, denoising paragraphs).
None
(log-linear model, Part-of, neural machine translation)(log-linear model, Used-for, representing prior knowledge sources as features)(prior knowledge sources, Part-of, neural machine translation)(neural network, Conjunction, log-linear model)(log-linear model, Used-for, guiding learning processing)(semantic role labeling, Part-of, neural machine translation)(RoBERTa, Evaluate-for, strong baseline)(linear decomposition, Used-for, interpret neural networks)(ReLU-activated Transformer, Hyponym-Of, neural networks)(linear model, Part-of, linear decomposition)(machine translation, Evaluate-for, sentiment classification)(error analysis, Used-for, applications with examples).
(deep q-network, Is-a-Prerequisite-of, reinforcement learning)(deep q-network, Part-of, deep learning)(deep q-network, Is-a-Prerequisite-of, neural network)(neural network, Part-of, deep q-network)(neural network, Used-for, sentence summarization)(neural network, Used-for, geolocation prediction)(neural network, Used-for, question answering)(attention mechanism, Part-of, sentence summarization)(attention mechanism, Part-of, geolocation prediction)(deep learning, Used-for, question answering)(deep learning, Used-for, machine comprehension)(deep learning, Part-of, neural network)(recurrent neural network, Hyponym-Of, neural network)(recurrent neural network, Used-for, sentence encoder)(recurrent neural network, Used-for, decoder)(encoder, Part-of, sentence summarization)(decoder, Part-of, sentence summarization)(language model, Used-for, text generation)
(highway network, Part-of, multi-layer recurrent highway network)(multi-layer recurrent highway network, Used-for, speech perception model)(highway network, Used-for, controlling useful neighbourhood expansion in GCN)(highway network, Is-a-Prerequisite-of, effective geolocation with GCN)(multilingual neural machine translation, Evaluate-for, accuracy improvements on low-resource languages)(conditional text generation, Part-of, Natural Language Generation)(conditional text generation, Used-for, controlling properties of generated content)(text generation module, Part-of, PPVAE)(condition representation module, Part-of, PPVAE)(PPVAE, Used-for, flexible conditional text generation)(sequential matching network, Compare, gated self-matching networks)(sequential matching network, Used-for, response selection in retrieval-based chatbots)(sequential matching network, Evaluate-for, outperforming state-of-the-art methods)(pointer networks, Used-for, locating answers in passages)(BERTScore, Compare, n-gram-based metrics
(Word-embedding models, Used-for, Caption generation)(Word-embedding models, Used-for, Word analogy questions)(Additive compositionality, Hyponym-Of, Compositionality)(Skip-Gram model, Hyponym-Of, Word-embedding models)(Skip-Gram model, Shows, Additive compositionality)(Distributional word embeddings, Used-for, Object naming)(Sequence-to-sequence neural models, Used-for, Chinese poems generation)(Attention mechanism, Part-of, Sequence-to-sequence neural models)(Object naming, Is-a-Prerequisite-of, Referring expression generation)(Sequence-to-Action, Hyponym-Of, Semantic parsing)(Multi-lingual neural relation extraction framework, Part-of, Relation extraction).
(heuristic search, Used-for, AMR-to-text generation)(heuristic search, Part-of, extraction algorithm)(heuristic extraction algorithm, Used-for, learning graph-to-string rules)(graph transducer, Used-for, collapsing input AMRs)(graph transducer, Used-for, generating output sentences)(benchmark, Evaluate-for, state-of-the-art result)(neural architecture search, Hyponym-Of, heuristic search)(neural architecture search, Part-of, neural architecture search system)(recurrent neural networks, Part-of, recurrent neural language modeling)(recurrent neural language modeling, Evaluate-for, strong baseline)(coNLL NER tasks, Is-a-Prerequisite-of, transferability)(word embeddings, Used-for, clustering forms by cell and paradigm)(string similarity, Used-for, clustering forms by cell and paradigm)(heuristic benchmark, Used-for, paradigm discovery problem)(argument structure constructions, Is-a-Prerequisite-of, Transformer-based language models)(psych
(context-sensitive grammar, Compare, synchronous context-free grammar)(context-sensitive grammar, Compare, synchronous tree-adjoining grammar)(context-sensitive grammar, Part-of, mildly context-sensitive grammars)(mildly context-sensitive grammars, Part-of, grammar induction)(context-sensitive grammar, Used-for, unsupervised discontinuous parsing)(probabilistic linear context-free rewriting system, Hyponym-Of, context-sensitive grammar)(context-sensitive grammar, Used-for, parameter learning)(context-sensitive grammar, Used-for, parsing)(mildly context-sensitive grammars, Compare, linear indexed grammars)(context-sensitive grammar, Used-for, maximum likelihood)(context-sensitive grammar, Compare, tree-adjoining grammars)(probabilistic context-free grammar, Part-of, context-sensitive grammar).
(machine translation, Used-for, translating high-level textual descriptions to formal representations in technical documentation)(machine translation, Used-for, generating translations from left to right as a linear sequence)(machine translation, Used-for, translating messages between agents in multiagent systems)(neural machine translation, Hyponym-of, machine translation)(recurrent networks, Used-for, encoding the source sentence in machine translation)(bi-directional LSTMs, Part-of, neural machine translation)(convolutional layers, Part-of, neural machine translation)(Deep Neural Networks, Hyponym-Of, neural machine translation)(linear associative units, Part-of, neural machine translation)(BLEU scores, Evaluate-for, machine translation)(syntactic trees, Used-for, improving neural machine translation)(syntactic encoders, Part-of, neural machine translation)(Sequence-to-Dependency Neural Machine Translation, Part-of, neural machine translation)(grammatical error correction, Hyponym-of, machine translation)(GEC, Used-for, correcting global
(activation function, Used-for, modeling memory processes)(modeling memory processes, Is-a-Prerequisite-of, measuring conceptual text complexity)(semantic composition, Compare, negation function)(representational similarity analysis, Evaluate-for, neural activation patterns)(semantic composition, Hyponym-Of, Representational Similarity Analysis)(simple word-embedding-based models, Compare, word-embedding-based RNN/CNN models)(morpho-syntactic features, Compare, lexical features)(differentially private generation method, Used-for, generate realistic user utterances)(decode fMRI patterns, Used-for, study semantic composition)(tree kernels, Evaluate-for, neural activation patterns)(hierarchical pooling, Hyponym-Of, pooling strategies)(task-oriented dialogue systems, Used-for, assist users)(social media writing style, Evaluate-for, socio-economic status)(morpho-syntactic features, Evaluate-for, predict stylistic variation)(simple word-embedding-based models,
(word embeddings, Hyponym-Of, vector semantics)(semantic relevance, Used-for, vector semantics)(distributional semantic models, Hyponym-Of, vector semantics)(Delta Embedding Learning, Used-for, vector semantics)(Pseudofit, Used-for, vector semantics).
(earley parsing, Compare, transition-based parsing)(dependency graph, Used-for, semantic dependency parsing)(Maximum Subgraph algorithms, Used-for, generate noncrossing graphs)(Rhetorical Structure Theory (RST) tree, Used-for, representing text structure)(neural encoder-decoder transition-based parser, Used-for, minimal recursion semantics)(token-based sequence tagging, Compare, token-based dependency parsing).
(labeled sequence transduction, Part-of, semi supervised learning)(multi-space variational encoder-decoders, Used-for, semi supervised learning)(semi supervised question answering, Part-of, semi supervised learning)(Generative Domain-Adaptive Nets, Used-for, semi supervised learning)(data-intensive techniques, Compare, Hybrid Code Networks)(Hybrid Code Networks, Used-for, dialog systems)(Neural network, Conjunction, regular expressions)(Neural network, Evaluate-for, spoken language understanding)(goal-acts, Evaluate-for, specific locations)(recursive neural network, Used-for, domain adaptation)(imitation and reinforcement approach, Used-for, grounded language learning).
(parsing, Is-a-Prerequisite-of, AMR generation)(parsing, Is-a-Prerequisite-of, question answering)(dependency parsing, Part-of, parsing)(token-based dependency parsing, Hyponym-Of, parsing)(semantic parsing, Hyponym-Of, parsing)(CCG parsing, Hyponym-Of, parsing)(neural parsing, Hyponym-Of, parsing)(parsing, Used-for, computational argumentation mining)(parsing, Used-for, generating text markup)(parsing, Used-for, semantic role labeling)(neural architecture, Used-for, parsing)(deep neural networks, Used-for, parsing)(dependency parsing, Compare, sequence tagging)(BiLSTM, Used-for, parsing)(sequence transduction, Used-for, parsing)(kernel methods, Used-for, parsing)(syntactic encoders, Used-for, parsing)(semantic dependency parsing, Hyponym-Of, parsing)(event extraction, Used-for, parsing)(constituency parsing
(convolutional neural network, Used-for, text categorization)(convolutional neural network, Used-for, zero pronoun resolution)(convolutional neural network, Used-for, local coherence model)(text categorization, Evaluate-for, word-level CNN)(deep pyramid CNN, Hyponym-Of, convolutional neural network)(word-level CNN, Compare, character-level CNN)(convolutional neural network, Part-of, deep pyramid CNN)(convolutional neural network, Part-of, local coherence model)(local coherence model, Used-for, entity grid representation)(convolutional neural network, Compare, recurrent neural network)(picturebook embeddings, Used-for, image search)(image search, Part-of, picturebook)(doubly-attentive decoder, Used-for, Neural Machine Translation)(picturebook, Hyponym-Of, convolutional neural network)(document dating, Used-for, Graph Convolutional Network)(convolutional neural network, Part-of, Graph Convolutional Network)(
(sentence boundary recognition, Part-of, segmental neural language model)(segmental neural language model, Used-for, word segmentation)(segmental neural language model, Compare, character LSTM models)(segmental neural language model, Compare, nonparametric Bayesian word segmentation models)(segmental neural language model, Is-a-Prerequisite-of, word discovery)(segmentation models, Part-of, word segmentation)(word discovery, Part-of, segmental neural language model)(visual context, Used-for, conditioning the model)(unconditional model, Compare, character LSTM models)(unconditional model, Evaluate-for, predictive distributions)(word meanings, Part-of, representations of nonlinguistic modalities)(language modeling, Part-of, segmental neural language model)(visual context, Used-for, improving performance).
(topic modeling, Used-for, understanding large collections of text)(topic modeling, Compare, syntactic textual relations)(topic modeling, Compare, word embedding models)(topic modeling, Part-of, aspect-based sentiment analysis)(interactive topic modeling, Hyponym-Of, topic modeling)(Tandem Anchors, Hyponym-Of, interactive topic modeling)(Tandem Anchors, Compare, single word anchor algorithms)(Tandem Anchors, Is-a-Prerequisite-of, fast and intuitive topic modeling)(coherent aspects, Evaluate-for, aspect extraction)(neural approach, Used-for, discovering coherent aspects)(attention mechanism, Used-for, de-emphasizing irrelevant words)(word embedding models, Used-for, improving coherence of aspects)(document modeling, Used-for, sentence extraction)(external information, Used-for, improving document modeling)(aspect extraction, Part-of, aspect-based sentiment analysis).
None
(response selection, Part-of, retrieval-based chatbots)(sequential matching network, Used-for, response selection)(multi-turn conversation, Part-of, retrieval-based chatbots)(position rank, Is-a-Prerequisite-of, keyphrase extraction)(tagging scheme, Conjunction, end-to-end models)(Open Information Extraction, Used-for, generating semi-structured knowledge)(Open IE, Used-for, question answering)(joint extraction of entities and relations, Conjunction, end-to-end models)(joint extraction task, Part-of, information extraction)(context-aware neural network model, Used-for, temporal information extraction)(information retrieval, Part-of, text classification)(n-grams overlap, Hyponym-Of, text similarity measures)(skip-grams overlap, Hyponym-Of, text similarity measures)(empirical study, Evaluate-for, response selection)(recurrent neural network, Part-of, sequential matching network)(grid-type recurrent neural networks, Part-of, Japanese PAS analysis)(convolution and pooling operations,
(weakly-supervised learning, Compare, supervised learning)(weakly-supervised learning, Compare, distant supervised learning)(weakly-supervised learning, Hyponym-Of, semi-supervised learning)(weakly-supervised learning, Evaluate-for, cross-lingual NER)(weakly-supervised learning, Evaluate-for, temporal relation classification)(weakly-supervised learning, Evaluate-for, discourse attachment)(weakly-supervised learning, Evaluate-for, spatio-temporally grounding natural sentence in video)(weakly-supervised learning, Evaluate-for, CTA transcript parsing)(RNN, Used-for, dialog systems)(Hybrid Code Networks, Part-of, dialog systems)(Hybrid Code Networks, Compare, end-to-end learning)(Hybrid Code Networks, Compare, current techniques)(Hybrid Code Networks, Evaluate-for, supervised learning)(Hybrid Code Networks, Evaluate-for, reinforcement learning)(named entity recognition, Evaluate-for, high accuracy)(named entity recognition,
(parser, Used-for, natural language understanding)(neural architecture, Used-for, semantic parsing)(dynamic programming, Used-for, projective dependency parsing)(integer linear programming, Used-for, multi-document summarization)(integer linear programming, Used-for, event coreference resolution)(programming language, Compare, natural language)(Snorkel framework, Hyponym-Of, data programming)(dynamic programming, Part-of, constituency parsing)(dynamic programming, Used-for, projective dependency parsing)(dynamic programming, Used-for, training model)(linear programming, Compare, integer linear programming)(code generation, Evaluate-for, natural language descriptions)(semantic parsing, Is-a-Prerequisite-of, event coreference resolution)(constituency parsing, Is-a-Prerequisite-of, dependency parsing)(data programming, Used-for, learning discourse structure).
(expert system, Evaluate-for, Natural Language Processing)(expert system, Compare, neural architecture)(neural architecture, Used-for, incorporating bilingual dictionaries)(graph neural networks, Used-for, incorporating multiple gazetteers)(LID, Used-for, processing multilingual text)(language classifiers, Evaluate-for, minority dialect speakers)(cycled reinforcement learning, Used-for, training on unpaired data)(sequence-to-sequence model, Used-for, LID)(model, Part-of, Natural Language Processing)(temporal relations, Evaluate-for, temporal reasoning)(semantic parsing, Is-a-Prerequisite-of, producing meaning representations)(evaluation in NLP, Evaluate-for, comparing system scores)(emotional support, Evaluate-for, conversation scenarios)(emotional support, Compare, sentiment translation)(sentiment-to-sentiment translation, Used-for, changing underlying sentiment of a sentence)(bilingual dictionaries, Used-for, neural machine translation)(AggGen, Used-for, neural data-to-text systems)(Helping Skills Theory, Part-of, ESC
(machine learning resource, Used-for, named entity recognition)(named entity recognition, Part-of, natural language processing)(natural language processing, Is-a-Prerequisite-of, text classification)(text classification, Compare, named entity recognition)(named entity recognition, Compare, cross-lingual NER)(cross-lingual NER, Used-for, named entity recognition)(named entity recognition, Used-for, sentiment analysis)(machine learning resource, Part-of, machine learning system)(machine learning system, Used-for, natural language inference)(machine learning resource, Used-for, automatic machine learning)(automatic machine learning, Used-for, knowledge discovery)(machine learning resource, Used-for, machine comprehension)(machine comprehension, Compare, sentiment analysis)(sentiment analysis, Part-of, natural language processing)(machine learning resource, Used-for, knowledge representation)(knowledge representation, Is-a-Prerequisite-of, automated reasoning)(machine learning resource, Used-for, training robust machine learning systems)(labeled documents, Used-for
(unlexicalized parsing, Part-of, neural encoder-decoder parser)(neural encoder-decoder parser, Used-for, semantic graph parsing)(semantic graph parsing, Evaluate-for, Minimal Recursion Semantics)(semantic graph parsing, Evaluate-for, Abstract Meaning Representation)(unlexicalized parsing, Used-for, token alignments)(unlexicalized predicates, Part-of, semantic graph parsing)(stack-based embedding features, Part-of, neural encoder-decoder parser)(Relational-Realizational grammar, Compare, lexicalized grammars)(topics, Part-of, Opinionated Natural Language Generation)(sentiments, Part-of, Opinionated Natural Language Generation)(neural model, Evaluate-for, WebNLG corpus)(grammar-based parser, Compare, neural encoder-decoder parser)(Referring Expression Generation, Hyponym-Of, Natural Language Generation)(bilexical dependencies, Part-of, L-PCFGs).
(data structure, Compare, algorithm)(argumentation dataset, Used-for, analyzing the content)(argumentation dataset, Used-for, analyzing the structure)(argumentation dataset, Used-for, analyzing the linguistic features)(argumentation dataset, Evaluate-for, debate outcome)(argumentation dataset, Evaluate-for, user traits)(user traits, Is-a-Prerequisite-of, debate outcome)(argumentation text, Part-of, debate)(linguistic features, Part-of, argumentation text)(structure, Part-of, argumentation text)(content, Part-of, argumentation text)(participant profiles, Conjunction, debates)(participant profiles, Evaluate-for, argument outcome)(data structure, Used-for, organizing participant profiles)(data structure, Used-for, storing debate data)(algorithm, Used-for, processing argumentation dataset)
(logistic regression, Used-for, PTLM probing)(logistic regression, Evaluate-for, harmful content quantification)(logistic regression, Used-for, template analysis)(human mind, Is-a-Prerequisite-of, mental processes)(continuous-time deconvolutional regressive neural network, Compare, continuous-time deconvolutional regression)(continuous-time deconvolutional regressive neural network, Part-of, deep neural extension)(continuous-time deconvolutional regressive neural network, Used-for, time-varying influence capture)(continuous-time deconvolutional regressive neural network, Used-for, non-linear influence capture)(continuous-time deconvolutional regressive neural network, Used-for, delayed influence capture)(continuous-time deconvolutional regression, Used-for, predictor influence analysis)(time-varying influence, Part-of, predictor influence analysis)(non-linear influence, Part-of, predictor influence analysis)(delayed influence, Part-of, predictor influence analysis)
(sequence classification, Part-of, natural language understanding)(sequence classification, Compare, span extraction)(sequence classification, Compare, span extraction)(sequence classification, Hyponym-Of, downstream tasks)(span extraction, Part-of, natural language understanding)(span extraction, Hyponym-Of, downstream tasks)(Multilingual Pre-trained Machine Reader, Used-for, natural language understanding)(Multilingual Pre-trained Machine Reader, Part-of, multilingual machine reading comprehension-style pre-training)(multilingual pre-trained language models, Part-of, multilingual natural language understanding)(cross-lingual generalization, Evaluate-for, source-language fine-tuning data)(Multilingual Pre-trained Machine Reader, Used-for, cross-lingual span extraction)(Multilingual Pre-trained Machine Reader, Used-for, sequence classification)(cross-lingual span extraction, Compare, sequence classification)
(Keyphrase boundary classification, Compare, scientific article summarization)(Keyphrase boundary classification, Used-for, detecting keyphrases)(detecting keyphrases, Part-of, Keyphrase boundary classification)(Keyphrase boundary classification, Used-for, labelling keyphrases)(labelling keyphrases, Part-of, Keyphrase boundary classification)(deep recurrent neural networks, Evaluate-for, multi-task learning)(semantic super-sense tagging, Conjunction, identification of multi-word expressions)(SummaReranker, Evaluate-for, selecting the best summary candidate)(Transformer, Used-for, enhancing copy mechanism)(degree centrality, Evaluate-for, identifying importance of source words)(copy module, Part-of, abstractive summarization models)(local self-attention, Evaluate-for, handling long-span dependencies)(explicit content selection, Evaluate-for, handling long-span dependencies)(sequence-to-sequence network, Used-for, text summarization)(critical role dungeons and dragons dataset, Used-for,
(deep learning introduction, Is-a-Prerequisite-of, adversarial domain adaptation)(deep learning introduction, Is-a-Prerequisite-of, data augmentation)(deep learning introduction, Is-a-Prerequisite-of, encoder-decoder model)(deep learning introduction, Is-a-Prerequisite-of, neural machine translation)(deep learning introduction, Is-a-Prerequisite-of, active learning)(adversarial domain adaptation, Used-for, text generation)(data augmentation, Used-for, deep learning methods)(encoder-decoder model, Used-for, text generation)(neural machine translation, Used-for, translation tasks)(active learning, Used-for, Named Entity Recognition)(data augmentation, Evaluate-for, deep learning methods)(adversarial domain adaptation, Evaluate-for, text generation)(neural machine translation, Evaluate-for, machine translation data sets)(text generation, Part-of, abstractive summarization model)(Named Entity Recognition, Part-of, information extraction)(graph neural network, Used-for, extraction model)(language model,
(neural network, Compare, regular expression)(regular expression, Used-for, improving supervised learning)(regular expression, Hyponym-Of, linguistic expression)(structured regex, Compare, prior datasets)(structured regex, Part-of, regex synthesis dataset)(probabilistic grammar, Used-for, generating complex regexes)(natural language descriptions, Evaluate-for, regex synthesis dataset)(crowdworkers, Used-for, generating diverse regex descriptions)(multimodal synthesis techniques, Evaluate-for, regex synthesis dataset)(regex generation, Is-a-Prerequisite-of, question generation)(regex, Compare, multiword expressions).
(morphology, Part-of, syntax)(semantics, Part-of, syntax)(machine translation, Used-for, dependency parsing)(machine translation, Used-for, semantic role labeling)(machine translation, Used-for, language modeling)(machine translation, Hyponym-Of, sequence-to-sequence transduction)(sequence-to-sequence transduction, Part-of, language processing applications)(neural network models, Evaluate-for, systematic generalization)(neural decoders, Used-for, lexical translation)(sequence modeling tasks, Evaluate-for, systematic generalization)(neural models, Used-for, sequence modeling tasks)(lexical ambiguity, Part-of, Machine Translation)(semantic biases, Used-for, translation errors)(polysemous nature of words, Used-for, semantic biases)(DiBiMT, Evaluate-for, semantic biases)(Transformer, Evaluate-for, compositional generalization)(consistency regularization training, Evaluate-for, compositional generalization)(representation consistency, Part-of, compositional generalization)(prediction consistency,
(Vector space representations of words, Part-of, word embeddings)(Vector space representations of words, Compare, sentence embeddings)(spectral clustering, Used-for, word embeddings)(synonyms, Compare, antonyms)(word embeddings, Part-of, sentence embeddings)(Multi-document summarization, Used-for, story clustering)(Multi-document summarization, Used-for, presentation of search results)(Multi-document summarization, Used-for, timeline generation)(Multi-document summarization, Is-a-Prerequisite-of, training supervised models)(Multi-document summarization, Hyponym-Of, clustering)(Wikipedia Current Events Portal, Used-for, building dataset)(transformer-based language models, Evaluate-for, universal sentence encoders)(deep metric learning, Evaluate-for, unsupervised textual representations)(autoencoding topic model with a mixture prior, Used-for, EnsLM)(autoencoding topic model with a mixture prior, Used-for, clustering)(EnsLM, Evaluate-for, shared knowledge among clusters)(noise learning methods, Compare, loss
(language modeling, Used-for, generating rhythmic poetry)(language modeling, Used-for, generating coherent poetry)(language modeling, Used-for, learning feature representations)(language modeling, Used-for, document context incorporation)(language modeling, Used-for, generating sentences)(language modeling, Used-for, fixed-size ordinally forgetting encoding)(language modeling, Used-for, hierarchical LSTM with caching)(language modeling, Is-a-Prerequisite-of, code generation)(language modeling, Is-a-Prerequisite-of, semantic parsing)(language modeling, Part-of, Recurrent Neural Networks)(language modeling, Used-for, evaluating model uncertainty)
(sentence representation, Used-for, semantic parsing)(sentence representation, Used-for, neural machine translation)(sequence-to-sequence models, Used-for, sentence representation)(convolutional neural networks, Evaluate-for, sentence representation)(recurrent neural networks, Evaluate-for, sentence representation)(predicate-argument structures, Part-of, sentence representation)(vector space models, Evaluate-for, sentence representation)(local coherence model, Evaluate-for, sentence representation)(Abstract Meaning Representation, Used-for, sentence representation)(Minimal Recursion Semantics, Used-for, sentence representation)(Stack-based embedding features, Evaluate-for, sentence representation)(transition-based parser, Evaluate-for, sentence representation)(bidirectional LSTM, Compare, convolutional layers)
(text mining, Conjunction, knowledge acquisition)(text mining, Conjunction, joint inference)(text mining, Used-for, evaluating semantic fluency responses)(sequence-to-sequence models, Compare, transformer models)(sequence-to-sequence models, Part-of, text mining)(AMR parsing, Conjunction, AMR generation)(AMR parsing, Hyponym-Of, text mining)(AMR generation, Hyponym-Of, text mining)(knowledge acquisition, Hyponym-Of, text mining)(joint inference, Hyponym-Of, text mining)(semantic fluency responses, Is-a-Prerequisite-of, neurological examination)(learning commonsense knowledge, Part-of, text mining)(semantic parsing models, Hyponym-Of, text mining)(classifier uncertainty, Evaluate-for, predicting native language from gaze fixations)(multimodal dataset, Part-of, learning tasks)(regularization method, Used-for, improving perplexity)(systematicity, Evaluate-for, linguistic signs)(rein
(semantic similarity, Used-for, paraphrase detection)(semantic similarity, Used-for, textual entailment recognition)(semantic similarity, Used-for, ranking relevance)(semantic similarity, Evaluate-for, document similarity)(semantic similarity, Evaluate-for, sentence clustering)(semantic similarity, Evaluate-for, adversarial examples)(semantic similarity, Evaluate-for, generation model)(similarity measures, Part-of, text similarity measures)(n-grams, Hyponym-Of, text similarity measures)(skip-grams, Hyponym-Of, text similarity measures)(TextFlow, Hyponym-Of, text similarity measures)(TextFlow, Used-for, compute similarity value)(semantic relevance, Compare, semantic similarity)(gated attention encoder, Used-for, source text representation)(sentence embeddings, Used-for, evaluate text)(multilingual embeddings, Hyponym-Of, embeddings)(semantic similarity prediction, Used-for, learned embeddings evaluation)(style-sensitive word embeddings, Hyponym-Of, word embeddings)(pseudo-sense,
(OSDM, Hyponym-Of, kernel graphical models)(Term ambiguity, Part-of, short text clustering)(CompareNet, Used-for, fake news detection)(OSDM, Used-for, short text stream clustering)(News content, Compare, knowledge base)(News document, Compare, trusted document)(News representation, Part-of, CompareNet)
None
(dirichlet processes, Part-of, hierarchical dirichlet process)(hierarchical dirichlet process, Used-for, learning multiple topic-sensitive representations)(conditional V-information, Used-for, rationale evaluation)(REV, Hyponym-Of, rationale evaluation)(REV, Evaluate-for, rationale-label pairs)
(Restricted non-monotonicity, Evaluate-for, projective arc-eager dependency parser)(non-monotonic system, Used-for, erroneous actions exploration)(dynamic oracle, Part-of, Covington parser)(Jensen-Shannon divergence, Used-for, dependency measurement)(skip-gram with negative sampling, Used-for, low-dimensional approximation)(named entity recognition, Is-a-Prerequisite-of, sequence prediction tasks)(machine translation, Is-a-Prerequisite-of, sequence prediction tasks)(continuous relaxation, Used-for, differentiable approximation)(Entity Equalization, Part-of, coreference resolution)(BERT embeddings, Used-for, coreference resolution)(semantic dependency parsing, Used-for, identifying semantic relationships)(second-order parsing, Used-for, interactions between pairs of edges)(mean field approximation, Used-for, second-order parsing)(loopy belief propagation, Used-for, second-order parsing)(neural pathways, Hyponym-Of, functional components)(Word Alignment Error Rate, Used-for,
(belief propagation, Used-for, mean field variational inference)(loopy belief propagation, Compare, mean field variational inference)(event coreference, Used-for, trigger detection)(event coreference, Used-for, event anaphoricity)(propagation trees, Used-for, rumor detection)(layer-wise relevance propagation, Used-for, neural machine translation)(layer-wise relevance propagation, Used-for, style classifier)(error propagation, Part-of, pipeline models)(error propagation, Used-for, performance limiting factor)(neural networks, Used-for, layer-wise relevance propagation)(joint models, Compare, pipeline models)(dependency parsing, Used-for, Chinese word segmentation)(dependency parsing, Used-for, POS tagging)(goal-acts, Used-for, learning activities for specific locations)(propagation Tree Kernel, Used-for, rumor detection)(recursive neural models, Used-for, rumor representation learning)(recursive neural models, Used-for, rumor classification)(Neural Open IE, Used-for, extracting relation tuples)(Neural
(morphologically rich languages, Part-of, language understanding systems)(lexical relations, Hyponym-Of, language understanding systems)(morph-fitting procedure, Used-for, improving distributional vector spaces)(dialogue state tracking, Evaluate-for, morph-fitted vectors)(Abstractive Sentence Summarization, Part-of, natural language processing)(sentence summarization, Hyponym-Of, natural language processing)(low-frequency word estimates, Used-for, boosting semantic quality)(aspect-based sentiment analysis, Hyponym-Of, sentiment classification)(neural networks, Used-for, language understanding tasks)(topic models, Evaluate-for, document retrieval)(sequence-to-sequence models, Used-for, keyphrase generation)(Variational Auto-Encoders, Used-for, natural language generation)(syntactic spaces, Part-of, Variational Auto-Encoders)(semantic spaces, Part-of, Variational Auto-Encoders)(dense representations, Used-for, document retrieval tasks)(neural topic models, Compare,
(gaussian graphical model, Compare, traditional independent word representation)(gaussian graphical model, Compare, new graphical model)(new graphical model, Used-for, short text clustering)(new graphical model, Part-of, Online Semantic-enhanced Dirichlet Model)(term ambiguity, Hyponym-Of, independent word representation)(OSDM, Used-for, short text stream clustering)(context, Part-of, online semantic-enhanced Dirichlet Model)(short text streams, Conjunction, sparse data representation)(short text streams, Conjunction, cluster evolution)(short text streams, Conjunction, infinite length).
(kkt condition, Part-of, optimization algorithm)(stochastic iterative algorithm, Is-a-Prerequisite-of, optimization algorithm)(derived segment scores, Hyponym-Of, SCRFs)(SCRFs, Hyponym-Of, CRFs)(CRFs, Is-a-Prerequisite-of, SCRFs)(joint inference framework, Part-of, integer linear programming)(bilingual embeddings, Evaluate-for, multilingual word similarity)(bilingual word embeddings, Compare, cross-lingual word embeddings)(pretraining strategy, Used-for, encoder-decoder model)(margin-based loss, Compare, cross entropy log-loss)(offline methods, Compare, joint learning)(isomorphism assumption, Part-of, cross-lingual word embeddings)(persona descriptions, Part-of, personalized dialogue models)(target-language decoder, Part-of, latent variable MMT)(non-goal oriented dialog agents, Hyponym-Of, chatbots)(question-answer pairs, Hyponym-Of, QA model)(annotation scheme, Part
(adversarial perturbation, Used-for, text classification)(neural machine translation, Is-a-Prerequisite-of, natural language processing)(LSTM-based models, Hyponym-Of, neural machine translation)(Transformer-based models, Hyponym-Of, neural machine translation)(graph neural network, Used-for, relation extraction)(multi-hop reasoning mechanism, Used-for, discovering accurate relations)(A* search algorithm, Used-for, parsing)(MGbank, Part-of, bi-LSTM neural network supertagger)(CompoundE, Used-for, knowledge graph embedding)(translation operations, Part-of, CompoundE)(rotation operations, Part-of, CompoundE)(scaling operations, Part-of, CompoundE)(link prediction, Part-of, KG prediction tasks)(path query answering, Part-of, KG prediction tasks)(entity typing, Part-of, KG prediction tasks)(scoring function design, Evaluate-for, KGC model performance)(loss function design, Evaluate-for, KGC model performance)(message passing
(markov random fields, Compare, conditional random fields)(conditional random fields, Part-of, semi-Markov conditional random fields)(semi-Markov conditional random fields, Used-for, sequence labeling)(conditional random fields, Used-for, sequence labeling)(conditional random fields, Part-of, state-of-the-art sequence labeling model architectures)(SeqVAT, Used-for, sequence labeling)(virtual adversarial training, Used-for, improving model robustness)(word-level labels, Used-for, segment scores)(weighted finite-state machines, Part-of, NLP systems)(key innovation, Part-of, paper)(word-level information, Conjunction, segment-level information)(africaPOS, Part-of, POS dataset)(POS, Hyponym-Of, part-of-speech)(masked language models, Compare, markov random fields)(segment scores, Used-for, SCRFs)(second-order expectations, Used-for, computing covariance matrices).
(singular value decomposition, Used-for, variance decomposition)(multimodal sentiment analysis, Part-of, multimodal information)(modality-invariant representations, Part-of, multimodal information)(modality-specific representations, Part-of, multimodal information)(multimodal information, Used-for, sentiment prediction)(contrastive representation learning, Part-of, ConFEDE)(contrastive feature decomposition, Part-of, ConFEDE)(similarity feature, Part-of, three modalities)(dissimilarity feature, Part-of, three modalities)(three modalities, Part-of, video sample)(text, Part-of, three modalities)(video frames, Part-of, three modalities)(audio, Part-of, three modalities)(bias-variance tradeoff, Evaluate-for, model complexity)(bias-variance tradeoff, Used-for, under-fitting minimization)(bias-variance tradeoff, Used-for, over-fitting minimization)(ensemble methods, Used-for, variance reduction)(neural language models,
(evaluation of dependency parsing, Evaluate-for, dependency parsing results accuracy)(semantic dependency parsing, Evaluate-for, evaluation of dependency parsing)(syntax dependency parsing, Is-a-Prerequisite-of, evaluation of dependency parsing)(dependency parsing models, Evaluate-for, evaluation of dependency parsing)(adversarial examples, Used-for, evaluation of dependency parsing)(multi-task learning, Improve, evaluation of dependency parsing)(multi-task learning, Evaluate-for, dependency parsing results accuracy)(word embeddings, Evaluate-for, dependency parsing results accuracy)(character string embeddings, Evaluate-for, dependency parsing results accuracy)(semantic relations, Part-of, dependency parsing)(POS tagging, Improve, evaluation of dependency parsing)(natural language processing tasks, Part-of, evaluation of dependency parsing)
(Kullback–Leibler divergence, Used-for, variational autoencoder)(Variational autoencoder, Part-of, generative model)(Posterior collapse, Evaluate-for, Kullback–Leibler divergence)(Kullback–Leibler divergence, Used-for, posterior)(Amortized variational inference, Used-for, variational autoencoder)(Deep neural networks, Used-for, variational autoencoder)(Posterior collapse, Part-of, variational autoencoder)(Batch Normalized-VAE, Hyponym-Of, variational autoencoder)(Conditional VAE, Hyponym-Of, variational autoencoder)(Language modeling, Used-for, text classification)(Language modeling, Used-for, dialogue generation)(Batch Normalized-VAE, Is-a-Prerequisite-of, conditional VAE)(Deep neural networks, Part-of, generative model)
(Text similarity measures, Used-for, plagiarism detection)(Text similarity measures, Used-for, information ranking)(Text similarity measures, Used-for, recognition of paraphrases)(Text similarity measures, Used-for, textual entailment)(Deep learning, Evaluate-for, sequential models)(n-grams overlap, Hyponym-Of, text similarity measures)(skip-grams overlap, Hyponym-Of, text similarity measures)(TextFlow, Hyponym-Of, text similarity measures)(TextFlow, Used-for, paraphrase detection)(TextFlow, Used-for, textual entailment recognition)(Multimodal sentiment analysis, Compare, text similarity measures)(LSTM-based model, Used-for, capturing contextual information)(Attention-based recurrent neural network, Used-for, extraction of entity mentions and relations)(Attention mechanism, Used-for, memory networks)(Word embeddings, Used-for, providing semantic information)(Gaussian mixtures, Hyponym-Of, multimodal word distributions)(Natural Language Inference, Used-for, recognizing textual entail
(expectation maximization algorithm, Used-for, parsing)(expectation maximization algorithm, Used-for, language modeling)(generative model, Conjunction, discriminative recognition model)(generative model, Used-for, language modeling)(generative model, Used-for, parsing)(discriminative model, Compare, generative model)(adversarial attacks, Evaluate-for, vulnerability of deep neural networks)(word-level attacking, Hyponym-Of, textual adversarial attacking)(particle swarm optimization algorithm, Used-for, search optimization)(denoising word alignment, Used-for, cross-lingual pre-training)(denoising word alignment, Used-for, predicting aligned tokens)(sequence labeling, Evaluate-for, pseudo-labeled datasets)(matrix product operator, Used-for, pre-trained language model compression)(opinion summarization, Used-for, generating summaries)(annotator group bias, Evaluate-for, annotation tasks)(GroupAnno, Used-for, capturing annotator group bias)(response generation, Evaluate-for, dialogue history
(semantic parser, Used-for, knowledge representation)(predicate-argument structures, Part-of, knowledge representation)(AMR graphs, Part-of, knowledge representation)(Abstract Meaning Representation, Used-for, knowledge representation)(neural language model, Used-for, knowledge representation)(relation detection, Used-for, knowledge representation)(Named Entity Recognition, Used-for, knowledge representation)(entity linking, Part-of, knowledge representation)(geolocation prediction, Used-for, knowledge representation)(neural sequence models, Used-for, knowledge representation)(SQL, Used-for, knowledge representation)(discriminative weighted finite state machine, Compare, generative neural language model)(feedforward neural network, Compare, recurrent neural networks)(sentence encoder, Part-of, selective encoding model)(attention mechanism, Part-of, geolocation prediction model)(translation quality, Evaluate-for, attention-based Neural Machine Translation)(word embeddings, Used-for, context embeddings)(Bidirectional LSTMs, Used-for, dependency path representations)(semantic parsing, Part-of, knowledge representation)
(Bi-directional LSTMs, Compare, Variable elimination)(State-of-the-art results, Compare, Variable elimination)(Parallel state for each word, Part-of, Alternative LSTM structure)(Attention mechanisms, Compare, Variable elimination)(Machine learning techniques, Used-for, Knowledge extraction)(Syntactic Transformations, Used-for, Data augmentation)(PosCal training, Compare, Variable elimination)(Probabilistic grammatical evolution, Used-for, AutoML strategy)(Attention distributions, Used-for, Model predictions)(Neural networks, Compare, Variable elimination)(Edge-enhanced Bayesian Graph Convolutional Network, Compare, Variable elimination)(Backdoor attacks, Compare, Variable elimination)(Abductive reasoning, Compare, Variable elimination).
(Oracle summary, Part-of, Compressive summarization paradigm)(Cognitive structure, Used-for, Predictive model of income)(Adversarial Attention Network, Used-for, Multi-dimensional emotion regression)(Dialog systems, Evaluate-for, Usability)(Dialog systems, Evaluate-for, User satisfaction)(Emotional dialogue system, Is-a-Prerequisite-of, Coherent structure)(CDRNN, Hyponym-Of, Continuous-time deconvolutional regression)(CDRNN, Evaluate-for, Reading time)(Regressions, Evaluate-for, Model update)(Negative flip rate, Evaluate-for, Regression measure)(CM-BART, Used-for, Metaphor generation)(S2G, Used-for, Interpretable operation trees)(Event schemas, Part-of, World knowledge)(IncSchema, Hyponym-Of, IncPrompt)(IncSchema, Evaluate-for, Temporal relations)(IncSchema, Evaluate-for, Hierarchical relations)
None
(None, <relation>, None).(None, <relation>, None)(triplets:("unsupervised learning",  "Compare", "supervised learning")(supervised learning, "Used-for", "multi-space variational encoder-decoders")(supervised learning, "Used-for", "Hybrid Code Networks (HCNs)")(supervised learning, "Used-for", "active learning")(supervised learning, "Used-for", "error detection")(HCNs, "Hyponym-Of", "unsupervised learning")("Hybrid Code Networks (HCNs)",  "Used-for", "dialog system")("unsupervised learning",  "Part-of", "language resources")("unsupervised learning",  "Compare", "supervised learning")(supervised learning, "Used-for", "HCNs")(supervised learning, "Used-for", "active learning")(supervised learning, "Used-for", "error detection")
(word embedding variation, Part-of, word embeddings)(word embedding variation, Compare, Gaussian mixtures)(Gaussian mixtures, Part-of, multimodal word distributions)(multimodal word distributions, Used-for, modeling multiple word meanings)(word embedding variation, Compare, diachronic word embeddings)(word embeddings, Hyponym-Of, fastText)(fastText, Compare, Probabilistic FastText)(Probabilistic FastText, Part-of, word embedding variation)(word embedding variation, Compare, adversarial training)(adversarial training, Part-of, cross-lingual word embeddings)
(naive bayes, Compare, neural machine translation)(neural machine translation, Exhibits, biases)(biases, Part-of, word embeddings)(word embeddings, Demonstrate, discriminative biases)(discriminative biases, Part-of, gender bias)(gender bias, Addressed-by, debiasing method)(debiasing method, Used-for, removing stereotypical gender biases)(discriminative biases, Evaluate-for, downstream NLP applications)(gender-related information, Preserved-by, debiasing method)(expertise style transfer, Conjunction, text simplification)(expertise style transfer, Improved-by, models with expert intelligence)(neural machine translation, Improved-by, minimum Bayes risk decoding)(beam search, Exhibits, length and token frequency bias)(compositional generalization, Improved-by, transformer models)(transformer models, Have, inductive biases)(pretrained language models, Encode, semantic content)(compositional generalization, Conjunction, human
(None)
(sequence to sequence, Compare, Statistical Machine Translation)(sequence to sequence, Compare, Tree-based Neural Machine Translation)(sequence to sequence, Used-for, AMR parsing)(sequence to sequence, Used-for, AMR generation)(sequence to sequence, Used-for, abstractive text summarization)(sequence to sequence, Used-for, video captioning)(sequence to sequence, Used-for, translation accuracy improvement)(sequence to sequence, Part-of, Neural Machine Translation)(sequence to sequence, Used-for, Chinese poem generation)(sequence to sequence, Used-for, text simplification)(sequence to sequence, Used-for, neural multi-source learning)(sequence to sequence, Used-for, named entity recognition)(sequence to sequence, Used-for, grammatical error correction)(sequence to sequence, Used-for, semantic parsing)(sequence to sequence, Used-for, dialogue systems)(sequence to sequence, Used-for, extractive summarization)(neural machine translation, Uses, Parallel RNN encoder)(neural machine translation, Uses, Hierarchical R
(visual qa, Hyponym-Of, visual question answering)(attention mechanisms, Used-for, visual question answering)(paragraph-style image captions, Used-for, visual question answering)(multi-grained attention, Used-for, visual question answering)(visual and textual question answering, Hyponym-Of, visual question answering)(catastrophic forgetting, Evaluate-for, visual question answering)(visually grounded neural syntax learner, Part-of, visual question answering)(paragraph-style image captions, Evaluate-for, visual question answering)(dynamic spatial memory network, Used-for, visual question answering)(dynamic spatial memory network, Part-of, visual question answering)(multi-modal neural machine translation, Hyponym-Of, neural machine translation)(show-and-fool, Evaluate-for, image captioning)
(deep neural network, Used-for, bilingual text embeddings)(deep neural network, Used-for, image description retrieval)(canonical correlation analysis, Part-of, PCCA)(canonical correlation analysis, Is-a-Prerequisite-of, generalized Canonical Correlation Analysis)(Partial Canonical Correlation Analysis, Hyponym-Of, canonical correlation analysis)(DPCCA, Hyponym-Of, Partial Canonical Correlation Analysis)(stochastic iterative algorithm, Used-for, DPCCA)(Canonical Correlation Analysis, Used-for, aligning word vectors)(Kernel CCA, Hyponym-Of, Canonical Correlation Analysis)(meta-embeddings, Used-for, Semantic Textual Similarity)(meta-embeddings, Used-for, STS Benchmark)(topic coherence metrics, Evaluate-for, topic models).
(neural belief tracking, Used-for, dialogue systems)(belief tracker, Part-of, dialogue systems)(Tandem Anchors, Compare, single word anchor)(Anchor methods, Used-for, topic identification)(Anchor methods, Compare, sampling-based interactive topic modeling)(representation learning, Used-for, neural belief tracking)(pre-trained word vectors, Used-for, neural belief tracking)(Riemannian optimization, Used-for, optimizing SGNS objective)(Bi-LSTM, Used-for, Chinese implicit discourse relations)(Cross-lingual model transfer, Used-for, low-resource language predictions)(neural paragraph-level question answering models, Used-for, document-level input)(Stochastic Gradient Descent, Compare, AllVec)(word2vec, Part-of, word embedding models)(negative sampling, Used-for, word representation learning)(active learning, Used-for, semantic parsing)(knowledge distillation, Used-for, transferring knowledge)(tree-structured neural topic model, Compare, prior
(markov chain, Hyponym-Of, semi-Markov chain)(semi-Markov chain, Hyponym-Of, SCRF)(SCRF, Hyponym-Of, hybrid semi-Markov CRF)(CRF, Hyponym-Of, SCRF)(CRF, Part-of, neural sequence labeling)(SCRF, Part-of, neural sequence labeling)(markov chain, Part-of, MemSum)(MemSum, Used-for, text summarization)(markov chain, Used-for, monolingual word alignment)(MemSum, Is-a-Prerequisite-of, high quality summaries)(text summarization, Evaluate-for, ROUGE performance)(CRF, Used-for, word-level labels)(SCRF, Used-for, segment-level labels)(hybrid semi-Markov CRF, Used-for, neural sequence labeling)(word alignment, Used-for, text-to-text generation)(text-to-text generation, Part-of, monolingual word alignment)(VisText, Evaluate-for
None
(autoencoders, Hyponym-Of, variational autoencoders)(variational autoencoders, Compare, Wasserstein autoencoders)(variational autoencoders, Part-of, syntax-infused variational autoencoder)(syntax-infused variational autoencoder, Used-for, text generation)(text generation, Evaluate-for, generative superiority)(syntax-infused variational autoencoder, Used-for, unsupervised paraphrasing)(unsupervised paraphrasing, Hyponym-Of, paraphrase generation)(paraphrase generation, Is-a-Prerequisite-of, natural language inference)(conditional VAE, Hyponym-Of, variational autoencoders)(Batch Normalized-VAE, Hyponym-Of, variational autoencoders)(Coupled-VAE, Hyponym-Of, variational autoencoders)(encoder, Part-of, autoencoders)(decoder, Part-of, autoencoders).
(None)
(manifold learning, Compare, deep neural networks)(manifold learning, Evaluate-for, state-of-the-art accuracy)(manifold learning, Used-for, embedding methods)(manifold learning, Used-for, a la carte embedding)(deep neural networks, Compare, kernel methods)(deep neural networks, Part-of, transfer learning)(Question answering, Evaluate-for, SQuAD)(Question answering, Used-for, understanding natural texts)(SQuAD, Evaluate-for, RC problem)(Pyramid scores, Part-of, multi-document summarization)(genetic algorithm, Used-for, training data generation)(Nystrom low-rank approximation, Part-of, pre-training deep architecture)(Pyramid scores, Used-for, optimization-based extractive summarization)(crowdsourcing, Evaluate-for, paraphrase correctness)(crowdsourcing, Evaluate-for, paraphrase grammaticality)(crowdsourcing, Evaluate-for, linguistic diversity)(embedding methods, Compare, SWEM)(embedding methods, Conjunction, Order Embeddings)(Order Emb
(latent variable grammars, Hyponym-Of, mixture models)(Gaussian mixture density, Part-of, Probabilistic FastText)(Gaussian mixtures, Part-of, Gaussian Mixture LVeGs)(Gaussian Mixture LVeGs, Hyponym-Of, latent vector grammars)(Gaussian Mixtures, Used-for, LVeGs)(Latent Vector Grammars, Hyponym-Of, latent variable grammars)(Gaussian Mixtures, Used-for, capturing different word senses)(mixture component, Part-of, Gaussian mixture density)(Gaussian Mixture Density, Used-for, representing multiple word senses).
(meta-learning, Compare, multi-task learning)(meta-learning, Used-for, extract common features)(meta-learning, Is-a-Prerequisite-of, fine-tuning models)(adversarial multi-task learning, Compare, meta-learning)(adversarial multi-task learning, Improve, shared feature extraction)(off-the-shelf knowledge, Hyponym-Of, shared knowledge)(adversarial multi-task learning, Used-for, alleviate interference)(BiLSTMs, Compare, dependency parsing)(BiLSTMs, Improve, classification performance)(multi-criteria learning, Compare, single-criterion learning)(multi-criteria learning, Improve, Chinese word segmentation)(question generation, Conjunction, entailment generation)(adversarial multi-task learning, Compare, multi-task learning)(conceptual parsing, Conjunction, semantic parsing)(conceptual parsing, Improve, argumentation mining)(dual learning game, Improve, semantic parser performance)(semantic super-sense tagging, Conjunction, identification of multi-word expressions)(multi-task
(None)
(tools for deep learning, Used-for, labeled sequence transduction)(tools for deep learning, Provides, variational encoder-decoders)(variational encoder-decoders, Part-of, labeled sequence transduction)(Hybrid Code Networks, Used-for, dialog systems)(Hybrid Code Networks, Combine, recurrent neural networks and domain-specific knowledge)(interactive topic models, Compare, anchor methods)(Tandem Anchors, Used-for, interactive topic modeling)(Tandem Anchors, Improve, sampling-based interactive topic modeling)(LSTM Noisy Channel Model, Used-for, disfluency detection)(LSTM Noisy Channel Model, Uses, Long Short-Term Memory language model)(Seq2Seq model, Used-for, slot filling)(pointer network, Part-of, Seq2Seq model)(Seq2Seq model, Evaluate-for, out-of-vocabulary problem)(Seq2Seq model, Used-for, spoken language understanding system)(machine translation, Compare, pretrained NMT model and multilingual joint training)(sphereRE,
None
(support vector machine, Used-for, MCI)(support vector machine, Compare, LSTM-based recurrent neural network)(MCI, Used-for, binary classification task)(complex networks, Used-for, automatically identify MCI in transcripts)(complex networks, Conjunction, word embedding)(word embedding, Used-for, better represent short texts)(word embedding, Hyponym-Of, CNE)(Bag of Words, Compare, complex networks enriched with embedding)(transcriptions, Part-of, Cinderella dataset)(abugida, Part-of, Brahmic family)(Thai, Hyponym-Of, Brahmic family)(Burmese, Hyponym-Of, Brahmic family)(Khmer, Hyponym-Of, Brahmic family)(Lao, Hyponym-Of, Brahmic family)(support vector machine, Evaluate-for, recovery performance)(transcripts, Part-of, neuropsychological assessments)(linguistic features, Hyponym-
(multi-agent system, Used-for, report generation)(co-operative multi-agent system, Part-of, report generation framework)(multi-agent self-play environment, Hyponym-Of, multi-agent system)(co-operative multi-agent system, Hyponym-Of, multi-agent system)(multi-agent system, Used-for, language learning)(multi-agent system, Used-for, communication with humans)(Transformer model, Used-for, neural word alignment).
(first-order logic, Part-of, chain reasoning paradigm)(chain reasoning paradigm, Used-for, document-level event argument extraction)(first-order logic, Used-for, decomposable logical rules)(neural encoders, Used-for, dependency parsers)(dependency parsers, Part-of, natural language processing)(higher-order features, Compare, first-order features)(first-order features, Used-for, pruning PLMs)(movement pruning, Hyponym-Of, first-order features)(Static Model Pruning (SMP), Hyponym-Of, first-order pruning)(TECHS, Used-for, extrapolation reasoning on TKGs)(temporal graph encoder, Part-of, TECHS)(logical decoder, Part-of, TECHS)(graph convolutional network, Part-of, temporal graph encoder)(temporal encoding, Part-of, temporal graph encoder)(heterogeneous attention, Part-of, temporal graph encoder)(propositional reasoning, Part-of, logical decoder)(first-order reasoning, Part-of, logical decoder)(
(cross entropy, Used-for, extractive summarization)(cross entropy, Part-of, objective function)(cross entropy, Used-for, knowledge distillation)(cross entropy, Used-for, structured prediction)(cross entropy, Compare, negative sampling)(knowledge distillation, Used-for, transferring knowledge between models)(extractive summarization, Evaluate-for, global context at the document level)(auxiliary pre-training tasks, Used-for, capturing document-level context)(CNN/DM dataset, Used-for, validating auxiliary pre-training tasks)(structured prediction, Part-of, knowledge distillation)(Bregman divergence, Used-for, interpreting cross-entropy and negative sampling)(token-level adaptive training, Evaluate-for, machine translation)(OOD detection, Used-for, task-oriented dialog system)(supervised contrastive learning, Used-for, learning discriminative semantic features)(autoregressive language models, Evaluate-for, forward cross-entropy)(MixCE, Evaluate-for, text generation quality)
None
(semantic role labeling, Used-for, recognizing predicate-argument structure)(semantic role labeling, Part-of, natural language understanding)(semantic role labeling, Compare, syntactic parsers)(semantic role labeling, Part-of, question-answer driven Semantic Role Labeling)(question-answer driven Semantic Role Labeling, Part-of, semantic role labeling)(question-answer driven Semantic Role Labeling, Hyponym-Of, semantic role labeling)(QA-SRL Bank 2.0, Part-of, question-answer driven Semantic Role Labeling)(QA-SRL Bank 2.0, Used-for, annotating semantic roles)(semantic role labeling, Compare, character-level models)(character-level models, Part-of, various SRL models)(semantic role labeling, evaluate-for, cross-lingual SRL)(cross-lingual SRL, Hyponym-Of, semantic role labeling)(semantic role labeling, Used-for, fact checking)(syntactic information, Used-for
(Neural Networks, Used-for, text classification)(Convolutional Neural Network, Hyponym-Of, Neural Networks)(Convolutional Neural Network, Used-for, text classification)(Visual Character Embedding, Evaluate-for, text classification)(Bi-LSTM, Used-for, relation extraction)(Bi-LSTM, Used-for, text classification)(ULMFiT, Used-for, text classification)(Explanation Methods, Evaluate-for, DNNs)(Text Deconvolution Saliency, Evaluate-for, text classification)(Active Learning Policy, Evaluate-for, text classification)(String Kernels, Used-for, text classification)(Bag-of-Super-Word-Embeddings, Conjunction, String Kernels)(String Kernels, Evaluate-for, automatic essay scoring)(Adversarial Regularization, Evaluate-for, text classification)(Adversarial Regularization, Evaluate-for, NMT models)(Structured Attention, Evaluate-for, text classification)(Structured Attention, Evaluate-for, discourse structure)(Adversarial Attacks,
(word sense disambiguation, Hyponym-Of, NLP system)(word sense disambiguation, Used-for, semantic disambiguation)(word sense disambiguation, Used-for, topic categorization)(word sense disambiguation, Used-for, polarity detection)(word sense disambiguation, Used-for, Named Entity Disambiguation)(word sense disambiguation, Used-for, Multimodal Named Entity Disambiguation)(word sense disambiguation, Used-for, prepositional disambiguation)(word sense disambiguation, Used-for, possessive disambiguation)(word sense disambiguation, Hyponym-Of, morphological disambiguation)(word sense disambiguation, Used-for, pronoun disambiguation)(word sense disambiguation, Used-for, commonsense reasoning)(sense inventory, Part-of, multilingual sense-annotated resource)(sense inventory, Used-for, Word Sense Disambiguation)(parallel corpora, Used-for, cross-lingual Word
(question classification, Compare, semantic parsing)(group sparse autoencoders, Used-for, question representation)(group sparse CNNs, Used-for, question representation)(semantic parsing, Used-for, transducing natural language utterances into machine executable meaning representations)(reranking, Is-a-Prerequisite-of, improve neural semantic parser)(reasoning, Used-for, database queries)(join, Part-of, database queries)(filtering, Part-of, database queries)(aggregation, Part-of, database queries)(WikiNLDB, Used-for, exploring database-style queries)(modular architecture, Used-for, database-style queries)
(labelling task, Part-of, multi-label classification)(multi-label classification, Hyponym-Of, classification)(imitation learning, Used-for, accomplishing BabySteps)(curriculum-based reinforcement learning, Used-for, maximizing rewards on navigation tasks)(autonomy, Part-of, self-determination theory)(instruction-guided agents, Used-for, comprehending complex instructions)(BabyWalk, Used-for, following long instructions)(emotion label semantics, Used-for, guiding model’s attention)(BabySteps, Part-of, BabyWalk)(high-level sub-tasks, Evaluate-for, demonstration parsing)(label embeddings, Used-for, tracking label-label correlations)(ternary classification task, Hyponym-Of, classification task)(autonomous agents, Used-for, vision-and-language navigation)(satisfaction of basic needs, Is-a-Prerequisite-of, self-determination)(training instances, Part-of, weakly supervised approach)(named high-level sub-tasks, Used
(finite state machine, Used-for, part-of-speech tagging)(finite state machine, Used-for, speech recognition)(finite state machine, Used-for, noisy channel models)(finite state machine, Used-for, morphological analysis)(finite state machine, Used-for, incremental building of complex words)(finite state machine, Part-of, finite state morphological analyzer)(weighted finite state transducers, Hyponym-Of, finite state machine)(weighted finite-state machines, Hyponym-Of, finite state machine)(finite state machine, Used-for, computing derivatives)(GPU implementation, Is-a-Prerequisite-of, FST composition operation)(bigram hashing, Used-for, document retrieval)(TF-IDF matching, Used-for, document retrieval)(multi-layer recurrent neural network model, Used-for, machine comprehension of text)(attention mechanism, Used-for, dependency information matching)(Wikipedia, Used-for, open-domain question answering)(neural paraphrasing model, Evaluate-for, syntactic transformations)
(neural parsing, Used-for, computational argumentation mining)(computational argumentation mining, Conjunction, neural techniques)(dependency parsing, Compare, sequence tagging)(dependency parsing, Part-of, neural parsing)(BiLSTMs, Compare, dependency parsing)(BiLSTMs, Used-for, long-range dependencies)(sequence tagging, Part-of, neural parsing)(sequence tagging, Used-for, computational argumentation mining)(multitask learning, Used-for, neural parsing)(multitask learning, Used-for, computational argumentation mining)(dependency parsing, Compare, semantic dependency parsing)(semantic dependency parsing, Part-of, neural parsing)(semantic dependency parsing, Part-of, parsing sentences)(parsing sentences, Part-of, neural parsing)(semantic dependency parsing, Used-for, semantic graph generation)(Maximum Subgraph algorithms, Used-for, noncrossing graphs)(Lagrangian Relaxation-based algorithm, Used-for, combine pages)(bilexical dependencies,
(statistical part of speech tagging, Compare, neural networks)(statistical part of speech tagging, Compare, recurrent neural networks)(neural networks, Part-of, statistical part of speech tagging)(statistical part of speech tagging, Compare, recurrent character-based representation)(statistical part of speech tagging, Compare, dynamically and pre-trained word embeddings)(statistical part of speech tagging, Compare, FSTs)(recurrent neural networks, Part-of, word or sub-word information)(recurrent neural networks, Used-for, sentence-level context)(word embeddings, Evaluate-for, part-of-speech tagging)(recurrent neural networks, Compare, neural networks)(recurrent character-based representation, Used-for, word embeddings)(FSTs, Used-for, part-of-speech tagging)(FSTs, Used-for, speech recognition)(FastText, Compare, BPEmb)(FastText, Compare, BERT)(BPEmb, Compare, BERT)(OSCAR corpus, Used-for, training mon
(language processing models, Compare, neural models)(neural models, Compare, deep neural networks)(deep neural networks, Part-of, neural machine translation)(neural machine translation, Used-for, grammatical error correction)(neural models, Part-of, dialog response generation)(neural models, Part-of, semantic dependency parsing)(neural models, Part-of, neural multimodal machine translation)(neural models, Part-of, language models)(language models, Used-for, resolving pronominal anaphora)(neural models, Compare, seq2seq models)(neural machine translation, Used-for, reducing word omission errors)(language processing models, Part-of, document classifiers)(document classifiers, Evaluate-for, changes in time)(nested attention mechanism, Used-for, correcting local errors)(ips algorithm, Hyponym-Of, iterative predicate selection algorithm)(contrastive learning, Used-for, reducing word omission errors)(multitask learning, Used-for, training IPS model)
(dynamic programming, Part-of, constituency parsing)(dynamic programming, Part-of, dependency parsing)(dynamic programming, Part-of, extractive multi-document summarization)(constituency parsing, Used-for, constituency parsing)(constituency parsing, Evaluate-for, Penn Treebank)(joint optimization, Conjunction, active learning)(joint optimization, Used-for, extractive multi-document summarization)(active learning, Used-for, extractive multi-document summarization)(scoring model, Used-for, extractive multi-document summarization)(Integer Linear Programming, Used-for, sentence compression)(syntactic features, Part-of, sentence compression)(syntactic constraints, Part-of, sentence compression)(dynamic programming, Part-of, Integer Linear Programming)(event coreference resolution, Evaluate-for, KBP 2016)(event coreference resolution, Evaluate-for, KBP 2017)(temporal and causal relations, Part-of, natural language understanding)(temporal relations, Conjunction, causal relations)(joint inference framework, Used-for
(text to speech generation, Used-for, generating speech mel-spectrograms)(text to speech generation, Encounter, over-smoothing problem)(over-smoothing problem, Part-of, speech mel-spectrogram generation)(NAR-TTS models, Encounter, over-smoothing problem)(NAR-TTS models, Incorporate, additional condition inputs)(additional condition inputs, Alleviate, over-smoothing problem)(advanced modeling methods, Reduce, over-smoothness)(GAN, Achieve, best voice quality)(Glow, Achieve, best voice quality)(voice quality, Alleviate, over-smoothing problem)(speech mel-spectrogram, Part-of, text to speech generation).
(word segmentation, Part-of, natural language processing)(neural word segmentation, Used-for, word embeddings)(statistical segmentation, Compare, neural word segmentation)(modular segmentation model, Used-for, neural word segmentation)(external training sources, Used-for, neural word segmentation)(adversarial multi-criteria learning, Used-for, Chinese word segmentation)(CWS, Part-of, Chinese word segmentation)(Bayesian model, Evaluate-for, text segmentation)(prosodic boundary, Evaluate-for, word segmentation)(data-driven sub-word units, Evaluate-for, morphological segmentation)(character CNN, Evaluate-for, word segmentation)(lattice-structured LSTM model, Evaluate-for, Chinese NER)(subword units, Used-for, neural machine translation)(derived word generation, Used-for, word segmentation).
(query expansion, Used-for, question answering)(query expansion, Used-for, query understanding)(query expansion, Used-for, taxonomy construction)(Operation Trees, Part-of, intermediate representation)(intermediate representation, Used-for, question answering)(OTTA, Compare, Spider)(OTTA, Compare, LC-QuaD 2.0)(OTTA, Used-for, semantic parsing)(morphological family, Evaluate-for, social media)(MFEP, Hyponym-Of, morphological family)(entity set expansion, Used-for, question answering)(entity set expansion, Used-for, query understanding)(entity set expansion, Used-for, taxonomy construction)(entity set expansion, Part-of, AVE)(knowledge-driven query expansion, Hyponym-Of, query expansion)(QA-based AVE, Used-for, attribute value extraction)(ACK-DEF, Part-of, dialogue generation)(ACK-DEF, Used-for, addressing hallucination problem)
(noisy channel model, Used-for, disfluency detection)(noisy channel model, Used-for, few-shot text classification)(noisy channel model, Part-of, LSTM Noisy Channel Model)(LSTM Noisy Channel Model, Used-for, disfluency detection)(LSTM language model, Used-for, scoring fluent sentences)(LSTM language model, Part-of, MaxEnt reranker)(MaxEnt reranker, Used-for, identifying plausible analysis)(weighted finite-state machines, Part-of, NLP systems)(weighted finite-state machines, Used-for, computing higher-order derivatives)(higher-order derivatives, Used-for, second-order expectations)(second-order expectations, Used-for, covariance matrices)(second-order expectations, Used-for, gradients of first-order expectations)(few-shot learning methods, Part-of, language model prompting)(prompt tuning, Hyponym-Of, channel prompt tuning)(channel models, Compare, direct models)(channel models, Used-for, language model prompting)
(answer rationales, Used-for, solving algebraic word problems)(answer rationales, Part-of, explanation process)(indirect supervision, Evaluate-for, program learning)(program learning, Is-a-Prerequisite-of, solving algebraic word problems)(Integer Linear Programming, Is-a-Prerequisite-of, summarization paradigm)(ROUGE, Evaluate-for, summarization systems)(compressive summarization, Compare, extractive summarization)(Quaternion algebra, Used-for, neural models)(neural models, Used-for, NLP tasks)(Quaternion Transformer, Hyponym-Of, Quaternion neural architectures)(Quaternion Transformer, Part-of, NLP models)(Expression-Pointer Transformer with Explanations, Is-a-Prerequisite-of, explainability)(plausibility, Part-of, explanation)(faithfulness, Part-of, explanation)(Expression-Pointer Transformer with Explanations, Used-for, algebraic word problem solving)(PEN dataset, Part-of, model evaluation)
(chomsky hierarchy, Compare, task hierarchy)(task hierarchy, Part-of, multi-task learning)(parallel architectures, Conjunction, serial architectures)(natural language understanding, Hyponym-Of, ATIS)(natural language understanding, Hyponym-Of, Snips)(International Classification of Diseases, Used-for, ICD coding)(Code Hierarchy, Part-of, ICD coding)(Code Co-occurrence, Part-of, ICD coding)(Hierarchical Text Classification, Evaluate-for, text-label semantics)(hierarchy-aware label semantics matching network, Evaluate-for, hierarchical information)(hierarchy-aware label semantics matching network, Used-for, hierarchical text classification)(HRQ-VAE, Evaluate-for, syntactic diversity)(HRQ-VAE, Used-for, paraphrase generation)(hierarchical duality learning, Part-of, HDLD)(token hierarchy, Part-of, HDLD)(utterance hierarchy, Part-of, HDLD)(mutual information, Used-for, hierarchical dualities).
(named entity recognition, Used-for, identifying named entities)(named entity recognition, Part-of, NLP tasks)(named entity recognition, Compare, mention detection)(named entity recognition, Evaluate-for, multilingual learning)(named entity recognition, Evaluate-for, weakly supervised cross-lingual approaches)(named entity recognition, Evaluate-for, local detection approach)(named entity recognition, Evaluate-for, collaborative training with other NER techniques)(mention detection, Compare, named entity recognition)(multilingual learning, Used-for, improving NER across multiple languages)(multilingual learning, Evaluate-for, symmetric KL divergence-based data selection strategy)(crowdsourcing, Used-for, named entity recognition data annotation)(crowdsourcing, Used-for, markable identification)(Hidden Markov Model variant, Used-for, aggregating sequential crowd labels)(Long Short Term Memory, Used-for, sequence prediction in unannotated text)(SynTime, Used-for, recognizing time expressions)(FOFE, Used-for, local detection approach
(dependency parsing, Is-a-Prerequisite-of, dependency syntax)(dependency parsing, Part-of, computational argumentation mining)(dependency parsing, Used-for, sequence tagging)(BiLSTM, Used-for, dependency parsing)(dependency trees, Part-of, dependency parsing)(non-monotonic transition system, Used-for, dependency parsing)(dynamic oracle, Used-for, dependency parsing)(word embeddings, Used-for, dependency parsing)(character embeddings, Used-for, dependency parsing)(attention mechanisms, Used-for, dependency parsing)(neural networks, Used-for, joint extraction)(neural networks, Used-for, dependency parsing)(stack-pointer networks, Used-for, dependency parsing)(Universal Dependencies, Part-of, dependency parsing)(semantic dependency graph formalisms, Part-of, dependency parsing)(Lagrangian Relaxation, Used-for, noncrossing graphs)(Maximum Subgraph algorithms, Used-for, dependency graph).
(shift-reduce parsing, Compare, novel constituency parsing scheme)(novel constituency parsing scheme, Used-for, predicting syntactic distance)(novel constituency parsing scheme, Compare, traditional shift-reduce parsing schemes)(syntactic distance, Used-for, determining the topology of grammar tree)(shift-reduce parsing, Compare, sequence-to-sequence constituent parsing)(dynamically-revised oracle procedure, Used-for, training the shift-reduce parser)(unsupervised data augmentation, Used-for, enhancing rhetorical relation recognition)(top-down tree linearizations, Hyponym-Of, tree linearizations)(in-order linearization, Used-for, achieving accuracy in sequence-to-sequence constituent parsing)(deterministic attention mechanisms, Used-for, matching the speed of transition-based parsers)
(latent semantic indexing, Part-of, semantic textual similarity tasks)(semantic textual similarity tasks, Evaluate-for, semantic dependency annotations)(semantic dependency annotations, Part-of, syntactic dependency annotations)(semantic dependency annotations, Used-for, graph-structured representations)(graph-structured representations, Part-of, syntactic dependency annotations)(syntactic dependency annotations, Used-for, syntactic parser)(syntactic parser, Used-for, graph structures)(graph structures, Part-of, low-rank subspace)(low-rank subspace, Used-for, word vectors)(word vectors, Part-of, sentence)(sentence, Part-of, natural language)(natural language, Used-for, downstream applications)(downstream applications, Evaluate-for, neural network models)(neural network models, Evaluate-for, skip-thought vectors)(low-rank subspace, Evaluate-for, sentences)(sentences, Is-a-Prerequisite-of, downstream applications)(low-rank subspace, Hyponym-Of, sentences).
None
(policy gradient method, Used-for, summarization model)(summarization model, Used-for, summarization)(summarization, Evaluate-for, CNN/Daily Mail dataset)(summarization, Evaluate-for, DUC-2002 dataset)(policy gradient method, Used-for, constituency parsers)(constituency parsers, Evaluate-for, F1 metric)(policy gradient method, Used-for, coreference resolution model)(coreference resolution model, Evaluate-for, English OntoNotes v5.0 benchmark)(policy gradient method, Used-for, historical text normalization)(historical text normalization, Evaluate-for, exact matches)(policy gradient method, Used-for, Semantic Dependency Parsing)(Semantic Dependency Parsing, Evaluate-for, SemEval 2015 Task 18 datasets)(policy gradient method, Used-for, visual question answering performance)(visual question answering performance, Evaluate-for, VQA v2 challenge)(policy gradient method, Used-for, active learning policy training)(active learning policy training, Evaluate-for, cross
(Cross-lingual text classification, Hyponym-Of, nlp for the humanity)(model distillation, Hyponym-Of, Cross-lingual text classification)(adversarial feature adaptation, Hyponym-Of, Cross-lingual text classification)(Context-Aware Rule Injection, Hyponym-Of, nlp for the humanity)(formality style transfer, Part-of, Context-Aware Rule Injection)(BERT-based encoder and decoder model, Part-of, Context-Aware Rule Injection)(GYAFC dataset, Evaluate-for, formality style transfer)(sentiment classification, Evaluate-for, Context-Aware Rule Injection)(Misgendered, Hyponym-Of, nlp for the humanity)(gender-neutral pronouns, Part-of, Misgendered)(neo-pronouns, Part-of, Misgendered)(Winogender, Hyponym-Of, nlp for the humanity)(BiasNLI, Hyponym-Of, nlp for
(kernel, Used-for, language learning)(kernel, Used-for, inference tasks)(kernel, Hyponym-Of, expressive kernels)(expressive kernels, Conjunction, Tree Kernels)(kernel, Compare, deep neural networks)(Nystrom low-rank approximation, Used-for, pre-training input layer)(kernelized neural network, Evaluate-for, three different tasks)(generic word embeddings, Used-for, large-scale generic corpora)(Domain Specific word embeddings, Used-for, domain of interest)(Domain Adapted word embeddings, Part-of, Canonical Correlation Analysis)(string kernels, Used-for, automatic essay scoring)(string kernels, Used-for, text classification)(string kernels, Used-for, aspect identification)(string kernels, Compare, syntactic features)(string kernels, Conjunction, bag-of-super-word-embeddings)(string subsequence kernel, Used-for, n-gram coverage in text generation)(RSA, Used-for, understanding neural models)(Representational Similarity Analysis
(loss function, Part-of, structured prediction)(loss function, Used-for, evaluating an output structure)(loss function, Used-for, coreference resolution)(loss-augmented setting, Part-of, structured prediction)(loss function, Is-a-Prerequisite-of, inconsistency loss function)(coreference measures, Used-for, learning loss functions)(loss function, Evaluate-for, model comparison)(inconsistency loss function, Used-for, penalizing inconsistency)(loss function, Compare, softmax cross-entropy loss function)(loss function, Compare, negative sampling loss function)(cross-entropy loss function, Part-of, token-level adaptive training)(cross-entropy loss function, Compare, adaptive objective).
(image retrieval, Used-for, action recognition)(image retrieval, Hyponym-Of, similarity search)(cross-modal applications, Used-for, image retrieval)(image retrieval, Is-a-Prerequisite-of, image editing)(semantic hashing, Used-for, similarity search)(semantic hashing, Part-of, information retrieval systems)(Information Retrieval, Hyponym-Of, Information Retrieval systems)(neural ranking models, Used-for, similarity search)(neural network mappings, Used-for, retrieval)(vision-language research, Is-a-Prerequisite-of, image retrieval)(vision-language research, Used-for, image annotation).
(parsing evaluation, Used-for, assessing parsing quality)(parsing evaluation, Evaluate-for, syntactic information)(parsing evaluation, Evaluate-for, multilingual model)(sequence-to-tree model, Used-for, parsing evaluation)(multilingual GeoQuery corpus, Part-of, parsing evaluation)(multilingual ATIS corpus, Part-of, parsing evaluation)(sequence-to-sequence learning, Used-for, parsing evaluation)(automatic metrics, Evaluate-for, parsing evaluation)(biased evaluation metrics, Compare, human judgements)(parsing evaluation, Evaluate-for, statistical machine translation)(conditional random fields, Used-for, parsing evaluation)(parsing evaluation, Evaluate-for, grammatical error correction evaluation)(parsing evaluation, Evaluate-for, cross-cultural understanding)(parsing evaluation, Part-of, information extraction)(textual entailment dataset, Evaluate-for, parsing evaluation)(parsing evaluation, Evaluate-for, dependency parser)
(natural language processing intro, Part-of, neural semantic parser)(neural semantic parser, Part-of, semantic parsing)(semantic parsing, Hyponym-Of, natural language processing)(neural semantic parser, Compare, grammar-based parser)(predicate-argument structures, Used-for, semantic parsing)(semantic parsing, Used-for, parsing natural language descriptions into source code)(knowledge bases, Used-for, various natural language processing tasks)(ITransF, Evaluate-for, knowledge base completion)(fixed-size ordinally forgetting encoding, Used-for, entity recognition)(neural networks, Hyponym-Of, recurrent neural networks)(neural networks, Hyponym-Of, convolutional neural networks)(event extraction, Part-of, natural language processing)(EuroSense, Part-of, sense-annotated resource)(parallel corpora, Used-for, event extraction)(SoPa, Used-for, text classification)(SQL generation, Used-for, mapping natural language to SQL queries).
(propositional logic, Compare, compositional logical forms)(propositional logic, Compare, non-compositional behaviour)(supervised methods, Evaluate-for, propositional logic)(semantic parsing, Evaluate-for, propositional logic)(propositional logic, Is-a-Prerequisite-of, semantic parser)(convolutional network, Evaluate-for, propositional logic)(positional encoding, Used-for, neural machine translation tasks)(weight sum operation, Compare, weight concatenation operation)(part-of-speech marking, Compare, propositional logic)(attention mechanism, Used-for, compositional logical forms).
(adversarial search, Used-for, detecting model vulnerabilities)(adversarial search, Part-of, adversarial training)(adversarial search, Used-for, generating adversarial examples)(adversarial search, Used-for, evaluating robustness of models)(adversarial examples, Part-of, adversarial search)(adversarial training, Improve-robustness-of, neural machine translation)(adversarial stability training, Part-of, adversarial training)(cross-lingual word embeddings, Achieve, mapping monolingual embeddings)(adversarial training, Enhance-performance, textual entailment models)(adversarial examples, Used-for, tricking neural classifiers)(adversarial examples, Improve-performance, visual language grounding)(adversarial learning framework, Used-for, why-question answering)(adversarial training, Improve-accuracy, style transfer techniques)(semantic-preserving perturbations, Part-of, detecting model vulnerabilities).
(Pre-trained models, Compare, pre-trained language models (PLMs))(Pre-trained models, Used-for, natural language processing (NLP))(encoder-decoder pre-training, Compare, replaced token detection)(encoder-decoder pre-training, Compare, replaced token denoising)(Generative Adversarial Networks (GANs), Hyponym-Of, pre-trained models)(GAN-style model, Is-a-Prerequisite-of, GanLM)(GAN-style model, Part-of, encoder-decoder pre-training)(GanLM, Used-for, replaced token detection)(GanLM, Used-for, replaced token denoising)(language understanding, Hyponym-Of, natural language processing (NLP))(generation, Hyponym-Of, natural language processing (NLP))(language generation benchmarks, Evaluate-for, GanLM)(Zero-shot stance detection (ZSSD), Hyponym-Of, stance detection)(C-STANCE, Used-for, zero-shot stance detection (ZSSD))(target-based Z
(information extraction, Used-for, relation extraction)(relation extraction, Hyponym-Of, information extraction)(information extraction, Used-for, entity mentions)(relation extraction, Used-for, knowledge base population)(joint extraction, Part-of, information extraction)(joint extraction, Used-for, relations and entities extraction)(information extraction, Used-for, distant supervision)(distant supervision, Used-for, reducing human efforts in building training data)(distant supervision, Compare, manual annotation)(distant supervision framework, Hyponym-Of, multi-lingual neural relation extraction framework)(mono-lingual attention, Part-of, multi-lingual neural relation extraction framework)(cross-lingual attention, Part-of, multi-lingual neural relation extraction framework)(multi-lingual neural relation extraction framework, Evaluate-for, relation extraction improvements)(dynamic transition matrix, Used-for, characterize noise in training data)(curriculum learning method, Used-for, training transition matrix)(curriculum learning method, Hyponym-
None
(training neural network, Used-for, language understanding)(training neural network, Part-of, Neural Symbolic Machine)(logging neural network, Used-for, handling compositionality)(sequence-to-sequence model, Part-of, Neural Symbolic Machine)(Lisp interpreter, Part-of, Neural Symbolic Machine)(Lisp interpreter, Used-for, performing program execution)(reinforcement learning, Used-for, optimizing task reward)(iterative maximum-likelihood training, Conjunction, reinforcement learning)(iterative maximum-likelihood training, Used-for, improving stability)(state-of-the-art, Compare, previous approaches)(WebQuestionsSP, Evaluate-for, performance of Neural Symbolic Machine)(zero pronoun resolution, Part-of, natural language processing)(annotated data, Used-for, zero pronoun resolution)(pseudo training data, Used-for, zero pronoun resolution)(cloze-style reading comprehension, Transferred-to, zero pronoun resolution)(knowledge bases, Used-for, question answering)(cross-att
(speech signal analysis, Used-for, discovering word-like acoustic units)(speech signal analysis, Used-for, learning to extract semantic information from speech)(spoken audio captions, Part-of, speech signal analysis)(spoken language acquisition, Part-of, speech signal analysis)(human language acquisition, Compare, spoken language acquisition)(human language acquisition, Evaluate-for, speech signal analysis)(child-directed speech, Is-a-Prerequisite-of, human language acquisition)(models trained on adult-directed speech, Compare, models trained on child-directed speech)(models trained on adult-directed speech, Evaluate-for, task performance)(models trained on child-directed speech, Evaluate-for, initial stages of learning)(linguistic properties, Compare, acoustic properties)(linguistic properties, Affect, task performance)(synthetic speech, Compare, child-directed speech)(synthetic speech, Compare, adult-directed speech).
(arguable unit type classification, Part-of, factor graph model)(argumentative relation prediction, Part-of, factor graph model)(factor graph model, Used-for, argument mining)(SVM parametrizations, Conjunction, RNN parametrizations)(transitivity, Part-of, structure constraints)(dependencies between adjacent relations and propositions, Part-of, factor graph model)(factor graph model, Evaluate-for, argumentative essay datasets)(factor graph model, Evaluate-for, web comments dataset)
(statistical parsing, Compare, neural parsing)(statistical parsing, Used-for, features extraction)(neural encoder-decoder transition-based parser, Part-of, neural parsing)(neural encoder-decoder transition-based parser, Used-for, predicting graphs)(predicting graphs, Is-a-Prerequisite-of, semantic representation)(stack-based embedding features, Used-for, predicting graphs)(statistical parsing, Used-for, bilexical dependencies)(statistical parsing, Used-for, domain-specific logical forms)(neural parsing, Used-for, Minimal Recursion Semantics)(neural parsing, Evaluate-for, MRS)(Abstract Meaning Representation, Conjunction, Minimal Recursion Semantics)(batch processing, Compare, high-precision grammar-based parser)(batch processing, Evaluate-for, GPU))(
(PageRank, Used-for, ranking)(PageRank, Part-of, Topical PageRank)(Topical PageRank, Used-for, ranking noun phrases)(Topical PageRank, Part-of, latent topic distribution)(latent topic distribution, Part-of, Latent Dirichlet Allocation)(pagerank, Compare, Salience Rank)(Salience Rank, Used-for, extracting keyphrases)(text embedding methods, Compare, conventional text embedding methods)(text embedding methods, Used-for, hyper-documents)(hyperdoc2vec, Evaluate-for, paper classification)(hyperdoc2vec, Evaluate-for, citation recommendation)(Entity linking, Used-for, disambiguating entity mentions)(Entity linking, Used-for, enriching texts with semantics)(Entity linking, Is-a-Prerequisite-of, knowledge bases)(annotation approach, Part-of, Entity linking)(annotation approach, Compare, strong baseline in ranking accuracy)(state-of-the-art transformers, Evaluate-for, natural language understanding)(state-of
(n-gram model, Part-of, Language models)(n-gram model, Compare, LSTM language model)(n-gram model, Compare, neural machine translation models)(n-gram model, Used-for, language model perplexity experiments)(LSTM language model, Hyponym-Of, neural language model)(LSTM language model, Part-of, neural machine translation models)(attention mechanism, Part-of, nested attention layers)(KBLSTM, Hyponym-Of, hybrid neural model)(state-of-the-art neural language models, Compare, character-level language models)(Affect-LM, Is-a-Prerequisite-of, LSTM language model)(Grammatical error correction systems, Evaluate-for, correcting grammatical errors)(Noisy Channel Model, Part-of, LSTM Noisy Channel Model)(sentence selector, Part-of, QA model)(disfluent sentence, Part-of, spontaneous speech)(rumors, Hyponym-Of, tweets), (LSTM, Hyponym
(bagging, Evaluate-for, classification)(bagging, Compare, neural network models)(classification, Used-for, joint extraction of entities and relations)(joint extraction of entities and relations, Evaluate-for, public dataset produced by distant supervision method)(classification, Part-of, natural language processing)(joint extraction of entities and relations, Evaluate-for, suspicious news posts)(classification, Used-for, predicting suspicious news posts)(classification, Compare, aggregation)(bagging, Used-for, increasing performance in natural language processing tasks).
(prosody, Compare, prosodic boundary information)(prosody, Part-of, speech)(prosody, Used-for, parsing disfluent speech)(prosody, Used-for, natural and expressive speech synthesis)(prosody, Used-for, detecting concealed information)(prosodic boundary information, Used-for, word segmentation)(speech, Part-of, corpus)(acoustic-prosodic indicators, Part-of, information concealment detection)(prosody, Is-a-Prerequisite-of, word segmentation in early language acquisition)(pitch and intensity features, Part-of, prosody)(acoustic features, Part-of, prosody)(speech disfluency, Compare, SU boundary)(prosodic features, Part-of, speech)(information concealment, Compare, deception)(parsing, Used-for, spoken dialogue)(prosody, Hyponym-Of, acoustic-prosodic features)(text-to-speech system, Used-for, synthesizing speech)(prosody, Used-for, understanding
(dependency parsing, Hyponym-Of, computational argumentation mining)(dependency parsing, Hyponym-Of, transition-based parser)(dependency parsing, Hyponym-Of, BiLSTM models)(dependency parsing, Used-for, extracting semantic relations)(dependency parsing, Hyponym-Of, Dynamic Oracle)(dependency parsing, Hyponym-Of, Maximum Subgraph algorithms)(dependency parsing, Hyponym-Of, projective arc-eager parser)(dependency parsing, Compare, tree-based LSTM model)(dependency parsing, Used-for, dependency treebank)(dependency parsing, Hyponym-Of, transition system)(dynamic oracle, Hyponym-Of, Covington parser)(Maximum Subgraph algorithms, Used-for, generating noncrossing graphs)(book embedding framework, Part-of, dependency parsing)(non-monotonic transition system, Hyponym-Of, non-projective Covington algorithm)(joint models, Used-for, Chinese word segmentation)(dependency trees, Hyponym
(q learning, Compare, reinforcement learning)(q learning, Is-a-Prerequisite-of, multi-task learning)(neural network models, Used-for, multi-task learning)(multi-task learning, Part-of, text classification)(hybrid code networks, Conjunction, recurrent neural networks)(deep learning model, Used-for, semantic role labeling)(semantic parser, Used-for, mapping natural language utterances into executable programs)(neural machine translation, Part-of, neural sequence-to-sequence models).
(disoucrse analysis, Compare, argument mining)(discourse analysis, Compare, discourse parsing)(discourse analysis, Compare, Rhetorical Structure Theory)(discourse analysis, Part-of, NLP)(discourse analysis, Used-for, revision analysis)(discourse analysis, Used-for, reasoning about coherence)(discourse analysis, Used-for, human scoring rubrics)(discourse analysis, Is-a-Prerequisite-of, RST annotation)(discourse analysis, Is-a-Prerequisite-of, predicate argument structure analysis)(discourse analysis, Used-for, discourse coherence)(discourse analysis, Used-for, identifying long-span dependencies)(discourse analysis, Part-of, Referring Expression Generation)(discourse analysis, Used-for, conversational speech)(discourse analysis, Used-for, understanding discourse structure)(discourse analysis, Used-for, RST Discourse Treebank)(revision analysis, Used-for, automatic revision purpose prediction)
(spoken utterances, Part-of, speech processing)(images, Part-of, speech processing)(multi-layer recurrent highway network, Used-for, speech processing)(temporal nature of spoken speech, Related-to, speech processing)(encoding of semantic aspects, Related-to, speech processing)(encoding of form-related aspects, Related-to, speech processing)(named entity recognition, Related-to, natural language processing)(mention detection, Related-to, natural language processing)(multi-layer recurrent highway network, Used-for, encoding of semantic aspects)(multi-layer recurrent highway network, Used-for, encoding of form-related aspects)(FOFE method, Related-to, named entity recognition)(FFNN, Used-for, named entity recognition)(FFNN, Used-for, mention detection)(language identification, Part-of, natural language processing)(local detection approach, Related-to, named entity recognition)(local detection approach, Related-to, mention detection)(NER, Evaluate-for, CoNLL 2003 NER task)(mention detection, Evaluate-for, Co
(chinese word segmentation, Part-of, chinese nlp)(segmentation criteria, Used-for, chinese word segmentation)(adversarial multi-criteria learning, Used-for, chinese word segmentation)(heterogeneous segmentation criteria, Compare, single-criterion learning)(joint models, Used-for, chinese nlp)(dependency parsing, Part-of, chinese nlp)(pos tagging, Part-of, chinese nlp)(character strings, Used-for, word embeddings)(bi-LSTM models, Used-for, chinese word segmentation)(memory augmented neural model, Used-for, chinese poem generation)(hypernyms, Part-of, taxonomy learning)(transductive learning, Used-for, hypernym prediction)(progressive learning model, Used-for, chinese semantic role labeling)(Chinese SemBank, Part-of, chinese semantic role labeling)(attention-based Bi-LSTM, Used-for, chinese implicit discourse relations)(bidirectional LSTM sequence labeling models, Used-for, word usage errors detection)(HS
(domain adaptation, Hyponym-Of, parse adaptation)(domain adaptation, Used-for, cross-domain tasks)(domain adaptation, Used-for, sentiment analysis)(domain adaptation, Used-for, neural machine translation)(domain adaptation, Hyponym-Of, sentiment domain adaptation)(domain adaptation, Hyponym-Of, neural machine translation adaptation)(domain adaptation, Used-for, named-entity recognition)(domain adaptation, Used-for, bilingual tasks)(domain adaptation, Used-for, parser adaptation)(domain adaptation, Used-for, CCG parsing)(domain adaptation, Used-for, deletion-based LSTM model)(domain adaptation, Used-for, lexicalized NMT model)(domain adaptation, Part-of, transfer learning)(domain adaptation, Part-of, feature learning)(domain adaptation, Is-a-Prerequisite-of, adaptation of sentiment lexicons)(domain adaptation, Used-for, dialog systems)(domain adaptation, Used-for, sentence compression)(domain adaptation, Part-of, a la carte embedding)(a la carte embedding, Is
(deep learning tool, Part-of, deep highway BiLSTM architecture)(deep learning tool, Used-for, semantic role labeling)(semantic role labeling, Evaluate-for, CoNLL 2005 test set)(semantic role labeling, Evaluate-for, CoNLL 2012 test set)(deep learning tool, Used-for, text similarity measures)(deep learning model, Used-for, semantic role labeling)(deep highway BiLSTM architecture, Part-of, deep learning model)(deep model, Conjunction, deep learning tool)(deep learning tool, Used-for, automatic question generation)(deep learning tool, Used-for, sentence scoring)(deep learning tool, Used-for, sentence selection)(deep learning tool, Used-for, fake news detection)(deep learning tool, Used-for, morphological disambiguation)(deep learning tool, Used-for, question answering)(deep learning tool, Used-for, multimodal affective computing)(deep learning tool, Used-for, semantic parsing)(deep learning tool, Used-for, slot filling
(None)
(toolkits for information retrieval, Used-for, volatility prediction)(sentiment analysis, Part-of, volatility prediction)(sentiment analysis, Used-for, forecast volatility)(information retrieval, Part-of, IR term weighting models)(word embeddings, Used-for, IR term weighting models)(market data, Part-of, mainstream approach to forecast market risk)(fusion methods, Used-for, combine text and market data resources)(word embedding-based approach, Compare, state-of-the-art methods)(reports of the companies in different financial sectors, Evaluate-for, characteristics)(dependency parsing, Used-for, constructing dependency treebank)(dependency treebank, Part-of, Universal Dependencies scheme)(neural network model, Used-for, training state-of-the-art parser)(English syntactic knowledge, Used-for, training state-of-the-art parser)(AliMe Chat, Is-a-Prerequisite-of, open-domain chatbot engine)(information retrieval, Conjunction, Sequence to Sequence based generation models)(attentive
(None)
(question answering, Is-a-Prerequisite-of, reading comprehension)(question answering, Used-for, generating answer)(question answering, Part-of, end-to-end neural network model)(question answering, Part-of, knowledge base)(question answering, Used-for, natural answer generation)(question answering, Evaluate-for, SQuAD dataset)(question answering, Evaluate-for, WikiReading dataset)(question answering, Evaluate-for, WebQuestions dataset)(question answering, Evaluate-for, WikiQA dataset)(question answering, Evaluate-for, SemEval-2016 dataset)(reading comprehension, Used-for, answering questions)(generating answer, Part-of, QA system)(natural answer generation, Part-of, QA system)(knowledge base, Used-for, QA system)(end-to-end neural network model, Part-of, QA system)(pointer networks, Used-for, locating positions of answers)(cross-attention mechanism, Used-for, representing questions)(entity linking, Part-of, KBQA system)(relation detection, Part-of, KBQA system
(neural semantic parser, Used-for, converting natural language utterances to intermediate representations)(predicate-argument structures, Part-of, neural semantic parser)(neural approach, Used-for, predicting sequences in unannotated text)(Long Short Term Memory, Part-of, neural approach)(neural model, Used-for, semantic role labeling)(semantic role labeling, Evaluate-for, recovering long-distance dependencies)(deep highway BiLSTM architecture, Part-of, semantic role labeling)(constrained decoding, Part-of, semantic role labeling)(neural reading comprehension model, Used-for, integrating external commonsense knowledge)(key-value memory, Part-of, neural reading comprehension model)(semantic parser, Part-of, NLP).
(morphological disambiguation, Part-of, morphological segmentation)(morphological disambiguation, Conjunction, surface context)(word embeddings, Used-for, morphological disambiguation)(deep learning-based approach, Used-for, morphological disambiguation)(dense representations, Used-for, morphological disambiguation)(language-independent approach, Used-for, morphological disambiguation)(CNN, Used-for, word embeddings)(bi-LSTMs, Used-for, character trigram representations)(word segmentation, Is-a-Prerequisite-of, Arabic NLP)(character representations, Compare, true morphological analyses)(neural machine translation, Evaluate-for, ratio of source and target tokens)(UBLI, Used-for, word translations)(morphology-aware alignment model, Used-for, UBLI)(BERT language model, Used-for, pronoun disambiguation)(decoupled prototype learning framework, Used-for, pseudo label disambiguation)(prototypical contrastive representation learning, Part-of, decoupled prototype learning
(Dependency grammar, Part-of, classic parsing method)(Constituency grammar, Part-of, classic parsing method)(classic parsing method, Compare, unsupervised parsing method)(classic parsing method, Compare, StructFormer)
(gradient descent, Used-for, learning word representations)(Stochastic Gradient Descent, Hyponym-Of, gradient descent)(negative sampling, Used-for, learning word representations)(sampling methods, Evaluate-for, bias)(SGD, Compare, AllVec)(SGD, Part-of, one-sample learning scheme)(batch gradient learning, Compare, SGD)(AllVec, Used-for, generating word representations)(AllVec, Evaluate-for, efficiency)(AllVec, Evaluate-for, performance on small training corpora)(adversarial training, Part-of, improving robustness of deep language models)(adversarial training, Evaluate-for, high time consumption)(multi-step gradient ascents, Part-of, adversarial training)(word substitutions, Part-of, adversarial training)(adversarial samples, Evaluate-for, grammatical quality)(adversarial samples, Evaluate-for, semantic consistency)(distribution shift risk minimization, Used-for, adversarial training with clean data)(distribution shift risk minimization, Evaluate-for,
(transliteration, Used-for, converting scripts)(neural machine translation, Used-for, translation tasks)(phrase-based systems, Compare, neural machine translation)(recurrent networks, Compare, convolutional layers)(convolutional layers, Part-of, neural machine translation architecture)(bi-directional LSTMs, Part-of, neural machine translation architecture)(bi-directional LSTMs, Compare, chunk-based decoders)(syntactic trees, Part-of, source-side syntax incorporation)(sequence-to-sequence models, Is-a-Prerequisite-of, neural machine translation)(BLEU score, Evaluate-for, translation performance)(dependency structure, Used-for, word generation)(nested attention, Used-for, grammatical error correction)
(multimodal sentiment analysis, Hyponym-Of, multi-modal learning)(multi-task learning, Used-for, multi-modal learning)(deep multimodal multi-task learning architecture, Part-of, multi-modal learning)(multimodal fusion, Part-of, multi-modal learning)(Dynamic Fusion Graph, Used-for, multimodal sentiment analysis)(Low-rank Multimodal Fusion, Used-for, multimodal fusion)(CMU Multimodal Opinion Sentiment and Emotion Intensity, Used-for, multimodal sentiment analysis)(speaker trait analysis, Is-a-Prerequisite-of, multimodal fusion)(emotion recognition, Is-a-Prerequisite-of, multimodal fusion)(multi-layer recurrent neural network model, Used-for, multi-modal learning).
(search, Used-for, information retrieval)(projective arc-eager dependency parser, Evaluate-for, search)(non-monotonic dynamic oracle, Evaluate-for, search)(KBQA system, Hyponym-Of, search)(entity linking, Part-of, KBQA system)(relation detector, Used-for, KBQA system)(neural word segmentation, Hyponym-Of, search)(segmentation model, Part-of, neural word segmentation)(pretraining, Used-for, segmentation model)(modular segmentation model, Part-of, neural word segmentation)(multimodal sentiment analysis, Compare, search)(abstractive summarization, Compare, search)(Grid Beam Search (GBS), Compare, search)(lexical constraints, Part-of, GBS)(pre-specified lexical constraints, Part-of, GBS)(knowledge base, Part-of, search)(entity mentions, Part-of, search)(dependency parsing, Evaluate-for, search)(cross-lingual dependency parsing, Hyponym-Of,
(paraphrasing, Used-for, training set augmentation)(paraphrasing, Used-for, improving question answering systems)(paraphrasing, Used-for, text simplification)(paraphrasing, Used-for, information ranking)(paraphrasing, Part-of, natural language understanding tasks)(paraphrase generation, Hyponym-Of, paraphrasing)(seq2seq models, Evaluate-for, paraphrase generation)(paraphrastic sentence embeddings, Part-of, paraphrasing)(sentence embeddings, Used-for, textual entailment recognition)(text similarity measures, Evaluate-for, paraphrase detection)(neural network, Used-for, paraphrase generation)(DNA sequence alignment algorithms, Evaluate-for, text similarity measures)(TextFlow, Is-a-Prerequisite-of, DNA sequence alignment algorithms)(LSTM recurrent networks, Evaluate-for, paraphrasing tasks)(Gated Recurrent Averaging Network, Compare, LSTM recurrent networks)(crowdsourcing, Used-for, paraphrase generation
(Neural machine translation, Used-for, Translating text)(Bi-directional LSTM, Part-of, Neural machine translation)(Convolutional layers, Used-for, Encoding source sentence)(A* CCG parsing model, Used-for, Parsing sentences)(Syntactic dependencies, Part-of, A* CCG parsing model)(Declarative knowledge, Used-for, Guiding neural network training)(Counterfactual statements, Used-for, Problem-solving)(Minimalist Grammar, Used-for, Parsing)(MGbank, Part-of, Minimalist Grammar)(Recurrent neural networks, Part-of, Sequential tasks)(Language acquisition, Part-of, Framework for connecting cognitive and social structures)(Attention mechanism, Used-for, Natural language generation)(Recurrent neural tensor networks, Compare, Recurrent neural networks)(Vector representations, Used-for, Sentence embeddings)(Snorkel framework, Used-for, Data programming)(Transformer, Used-for, Neural machine translation)(Restricted recurrent neural
(labeled sequence transduction, Used-for, transforming sequences)(multi-space variational encoder-decoders, Used-for, labeled sequence transduction)(neural networks, Used-for, handling discrete and continuous latent variables)(SIGMORPHON morphological inflection benchmark, Evaluate-for, multi-space variational encoder-decoders)(pairwise cooccurrence, Part-of, relations between ideas)(prevalence correlation, Part-of, relations between ideas)(cooccurrence, Compare, prevalence correlation)(bi-directional LSTMs, Compare, parallel state LSTM)(feed-forward networks, Used-for, cross-modal applications)(feed-forward networks, Used-for, bridging modalities)(support vector machine, Compare, LSTM-based recurrent neural network)(negative sampling, Used-for, word2vec)(distributed word representation learning, Part-of, word2vec)(global corpus-level information, Used-for, generating noise distribution)(named entity disambiguation, Used-for, distinguishing entities)(
(one-shot learning, Compare, zero-shot learning)(one-shot learning, Evaluate-for, language classification)(one-shot learning, Used-for, few-shot learning)(one-shot learning, Hyponym-Of, machine learning)(one-shot learning, Is-a-Prerequisite-of, learning efficiency)
(concept induction, Compare, topic models)(analogical reasoning, Evaluate-for, Chinese lexical knowledge)(cross-lingual word embeddings, Used-for, cross-lingual sentiment classification)(cross-lingual word embeddings, Evaluate-for, low-resource languages)(cross-lingual word embeddings, Part-of, multilingual sentence embedding)(noise contrastive estimation, Hyponym-Of, contrastive learning)(BLSE, Hyponym-Of, bilingual embeddings)(BLSE, Used-for, sentiment classification)(Pseudofit, Used-for, acquiring synonyms)(embedding learning, Conjunction, concept induction)(embedding learning, Conjunction, order embeddings)(spectral clustering, Used-for, word embeddings)(attention mechanism, Used-for, improving coherence of aspects)(constrastive learning, Used-for, learning word embeddings)(joint training, Used-for, cross-lingual word embeddings).
(document ranking, Used-for, information retrieval)(document ranking, Part-of, dense retrieval)(dense retrieval, Evaluate-for, query-document retrieval)(dense retrieval, Used-for, retrieving relevant documents)(dense retrieval, Part-of, neural summarization models)(WikiSum, Evaluate-for, summarization model performance)(MultiNews, Evaluate-for, summarization model performance)(Hibert, Used-for, document encoding)(DAR, Used-for, document augmentation)(query-based summarization, Hyponym-Of, summarization)(query-based summarization, Used-for, highlighting query-specific information)(DuoRC, Used-for, reading comprehension evaluation)(BERT, Used-for, document encoding)(Transformer, Used-for, encoding documents)(transformer-based encoder-decoder framework, Part-of, Transformer)(neural coreference models, Part-of, coreference resolution)(Graph Convolutional Networks, Used-for, encoding discourse graphs)(document clustering, Used-for, document analysis)(word
(None)
(evaluation of information retrieval, Conjunction, sentiment analysis methods)(evaluation of information retrieval, Used-for, forecast volatility)(evaluation of information retrieval, Part-of, research on volatility prediction)(evaluation of information retrieval, Used-for, study of fusion methods combining text and market data resources)(evaluation of information retrieval, Hyponym-Of, sentiment analysis methods)(forecast volatility, Used-for, study of fusion methods combining text and market data resources)(forecast volatility, Hyponym-Of, volatility prediction)(sentiment analysis methods, Part-of, volatility prediction)(volatility prediction, Is-a-Prerequisite-of, financial markets)(sentiment analysis methods, Is-a-Prerequisite-of, forecast volatility)(market data, Part-of, factual market data)(sentiment analysis, Evaluate-for, volatility prediction)(market data, Used-for, forecast market risk)(volatility prediction, Hyponym-Of, financial markets)(sentiment analysis, Conjunction, NLP)(tutorialbank, Used-for, NLP education
(neural techniques, Used-for, computational argumentation mining)(computational argumentation mining, Part-of, sequence parsing)(token-based dependency parsing, Compare, token-based sequence tagging)(token-based dependency parsing, Compare, local tagging models)(multi-task learning setup, Compare, single-task learning)(multi-task learning setup, Used-for, computational argumentation mining)(BiLSTM, Used-for, sequence tagging)(text similarity measures, Used-for, plagiarism detection)(text similarity measures, Used-for, information ranking)(text similarity measures, Used-for, paraphrase detection)(text similarity measures, Used-for, textual entailment)(n-grams, Part-of, text similarity measures)(skip-grams overlap, Part-of, text similarity measures)(TextFlow, Hyponym-Of, text similarity measures)(abstract syntax networks, Used-for, code generation)(abstract syntax networks, Used-for, semantic parsing)(abstract syntax trees, Part-of, abstract syntax networks)(attention-based sequence learning, Used-for, automatic question
(combinatory categorial grammar, Part-of, L2)(combinatory categorial grammar, Part-of, Weir's hierarchy of language classes)(combinatory categorial grammar, Part-of, statistical parsing)(combinatory categorial grammar, Compare, tree-adjoining grammars)(combinatory categorial grammar, Compare, linear indexed grammars)(combinatory categorial grammar, Compare, head grammars)(combinatory categorial grammar, Part-of, evaluation method based on predicate-argument structure)(combinatory categorial grammar, Used-for, automatic generation of CCG corpora)(automatic generation of CCG corpora, Used-for, domain adaptation)(automatic generation of CCG corpora, Evaluate-for, performance gains)(weir, Defined, hierarchy of language classes)(hierarchy of language classes, Part-of, L2)(L2, Is-a-Prerequisite-of, controllable CFG)(decomposed scoring, Part-of, evaluation method
(MultiMedia Event Extraction, Used-for, extracting events and their arguments)(MultiMedia Event Extraction, Part-of, multimedia documents)(Weakly Aligned Structured Embedding, Used-for, encoding structured representations)(textual data, Conjunction, visual data)(structured representations, Compare, unstructured representations)(Bidirectional LSTM, Used-for, learning flat entities and their inner dependencies)(graph convolutional network, Used-for, learning flat entities and their inner dependencies)(nested named entity recognition, Part-of, BiFlaG)(BiFlaG, Used-for, nested named entity recognition)(BiFlaG, Compare, previous state-of-the-art models)(multilingual transfer learning, Used-for, detecting multiple frames in different languages)(dictionary, Part-of, elementary resources)(few annotations, Part-of, elementary resources)(frames, Part-of, news headline)(news headline, Used-for, detecting frames)(extractive summarization, Used-for, collaborative filtering
(neural question answering, Used-for, reading comprehension style question answering)(reading comprehension style question answering, Is-a-Prerequisite-of, answer questions)(gated self-matching networks, Used-for, reading comprehension style question answering)(neural question answering, Used-for, gated attention-based recurrent networks)(gated attention-based recurrent networks, Part-of, question matching)(self-matching attention mechanism, Part-of, passage matching)(pointer networks, Used-for, locating answer positions)(COREQA, Is-a-Prerequisite-of, neural question answering)(sequence-to-sequence learning, Part-of, COREQA)(copying mechanisms, Part-of, COREQA)(retrieving mechanisms, Part-of, COREQA)(encoder-decoder framework, Used-for, COREQA)(Generative Domain-Adaptive Nets, Is-a-Prerequisite-of, neural question answering)(reinforcement learning, Used-for, training latent variables)(cross-attention mechanism, Part-of, question representation)(global knowledge, Part
(named entity recognition, Is-a-Prerequisite-of, facial recognition system)(heuristic scheme, Used-for, annotation projection)(word embeddings, Used-for, cross-lingual NER)(sequence-to-sequence approach, Used-for, AMR semantic graphs)(monotonic hard attention model, Used-for, transition framework)(automatic evaluation model CMADE, Used-for, dialog evaluation)(state-of-the-art end-to-end ASR system, Evaluate-for, different English accents)(meta-transfer learning, Used-for, code-switched speech recognition)(span prediction model, Compare, sequence labeling framework)(emotion recognition, Part-of, high-profile events)(ethical considerations, Used-for, AI tasks)(question answering systems, Used-for, moral judgments)(state-of-the-art end-to-end ASR system, Conjunction, convolutional and recurrent layers)(co-decoding schemes, Used-for, combine outputs of projection-based approaches)(open domain dialog system evaluation, Evaluate-for, conversational systems)(automatic
(programming language, Part-of, code generation)(programming language, Used-for, analyzing data)(programming language, Used-for, manipulating text)(programming language, Used-for, querying databases)(natural language descriptions, Used-for, generating source code)(semantic parsing, Used-for, code generation)(neural architecture, Used-for, code generation)(constituency parsing, Part-of, parsing)(constituency parsing, Hyponym-Of, dependency parsing)(Penn Treebank, Evaluate-for, constituency parsing)(French Treebank, Evaluate-for, constituency parsing)(core language, Hyponym-Of, programming language)(natural language interfaces, Compare, programming language)(DAG transducer, Used-for, graph-to-program transformation)(DAG automaton, Part-of, DAG transducer)(Elementary Dependency Structures, Evaluate-for, NLG)(English Resource Semantics, Part-of, Elementary Dependency Structures)(MH₄ algorithm, Used-for, dependency parsing)(LSTM features, Part-of,
None
(bag of word model, Compare, paragraph embedding model)(bag of word model, Used-for, feature-based models)(bag of word model, Part-of, sentence content task)(feature-based models, Compare, neural models)(paragraph embedding model, Evaluate-for, downstream classification tasks)(paragraph embedding model, Part-of, Zhang et al. (2017) method)(bag of word model, Compare, reconstruction-based objective)
(conjecture, Part-of, mathematical premise)(mathematical reasoning, Used-for, solving arithmetic word problems)(quantity span, Evaluate-for, tagging quantity in text)(equation generation, Used-for, solving arithmetic word problems)(statistical models, Part-of, sentence summarization)(neural models, Part-of, sentence summarization)(redundancy, Part-of, sentence summarization)(relevance, Part-of, sentence summarization)(informativeness, Part-of, sentence summarization)(conjecture, Part-of, mathematical model)(relevance, Evaluate-for, sentence summarization)(informativeness, Evaluate-for, sentence summarization)(matrix factorization, Hyponym-Of, mathematical model)
(bias-variance, Used-for, evaluating model performance)(bias-variance, Part-of, machine learning)(bias-variance, Compare, bias)(bias-variance, Compare, variance)(bias-variance, Evaluate-for, overfitting)(bias-variance, Evaluate-for, underfitting)(bias-variance, Compare, empirical work)(bias-variance, Compare, theoretical analysis)(over-parameterized neural networks, Compare, classic bias-variance trade-off)(variance, Hyponym-Of, noise)(variance, Part-of, error decomposition)(bias, Hyponym-Of, error decomposition)(bias-variance, Evaluate-for, model complexity)(supervised contrastive learning, Used-for, minimizing intra-class variance)(supervised contrastive learning, Used-for, maximizing inter-class variance)(bidirectional long short-term memory, Hyponym-Of, recurrent neural networks)(difference between systems, Compare, bias)(difference between systems, Compare, variance)(adversarial augmentation, Used
(event detection, Used-for, identifying events)(event detection, Used-for, categorizing events)(event detection, Hyponym-Of, event extraction)(event detection, Evaluate-for, supervised attention mechanisms)(supervised attention mechanisms, Part-of, event detection)(event detection, Is-a-Prerequisite-of, event coreference)(arguments, Used-for, event detection)(event coreference, Hyponym-Of, event detection)(Nugget Proposal Networks, Used-for, event detection)(Nugget Proposal Networks, Evaluate-for, TAC KBP 2017 datasets)(supervised attention mechanisms, Evaluate-for, ACE 2005 dataset)(event coreference, Compare, joint models)(event coreference, Part-of, end-to-end event extraction systems)(event extraction, Used-for, knowledge base population)(automatically labeled data, Conjunction, human-labeled data)(disfluency detection, Part-of, natural language processing)(fake news detection, Part-of, fact-checking research)(
(dialog system, Part-of, task-oriented dialogue system)(dialog system, Part-of, open-domain dialogue system)(dialog system, Used-for, end-to-end learning)(dialog system, Used-for, language grounding)(dialog system, Part-of, Hybrid Code Networks)(State tracking, Part-of, task-oriented dialogue system)(belief tracker, Part-of, task-oriented dialogue system)(Neural Belief Tracking, Part-of, task-oriented dialogue system)(dialog system, Evaluate-for, model complexity)(dialog system, Compare, customer-facing dialog system)(dialog system, Compare, pipeline-based methods).
(labeled sequence transduction, Is-a-Prerequisite-of, semi-supervised learning)(semi-supervised learning, Used-for, labeled sequence transduction)(multi-space variational encoder-decoders, Used-for, labeled sequence transduction)(multi-space variational encoder-decoders, Is-a-Prerequisite-of, semi-supervised learning)(hybrid code networks, Used-for, dialog systems)(semi-supervised question answering, Used-for, question answering)(generative domain-adaptive nets, Used-for, question answering)(generative domain-adaptive nets, Is-a-Prerequisite-of, semi-supervised learning)(pre-trained word embeddings, Part-of, NLP tasks)(NLP tasks, Evaluate-for, semi-supervised learning)(word embeddings, Evaluate-for, NLP tasks)(distant supervised learning, Is-a-Prerequisite-of, semi-supervised learning)(fine-grained opinion analysis, Evaluate-for, opinion summarization)(recursive neural networks, Used-for, domain adaptation)(generative latent-variable
(language identification, Used-for, multilingual text)(language identification, Is-a-Prerequisite-of, text processing)(language identification, Part-of, code-switching analysis)(language identification, Evaluate-for, health tracking)(language identification, Conjunction, dialectal variety handling)(Twitter, Used-for, health tracking)(multilingual speakers, Improve, health tracking)(multilingual speakers, Is-a-Prerequisite-of, dialect handling)(LID benchmarks, Evaluate-for, language identification)(code-switched text, Part-of, multilingual text).
(harmonic function, Compare, negation function)(negation function, Evaluate-for, fMRI patterns)(language modeling, Used-for, text infilling)(task-specific transformation functions, Part-of, Supervised Directional Similarity Network)(distributional vector space, Compare, general-purpose word embeddings)(HyperLex dataset, Evaluate-for, lexical entailment)(neural networks, Is-a-Prerequisite-of, model performance)(reward functions, Used-for, reinforcement learning)(synthetic user utterances, Used-for, increasing functional coverage)(humor mining, Compare, sentiment analysis)(prototypical function, Evaluate-for, commonsense knowledge)(natural language generation, Part-of, conditional text generation)(action-semantic processing, Used-for, decoding fMRI patterns)(classification tasks, Evaluate-for, auxiliary loss functions)
(neural dialogue generation, Part-of, dialogue generation)(neural knowledge diffusion, Used-for, dialogue generation)(entity diffusion, Used-for, neural knowledge diffusion)(convergent thinking, Used-for, neural knowledge diffusion)(divergent thinking, Used-for, neural knowledge diffusion)(Rumor detection, Is-a-Prerequisite-of, learning from social-media conversations)(binarized constituency trees, Used-for, social-media conversations)(Tree LSTM models, Used-for, stance classification)(rumor classification, Is-a-Prerequisite-of, stance classification)(multi-task learning, Part-of, Tree LSTM models)(transformers, Hyponym-Of, deep transformers)(optimization, Used-for, training deep transformers)(pre-trained models, Is-a-Prerequisite-of, fine-tuning)(Text-to-SQL parsing, Hyponym-Of, semantic parsing)(RoBERTa, Used-for, pre-trained models)(shallow decoder, Compare, balanced encoder-decoder depth)(SAD
(output layer, Part-of, neural machine translation)(vector space representations, Used-for, capturing word similarity)(error-correcting codes, Used-for, improving model robustness)(softmax, Compare, binary code)(binary code, Compare, softmax)(synonyms, Part-of, spectral clustering)(antonyms, Part-of, spectral clustering)(word embeddings, Used-for, signed spectral normalized graph cut algorithm)(word embeddings, Evaluate-for, semantic similarity)(rank-based metric, Compare, vector cosine)(prototypical networks, Compare, MLMAN model)(self-attention networks, Used-for, assembling global information)(machine translation tasks, Evaluate-for, proposed method)(tree-LSTMs, Used-for, tree-based sentiment analysis)(graph convolutional neural network, Used-for, tree communication model)(graph recurrent neural network, Used-for, tree communication model)(attention mechanisms, Used-for, Visual Question Answering)(EigenSent, Used-for, word-sequence embeddings)(dependency tree linearization, Used-for
(conjunctive normal form, Used-for, cky parsing)(cky parsing, Evaluate-for, efficiency)(cky parsing, Part-of, parsing techniques)(parsing techniques, Conjunction, token-based dependency parsing)(parsing techniques, Conjunction, token-based sequence tagging)(parsing techniques, Conjunction, bi-LSTM models)(bi-LSTM models, Used-for, capturing long-range dependencies)(bi-LSTM models, Used-for, classification scenarios)(token-based dependency parsing, Compare, token-based sequence tagging)(token-based sequence tagging, Compare, dependency parsing)(bi-directional LSTMs, Used-for, capturing dependencies)(token-based sequence tagging, Part-of, AM)(dependency parsing, Used-for, building semantic graphs)(dependency parsing, Compare, token-based sequence tagging)(token-based dependency parsing, Compare, AM)(word representations, Used-for, dependency parsing)(token alignments, Part-of, semantic graph parsing)(bilexical dependencies, Compare,
(None
(markov decision process, Hyponym-Of, decision-making process)(markov decision process, Used-for, reinforcement learning)(MemSum, Used-for, summarizing long documents)(MemSum, Part-of, reinforcement-learning-based extractive summarizer)(extractive summarizer, Hyponym-Of, summarizer)(reinforcement learning, Part-of, MemSum)(state bill, Part-of, legislative body’s vote breakdown)(attention mechanism, Part-of, rumor detection process)(rumor detection layer, Part-of, rumor detection process)(stance detection layer, Part-of, rumor detection process)(hypothesis generator, Part-of, two-phase approach)(reasoner, Part-of, two-phase approach)(hypothesis, Part-of, prediction)(neural metrics, Evaluate-for, machine translation evaluation)(Co-UCB, Hyponym-Of, dueling framework)(dueling framework, Compare, multi-armed bandit learning).
(syntax, Part-of, natural language processing)(syntax, Used-for, code generation)(syntax, Used-for, semantic parsing)(syntax, Used-for, neural machine translation)(parse trees, Part-of, syntax)(syntactic encoders, Part-of, neural machine translation)(morpheme segmentation, Part-of, syntax)(abstract syntax trees, Used-for, code generation)(parse trees, Used-for, syntactic analysis)(BiLSTM, Used-for, detecting syntactic information)(RNN encoder, Used-for, processing syntactic information)(positional tags, Used-for, syntactic analysis).
(Dependencies, Evaluate-for, Relation Extraction)(Molecular Structures, Is-a-Prerequisite-of, Graph Convolutional Networks)(Dependencies, Part-of, Dependency Trees)(Gated Convolutional Network, Evaluate-for, Sentence-level Representations)(Graph Convolutional Network, Evaluate-for, Document Dating)(Multiview Geolocation Model, Used-for, Graph Convolutional Networks)(Graph Convolutional Network, Part-of, NeuralDater)(Graph Convolutional Network, Part-of, GCN)(Attention Guided Graph Convolutional Networks, Improve, Relation Extraction)(Graph Convolutional Network, Hyponym-Of, Neural Network)(Graph Convolutional Network, Used-for, Drug-drug Interaction Extraction)(Graph Convolutional Network, Used-for, Social media user geolocation)(Graph Convolutional Network, Used-for, Knowledge Base Completion)(Graph Convolutional Networks, Is-a-Prerequisite-of, AGGCNs).
(spectral method, Evaluate-for, weighted non-deterministic automata)(weighted non-deterministic automata, Part-of, language modeling tasks)(spectral method, Used-for, scaling up spectral learning)(scaling up spectral learning, Part-of, improve performance)(improve performance, Hyponym-Of, evaluate language models)(language models, Used-for, character-based language modeling)(character-based language modeling, Part-of, language modeling tasks)
(pointer network, Used-for, question answering)(pointer network, Used-for, machine reading comprehension)(pointer network, Used-for, neural machine translation)(labeled sequence transduction, Hyponym-Of, sequence-to-sequence learning)(deep neural networks, Part-of, neural networks)(recurrent neural networks, Part-of, neural networks)(question answering, Is-a-Prerequisite-of, question representation)(attention mechanism, Part-of, pointer network)(kernel methods, Compare, deep neural networks)(AMR graphs, Used-for, semantic representation)(tree kernels, Part-of, kernel methods)(LAU, Compare, LSTM unit)(LAU, Compare, GRU)(reinforcement learning, Is-a-Prerequisite-of, sentence selection)(deep pyramid CNN, Hyponym-Of, convolutional neural network)(KBLSTM, Hyponym-Of, recurrent neural networks)(state-of-the-art system, Evaluate-for, F-score)(LSTM, Part-of, recurrent neural networks).
(structured prediction, Used-for, natural language processing)(structured prediction, Is-a-Prerequisite-of, neural machine translation)(structured prediction, Evaluate-for, coreference resolution)(structured prediction, Evaluate-for, transition-based dependency parsing)(structured prediction, Evaluate-for, reward augmented maximum likelihood learning)(structured prediction, Used-for, sequence labelling)(structured prediction, Part-of, deep neural networks)(structured prediction, Evaluate-for, semantic parsing)(structured prediction, Evaluate-for, code generation)(deep neural networks, Compare, kernel methods)(Nystrom low-rank approximation, Part-of, deep neural networks)(kernelized neural network, Evaluate-for, three different tasks)(MELA, Part-of, coreference measures)(bi-directional LSTMs, Compare, proposed LSTM structure)(neural machine translation, Evaluate-for, English translations)(neural machine translation, Evaluate-for, document-level translations)(memory networks, Part-of, document-level neural machine translation model)(graph neural network, Part-of, semantic
(integrating text and knowledge, Used-for, search engine)(query auto-completion, Used-for, search engine)(language model, Is-a-Prerequisite-of, query auto-completion)(answer content, Used-for, search engine)(machine reading comprehension, Used-for, search engine)(semantic space, Used-for, search engine)(MRC, Compare, search engine)(answer boundary, Used-for, search engine)(cross-passage answer verification, Used-for, search engine)(end-to-end neural model, Used-for, search engine)(recurrent neural network, Used-for, search engine)(BLEU points, Evaluate-for, search engine)(unsupervised machine translation, Used-for, search engine)(state-of-the-art performance, Evaluate-for, search engine)(entity linking, Used-for, search engine).
(aspect extraction, Part-of, sentiment analysis)(topic models, Used-for, aspect extraction)(neural word embeddings, Used-for, aspect extraction)(attention mechanism, Used-for, aspect extraction)(target-oriented sentiment classification, Part-of, sentiment analysis)(polarity orientation, Evaluate-for, sentiment analysis)(cross-domain sentiment classification, Part-of, sentiment analysis)(memory networks, Used-for, aspect sentiment classification)(target-sensitive sentiment, Used-for, aspect sentiment classification)(word embeddings, Used-for, sentiment analysis)(multimodal sentiment analysis, Part-of, sentiment analysis)(utterances, Part-of, multimodal sentiment analysis)(linguistic role, Used-for, sentence-level sentiment classification)(domain adaptation, Used-for, sentiment analysis)(sentiment classifier, Evaluate-for, sentiment analysis)(document clustering, Compare, sentiment analysis)(Text Deconvolution Saliency, Used-for, text classification)(multilingualism, Part-of, sentiment analysis)(compositor attribution, Compare, sentiment analysis)(semantic role labeling,
(projective arc-eager dependency parser, Compare, non-projective Covington algorithm)(non-monotonic transition system, Used-for, exploring erroneous actions during training process)(dynamic oracle, Hyponym-Of, Covington parser)(dynamic oracle, Compare, non-monotonic dynamic oracle)(dynamic oracle, Compare, monotonic dynamic oracle)(neural word segmentation, Used-for, pretraining character and word embeddings)(statistical segmentation, Used-for, leveraging punctuation and automatic segmentation)(statistical segmentation, Used-for, exploiting POS)(modular segmentation model, Part-of, neural word segmentation)(LSTM-based model, Used-for, capturing contextual information in videos)(graph-based attention mechanism, Part-of, sequence-to-sequence framework)(graph-based attention mechanism, Used-for, addressing saliency factor)(Grid Beam Search, Hyponym-Of, beam search)(Grid Beam Search, Used-for, incorporating lexical constraints)(lexical constraints, Used-for, neural interactive-predictive translation)(Multi-
(automated essay scoring, Used-for, grading essays)(automated essay scoring, Is-a-Prerequisite-of, automated writing evaluation)(automated essay scoring, Evaluate-for, argument persuasiveness)(automated essay scoring, Evaluate-for, thesis strength)(automated essay scoring, Used-for, prompt-independent rating)(automated essay scoring, Used-for, dimension-specific essay scoring)(automated essay scoring, Used-for, holistic scoring)(automated essay scoring, Used-for, cross-prompt scoring)(automated essay scoring, Used-for, rubric-based features)(automated essay scoring, Part-of, automated processing of historical texts)(automated essay scoring, Used-for, ForecastQA)
(matrix multiplication, Part-of, neural networks training)(GPUs, Used-for, matrix multiplication)(matrix multiplication, Used-for, neural networks evaluation)(matrix multiplication, Is-a-Prerequisite-of, few-hot vector multiplication)(few-hot vector multiplication, Hyponym-Of, matrix multiplication)(matrix multiplication, Used-for, sparse data operations)(neural networks, Used-for, Question Answering)(TAGOP, Used-for, Question Answering)(TAT-QA, Used-for, Question Answering)(TAGOP, Evaluate-for, numerical reasoning)(TAGOP, Part-of, hybrid data QA models)(numerical reasoning, Part-of, TAT-QA)
(evaluation of language modeling, Part-of, language modeling)(language modeling, Used-for, capturing morphological regularities)(Bayesian learning algorithm, Evaluate-for, RNNs)(Taylor's law, Used-for, quantifying structural complexity in linguistic time series)(Taylor's exponent, Evaluate-for, structural complexity underlying linguistic time series)(subword units, Conjunction, characters)(subword units, Conjunction, word segments)(subword units, Conjunction, character n-grams)(contrastive study, Evaluate-for, subword unit interactions with different morphological typologies)(image caption models, Evaluate-for, image caption annotations)(Inception-ResNetv2, Used-for, image-feature extraction)(few-shot natural language generation, Evaluate-for, generalization across domains)(morphological supervision, Used-for, improving character language models)(Transformer-XL, Used-for, learning longer-term dependencies)
(collaborative filtering, Compare, collaborative task)(collaborative task, Used-for, goal-driven collaborative task)(BERT, Used-for, collaborative filtering model ESCOFILT)(NLP models, Used-for, collaborative stories)(HMCEval, Used-for, evaluating malevolence in dialogues)(federated learning, Used-for, collaboratively train a shared model)(legal artificial intelligence, Evaluate-for, privacy-preserving and decentralized learning methods)(CoLaDa, Used-for, Collaborative Label Denoising)
(lexical semantics, Part-of, language understanding systems)(Distributional vector space models, Compare, curated semantic lexicons)(morph-fitting procedure, Used-for, improving distributional vector spaces)(inflectional forms, Conjunction, derivational antonyms)(semantic textual similarity tasks, Hyponym-Of, distributional representation of sentences)(low-frequency word estimates, Evaluate-for, morph-fitting procedure)(abstract meaning representations, Hyponym-Of, sentence-level semantic representations)(AMR parsing, Hyponym-Of, abstract meaning representations)(semantic parser, Used-for, mapping natural language utterances into executable programs)(confidence model, Evaluate-for, neural semantic parsers)(Sequence-to-Action, Is-a-Prerequisite-of, semantic graph generation)(AMR-to-text generation, Used-for, recovering text representing the same meaning as an AMR graph)(dynamic knowledge graph embeddings, Used-for, modeling both structured knowledge and unstructured language)(part-of-speech induction, Used-for,
(labeled sequence transduction, Is-a-Prerequisite-of, structured learning)(semi-supervised learning, Used-for, structured learning)(variational encoder-decoders, Used-for, labeled sequence transduction)(deep neural networks, Part-of, structured learning)(unsupervised learning techniques, Part-of, structured learning)(Tree Long Short-Term Memory Networks, Part-of, structured learning)(automatic dialogue evaluation, Used-for, evaluating dialogue models)(Nystrom low-rank approximation, Used-for, kernelized neural network)(kernel methods, Used-for, structured learning)(neural networks, Used-for, detecting events)(genetic adversarial network, Conjunction, neural networks)(semantic parsing, Used-for, mapping natural language utterances)(goal-acts, Part-of, learning goals for locations)(neural networks, Used-for, spoken language understanding)(Recurrent Neural Networks, Part-of, structured learning)(attention mechanisms, Part-of, Recurrent Neural Networks)(metaphor identification models, Part-of
(transfer learning, Used-for, question answering)(transfer learning, Used-for, reading comprehension)(transfer learning, Used-for, domain adaptation)(transfer learning, Used-for, text classification)(SQuAD, Is-a-Prerequisite-of, question answering)(multi-task learning, Part-of, neural network models)(adversarial training, Used-for, domain adaptation)(BiLSTM, Used-for, sequence tagging)(BiLSTM, Used-for, classification)(kernel methods, Used-for, learning feature representations)(Tree Kernels, Hyponym-Of, kernel methods)(embedding methods, Used-for, model transitive relational data)(Order Embeddings, Hyponym-Of, embedding methods)(probabilistic modeling, Used-for, calibration of denotational probabilities)(multi-task learning, Used-for, abstractive summarization)(Nystrom low-rank approximation, Used-for, kernel spaces)(a la carte embedding, Used-for, inducing embeddings)(SQuAD, Used-for, text
(genetic algorithm, Used-for, automatic training data generation)(automatic training data generation, Part-of, supervised framework)(supervised framework, Used-for, optimization-based extractive multi-document summarization)(optimization-based extractive multi-document summarization, Evaluate-for, automatic Pyramid scores)(genetic algorithm, Evaluate-for, automatic Pyramid scores).
(memory network, Used-for, zero pronoun resolution)(zero pronoun resolution, Part-of, pronoun resolution)(annotated data, Used-for, training models)(gated self-matching network, Used-for, reading comprehension)(recurrent neural networks, Used-for, sequence tasks)(multi-space variational encoder-decoder, Used-for, labeled sequence transduction)(cold-start problem, Part-of, review spam detection)(geolocation prediction, Part-of, social media analysis)(AMR graphs, Part-of, Abstract Meaning Representations)(question answering, Used-for, accessing knowledge bases)(bandit structured prediction, Used-for, stochastic optimization)(pointer networks, Used-for, locating answers in passages)(self-matching attention mechanism, Used-for, refining passage representation)
(context sensitive grammar, Part-of, language model)(context sensitive grammar, Used-for, NLP tasks)(context sensitive grammar, Compare, context-free grammar)(context sensitive grammar, Evaluate-for, streaming content)(language model, Used-for, grammar induction)(sequence labeling, Evaluate-for, NLP tasks)(pretrained context embeddings, Used-for, sequence labeling)(pretrained context embeddings, Part-of, bidirectional language models)(collapsed variational inference, Used-for, grammar induction)(context-free rule probabilities, Part-of, compound probabilistic context free grammar)(context-free rule probabilities, Conjunction, per-sentence continuous latent variable)(machine reading comprehension, Evaluate-for, question answering)(machine reading comprehension, Part-of, sequence labeling)(TTE prediction, Evaluate-for, streaming content).
(part of speech, Is-a-Prerequisite-of, morphological tagging)(part of speech, Used-for, part-of-speech induction)(lexical resources, Used-for, part-of-speech induction)(recurrent neural networks, Used-for, part-of-speech tagging)(weighted finite state transducers, Used-for, part-of-speech tagging)(taggers, Evaluated-for, part-of-speech tags)(gender information, Used-for, part-of-speech tagging)(text infilling, Used-for, natural language generation)(neural machine translation, Evaluate-for, morphology)(sequence-to-sequence framework, Used-for, speech recognition)(automatic speech recognition, Used-for, speech translation)(speech encoder, Part-of, SimulSpeech)(speech segmenter, Part-of, SimulSpeech)(text decoder, Part-of, SimulSpeech)(recurrent neural networks, Part-of, end-to-end automatic speech recognition system)(accents, Evaluated-for, internal representation of
(greedy algorithm, Used-for, text adversarial attack)(greedy algorithm, Used-for, recursive partitioning)(text adversarial attack, Part-of, text classification)(recursive partitioning, Part-of, inference algorithm)(attentive neural architectures, Compare, encoder-decoder sequence-to-sequence models)(multi-head attention, Part-of, attentive neural architectures)(Multi-head attentive neural architectures, Compare, mixture of attentive experts model)(Neural Machine Translation (NMT), Evaluate-for, German↔English translation)(Neural Machine Translation (NMT), Evaluate-for, Chinese→English translation)(Mixture of attentive experts model, Evaluate-for, machine translation)(Mixture of attentive experts model, Evaluate-for, language modeling)(FSRE, Hyponym-Of, few-shot learning)(Neural Sparse Topical Coding (NSTC), Used-for, topic models)(topic models, Evaluate-for, document aggregation)(topic models, Evaluate-for, text corp
(neural machine translation, Compare, statistical machine translation)(phrase-based methods, Compare, neural machine translation)(chunk-based decoders, Used-for, neural machine translation)(posterior regularization, Used-for, neural machine translation)(syntactic knowledge, Used-for, statistical machine translation)(syntactic knowledge, Used-for, neural machine translation)(compound splitting, Evaluate-for, statistical machine translation)(sentence splitting, Used-for, neural machine translation)(neural network-based lexicon, Part-of, direct HMM)(alignment models, Part-of, direct HMM)(sentence embedding similarity, Used-for, neural machine translation)(translation model, Hyponym-Of, statistical machine translation)(NMT+RNNG, Used-for, neural machine translation)(morphological transformations, Part-of, statistical machine translation)(pseudo-parallel sentences, Used-for, statistical machine translation).
(thesaurus-based similarity, Used-for, checking procedure)(checking procedure, Evaluate-for, thesauri)(corpus-based similarity, Compare, thesaurus-based similarity)(Russian wordnet, Part-of, word sense description)(checking procedure, Evaluate-for, word sense description)
(semantic parsing, Used-for, converting natural language to intermediate representations)(semantic parsing, Used-for, mapping natural language utterances into executable programs)(semantic parsing, Used-for, translating natural language questions to structured queries)(semantic parsing, Used-for, generating formal semantic representations)(predicate-argument structures, Part-of, intermediate representations)(neural semantic parser, Compare, grammar-based parser)(reinforcement learning, Conjunction, maximum marginal likelihood)(neural semantic parser, Part-of, state-of-the-art results)(semantic representation, Evaluate-for, semantic parsing)(UCCA parser, Part-of, multilingual context)(semantic parsing, Compare, syntactic schemes)(semantic graph, Part-of, semantic parsing)(abstract syntax networks, Used-for, code generation)(abstract meaning representations, Evaluate-for, semantic parsing)(semantic graphs, Part-of, meaning representations)(Variational autoencoding framework, Used-for, probabilistic assertions)(discourse representation structures, Compare, abstract meaning representations)(confidence modeling, Evaluate-for, neural semantic pars
(graph theory, Used-for, graph convolutional network)(psycholinguistic metrics, Evaluate-for, dialogues in movies)(distantly supervised open-domain question answering, Part-of, open-domain question answering)(distant supervision data, Evaluate-for, DS-QA)(inter-sentence relation extraction, Part-of, natural language processing)(graph convolutional neural network, Hyponym-Of, neural network)(smatch, Used-for, AMR parsing accuracy)(sembleu, Compare, smatch)(Triples, Part-of, Open Information Extraction systems)(knowledge graph, Part-of, open knowledge graph)(link prediction, Used-for, predicting test facts)(entity graph, Part-of, BiFlaG)(graph rewriting system, Part-of, semantic parser)(bipartite flat-graph network, Part-of, named entity recognition)(graph network, Part-of, Multimodal Neural Graph Memory Networks)(graph network, Used-for, understanding object interactions)(knowledge graph completion, Part-of,
(neural turing machine, Inspired, Global Context Layer)(Global Context Layer, Part-of, temporal information extraction model)(Global Context Layer, Has, long-term memory)(Global Context Layer, Has, attention mechanisms)(neural turing machine, Used-for, processing global context in discourse-scale texts)(Global Context Layer, Evaluate-for, irregular long-distance dependencies).
(neural machine translation, Compare, phrase based machine translation)(encoder, Part-of, neural machine translation)(decoder, Part-of, neural machine translation)(phrase based machine translation, Compare, encoder-decoder framework)(phrase, Used-for, modeling local dependencies)(phrase based machine translation, Compare, chunk based translation)(compound splitting, Evaluate-for, phrase based machine translation)(sentence alignment, Evaluate-for, phrase based machine translation)(compound splitting methods, Part-of, phrase based machine translation)(neural machine translation, Is-a-Prerequisite-of, supervised machine translation)(direct HMM, Evaluate-for, phrase based machine translation)
(uncertainty, Evaluate-for, RNN training)(uncertainty, Evaluate-for, model averaging)(uncertainty, Evaluate-for, stochastic gradient Markov Chain Monte Carlo)(uncertainty, Evaluate-for, Gaussian mixture word distributions)(uncertainty, Evaluate-for, word similarity benchmarks)(uncertainty, Evaluate-for, dependency parsing)(uncertainty, Evaluate-for, neural semantic parsers)(uncertainty, Evaluate-for, goal-oriented visual dialogue)(uncertainty, Evaluate-for, rumour verification)(uncertainty, Evaluate-for, Natural Language Inference)(uncertainty, Evaluate-for, clinical trials)(uncertainty, Evaluate-for, sequence modeling)(uncertainty, Evaluate-for, event detection)(uncertainty, Evaluate-for, named entity recognition)(uncertainty, Evaluate-for, vision and language navigation)(uncertainty, Evaluate-for, misclassification detection)(uncertainty, Evaluate-for, negation modeling)(uncertainty, Is-a-Prere
(neural summarization, Part-of, neural sequence-to-sequence models)(neural summarization, Part-of, sequence-to-sequence framework)(neural summarization, Part-of, encode-attend-decode paradigm)(neural summarization, Compare, extractive summarization)(neural summarization, Used-for, abstractive summarization)(neural summarization, Hyponym-Of, abstractive sentence summarization)(neural sequence-to-sequence models, Used-for, abstractive text summarization)(neural sequence-to-sequence models, Part-of, encode-attend-decode paradigm)(neural sequence-to-sequence models, Used-for, improving information reproduction)(abstractive summarization, Part-of, neural summarization)(abstractive sentence summarization, Hyponym-Of, abstractive summarization)(abstractive sentence summarization, Part-of, neural summarization)(encode-attend-decode paradigm, Used-for, query-based summarization)(query-based summarization
(generative adversarial network, Hyponym-Of, generative conversational systems)(context information, Part-of, dialog processing)(sequence learning, Is-a-Prerequisite-of, Dialogue Act classification)(Recurrent Neural Network, Hyponym-Of, neural networks)(local GAN, Hyponym-Of, generative adversarial network)(global GAN, Hyponym-Of, generative adversarial network)(GAN-BERT, Hyponym-Of, generative adversarial network)(DSGAN, Hyponym-Of, generative adversarial network)(generative adversarial network, Used-for, relation extraction)(self-regulated learning, Used-for, event detection)(adversarial learning, Used-for, generation of spurious features)(HotFlip, Used-for, adversarial training)(probability weighted word saliency (PWWS), Used-for, text adversarial attack)(adversarial learning framework, Used-for, why-question answering)(Adversarial networks for Generating
(None)
(attention model, Hyponym-Of, neural model)(neural model, Part-of, machine learning)(Propagation Tree Kernel, Hyponym-Of, kernel-based method)(kernel-based method, Used-for, detecting rumors)(discourse structure, Part-of, Rhetorical Structure Theory)(neural model, Used-for, text categorization)(sequence-to-sequence neural models, Used-for, Chinese poem generation)(memory mechanism, Part-of, memory augmented neural model)(NMT, Hyponym-Of, neural model)(NMT+RNNG, Used-for, machine translation)(hard attention mechanism, Part-of, neural model)(soft attention models, Compare, hard attention mechanism)(Propagation Tree Kernel, Compare, state-of-the-art rumor detection models)(Chinese poem generation, Used-for, sequence-to-sequence neural models)(text-based user geolocation model, Evaluate-for, state of the art performance)(question classification, Used-for, question modeling)
(probabilistic grammar, Used-for, generating regexes)(probabilistic grammar, Used-for, modeling latent representation)(probabilistic grammar, Compare, simulation-based approaches)(probabilistic grammar, Conjunction, probabilistic generative models)(probabilistic grammar, Part-of, linguistic typology)
None
(spectral clustering, Hyponym-Of, clustering)(word embeddings, Used-for, spectral clustering)(spectral clustering, Compare, signed spectral normalized graph cut algorithm)(Taylor's law, Part-of, Taylor analysis)(Taylor analysis, Evaluate-for, language data)(Machine Translation, Used-for, TED talks)(Monotonic Infinite Lookback attention, Used-for, simultaneous machine translation)(Episodic Memory Reader, Evaluate-for, QA accuracy)(dependency triples, Used-for, unsupervised semantic frame induction)(frame induction, Hyponym-Of, clustering)(bi-directional interrelated model, Hyponym-Of, slot filling)(density-based novelty detection algorithm, Used-for, unknown intent detection)(Online Semantic-enhanced Dirichlet Model, Hyponym-Of, short text clustering)(Wikipedia Current Events Portal, Part-of, multi-document summarization dataset)(graph autoencoder, Used-for, document clustering)(sentence meta-embeddings
(neural network architecture, Evaluate-for, computer vision)(syntax parser, Evaluate-for, neural network architecture)(transfer learning, Evaluate-for, computer vision)(data augmentation, Used-for, neural machine translation)(universal language model fine-tuning, Used-for, text classification)(image captioning, Used-for, computer vision)(adversarial attacks, Evaluate-for, computer vision)(deep neural networks, Evaluate-for, neural machine translation)(machine learning, Part-of, computer vision)(political ideologies, Correlated-with, moral foundations)(language model, Used-for, textual data analysis)(text classification, Evaluate-for, natural language processing)(domain data selection, Used-for, neural machine translation)(transkimmer architecture, Used-for, transformer models)(machine unlearning, Part-of, natural language processing)(fMRI, Used-for, text decoding from cognitive signals)
(finite state transducer, Used-for, part-of-speech tagging)(finite state transducer, Used-for, speech recognition)(finite state transducer, Evaluate-for, GPU implementation)(finite state transducer, Evaluate-for, CPU implementation)(finite state transducer, Part-of, finite state approach)(finite state transducer, Used-for, morph-based auto-completion)(finite state transducer, Evaluate-for, morphological analyzer)(neural semantic parsers, Used-for, semantic parsing)(encoder-decoder framework, Part-of, neural semantic parsers)(morphological analyzer, Used-for, morph-based auto-completion).
(dual decomposition, Compare, hierarchical pooling operation)(dual decomposition, Compare, max-pooling operation)(dual decomposition, Compare, multi-hop RC)(dual decomposition, Evaluate-for, question decomposer)(Neural Symbolic Machine, Part-of, dual decomposition)(dual decomposition, Used-for, semantic parsing)(dual decomposition, Evaluate-for, logical form)(dual decomposition, Evaluate-for, decompositionality)(dual decomposition, Compare, convolutional network)(dual decomposition, Compare, recurrent networks)(dual decomposition, Compare, DihEdral)(dual decomposition, Used-for, knowledge graph embeddings)
(supertagging, Used-for, dependency tree parsing)(supertagging, Used-for, constituency parsing)(supertagging, Used-for, CCG parsing)(Markovian supertaggers, Used-for, MG parsing)(holographic composition, Used-for, CCG parsing)(supervised learning models, Evaluate-for, retrieval benchmarks)(supervised learning models, Evaluate-for, similarity benchmarks)(supervised learning models, Evaluate-for, relatedness benchmarks)(tagging scheme, Used-for, entity and relation extraction)(distant supervision method, Used-for, public dataset generation)(neural techniques, Used-for, dependency tree parsing)(neural techniques, Used-for, AMR parsing).
None
(tree adjoining grammar, Is-a-Prerequisite-of, second member )(second member, Used-for, hierarchy of language classes)(tree adjoining grammar, Part-of, synchronous tree-adjoining grammar)(synchronous tree-adjoining grammar, Hyponym-Of, tree adjoining grammar)(synchronous tree-adjoining grammar, Used-for, transformation)(tree adjoining grammar, Compare, linear indexed grammars)(tree adjoining grammar, Compare, combinatory categorial grammars)(tree adjoining grammar, Compare, head grammars)(Greibach normal form, Part-of, context-free grammar)(Greibach normal form, Part-of, synchronous context-free grammar)(context-free grammar, Used-for, hierarchy of language classes)(context-free grammar, Is-a-Prerequisite-of, synchronous context-free grammar)(context-free grammar, Compare, pushdown automata)(pushdown automata, Part-of, labeled distinguished pushdown automata)(context-free grammar, Used-for, labeled distinguished pushdown automata)(context-free grammar,
(bidirectional recurrent neural network, Used-for, text categorization)(bidirectional recurrent neural network, Hyponym-Of, recurrent neural network)(bidirectional recurrent neural network, Conjunction, residual learning)(bidirectional recurrent neural network, Conjunction, bidirectional gated recurrent structures)(recurrent neural network, Hyponym-Of, neural network)(recurrent neural network, Used-for, abstractive sentence summarization)(recurrent neural network, Used-for, entity extraction)(recurrent neural network, Used-for, event extraction)(recurrent neural network, Used-for, grid-type neural networks)(recurrent neural network, Used-for, word-lemma pair transformation)(recurrent neural network, Used-for, Japanese sentence compression)(residual learning, Used-for, hierarchical recurrent neural network)(hierarchical recurrent neural network, Used-for, relation detection)(relation detection, Part-of, Knowledge Base Question Answering)(recurrent neural network, Used-for, supervised context sensitive lemmat
(regularization, Used-for, integrating prior knowledge)(posterior regularization, Used-for, integrating prior knowledge)(posterior regularization, Evaluate-for, Chinese-English dataset)(regularization, Evaluate-for, performance improvement)(sense-level information integration, Hyponym-Of, regularization)(posterior regularization, Part-of, regularization)(posterior regularization, Hyponym-Of, regularization)(features, Part-of, log-linear model)(log-linear model, Part-of, posterior regularization)
(beam search, Used-for, decoding sentences)(Grid Beam Search, Is-a-Prerequisite-of, beam search)(Grid Beam Search, Used-for, Neural Interactive-Predictive Translation)(Grid Beam Search, Used-for, Domain Adaptation for Neural Machine Translation)(Neural Machine Translation, Used-for, beam search)(RNNGs, Used-for, beam search)(beam search, Compare, lexical constraints)(Grid Beam Search, Compare, beam search)(target syntax, Part-of, beam search)(monotonic left-to-right order, Used-for, beam search)(monotonic constraint, Evaluate-for, decoding performance)(priority queue, Used-for, beam search)(hypothesis selection, Used-for, beam search)(priority queue, Compare, beam search)
(wordnet, Used-for, synsets)(synsets, Part-of, semantic concepts)(wordnet, Compare, word embeddings)(word embeddings, Used-for, word analogy questions)(word embeddings, Used-for, caption generation)(skip-gram model, Used-for, learning word vectors)(word vectors, Part-of, word embeddings)(word2vec, Hyponym-Of, skip-gram model)(word embeddings, Evaluate-for, document clustering)(abstractive summarization, Used-for, document summarization)(document summarization, Part-of, NLP tasks)(character-level language models, Compare, word-level language models)(CNN, Used-for, text categorization)(neural networks, Part-of, NLP tasks)(SGNS, Hyponym-Of, skip-gram model)(languages, Conjunction, lexicons)(cross-lingual signals, Evaluate-for, bilingual lexicon induction)
None
(distant supervision, Is-a-Prerequisite-of, classification)(noise, Affect, classification)(dynamic transition matrix, Evaluate-for, noise)(adversarial model, Used-for, classification)(group sparse autoencoders, Use-for, question classification)(group sparse CNNs, Evaluate-for, question representation)(keyphrase boundary classification, Used-for, scientific articles)(temporal relation classification, Hyponym-Of, classification)(Recurrent Neural Networks, Used-for, text classification)(sequence to sequence neural networks, Used-for, text simplification)(neural Machine Translation, Used-for, sentence splitting)(Universal Language Model Fine-tuning (ULMFiT), Used-for, text classification)(Text Deconvolution Saliency (TDS), Used-for, text classification)(bidirectional long short-term memory (Bi-LSTM), Used-for, relation extraction)(Memory networks (MNs), Used-for, aspect sentiment classification)(question classification, Hyponym-Of, classification)
(constraint satisfaction, Evaluate-for, conjunctive constraint satisfaction)  (conjunctive constraint satisfaction, Part-of, BERE)  (BERE, Used-for, guarantee coherence)  (coherence, Used-for, understanding event relations)  (event relation extraction, Part-of, understanding story with multiple events)  (constraint satisfaction, Evaluate-for, coherence)  (coherence, Is-a-Prerequisite-of, dialogue system performance)  (staged query graph generation, Used-for, handling multiple complexity types)  (multiple complexity types, Part-of, KBQA)  (EmoDS, Evaluate-for, emotional expression quality)
(syntaxnet, Compare, structured attention mechanism)(syntaxnet, Compare, Tree-LSTM model)(syntaxnet, Compare, neural sequence-to-sequence model)(syntaxnet, Compare, multi-stage paradigm decoder)(syntaxnet, Evaluate-for, subject-verb agreement diagnostic)(syntaxnet, Evaluate-for, F1 scores against gold parse trees)(syntaxnet, Evaluate-for, universal dependency tree structure)(syntaxnet, Is-a-Prerequisite-of, syntax-infused variational autoencoder)(syntaxnet, Compare, LVG-NSL)(syntaxnet, Used-for, code generation)(structured attention mechanism, Hyponym-Of, unsupervised parsers)(Tree-LSTM model, Part-of, imitation learning)(neural sequence-to-sequence model, Used-for, DRTS parsing)(subject-verb agreement diagnostic, Evaluate-for, LSTMs)(VG-NSL, Part-of, visually grounded neural models)(universal dependency tree structure, Part-of, cross-lingual transfer
(kNN-MT, Hyponym-Of, k-NN)(kNN-MT, Used-for, neural machine translation)(Adaptive kNN-MT, Hyponym-Of, kNN-MT)(Meta-k Network, Used-for, Adaptive kNN-MT)(Meta-k Network, Part-of, Adaptive kNN-MT)(k-NN, Used-for, token-level retrieval)(token, Part-of, kNN-MT)(language model, Part-of, ReadOnce Transformers)(ReadOnce Transformers, Used-for, multi-hop QA)(ReadOnce Transformers, Used-for, abstractive QA)(ReadOnce Transformers, Used-for, long-document summarization)(parameters, Part-of, GP-GNNs)(GP-GNNs, Used-for, relation extraction)(DocRED, Used-for, relation extraction).
(weighted finite state transducers, Used-for, part of speech tagging)(recurrent neural networks, Used-for, part of speech tagging)(part of speech tagging, Evaluate-for, Stanford Parser and baseline bidirectional networks)(Penn Treebank, Part-of, part of speech tagging)(pretrained contextual and non-contextual subword embeddings, Used-for, part of speech tagging)(FastText, Hyponym-Of, non-contextual subword embeddings)(BPEmb, Hyponym-Of, non-contextual subword embeddings)(BERT, Hyponym-Of, contextual subword embeddings)(character representations, Conjunction, part of speech tagging)(dataset similarity measures, Used-for, part of speech tagging)(cross-lingual transfer learning, Used-for, part of speech tagging)(OSCAR-based ELMo embeddings, Compare, Wikipedia-based ELMo embeddings)(male and female author texts, Evaluate-for, part of speech tagging)(contrastive learning, Used-for, part of speech
(response selection, Part-of, chat bot)(multi-turn conversation, Part-of, chat bot)(retrieval based chatbots, Hyponym-Of, chat bot)(sequential matching network, Used-for, response selection)(recurrent neural network, Part-of, sequential matching network)(representation, Used-for, user interests)(comments, Used-for, user engagement)(representation, Used-for, news recommendation)(evaluations, Used-for, comment quality)(customer service, Is-a-Prerequisite-of, specific responses)(Seq2Seq models, Used-for, response generation)(DuoRC, Used-for, language understanding)(diverse requirements, Is-a-Prerequisite-of, diverse responses)(language modeling, Used-for, word predictions)(hybrid language modeling, Used-for, string probability)(BLEU scores, Evaluate-for, machine translation models)(NLP applications, Used-for, neural networks)(cross-lingual transfer learning, Used-for, multilingual models)(discriminating antonyms and synonyms
(pseudo-relevance feedback, Used-for, search engine indexing)(pseudo-relevance feedback, Part-of, lexical methods)(pseudo-relevance feedback, Part-of, dense methods)(BM25, Hyponym-Of, lexical methods)(ANCE, Hyponym-Of, dense methods)(ColBERT, Hyponym-Of, dense methods)(ColBERT-PRF, Hyponym-Of, pseudo-relevance feedback)(inverse document frequency, Part-of, ColBERT-PRF)(CWPRF, Part-of, pseudo-relevance feedback)(deep language model, Used-for, CWPRF)(contrastive weighting model, Is-a-Prerequisite-of, CWPRF)(contrastive weighting model, Used-for, selecting useful embeddings)(contrastive weighting model, Used-for, semantic search)(search engine indexing, Used-for, retrieval effectiveness)(search engine indexing, Evaluate-for, user’s search intent)(query representation, Part-of, search engine indexing)(embeddings,
(logic and reasoning, Part-of, human reasoning and decision-making)(argument construction, Is-a-Prerequisite-of, human reasoning and decision-making)(argument construction, Difficult-For, both human and machine)(encoder-decoder model, Used-for, argument generation)(encoder-decoder model, Enriched-By, evidence from Wikipedia)(talking point phrases, Part-of, encoder-decoder model)(talking point phrases, Used-for, intermediate representation)(endpoint decoder, Used-for, producing final argument)(encoder-decoder model, Evaluate-for, argument generation)(coarse-to-fine reasoning, Used-for, generating clarification questions)(conversational machine reading, Used-for, answer user questions given knowledge base text)(conversational machine reading, Part-of, reading comprehension)(Explicit Memory Tracker, Used-for, tracking conditions listed in rule text)(Explicit Memory Tracker, Part-of, conversational machine reading)(logical reasoning, Part-of, multiple-choice question answering)(multi-source meta transfer, Used-for, low
(convolutional network, Used-for, word embedding)(pre-trained neural network, Is-a-Prerequisite-of, N3)(task-specific classification layer, Part-of, N3)(object detection, Part-of, vision-language pre-training)(E2E-VLP, Used-for, visual learning)(object detector, Used-for, visual representation)(visual representation, Part-of, E2E-VLP)(visual representation, Part-of, Unified Transformer framework)(region-based visual features, Used-for, vision-language pre-training)(image representation, Used-for, concatenation with text embedding)(text embedding, Used-for, concatenation with image representation)(image-text pairs, Used-for, vision-language pre-training)(large-scale dataset, Part-of, vision-language pre-training)(semantic alignments, Used-for, vision-language pre-training)(Transformer encoder-decoder architecture, Used-for, pre-training)(image captioning, Used-for, visual learning)(bilingual text embeddings, Used-for, cross-lingual image description
(backpropagation, Used-for, neural networks)(Deep Neural Networks, Part-of, neural networks)(stochastic gradient Markov Chain Monte Carlo, Used-for, learning weight uncertainty)(Structured projection of intermediate gradients, Part-of, backpropagation)(gradient noise, Part-of, backpropagation)(MultiScale Collaborative framework, Used-for, easing training of NMT models)(block-scale collaboration mechanism, Used-for, boosting gradient backpropagation)(straight-through estimators, Part-of, backpropagation)(deep RNN architecture, Used-for, modeling complex linguistic structures)(linear associative units, Used-for, reducing gradient propagation path).
(text similarity, Used-for, plagiarism detection)(text similarity, Used-for, information ranking)(text similarity, Used-for, recognition of paraphrases)(text similarity, Used-for, textual entailment)(text similarity, Part-of, document matching approach)(document matching approach, Evaluate-for, documents with non-comparable lengths)(document matching approach, Used-for, compare texts in a common space of hidden topics)(document matching approach, Evaluate-for, two matching tasks).
(capsule network, Used-for, multi-label text classification)(capsule network, Used-for, question answering)(capsule network, Evaluate-for, scalability)(capsule network, Evaluate-for, routing processes)(capsule network, Evaluate-for, low-resource settings)(routing processes, Evaluate-for, performance)(scalability, Evaluate-for, performance)(adaptive optimizer, Used-for, capsule network)(agreement score, Used-for, capsule network)(Nugget Proposal Networks, Compare, capsule network)(gated graph neural network, Compare, capsule network).
(bayes theorem, Is-a-Prerequisite-of, neural NLP techniques)(English Resource Grammar, Used-for, semantic parsing)(TLE, Used-for, grammatical error correction)(semantic parsing, Used-for, analyzing ESL data)(grammatical error correction models, Evaluate-for, grammatical error correction)(NLLB-200, Used-for, Neural Machine Translation)(Neural Machine Translation, Used-for, translating languages)(Mixture of Experts, Hyponym-Of, NLLB-200)(pruning method, Used-for, optimizing NLLB-200)(pruning method, Evaluate-for, translation quality)(finetuning, Used-for, improving model performance)(English as a second language, Evaluate-for, semantic parsing quality)
(bootstrapping, Used-for, learning specific dependency patterns)(bootstrapping, Used-for, extracting numerical relations)(bootstrapping, Used-for, selecting initial seeds)(bootstrapping, Used-for, noise reduction in distantly supervised relation extraction)(Iterative Rectification Network, Used-for, improving general NLG systems)(classification, Used-for, intent classification)(self-supervision, Used-for, dataset development)(ClarQ, Used-for, enhancement of dialog systems)(ClarQ, Hyponym-Of, dataset)(contrastive clustering-based bootstrapping, Used-for, refining labels of passages)(classification, Used-for, question-answering)
(abstractive summarization, Hyponym-Of, text summarization)(query-based summarization, Hyponym-Of, text summarization)(extractive summarization, Hyponym-Of, text summarization)(encode-attend-decode paradigm, Used-for, text summarization)(encode-attend-decode paradigm, Part-of, machine translation)(query attention model, Part-of, query-based summarization)(diversity based attention model, Used-for, reducing repeated phrases)(new query-based summarization dataset, Used-for, testing query-based summarization model)(neural sequence-to-sequence models, Used-for, abstractive text summarization)(pointer-generator network, Hyponym-Of, neural sequence-to-sequence models)(coverage, Used-for, tracking summarized content)(Pyramid scores, Used-for, evaluating generation systems)(genetic algorithm, Used-for, automatic training data generation)(selective encoding model, Hyponym-Of, sequence-to-sequence framework)(G
(recurrent neural network, Used-for, sequence transduction)(recurrent neural network, Compare, convolutional neural network)(recurrent neural network, Part-of, neural network architecture)(recurrent neural network, Used-for, neural machine translation)(recurrent neural network, Used-for, language modeling)(recurrent neural network, Used-for, reading comprehension)(recurrent neural network, Used-for, dialog systems)(recurrent neural network, Used-for, question answering over knowledge base)(recurrent neural network, Used-for, sentiment classification)(recurrent neural network, Used-for, text categorization)(recurrent neural network, Used-for, sequence-to-sequence frameworks)(recurrent neural network, Used-for, joint extraction of entity mentions and relations)(recurrent neural network, Used-for, implicit discourse relation classification)(recurrent neural network, Used-for, zero pronoun resolution)(recurrent neural network, Used-for, morphological inflection)(recurrent neural network, Used-for, knowledge representation)(convolutional
(normalization, Used-for, parser adaptation)(parser adaptation, Evaluate-for, parsing performance)(deep highway BiLSTM architecture, Used-for, semantic role labeling)(convolutional neural network, Used-for, local coherence model)(grapheme-to-phoneme dictionary, Used-for, multi-task learning architecture)(multi-task learning architecture, Evaluate-for, performance)(attention mechanisms, Conjunction, decoder)(attention mechanisms, Used-for, encoder-decoder architectures)(bandit structured prediction, Conjunction, partial feedback)(attention, Part-of, Tree-LSTMs)(attention, Hyponym-Of, attention mechanisms)(semantic role labeling, Used-for, deep highway BiLSTM architecture).
(evaluation of question answering, Part-of, question answering system)(question answering system, Used-for, generating natural answers)(EviNets, Evaluate-for, factoid question answering)(factoid question answering, Hyponym-Of, question answering)(EviNets, Evaluate-for, supporting evidence)(supporting evidence, Conjunction, structured knowledge bases and unstructured text documents)(semantic parsing, Used-for, answering inter-related questions)(question answering, Hyponym-Of, machine comprehension)(COREQA, Evaluate-for, question answering)(Masque, Evaluate-for, Q&A task)(evaluation of question answering, Used-for, measuring correctness and coherence)(question answering, Hyponym-Of, open-domain question answering)(cross-lingual OpenQA, Hyponym-Of, OpenQA)(SAN, Evaluate-for, machine reading comprehension)(machine reading comprehension, Hyponym-Of, question answering)(universal schema, Used-for,
(knowledge graph, Part-of, Knowledge Base)(KB-QA, Part-of, Knowledge Base)(Knowledge Base verbalisers, Used-for, KB-QA)(micro-planning, Part-of, NLG models)(question representation, Evaluate-for, NN-based KB-QA)(cross-attention mechanism, Used-for, NN-based KB-QA)(global knowledge, Used-for, cross-attention model)(joint inference, Part-of, knowledge acquisition)(KB-InfoBot, Used-for, search Knowledge Bases)(KB-InfoBot, Used-for, goal-oriented dialogue agents)(reinforcement learner, Used-for, KB-InfoBot)(deep residual bidirectional LSTMs, Part-of, hierarchical recurrent neural network)(entity linking, Part-of, KBQA system)(relation detector, Part-of, KBQA system)(joint semantic space, Used-for, visually grounded model of speech perception)(semantic aspects, Part-of, visually grounded model of
(disourse parsing, Used-for, discourse segmentation)(disourse parsing, Used-for, discourse relation identification)(disourse segmentation, Evaluate-for, building end-to-end discourse parsers)(disourse parsing, Evaluate-for, discourse coherence)(disourse cohort, Evaluate-for, text quality)(disourse parsing, Used-for, Rhetorical Structure Theory)(disourse parsing, Evaluate-for, coherence assessment)(disourse parsing, Conjunction, implicit discourse relation classification)(implicit discourse relation classification, Used-for, identification of long-span dependencies)(disourse parsing, Conjunction, discourse segmentation)(disourse parsing, Conjunction, discourse parsing evaluation tasks)(coherence assessment, Evaluate-for, discourse parsing)(disourse parsing, Evaluate-for, implicit discourse relation identification)(disourse relation, Evaluate-for, discourse coherence)(Rhetorical Structure Theory, Part-of, discourse parsing)(disourse parsing, Compare, hierarchical neural network used for coherence assessment).
(variational autoencoders, Part-of, text generation)(variational autoencoders, Part-of, semi-supervised learning)(variational autoencoders, Is-a-Prerequisite-of, multi-space variational encoder-decoders)(variational autoencoders, Part-of, neural topic model)(variational autoencoders, Part-of, Syntax-infused variational autoencoder)(variational autoencoders, Part-of, Batch Normalized-VAE)(variational autoencoders, Part-of, SIGNAL model)(variational autoencoders, Part-of, Coupled-VAE)(variational autoencoders, Part-of, Bayesian Hierarchical Words Representation)(variational autoencoders, Part-of, disentangled syntactic and semantic spaces)(variational autoencoders, Part-of, residual variant of vector-quantized variational auto-encoder)(variational autoencoders, Part-of, hierarchical conditional variational autoencoder)(Batch Normalized-VAE, Used-for, mitigating
(maximum likelihood estimation, Is-a-Prerequisite-of, reward-augmented maximum likelihood learning)(maximum likelihood estimation, Compare, entropy regularized reinforcement learning)(maximum likelihood estimation, Compare, reward-augmented maximum likelihood learning)(maximum likelihood estimation, Compare, mutual-information-based decoding)(maximum likelihood estimation, Compare, reinforcement learning)(maximum likelihood estimation, Used-for, sequence-to-sequence models)(maximum likelihood estimation, Used-for, autoregressive neural machine translation)(maximum likelihood estimation, Used-for, document-level neural machine translation)(maximum likelihood estimation, Used-for, generative dialogue models)(reward-augmented maximum likelihood learning, Part-of, sequence prediction algorithms)(token-level loss smoothing, Part-of, reward-augmented maximum likelihood learning)(sequence-level smoothing, Part-of, reward-augmented maximum likelihood learning)(entropy regularized reinforcement learning, Part-of, sequence prediction algorithms)(unlikelihood loss, Used-for, generative dialogue models)(iter
(SGNS word embedding model, Compare, SVD over SPPMI matrix)(SGNS word embedding model, Compare, original method to train SGNS)(Riemannian optimization, Evaluate-for, SGNS objective)(SGNS word embedding algorithm, Evaluate-for, JS dependency measure)(attention-based Bi-LSTM, Evaluate-for, Chinese implicit discourse relations)(semantic parsing, Evaluate-for, training data)(active learning, Evaluate-for, training hyperparameters)(uncertainty sampling, Compare, traditional data collection)(uncertainty sampling, Compare, overnight data collection)(word co-occurrence statistics, Used-for, noise distribution generation)(Temporal Referencing method, Compare, alignment models)(lexical semantic change detection, Evaluate-for, noise stemming from vector space alignment)(skip-gram with negative sampling, Evaluate-for, lexical semantic change)(sequence modeling, Compare, word order-agnostic approaches)(Pinyin-to-character (P2C) conversion, Part-of, Chinese input method engine (IME))(P
(random walk, Used-for, reasoning models)(random walk, Evaluate-for, reasoning tasks)(pretrained language models, Used-for, random walk models)(symbolic representations, Used-for, random walk models)(random walk models, Enhance, pretrained language models)(GPT-2, Use, symbolic representations)(random walk models, Use, schemata)(pretrained language models, Enhance, zero-shot generalization)(zero-shot generalization, Improve, GAP method)(GAP method, Used-for, pretrained LMs)(few-shot learning, Compare, zero-shot learning)(in-context learning, Compare, zero-shot learning)(self-adaptive ICL, Improve, in-context learning)(deep contextual embeddings, Compare, classic pretrained embeddings)(classic pretrained embeddings, Compare, random word embeddings)(JS divergence, Evaluate-for, dependency measure)(skip-gram embedding algorithm, Evaluate-for, JS dependency measure)(word2vec's skip-gram, Use, JS divergence)(SCRFs, Improve,
(semantic parser, Is-a-Prerequisite-of, object detection)(reinforcement learning, Used-for, object detection)(event coreference, Compare, object detection)(trigger detection, Part-of, event detection)(event anaphoricity, Part-of, event coreference)(emotion detection, Compare, object detection)(fine-grained emotions, Hyponym-Of, emotion detection)(deep learning models, Used-for, emotion detection)(Joint model, Used-for, event coreference)(MaxEnt reranker, Used-for, disfluency detection)(lambdaMART ranker, Used-for, supporting argument detection)(fake news detection, Compare, object detection)(Hybrid convolutional neural network, Used-for, fake news detection)(dialect priming, Used-for, racial bias reduction)(race priming, Used-for, racial bias reduction)(LSTM language model, Part-of, disfluency detection)(supporting argument detection, Compare, object detection)(z
(monte carlo tree search, Compare, constant computation efforts)(monte carlo tree search, Part-of, Dynamic-Tree Driven Theorem Solver)(monte carlo tree search, Used-for, theorem proving)(monte carlo tree search, Used-for, dynamic-tree Monte-Carlo search algorithm)(Dynamic-Tree Driven Theorem Solver, Evaluate-for, state confidence)(Dynamic-Tree Driven Theorem Solver, Evaluate-for, proof-level values)(multi-task learning framework, Used-for, sentiment analysis)(multi-task learning framework, Evaluate-for, unimodal representations)(action recognition, Used-for, image annotation)(action recognition, Used-for, scene understanding)(action recognition, Used-for, image retrieval)(multi-modal sentiment analysis, Compare, unimodal sentiment analysis)(multimodal annotations, Compare, unimodal annotations)(dataset, Conjunction, methods)(dataset, Conjunction, codes)(theorem proving, Compare, action recognition)
(variational bayes model, Part-of, BHWR)(BHWR, Used-for, word representation learning)(BHWR, Used-for, semantic taxonomy modeling via hierarchical priors)(BHWR, Hyponym-Of, Bayesian modeling)(variational bayes model, Part-of, Variational Autoencoder)(Variational Autoencoder, Used-for, text generation)(Variational Autoencoder, Used-for, approximating model’s posterior on latent variables)(Coupled-VAE, Hyponym-Of, Variational Autoencoder)(Coupled-VAE, Used-for, improving the encoder and decoder parameterizations)(Coupled-VAE, Used-for, avoiding posterior collapse)
(probability, Used-for, evaluating RC datasets)(probability, Part-of, A* CCG parsing model)(probability, Used-for, learning representations)(deep stochastic point processes, Compare, previous computational approaches)(traditional non-neural-network-based model, Compare, deletion-based Long Short-Term Memory neural network model)(prerequisite skills, Evaluate-for, RC datasets)(readability, Evaluate-for, RC datasets)(quality, Evaluate-for, reading comprehension datasets)(character-level models, Compare, word and morphology level information)(domain adaptability, Evaluate-for, deletion-based LSTM neural network model)(weight sharing, Used-for, model compression)(weight sharing, Used-for, incorporating prior knowledge)(model distillation, Used-for, model compression)(score, Evaluate-for, cross-domain setting)(model, Evaluate-for, state-of-the-art results)(sarcasm, Part-of, social communities)(constrains, Evaluate-for, quality and diversity of puns)(contrast, Evaluate-for
(recursive neural network, Hyponym-Of, neural network)(recursive neural network, Used-for, computing a representation of text)(recursive neural network, Conjunction, attention mechanism)(recursive neural network, Hyponym-Of, deep learning model)(neural network, Part-of, deep learning model)(recursive neural network, Evaluate-for, text categorization)(attention mechanism, Part-of, recurrent neural network)(neural network, Conjunction, recurrent neural network)(deep learning model, Hyponym-Of, machine learning)
(sentence simplification, Compare, lexical simplification)(sentence simplification, Compare, content reduction)(sentence simplification, Used-for, improving understandability)(sentence simplification, Evaluate-for, quality of text)(sentence simplification, Evaluate-for, understandability of text)(neural Machine Translation, Used-for, sentence simplification)(splitting based on semantic parsing, Evaluate-for, sentence simplification)(sequence modeling, Evaluate-for, Complex Word Identification)(Complex Word Identification, Part-of, sentence simplification)(neural text simplification, Compare, automated text simplification)(unsupervised neural text simplification, Part-of, neural text simplification)(sentence encoder, Part-of, selective encoding model)(attention equipped decoder, Part-of, selective encoding model)(recurrent neural networks, Part-of, sentence encoder)(recurrent neural networks, Part-of, attention equipped decoder)(selective gate network, Used-for, controlling information flow)(selective encoding model, Evaluate-for, English Gigaword Datasets)(
(dis.coarse mode, Compare, dis.coarse segmentation)(dis.coarse model, Evaluate-for, identifying salient discussion points)(dis.coarse model, Evaluate-for, automatic essay scoring)(dis.coarse segmentation, Part-of, dis.coarse model)(dis.coarse model, Evaluate-for, predicting the consistency among team members’ understanding)(dis.coarse model, Evaluate-for, implicit discourse relation classification)(dis.coarse model, Evaluate-for, discourse relation prediction tasks)(dis.coarse coherence, Part-of, dis.coarse model)(dis.coarse specific word embeddings, Used-for, implicit discourse relation recognition)(dis.coarse representation theory, Part-of, dis.coarse model)(dis.coarse connective classification, Used-for, implicit discourse relation recognition)(annotation corpus, Used-for, discourse relations)(Rhetorical Structure Theory, Part-of, dis.coarse model)(dis.coarse model, Evaluate-for, contractual obligations and prohibitions detection)(Groningen Meaning Bank, Used-for, formal meaning representations)(
(social medium analysis, Used-for, estimating a user’s socio-economic profile)(estimating a user’s socio-economic profile, Used-for, social science research)(estimating a user’s socio-economic profile, Used-for, various downstream applications)(socio-economic profile, Part-of, income classification)(user cognitive structure, Used-for, predictive model of income)(classifier, Used-for, time-tagging tweets)(time-tagging tweets, Used-for, temporal orientation measurement)(temporal orientation, Used-for, predictive model of income)(future temporal orientation, Evaluate-for, income)(universal dependencies 2.0 framework, Used-for, dependency parsing for social media English)(annotated dataset, Used-for, dependency parsing for social media English)(parsing strategies, Evaluate-for, performance disparities between AAE and Mainstream American English tweets)(echo chambers, Part-of, social media framework)(fake news detection, Evaluate-for, preventing misinformation)(social medium analysis, Used-for, understanding social media framework).
(neural network models, Used-for, learning)(adversarial multi-task learning framework, Used-for, alleviating interference between shared and private latent feature spaces)(extensive experiments, Evaluate-for, demonstrating benefits of approach)(shared knowledge, Used-for, transferring to new tasks)(decentralized deep multiagent policies, Used-for, coordinate via a differentiable communication channel)(agent messages, Used-for, translating in communication strategies)(translation model, Used-for, interpreting agents' messages)(distributional word embeddings, Used-for, linking visual to lexical information)(object names, Part-of, referring expression generation)(learning, Evaluate-for, acquiring commonsense knowledge from text)(joint inference, Used-for, learning physical knowledge of objects and actions)(aspect extraction, Part-of, aspect-based sentiment analysis)(neural word embeddings, Used-for, improving aspect coherence)(automatic labeling, Used-for, event extraction)(bilingual word embeddings, Used-for, translating with minimal pairs)(semantic role
(reinforcement learning, Conjunction, neutralization module)(reinforcement learning, Conjunction, emotionalization module)(reinforcement learning, Evaluate-for, content preservation performance)(reinforcement learning, Used-for, supervised learning)(reinforcement learning, Used-for, credit assignment problem)(reinforcement learning, Conjunction, reward augmented maximum likelihood)(reinforcement learning, Compare, Actor-Critic)(reinforcement learning, Evaluate-for, token-level RAML)(reinforcement learning, Used-for, reading comprehension)(reinforcement learning, Evaluate-for, sequence prediction algorithms)(reinforcement learning, Conjunction, Actor-Critic)(reinforcement learning, Evaluate-for, human bandit feedback)(reinforcement learning, Used-for, neural networks)(reinforcement learning, Evaluate-for, interactive conversational game)(reinforcement learning, Evaluate-for, coreference resolution)(reinforcement learning, Conjunction, imitation learning)(reinforcement learning, Used-for, policy gradient training)(reinforcement learning, Evaluate-for, sequence-to-sequence learning)(rein
(robotics, Used-for, human-robot communication)(robotics, Used-for, human-robot collaboration)(grounded verb semantics, Used-for, human-robot communication)(grounded verb semantics, Used-for, human-robot collaboration)(grounded verb semantics, Part-of, robotics)(parallel data, Part-of, grounded verb semantics)(primitive actions, Part-of, parallel data)(reinforcement learning, Used-for, interactive learning approach)(reinforcement learning, Used-for, grounded verb semantics)(interactive learning approach, Part-of, robotics)(question-asking behaviors, Part-of, interactive learning approach)(optimal policy, Evaluate-for, question-asking behaviors)(teachers, Conjunction, students)(models, Evaluate-for, grounded verb semantics)
(long short term memory network, Hyponym-Of, recurrent neural network)(long short term memory network, Used-for, predicting sequences)(long short term memory network, Used-for, language model prediction)(long short term memory network, Used-for, disfluency detection)(long short term memory network, Used-for, semantic relation extraction)(long short term memory network, Used-for, aspect-level sentiment classification)(long short term memory network, Used-for, generating sentences)(long short term memory network, Used-for, sentence compression)(long short term memory network, Part-of, Affect-LM)(long short term memory network, Part-of, attention-based recurrent neural network)(long short term memory network, Part-of, syntax-infused variational autoencoder)(hidden Markov model, Hyponym-Of, probabilistic model)(hidden Markov model, Used-for, aggregating crowd labels)(recurrent neural network, Hyponym-Of, neural network)(attention mechanism, Part-of,
(inference, Hyponym-Of, Natural Language Inference)(inference, Hyponym-Of, Recognizing Textual Entailment)(inference, Used-for, reasoning)(inference, Part-of, human language modeling)(inference, Used-for, artificial intelligence)(inference, Used-for, neural network-based models)(inference, Used-for, understanding of a cohort)(neural network-based models, Compare, deep neural networks)(deep neural networks, Compare, expressive kernels)(coreference resolution, Used-for, inference)(modeling inference, Used-for, better understanding of language tasks)(syntactic parsing information, Used-for, inference improvements)(external knowledge, Used-for, improving inference models)(event coreference resolution, Part-of, inference tasks)(semantic dependency graph, Part-of, inference tasks)
(summarization evaluation, Used-for, automatic essay scoring (AES))(summarization evaluation, Evaluate-for, discourse modes)(abstractive summarization, Hyponym-Of, summarization evaluation)(query-based summarization, Part-of, summarization evaluation)(ROUGE-L scores, Used-for, summarization evaluation)(human judgments, Used-for, summarization evaluation)(ROUGE, Used-for, summarization evaluation)(BLEU, Used-for, summarization evaluation)(generation systems, Evaluate-for, summarization evaluation)(summarization evaluation, Part-of, compressive summarization)(summarization evaluation, Part-of, extractive summarization)(summarization evaluation, Part-of, abstractive summarization)(abstractive summarization, Compare, extractive summarization)(abstractive summarization, Compare, compressive summarization)(neural sequence-to-sequence models, Used-for, abstractive summarization)(sequence-to-sequence framework, Used-for, abstractive summarization)(
(transition based dependency parsing, Compare, token-based sequence tagging)(transition based dependency parsing, Compare, dependency parsing)(transition based dependency parsing, Part-of, structured prediction)(transition based dependency parsing, Evaluate-for, attachment decisions)(shift and reduce operations, Part-of, transition based dependency parsing)(arc-swift, Compare, existing transition systems)(arc-swift, Evaluate-for, performance)(convolutional neural network, Used-for, word representation composition)(token-based sequence tagging, Compare, dependency parsing)(BiLSTM, Used-for, tagging models)(BiLSTM, Used-for, structured prediction tasks)(deep dependency parsing, Part-of, dependency parsing)(MH₄ algorithm, Is-a-Prerequisite-of, transition based dependency parsing)(GNN’s updating functions, Evaluate-for, high-order feature capturing)(graph neural networks, Used-for, dependency tree node representation learning)
(multi-task learning, Used-for, learning shared layers)(shared layers, Used-for, extracting common features)(task-specific features, Compare, common features)(task-specific features, Part-of, multi-task learning)(shared features, Evaluate-for, noise contamination)(adversarial multi-task learning, Used-for, separating shared and private features)(text classification tasks, Evaluate-for, performance improvement)(shared knowledge, Used-for, transferring to new tasks)(argumentation mining, Is-a-Prerequisite-of, dependency parsing)(dependency parsing, Compare, sequence tagging)(multi-task learning, Used-for, jointly learning tasks)(pre-normalization, Used-for, historical text processing)(encoder-decoder architectures, Used-for, solving pre-normalization)(multi-task learning, Evaluate-for, attention mechanisms)(Wikipedia, Used-for, open-domain question answering)(document retrieval, Part-of, machine reading at scale)(machine comprehension of text, Part-of, machine reading at scale)(bigram hashing, Used-for
(social network extraction, Used-for, user’s occupational class classification)(user’s tweets, Part-of, social network extraction)(user’s follower/following community profile descriptions, Part-of, social network extraction)(graph convolutional network, Used-for, social network extraction)(social network homophily, Used-for, social network extraction)(profile descriptions, Part-of, content information)(social network, Part-of, content information)(social network information, Compare, content information)
(knowledge graph embeddings, Part-of, knowledge graphs)(hierarchical patterns, Used-for, hierarchical data)(hyperbolic embedding methods, Used-for, high-fidelity representations)(hyperbolic reflections, Part-of, hyperbolic KG embedding models)(hyperbolic rotations, Part-of, hyperbolic KG embedding models)(attention-based transformations, Used-for, modeling complex relational patterns)(mean reciprocal rank, Evaluate-for, knowledge graph embedding models)(WN18RR, Evaluate-for, knowledge graph embedding models)(YAGO3-10, Evaluate-for, knowledge graph embedding models)(translationese, Compare, natural text)(BLEU score, Evaluate-for, machine translation)(sentence-level classifier, Used-for, distinguishing translationese from natural text)(tagging the training data, Used-for, biasing the NMT model outputs)(deep transformers, Part-of, transformer models)(initialization and optimization, Is-a-Prerequisite-of, effective training of deep transformers)(RoBERTa, Part-of, deep
(perceptron, Part-of, neural networks)(neural networks, Used-for, generative model)(neural networks, Used-for, speech perception)(neural networks, Used-for, automatic decipherment)(neural networks, Used-for, coreference resolution)(neural networks, Used-for, speech processing)(neural networks, Evaluate-for, model complexity)(generative model, Used-for, labeled sequence transduction)(generative model, Part-of, multi-space variational encoder-decoders)(speech perception, Is-a-Prerequisite-of, image-grounded conversation)(coreference resolution, Used-for, quality estimation)(automatic decipherment, Used-for, deciphering Ugaritic)(automatic decipherment, Used-for, deciphering Linear B)(speech processing, Used-for, representational spaces).
(generative and discriminative model, Part-of, parsing and language modeling framework)(parsing and language modeling framework, Evaluate-for, English Penn Treebank)(data programming paradigm, Is-a-Prerequisite-of, Snorkel framework)(Snorkel framework, Used-for, learning discourse structure)(generative step, Used-for, transforming heuristics into probability distributions)(Generative models, Compare, discriminative models)(cross-lingual transfer, Used-for, syntactic analysis in low-resource languages)(structured prior, Part-of, generative model)(structured prior, Used-for, cross-lingual transfer within a generative model)(deep neural network classifiers, Compare, discriminative models)(neural model, Is-a-Prerequisite-of, coherence models)(Hyperbolic Capsule Networks, Part-of, Multi-Label Classification)(Hyperbolic Capsule Networks, Evaluate-for, MLC performance improvements)(unsupervised relation extraction, Used-for, extracting relations between entities)(generative URE approaches,
(kernel methods, Part-of, data structure)(deep neural networks, Part-of, data structure)(Tree Kernels, Hyponym-Of, kernel methods)(Propagation Tree Kernel, Hyponym-Of, kernel methods)(Neural Machine Translation (NMT), Used-for, data structure)(Sequence-to-Dependency Neural Machine Translation (SD-NMT), Hyponym-Of, Neural Machine Translation (NMT))(propagation trees, Part-of, data structure)(dependency structure, Part-of, data structure)(abstract syntax trees (ASTs), Part-of, data structure)(transition set, Part-of, data structure)(Flickr entailment graphs, Part-of, data structure)(WordNet entailment graphs, Part-of, data structure)(entity and type labels, Part-of, data structure)(lattice structure, Part-of, data structure)(set of concepts, Part-of, data structure)(syntactic structures, Part-of, data structure)(syntactic knowledge, Part-of, data structure)(
(word distribution, Compare, word embeddings)(word distribution, Part-of, text data)(word distribution, Part-of, natural language)(word distribution, Used-for, word similarity)(word distribution, Used-for, entailment)(word embeddings, Used-for, aspect extraction)(word embeddings, Used-for, cross-lingual text classification)(multi-modal word distributions, Compare, word distributions)
(neural language modeling, Used-for, automatic generation of rhythmic poetry)(neural language modeling, Used-for, parsing natural language descriptions into source code)(neural language modeling, Used-for, generation of conversational text)(neural language modeling, Used-for, opinionated natural language generation)(neural language modeling, Used-for, open-vocabulary language modeling corpus)(neural language modeling, Used-for, sequence labeling framework).(phonetic encoding, Part-of, neural language modeling)(poetry generation, Is-a-Prerequisite-of, neural language modeling)(affective information, Part-of, neural language modeling)(topic model-like architecture, Part-of, neural language modeling)(character-level language models, Hyponym-Of, neural language modeling).(recurrent neural networks, Hyponym-Of, neural language modeling)(RNN, Hyponym-Of, neural language modeling)(LSTM, Hyponym-Of, neural language modeling)(recurrent networks, Conjunction, LSTM).
(speech recognition, Part-of, automatic speech recognition)(speech recognition, Evaluate-for, word error rate)(end-to-end automatic speech recognition, Compare, conventional DNN/HMM systems)(attention-based methods, Part-of, end-to-end automatic speech recognition)(connectionist temporal classification, Part-of, end-to-end automatic speech recognition)(hybrid CTC/attention architecture, Used-for, end-to-end automatic speech recognition)(speech recognition, Used-for, decoding)(Weighted finite state transducers, Used-for, speech recognition)(multi-speaker speech recognition, Hyponym-Of, speech recognition)(end-to-end automatic speech recognition, Used-for, multi-speaker speech recognition)(conversational-context aware end-to-end speech recognizer, Used-for, speech recognition)(word embedding, Used-for, speech recognition)(sentence embedding, Used-for, speech recognition)(e-WER, Evaluate-for, speech recognition)(transcribed data, Is-a-Prerequisite-of, measuring the performance
(crawling the web, Used-for, create parallel corpora)(parallel corpora, Used-for, machine translation systems)(sentence alignment, Part-of, parallel corpora)(sentence pair filtering, Part-of, parallel corpora)(fine-tuning, Compare, regularization)(fine-tuning, Used-for, improve NLP performance)(HiddenCut, Used-for, data augmentation)(HiddenCut, Used-for, improve generalization)(demographic biases, Part-of, pretrained language models)(social stereotypes, Part-of, pretrained language models)(Causal-Debias, Used-for, mitigate bias)(pretrained language models, Used-for, various natural language processing tasks)(matrix product operator, Used-for, over-parameterization)(parameter matrices, Part-of, pretrained language models)(parameter matrices, Used-for, fine-tuning)(statistics strategies, Used-for, select important parameter matrices)(dynamic strategies, Used-for, select important parameter matrices)(causal invariant perspective, Used
(conventional DNN/HMM systems, Part-of, ASR)(end-to-end automatic speech recognition, Used-for, acoustic frames)(end-to-end automatic speech recognition, Compare, conventional DNN/HMM systems)(attention-based methods, Part-of, end-to-end architectures)(connectionist temporal classification (CTC), Part-of, end-to-end architectures)(connectionist temporal classification (CTC), Compare, attention-based methods)(deep convolutional neural network (CNN), Hyponym-Of, neural networks)(deep pyramid CNN, Part-of, CNN architecture)(topic categorization, Used-for, text categorization)(sentiment classification, Used-for, text categorization)(free trade, Conjunction, Trans-Pacific Partnership)(matrix factorization, Compare, item recommendation)(text categorization, Used-for, domain classification)(state-of-the-art classification model, Part-of, sense representation research)(tf-idf, Used-for, text categorization)(context-dependency trees, Part-of,
(Deep Neural Networks, Hyponym-Of, Neural Machine Translation)  (Deep Neural Networks, Used-for, modeling complex functions)  (Neural Machine Translation, Part-of, optimization)  (recurrent neural networks, Hyponym-Of, Deep Neural Networks)  (gradient diffusion, Used-for, recurrent neural networks)  (linear associative units, Used-for, reduce gradient propagation path)  (linear associative units, Compare, LSTM unit)  (linear associative units, Compare, GRU)  (stochastic optimization, Compare, stochastic gradient Markov Chain Monte Carlo)  (stochastic gradient Markov Chain Monte Carlo, Used-for, optimization)  (attention mechanisms, Hyponym-Of, multi-task learning)  (constrained decoding, Hyponym-Of, deep highway BiLSTM)  (optimization, Used-for, automatic Pyramid)  (automatic Pyramid, Hyponym-Of, multi-document summarization)  (BONIE, Hypon
(predicate logic, Used-for, logical reasoning)(predicate logic, Used-for, entailment)(predicate logic, Part-of, logical form)(predicate logic, Compare, rule-based factuality prediction)(predicate logic, Compare, hybrid approach)(predictive coding, Used-for, learning representations)(rule-based factuality prediction, Evaluate-for, factuality prediction)(coreference resolution, Conjunction, predicate argument structure analysis)(typed entailment graphs, Evaluate-for, entailment relations)(Open Information Extraction, Evaluate-for, extracting triples)
(hidden markov model, Used-for, aggregating sequential crowd labels)(hidden markov model, Used-for, predicting sequences in unannotated text)(hidden markov model, Used-for, merging annotations from labelling functions)(hidden markov model, Used-for, named entity recognition)(hidden markov model, Used-for, neural alignment in NMT)(named entity recognition, Is-a-Prerequisite-of, transfer learning technique)(named entity recognition, Part-of, information extraction)(neural hidden markov model, Compare, attention-based NMT)(weak supervision, Used-for, named entity recognition)(conditional hidden markov model, Used-for, inferring true labels from multi-source noisy labels)(conditional hidden markov model, Compare, hidden markov model)(BERT embeddings, Part-of, conditional hidden markov model)(bert-ner model, Part-of, conditional hidden markov model)(attention-based neural machine translation, Compare, pure encoder-decoder sequence-to-sequence models)(attention-based neural
(decision tree, Part-of, machine learning)(decision tree, Used-for, prediction)(prediction, Hyponym-Of, machine learning task)(neural network model, Compare, decision tree)(neural network model, Used-for, prediction)(tree communication model, Used-for, prediction)(tree communication model, Hyponym-Of, decision tree)(graph convolutional neural network, Part-of, tree communication model)(graph recurrent neural network, Part-of, tree communication model)(Tree-LSTM, Hyponym-Of, decision tree)(decision tree, Compare, neural ranking approach)(reinforcement-learning, Used-for, decision making)(MemSum, Hyponym-Of, reinforcement-learning-based system)(crowdsourcing, Used-for, training data collection)(CrowdOpinion, Hyponym-Of, crowdsourcing approach)(Chinese Spelling Correction, Used-for, error correction)(error model, Part-of, Chinese Spelling Correction)(language model, Part-of, Chinese Spelling Correction
(bayesian network, Used-for, automatic text-based diagnosis system)(bayesian network, Used-for, learning interpretable relationships)(bayesian network, Compare, neural networks)(neural networks, Part-of, automatic text-based diagnosis system)(Entity-Aware Convolutional Neural Networks, Part-of, automatic text-based diagnosis system)(stacking Bayesian Network Ensembles, Part-of, proposed diagnosis framework)(collective classification, Used-for, predicting political frames)(neural networks, Used-for, event detection)(probabilistic inference, Used-for, estimating network parameters)(generative adversarial networks, Used-for, enhancing cross-language translation)(SocAoG, Used-for, inferring social relations)(recurrent network, Used-for, eliminating spurious features)(GAN-BERT, Used-for, sentence classification tasks)(neural topic model, Hyponym-Of, Wasserstein autoencoders framework)(neural topic model, Used-for, topic modeling)(split encoder, Con
(conditional probability, Used-for, computing the similarity between relations)(conditional probability, Part-of, divergence between entity pairs distributions)(conditional probability, Part-of, computing the likelihood of the label given the input)(divergence, Used-for, approximating similarity)(conditional probability, Used-for, estimating probabilities of candidate summaries)(conditional probability, Used-for, evaluating summaries' quality level).
(feature learning, Evaluate-for, language identification)(automatic question generation, Used-for, reading comprehension)(feature hashing, Evaluate-for, language identification)(human supervision, Part-of, active learning)(generative model, Compare, supervised learning)(guessing entities, Is-a-Prerequisite-of, GuessTwo)(neural word segmenter, Compare, current neural models)(generative neural network, Compare, traditional methods)(Several standard video captioning datasets, Conjunction, diverse automatic)(neural networks, Used-for, feature learning)(End-to-end learning, Part-of, neural models)(neural models, Compare, traditional methods).
(linguistics, Part-of, linguistics basic)(computational linguistics, Is-a-Prerequisite-of, linguistics basic)(neuropsychological assessments, Used-for, detecting MCI)(cognitive impairment, Evaluate-for, linguistic features)(sentence segmentation, Part-of, micro-planning)(referring expression generation, Part-of, micro-planning)(surface realization, Part-of, micro-planning)(lexicalisation, Part-of, micro-planning)(aggregation, Part-of, micro-planning)(symptomatic disfluencies, Evaluate-for, linguistic features)(deep neural networks, Used-for, neural machine translation)(syntactic information, Part-of, source syntax)(syntactic encoders, Used-for, neural machine translation)(heterogeneous segmentation criteria, Part-of, Chinese word segmentation)(dependency treebank, Part-of, dependency parsing)(internalization, Part-of, enculturation mechanism)(self-regulation, Part-of, enculturation mechanism
(seq2seq, Hyponym-Of, neural sequence-to-sequence models)(neural sequence-to-sequence models, Used-for, named entity recognition)(neural sequence-to-sequence models, Used-for, machine translation)(neural sequence-to-sequence models, Used-for, conversation)(seq2seq, Used-for, response generation)(seq2seq, Conjunction, BiLSTMs)(BiLSTMs, Compare, models that operate on argument component level)(neural sequence-to-sequence models, Is-a-Prerequisite-of, AliMe Chat)(AliMe Chat, Used-for, open-domain chatbot engine)(seq2seq, Part-of, AliMe Chat)(seq2seq, Used-for, open-domain chatbot engine)(seq2seq, Is-a-Prerequisite-of, AliMe Chat rerank model)(seq2seq, Compare, convolutional seq2seq model)(convolutional seq2seq model, Compare, Transformer model)(Transformer model,
(informed search, Used-for, neural search systems)(neural search systems, Part-of, information retrieval)(Entity-Duet Neural Ranking Model, Hyponym-Of, neural search systems)(Entity-Duet Neural Ranking Model, Used-for, informed search)(knowledge graphs, Used-for, improving neural ranking models)(knowledge graphs, Used-for, neural search systems)(ranking, Part-of, neural search systems)(discourse coherence, Evaluate-for, scoring rubrics)(discourse coherence, Hyponym-Of, language assessment metrics)(RST annotations, Part-of, discourse structure)(attention mechanism, Part-of, Transformer architecture)(context gates, Used-for, controlling source and target contributions)(lattice transformer, Used-for, speech translation)(JLPC, Used-for, analyzing interview transcripts)(contextualization, Evaluate-for, contextualized language models)(multi-head attention, Part-of, Transformer architecture)(multi-head attention, Used-for, capturing pairwise interactions in sequences)(Transformer, Used-for, natural language
(problem solving and search, Used-for, math problem solving)(problem solving and search, Used-for, reasoning)(problem solving and search, Used-for, geometry problem solving)(math problem solving, Part-of, end-to-end math problem solving system)(end-to-end math problem solving system, Is-a-Prerequisite-of, logical forms)(natural language input, Part-of, end-to-end math problem solving system)(hybrid approach, Conjunction, syntactic analyzer)(hybrid approach, Conjunction, lexicalized grammar)(formal ontology, Used-for, semantic disambiguation)(hybrid system, Evaluate-for, logical form precision)(hybrid system, Evaluate-for, logical form recall)(numerical values, Hyponym-Of, number symbols)(NumS2T, Used-for, sequence-to-tree network)(numerical properties prediction mechanism, Used-for, numerical categorization)(numerical properties prediction mechanism, Used-for, numerical comparison)(numerical values, Used
(neural machine translation nmt, Part-of, deep neural networks dnns)(neural machine translation nmt, Used-for, translation)(neural machine translation nmt, Compare, statistical machine translation)(neural machine translation nmt, Hyponym-Of, machine translation)(neural machine translation nmt, Part-of, natural language processing nlp)(bi-directional lstms, Used-for, encoding source sentence)(convolutional layers, Used-for, encoding source sentence)(deep architecture, Used-for, encoder rnns)(deep architecture, Used-for, decoder rnns)(gradient diffusion, Used-for, encoder rnns)(gradient diffusion, Used-for, decoder rnns)(linear associative units laus, Compare, lstm unit)(linear associative units laus, Compare, gru)(linear associative units laus, Used-for, addressing gradient diffusion)(nested attention layers, Used-for, grammatical error correction gec)(binary code, Used-for, calculating output layer in nmt)(layer-wise relevance propagation l
(phonological feature, Part-of, phoneme)(lexical resource, Used-for, phonological research)(distinctive feature, Hyponym-Of, phonological feature)(phonological feature, Used-for, phonotactic learning)(grapheme-to-phoneme conversion, Used-for, phonological transduction)(phonological information, Part-of, phonological feature)(phonological typology, Used-for, linguistic typology)(deep stochastic point process, Evaluate-for, phonological typology)(phonological distinctive features, Used-for, segment-level phonotactic acquisition)(distinctive features, Compare, feature-naive model).
(tuning pre trained language, Part-of, text classification approaches)(tuning pre trained language, Used-for, text generation tasks)(tuning pre trained language, Used-for, low-resource Named Entity Recognition)(tuning pre trained language, Part-of, text-based transfer learning techniques)(tuning pre trained language, Compare, task-specific model architectures)(tuning pre trained language, Used-for, multimodal sentiment analysis)(pragmatic reasoning, Used-for, improving the quality of learned meanings)(spelling error features, Evaluate-for, native language detection)(spelling error features, Part-of, lexical features)(character n-grams, Used-for, native language detection)(hierarchical attention network, Evaluate-for, reading comprehension style question answering)(language models, Evaluate-for, predicting distant supervision relation types)(pre-trained language models, Used-for, multimodal language)(attention and fusion, Part-of, hierarchical attention network)(dual adversarial transfer network, Used-for, low-resource Named Entity Recognition)(machine translation evaluation
(semantic representation, Used-for, NLP)(AMR, Hyponym-Of, semantic representation)(UCCA, Hyponym-Of, semantic representation)(GMB, Hyponym-Of, semantic representation)(UDS, Hyponym-Of, semantic representation)(AMR parsing, Used-for, AMR)(AMR generation, Used-for, AMR)(SMATCH, Evaluate-for, AMR parsing)(BLEU, Evaluate-for, AMR generation)(sequence-based AMR models, Evaluate-for, AMR parsing)(semantic roles, Part-of, AMR)(coreference, Part-of, AMR)(negation, Part-of, AMR)(Nystrom low-rank approximation, Used-for, kernel spaces)(kernelized neural network, Used-for, NLP tasks)(Tree Kernels, Hyponym-Of, expressive kernels)(deep neural networks, Compare, expressive kernels)(attention-over-attention reader, Used
(question answering, Compare, commonsense question answering)(knowledge bases, Part-of, question answering)(sequential question answering, Hyponym-Of, question answering)(semi-supervised question answering, Hyponym-Of, question answering)(conversational QA, Hyponym-Of, question answering)(open-domain question answering, Hyponym-Of, question answering)(reading comprehension, Hyponym-Of, question answering)(question representation, Part-of, question answering)(end-to-end neural network model, Used-for, question representation)(recurrent network, Evaluate-for, machine comprehension)(global knowledge, Used-for, question representation)(parse tree, Used-for, constituent-centric neural architecture)(parse tree, Evaluate-for, candidate answers)(generative model, Is-a-Prerequisite-of, Generative Domain-Adaptive Nets)(domain adaptation, Part-of, Generative Domain-Adaptive Nets)(sequential context, Used-for, sequential question answering)(knowledge base, Compare,
(code generation, Used-for, software development)(code generation, Used-for, generating Abstract Syntax Trees)(semantic parsing, Used-for, mapping natural language to executable programs)(semantic parsing, Used-for, question answering)(semantic parsing, Used-for, code generation)(semantic parsing, Used-for, translating natural language to SQL queries)(semantic parsing, Evaluate-for, natural language variation)(semantic parsing, Evaluate-for, compositional generalization)(neural semantic parser, Part-of, semantic parsing)(abstract syntax networks, Part-of, code generation)(abstract syntax networks, Used-for, representing outputs as abstract syntax trees)(contextual embeddings, Used-for, word sense disambiguation)(sequence-to-sequence models, Compare, span-based parser)(reinforcement learning, Conjunction, maximum marginal likelihood)(counterfactual learning, Evaluate-for, improving neural semantic parsing)(QA systems, Compare, conversational QA setting)(relation-aware self-attention mechanism, Part-of, text
(linguistic knowledge identification, Is-a-Prerequisite-of, alias identification)(alias identification, Used-for, proper nouns)(alias identification, Used-for, pronouns)(alias identification, Used-for, noun phrases)(languages, Conjunction, English)(languages, Conjunction, Chinese)(languages, Conjunction, Spanish)(languages, Conjunction, Italian)(linguistic knowledge, Used-for, pre-trained language models)(pre-trained language models, Evaluate-for, sentiment classification)(sentiment lexicon, Hyponym-Of, sentiment linguistic knowledge)(negation words, Hyponym-Of, sentiment linguistic knowledge)(intensity words, Hyponym-Of, sentiment linguistic knowledge)(multi-sentiment-resource Enhanced Attention Network, Used-for, sentiment classification)(sentiment-relevant information, Part-of, sentiment prediction)(linguistic knowledge, Part-of, Heterogeneous Linguistics Graph)(Heterogeneous Linguistics Graph, Used-for, language models)(l
(Controlled Text Generation, Is-a-Prerequisite-of, Conditional Text Generation)(Controlled Text Generation, Part-of, Conditional Masked Language Modeling)(Controlled Text Generation, Part-of, Pre-train and Plug-in Variational Auto-Encoder)(Conditional Text Generation, Hyponym-Of, Controlled Text Generation)(Conditional Masked Language Modeling, Used-for, Text Generation)(Pre-train and Plug-in Variational Auto-Encoder, Used-for, Flexible Conditional Text Generation)(Finite State Machine, Used-for, Neural Sequential Generation Process)(SAMA Model, Used-for, Job Posting Generation)(BART, Used-for, Text Summarization)(BART, Used-for, Question Answering)(BERT, Used-for, Machine Translation)(BERT, Used-for, Text Summarization)(Question-Answer Hierarchies, Part-of, SQUASH)(Salient Information, Used-for, Abstractive Summarization)(Structured Data, Used-for
(multilingual neural, Part-of, Multi-modal Neural Machine Translation)(multilingual neural, Part-of, multilingual model)(multilingual model, Used-for, multi-task learning)(multilingual model, Used-for, translating)(multilingual model, Used-for, language identification)(multilingual model, Used-for, semantic parsing)(multilingual model, Used-for, named entity recognition)(multilingual model, Used-for, crosslingual coreference)(multilingual model, Used-for, transferring pretrained NMT model)(multilingual model, Used-for, zero-shot translation)(semantic parsing, Part-of, multilingual model)(semantic parsing, Evaluate-for, multilingual GeoQuery dataset)(semantic parsing, Evaluate-for, multilingual ATIS corpus)(language identification, Evaluate-for, multilingual LID benchmarks)(language identification, Used-for, health tracking on Twitter)(multi-task learning, Used-for, sequence labeling)(semantic parsing, Used-for, natural language sentences)(sequence-to-tree model, Part-of, multi-task learning
(learn event, Compare, classify news)(learn event, Used-for, improving event coreference resolution performance)(learn event, Used-for, predicting fine-grained temporal relations)(learn event, Part-of, temporal information extraction)(event coreference chains, Compare, topic transition sentences)(event coreference chains, Compare, sub-event structure)(event coreference chains, Compare, inter-coreference chain correlations)(semantic parsing, Used-for, temporal information extraction)(latent meanings of morphemes, Compare, morphemes)(Global Context Layer, Used-for, temporal relations storage)(context-aware neural network model, Used-for, temporal information extraction)(linguistic features, Is-a-Prerequisite-of, classification improvements)(social interaction features, Part-of, finer-grained separation)(Nugget Proposal Networks, Used-for, event detection).
(visual dialogue, Part-of, multi-modal dialogue systems)(multi-modal dialogue systems, Conjunction, visual language grounding)(visual language grounding, Evaluate-for, robustness to adversarial perturbations)(adversarial perturbations, Compare, visually-similar adversarial examples)(visually-similar adversarial examples, Used-for, misleading neural image captioning systems)(Show-and-Fool, Is-a-Prerequisite-of, robustness implications)(image feature extraction, Part-of, visual language grounding)(CNN, Part-of, image feature extraction)(RNN, Part-of, language caption generation)(Recurrent Dual Attention Network, Hyponym-Of, visual dialogue)(ReDAN, Used-for, multi-step reasoning to answer image-related questions)(question-answering turn, Part-of, visual dialogue)(VisDial v1.0, Evaluate-for, ReDAN model's performance).
(commonsense evaluation, Used-for, assessing performance of models on commonsense reasoning tasks)(commonsense reasoning, Part-of, commonsense evaluation)(Commonsense Knowledge Graphs, Used-for, retrieving sets of commonsense facts for commonsense evaluation)(neural coreference models, Evaluate-for, their ability to resolve coreference)(common sense, Is-a-Prerequisite-of, intelligent systems)(ranking metric, Used-for, similarity estimation)(semantic similarity, Used-for, evaluating word embeddings)(BLEU, Used-for, evaluating machine translation systems)(co-reference resolution, Part-of, commonsense reasoning)(coreference linking actions, Part-of, neural coreference models)(reinforcement learning, Used-for, optimizing coreference evaluation metrics)(mention boundary detection, Part-of, co-reference resolution)(WSCR dataset, Used-for, fine-tuning language models for pronoun disambiguation problems)(multilingual contrastive pretraining, Used-for, enhancing sentence representations)(AmbER sets, Evaluate-for, entity disambigu
(sequence labeling model, Part-of, neural sequence labeling)(sequence labeling model, Used-for, named entity recognition)(neural sequence labeling, Compare, traditional sequence labeling)(sequence labeling model, Used-for, automatic identification of discourse modes)(sequence labeling model, Used-for, error detection in learner texts)(sequence labeling model, Used-for, chunking)(sequence labeling model, Used-for, POS-tagging)(neural sequence labeling, Evaluate-for, identifying discourse modes)(discourse modes, Used-for, improving automatic essay scoring)(Long Short Term Memory, Used-for, predicting sequences in unannotated text)(neural sequence labeling, Conjunction, Hidden Markov Model)(sequence labeling model, Used-for, question generation)(Feedforward Neural Network, Compare, Long Short Term Memory)(FOFE method, Used-for, encoding sentence fragments)(sequence labeling model, Compare, local detection approach)(graph-to-sequence learning, Compare, sequence-to-sequence learning)(semantic role labeling, Used-for
(argument mining, Part-of, argument invention)(factor graph model, Used-for, argument mining)(SVM, Used-for, argumentative relation prediction)(RNN, Used-for, argumentative relation prediction)(transitivity, Part-of, structure constraints)(adjacent relations, Part-of, dependencies)(propositions, Part-of, dependencies)(LambdaMART, Used-for, sentence-level supporting argument detection)(hyperdoc2vec, Conjunction, text embedding methods)(paper classification, Evaluate-for, hyperdoc2vec)(citation recommendation, Evaluate-for, hyperdoc2vec)(CANDELA, Used-for, counter-argument generation)(retrieval system, Part-of, CANDELA)(text planning decoder, Part-of, CANDELA)(content realization decoder, Part-of, CANDELA)(12 million articles, Used-for, CANDELA)(BERT, Evaluate-for, Argument Reasoning Comprehension Task)(argument trees, Part-of, complex arguments)(premises, Part-of, argument components)(
(conversational text, Part-of, LSTM language model)(Affect categories, Part-of, LSTM language model)(conversational text, Used-for, Affect-LM)(sentences, Part-of, conversational text)(Amazon Mechanical Turk, Evaluate-for, Perception studies)(word representations, Part-of, Affect-LM)(NER systems, Part-of, supervised machine learning models)(large amounts of manually annotated data, Part-of, NER systems)(cross-lingual NER, Used-for, annotation projection)(word embeddings, Hyponym-Of, distributed representations of words)(annotation projection, Part-of, creating automatically labeled NER data)(target language, Evaluate-for, NER system)(word embeddings, Used-for, cross-lingual NER)(classifier, Hyponym-Of, BabbleLabble)(natural language explanation, Used-for, BabbleLabble)(semantic parser, Used-for, converting explanations into labeling functions)(training classifiers, Used-for, noisy labels
(Neural Machine Translation, Used-for, word-level training)(EED network, Used-for, generate responses)(TF-IDF retrieval model, Used-for, retrieve conversation examples)(context, Used-for, retrieve responses)(similarity scores, Used-for, weigh retrieved responses)(sequence-level smoothing, Used-for, improve results)(token-level loss smoothing, Used-for, improve results)(segmentation error, Evaluate-for, Segment Pooling LSTM)(segment labeling, Part-of, text segmentation)(text segmentation, Hyponym-Of, text processing)(image caption evaluation, Compare, dialogue response evaluation)(BERTScore, Compare, new metric)(conversational QA benchmarks, Compare, human evaluation)(ground truth sequence, Evaluate-for, word-level training)(Entity Types, Used-for, enhance extraction accuracy)(Fine-grained entity types, Part-of, sentences)(detection-based OIE methods, Used-for, information extraction)(Hungarian algorithm, Compare, dynamic label assignment)(question
(interpretability method, Used-for, rationales)(interpretability method, Used-for, attention mechanisms)(interpretability method, Used-for, influence functions)(interpretability method, Evaluate-for, multi-hop reading comprehension)(interpretability method, Evaluate-for, operational risk classification)(interpretability method, Evaluate-for, claim verification)(interpretability method, Evaluate-for, automatic text-based diagnosis)(rationales, Used-for, text classifiers)(attention mechanisms, Used-for, NLP tasks)(influence functions, Used-for, GNN-based models)(attention-based neural model, Evaluate-for, sarcasm)(EPAr model, Evaluate-for, multi-hop reading comprehension)(Document Explorer, Is-a-Prerequisite-of, EPAr model)(Answer Proposer, Is-a-Prerequisite-of, EPAr model)(Evidence Assembler, Is-a-Prerequisite-of, EPAr model)(Word2Sense, Evaluate-for, computational NLP tasks)(SemiORC, Evaluate-for, operational risk prediction)(DTCA, Evaluate
(generated explanation, Used-for, training classifiers)(training classifiers, Used-for, annotations)(annotations, Part-of, labeling functions)(labeling functions, Part-of, BabbleLabble)(BabbleLabble, Used-for, generating noisy labels)(generated explanation, Compare, labels)(generated explanation, Evaluate-for, classifiers)(classifiers, Used-for, relation extraction tasks)(relation extraction tasks, Part-of, natural language processing)(generated explanation, Evaluate-for, model performance)(model performance, Evaluate-for, experimental results)(experimental results, Part-of, machine learning workflows)
(topic model, Used-for, language modeling)(topic model, Compare, LDA)(topic model, Is-a-Prerequisite-of, neural language model)(topic model, Used-for, text analysis)(topic model, Used-for, aspect extraction)(topic model, Used-for, interactive topic modeling)(aspect extraction, Hyponym-Of, topic model)(embedding space, Part-of, word embedding models)(distributional vector spaces, Part-of, language understanding tasks)(morph-fitting, Improve, distributional vector spaces)(adversarial multi-task learning, Improve, multi-task learning)(BiLSTMs, Used-for, classification)(linear associative units, Improve, gradient propagation)(multi-task learning setup, Improve, dependency parsing)(multi-task learning setup, Improve, sequence tagging)(sentence context, Improve, language modeling)(document context, Improve, language modeling)(error detection, Improve, annotation quality)(joint modeling, Improve, discourse relation prediction)(token-based sequence
(deep learning model, Used-for, semantic role labeling)(deep learning model, Part-of, NLP pipeline)(deep learning model, Used-for, question generation)(deep learning model, Used-for, named entity recognition)(deep learning model, Used-for, fake news detection)(deep learning model, Used-for, morphological disambiguation)(deep learning model, Used-for, translation accuracy)(deep learning model, Part-of, neural network)(deep learning model, Hyponym-Of, sequence-to-sequence learning)(deep learning model, Used-for, sentiment classification)(deep learning model, Used-for, text classification)(deep learning model, Used-for, email subject line generation)(neural network, Used-for, semantic role labeling)(neural network, Used-for, NLP)(sentence-level information, Is-a-Prerequisite-of, paragraph-level information)(attention-based sequence learning model, Used-for, question generation)(attention-based sequence learning model,
(relation classification, Part-of, relation extraction)(relation extraction, Hyponym-Of, natural language processing)(multimodal sentiment analysis, Part-of, natural language processing)(multimodal sentiment analysis, Compare, sentiment classification)(automatic question answering, Part-of, natural language processing)(cognitive NLP systems, Part-of, cognitive computing)(cognitive features, Used-for, sentiment polarity task)(cognitive features, Used-for, sarcasm detection task)(eye-movement patterns, Part-of, cognitive features)(EEG signals, Part-of, cognitive features)(brain-imaging, Part-of, cognitive features)(Convolutional Neural Network, Used-for, cognitive NLP systems)(gaze information, Part-of, cognitive features)(sarcasm labelled datasets, Evaluate-for, cognitive NLP systems)(sentiment labelled datasets, Evaluate-for, cognitive NLP systems)(Recurrent Neural Network, Used-for, dialogue act classification)(RNN, Hyponym-Of, Re
(automatic dialogue, Used-for, collecting additional symptoms)(task-completion dialogue agent, Used-for, training via reinforcement learning)(Deep Dyna-Q, Is-a-Prerequisite-of, automatic dialogue)(belief spans, Part-of, automatic dialogue)(dialogue agent, Evaluate-for, real user experience)(response selection, Part-of, automatic dialogue)(encoder-decoder style neural network, Used-for, argument generation)(task-oriented dialogue systems, Compare, pipeline-based methods)(dialogue policy learning, Part-of, automatic dialogue)(belief spans, Hyponym-Of, text spans).
(faceted summarization, Compare, abstractive summarization)(faceted summarization, Compare, query-based summarization)(faceted summarization, Part-of, document summarization)(query-based summarization, Used-for, highlighting relevant points)(abstractive summarization, Part-of, document summarization)(encode-attend-decode paradigm, Used-for, query-based summarization)(query attention model, Part-of, encode-attend-decode paradigm)(diversity based attention model, Used-for, preventing repeating phrases)(Neural sequence-to-sequence models, Used-for, abstractive text summarization)(hybrid pointer-generator network, Used-for, accurate information reproduction)(supervised framework, Used-for, optimization-based extractive multi-document summarization)(genetic algorithm, Used-for, automatic training data generation)(selective encoding model, Used-for, abstractive sentence summarization)(sentence encoder, Part-of, selective encoding model)(attention equipped decoder, Part-of, selective encoding model)(compressive summar
(social bias, Used-for, political ideology prediction)(demographic inference, Evaluate-for, social bias)(political leaning, Part-of, political ideology)(automatic political orientation prediction, Evaluate-for, political ideology)(propagation pattern, Compare, real stories)(rumor detection, Used-for, identifying fake news)(microblog posts diffusion, Part-of, social media)(multilingual connotation frames, Used-for, studying public sentiments)(user cognitive structure, Used-for, predictive model of income)(social interaction features, Evaluate-for, classification results)(antisocial behavior, Used-for, predicting conversation trajectory)(multimodal posts, Part-of, social media)(GCN, Used-for, social media user geolocation)(fake news, Compare, real stories)(Propagation Tree Kernel, Used-for, detecting rumors)(Semantic Relevance Based neural model, Compare, baseline systems)(language use, Used-for, political engagement characterization)
(distributed word representation, Used-for, modeling words)(distributed word representation, Part-of, NLP tasks)(distributed word representation, Used-for, word vectors)(distributed word representation, Evaluate-for, word similarity and analogy tasks)(distributed word representation, Evaluate-for, sentiment analysis)(character composition model, Compare, word-lookup model)(hierarchical Dirichlet process, Used-for, topic-sensitive representations)(constants, Part-of, low-rank subspace)(multilingual distributed representation, Used-for, cross-lingual document classification)(multilingual distributed representation, Part-of, multi-task modeling approach)
(domain aspect based sentiment, Part-of, cross-domain sentiment analysis)(domain aspect based sentiment, Compare, single-domain sentiment analysis)(aspect term extraction, Part-of, aspect-based sentiment analysis)(aspect sentiment classification, Part-of, aspect-based sentiment analysis)(dual recurrent neural network, Used-for, aspect term extraction)(dual recurrent neural network, Used-for, aspect sentiment classification)(χ2 test, Used-for, cross-domain sentiment classification)(cosine-similarity, Used-for, cross-domain sentiment classification)(opinion targets, Part-of, targeted sentiment analysis)(sentiment polarities, Part-of, targeted sentiment analysis)(span-based extract-then-classify framework, Used-for, targeted sentiment analysis)(OpinionDigest, Used-for, opinion summarization)(Key Point Analysis, Used-for, review summarization)(dependency trees, Used-for, aspect-based sentiment classification)(multimodal aspect-based sentiment analysis, Part-of, aspect-based sentiment analysis)(cross-domain data augmentation, Used-for, cross-domain aspect-based sentiment analysis
(reinforcement learning, Conjunction, maximum marginal likelihood)(reinforcement learning, Used-for, learning algorithm)(semantic parser, Part-of, semantic parsing)(syntactic, Evaluate-for, semantic fluency)(Sequence-to-Action, Used-for, semantic graph generation)(StructVAE, Used-for, semi-supervised semantic parsing)(UCCA parsing, Hyponym-Of, semantic parsing)(AMR, Conjunction, SDP)(AMR, Conjunction, Universal Dependencies)(semantic graph, Part-of, sentence meaning)(semantic parsing, Is-a-Prerequisite-of, natural language understanding tasks)(semantic parsing, Part-of, machine learning)(counterfactual learning, Used-for, neural semantic parsing)(sequence-to-sequence models, Evaluate-for, handling natural language variation)(semantic parse correction, Is-a-Prerequisite-of, system generating accurate interpretation)(semantic parsing, Hyponym-Of, Discourse Representation Theory)(semantic parsing, Is-a-Prerequisite-of, mapping natural language to formal
(representation learning, Evaluate-for, word similarity tasks)(representation learning, Evaluate-for, word analogy tasks)(morph-fitted vectors, Used-for, dialogue state tracking)(sequence-based AMR models, Conjunction, ordering variations of graph-to-sequence conversions)(sequence-to-sequence models, Compare, neural encoder-decoder transition-based parser)(distributed word representations, Used-for, modeling words in NLP tasks)(semantic representation, Compare, syntactic schemes)(sequence-based AMR models, Compare, traditional AMR models)(entity grid representation, Part-of, local coherence model)(bidirectional language models, Used-for, pretrained context embeddings)(pretrained context embeddings, Used-for, sequence labeling tasks).
(specializing word embeddings, Used-for, acquiring synonyms)(specializing word embeddings, Used-for, capturing stylistic similarity)(word embeddings, Hyponym-Of, vector space representations)(Pseudofit, Used-for, specializing word embeddings)(pseudo-sense, Part-of, Pseudofit)
(annotated training, Used-for, building training data)(distant supervision, Evaluate-for, annotated training)(lexical resources, Used-for, annotated training)(lexical features, Evaluate-for, annotated training)(human supervision, Part-of, annotated training)(error detection, Used-for, annotated training)(lexicon, Part-of, lexical resources)(question answering, Evaluate-for, annotated training)(question answering models, Part-of, annotated training)(machine comprehension, Evaluate-for, annotated training)(NLI models, Evaluate-for, annotated training)(semantic parsing, Evaluate-for, annotated training)(multi-document summarization, Evaluate-for, annotated training)(neural machine translation, Evaluate-for, annotated training).
(unsupervised bilingual word embeddings, Used-for, cross-lingual model transfer)(unsupervised bilingual word embeddings, Part-of, bilingual dictionary induction)(unsupervised bilingual word embeddings, Hyponym-Of, word embedding)(cross-lingual model transfer, Is-a-Prerequisite-of, unsupervised neural machine translation)(unsupervised bilingual word embeddings, Evaluate-for, mining parallel sentences)(cross-lingual word embeddings, Conjunction, denoising)(unsupervised bilingual word embeddings, Used-for, unsupervised neural machine translation)(unsupervised bilingual word embeddings, Evaluate-for, syntactic analysis tools)(denoising, Part-of, unsupervised neural machine translation)(back-translation, Part-of, unsupervised neural machine translation)(unseen words, Used-for, mining target-language sentences)(byte-pair encoding, Part-of, neural machine translation)(byte-pair encoding, Evaluate-for, out-of-vocabulary
(natural language explanations nles, Part-of, real-world question answering systems)(real-world question answering systems, Conjunction, question answering)(real-world question answering systems, Used-for, generating answer with natural language sentence)(COREQA, Part-of, sequence-to-sequence learning)(COREQA, Used-for, generating natural answers)(semantic units, Part-of, natural answer)(natural answer, Used-for, evaluating coherence of response)(encoder-decoder framework, Part-of, sequence-to-sequence learning)(encoder-decoder framework, Used-for, incorporating copying and retrieving mechanisms)(copying and retrieving mechanisms, Part-of, encoder-decoder framework)(copying mechanisms, Conjunction, retrieving mechanisms)(reading comprehension RC datasets, Used-for, developing natural-language understanding systems)(prerequisite skills, Conjunction, readability)(FOFE method, Used-for, encoding sentence fragment)(FFNN, Used-for, predicting entity label for text fragment)(generative grammar, Part-of, ONLG)(ONLG, Hyper
(dense retrieval, Compare, sparse retrieval)(dense retrieval, Compare, BM25)(dense retrieval, Used-for, document retrieval)(dense retrieval, Used-for, similarity search)(dense retrieval, Conjunction, neural network-based models)(dense retrieval, Conjunction, cross-lingual word embeddings)(dense retrieval, Used-for, cross-modal applications)(neural network-based models, Used-for, OpenQA)(neural network-based models, Used-for, counter-argument generation)(OpenQA, Used-for, text retrieval)(OpenQA, Used-for, reading comprehension)(knowledge graphs, Evaluate-for, dense retrieval)(retrieval systems, Compare, generation-based models).
(compositional distributional semantics model, Part-of, distributional semantic models)(Polish evaluation dataset, Used-for, compositional distributional semantics model)(compositional distributional semantics model, Evaluate-for, prediction of compositionality)(multiword expressions, Part-of, distributional semantic models)(distributional semantic models, Used-for, detecting compositionality)(Poincaré embeddings, Used-for, predicting compositionality)(distributional semantic models, Used-for, decoding fMRI patterns)(negation function, Part-of, semantic composition)(multiword expressions, Part-of, metaphor)(SentiBERT, Hyponym-Of, BERT)(phrase-level sentiment classification, Evaluate-for, SentiBERT)(negation, Part-of, compositional sentiment semantics)(Pixie Autoencoder, Used-for, functional distributional semantics)(Visual Genome dataset, Used-for, training functional distributional semantics)(functional distributional semantics, Hyponym-Of, distributional semantic models)(semantic
(aspect extraction, Part-of, aspect-based sentiment analysis)(aspect-based sentiment analysis, Used-for, evaluating sentiment polarities of given aspects or entities)(aspect-based sentiment analysis, Compare, general sentiment analysis)(aspect-category sentiment analysis, Hyponym-Of, aspect-based sentiment analysis)(aspect-term sentiment analysis, Hyponym-Of, aspect-based sentiment analysis)(attention mechanism, Used-for, improving aspect coherence)(topic models, Compare, word embedding models)(word embeddings, Is-a-Prerequisite-of, word embedding-based approach)(word embeddings, Used-for, learning semantic relationships)(convolutional neural networks, Used-for, predicting sentiment polarity)(gating mechanisms, Used-for, selecting sentiment features)(Semantic Parsing, Used-for, understanding sentences' structure)(Transformer, Used-for, machine translation)(Synaptic neural language models, Used-for, syntactic collocation of words)(word embeddings, Part-of, sentiment classification)(domain-sensitive embeddings, Evaluate-for
(object naming, Part-of, referring expression generation)(referring expression generation, Evaluate-for, real-world images)(object recognition, Compare, object names)(visual information, Conjunction, lexical information)(distributional word embeddings, Used-for, linking visual to lexical information)(individual predictors, Used-for, object names)(zero-shot learning, Evaluate-for, combining lexical and visual information)(different ways, Compare, similar performance)(aspect extraction, Part-of, aspect-based sentiment analysis)(topic models, Part-of, aspect extraction)(neural word embeddings, Used-for, discovering coherent aspects)(attention mechanism, Used-for, de-emphasizing irrelevant words)(vector space representations, Evaluate-for, word similarity)(spectral clustering, Used-for, word embeddings)(positive weights, Compare, negative weights)(sentiment analysis, Used-for, volatility prediction)(term weighting models, Part-of, sentiment analysis)(word embeddings, Conjunction, fact market data)(pre-trained word embeddings, Part
(sarcasm detection, Used-for, detecting sarcasm in tweets)(sarcasm detection, Evaluate-for, multi-modal messages)(multi-modal messages, Conjunction, texts and images)(text features, Part-of, multi-modal sarcasm detection)(image features, Part-of, multi-modal sarcasm detection)(attribute features, Part-of, multi-modal sarcasm detection)(bidirectional LSTM network, Used-for, extract text features)(text features, Used-for, sarcasm detection)(image features, Used-for, sarcasm detection)(attribute features, Used-for, sarcasm detection)(multi-modal hierarchical fusion model, Used-for, sarcasm detection)(sarcasm detection, Compare, textual sarcasm detection)(sarcasm detection, Compare, multi-modal sarcasm detection)(MUStARD dataset, Used-for, sarcasm detection research)(audiovisual utterances, Part-of, MUStARD dataset)(historical utterances, Part-of, MUSt
(recurrent neural tensor, Hyponym-Of, recurrent neural network)(recurrent neural network, Used-for, language modeling)(recurrent neural network, Used-for, automatic question answering)(recurrent neural network, Used-for, sentence summarization)(language modeling, Part-of, natural language processing)(automatic question answering, Part-of, natural language processing)(sentence summarization, Part-of, natural language processing)(recurrent neural network, Compare, hybrid code network)(recurrent neural network, Evaluate-for, dialog systems)(hybrid code network, Evaluate-for, dialog systems)(recurrent neural network, Evaluate-for, Chinese analysis)(back-propagation through time, Used-for, traditional training)(stochastic gradient Markov Chain Monte Carlo, Used-for, learn weight uncertainty)(attention-based recurrent neural network, Evaluate-for, entity extraction)(attention-based recurrent neural network, Evaluate-for, relation extraction)(recurrent neural network, Is-a-Prerequisite-of, conditional variational autoencoder)(deep residual bid
(neural parser, Compare, non-neural parser)(neural parser, Used-for, AMR parsing)(neural parser, Used-for, constituency parsing)(neural parser, Part-of, semantic parsing)(AMR parsing, Part-of, natural language understanding)(constituency parsing, Part-of, syntactic analysis)(syntactic analysis, Hyponym-Of, natural language parsing)(neural parser, Evaluate-for, generalization)(semantic parsing, Compare, syntactic analysis)(information extraction, Compare, summarization)(OONP parser, Hyponym-Of, neural parser)(semantic parsing, Part-of, neural representation)(neural representations, Evaluate-for, neural retrievers)
(fact checking article, Used-for, verifying claims)(fact checking article, Part-of, manual fact-checking initiatives)(manual fact-checking, Used-for, saving effort on repeated claims)(manual fact-checking, Evaluate-for, time efficiency)(fact checking article, Used-for, generating justifications for verdicts)(fact checking article, Used-for, identifying misinformation types)(misinformation, Conjunction, disinformation)(misinformation, Hyponym-Of, fake news)(fact checking article, Used-for, structuring misinformation stories)(fact-check article, Part-of, retrieval systems for fact-checking)(retrieval systems, Evaluate-for, accuracy)(retrieval systems, Evaluate-for, retrieving authoritative evidence)(retrieval systems, Evaluate-for, robustness on various problems)(retrieval systems, Hyponym-Of, information retrieval)(retrieval systems, Part-of, open-domain NLP tasks)(retrieval systems, Used-for,
(generative neural models, Part-of, generative retrieval)(constituency parsing, Is-a-Prerequisite-of, generative neural models)(sequence-to-sequence model, Used-for, slot filling)(generative adversarial networks (GANs), Part-of, generative retrieval)(neural machine translation, Part-of, generative retrieval)(generative conversational models, Part-of, generative retrieval)(Dialogue Act classification, Is-a-Prerequisite-of, generative conversational models)(generative probabilistic model, Part-of, generative retrieval)(text adversarial attack, Evaluate-for, generative retrieval)(unsupervised relation extraction, Evaluate-for, generative retrieval)(distantly supervised relation extraction, Evaluate-for, generative retrieval)(story generation, Part-of, generative retrieval)(context information, Used-for, generative conversational models)(specificity control variable, Used-for, generative conversational models)(emotional language generation, Evaluate-for, generative conversational models)(HEAD-QA,
(abstractive summarization, Is-a-Prerequisite-of, summarization task)(query-based summarization, Is-a-Prerequisite-of, summarization task)(extractive summarization, Is-a-Prerequisite-of, summarization task)(encode-attend-decode paradigm, Evaluate-for, query-based summarization)(encode-attend-decode paradigm, Evaluate-for, abstractive summarization)(query attention model, Part-of, encode-attend-decode paradigm)(diversity based attention model, Part-of, encode-attend-decode paradigm)(Neural sequence-to-sequence models, Used-for, abstractive text summarization)(coverage, Used-for, discouraging repetition)(pointer-generator network, Used-for, accurate reproduction of information)(query attention model, Used-for, query-based summarization)(selective encoding model, Used-for, abstractive sentence summarization)(sentence encoder, Part-of, selective encoding model)(attention equipped decoder, Part-of, selective encoding model)(selective gate network,
(neural machine translation, part-of, machine translation)(convolutional layers, used-for, encoding source sentence)(bi-directional LSTMs, compare, convolutional layers)(deep neural networks, used-for, modeling complex functions)(linear associative units, used-for, reducing gradient diffusion)(decentralized deep multiagent policies, used-for, learning communication strategies)(syntactic encoders, used-for, improving translation accuracy)(RNN encoder, hyponym-of, syntactic encoders)(Mixed RNN encoder, compare, Parallel RNN encoder)(sequence-to-dependency NMT, hyponym-of, machine translation)(grammar error correction, used-for, correcting word order and usage)(nested attention layers, used-for, grammatical error correction)(binary codes, used-for, reducing memory usage)(layer-wise relevance propagation, used-for, interpreting neural MT)(multi-modal neural machine translation, used-for, incorporating visual features)(zero-resource NMT, used-for
(dialogue state tracking dst, Part-of, task-oriented dialogue systems)(task-oriented dialogue systems, Used-for, estimating user goals and requests)(dialogue state tracking dst, Used-for, handling unknown slot values)(dialogue state tracking dst, Used-for, predicting current dialogue state)(Global-Locally Self-Attentive Dialogue State Tracker, Hyponym-Of, dialogue state tracking dst)(Neural Belief Tracking, Hyponym-Of, dialogue state tracking dst)(Selectively Overwriting Memory for Dialogue State Tracking, Hyponym-Of, dialogue state tracking dst)(Transferable Dialogue State Generator, Hyponym-Of, dialogue state tracking dst)(Global-Locally Self-Attentive Dialogue State Tracker, Used-for, improving tracking of rare states)(Neural Belief Tracking, Used-for, dialogue state update mechanism)(Selectively Overwriting Memory for Dialogue State Tracking, Used-for, efficient DST in open vocabulary-based settings)(Transferable Dialogue State Generator, Used-for
(cognate detection, Used-for, state of the art methods)(global constraints, Part-of, cognate detection)(global constraints, Used-for, rescoring)(rescoring, Evaluate-for, performance improvements)(cognate detection, Evaluate-for, different language pairs).
(generated sentence, Used-for, sentence summarization)(sentence encoder, Part-of, sequence-to-sequence framework)(selective gate network, Part-of, selective encoding model)(attention equipped decoder, Used-for, abstractive sentence summarization)(recurrent neural networks, Used-for, sentence encoder)(vector, Part-of, sentence)(lexical constraints, Used-for, incorporating auxiliary knowledge)(neural models, Used-for, poem generation)(memory augmented neural model, Conjunction, neural model and augmented memory)(attention mechanism, Conjunction, neural models and augmented memory)(Grid Beam Search, Used-for, sequence generation)(lexically constrained decoding, Used-for, incorporating constraints in models)(ROC story cloze task, Evaluate-for, model performance)(hierarchical recurrent networks, Part-of, ROC story cloze task model)(attention, Part-of, hierarchical recurrent networks)(discriminative models, Compare, generative models)(target-specific representations, Part-of, sentence)(target-sensitive memory networks, Used-for,
(visual semantic pretraining, Compare, distributional semantic representations)(visual semantic pretraining, Used-for, contrastive visual semantic pretraining)(CLIP, Part-of, visual semantic pretraining)(visual semantic pretraining, Evaluate-for, GPT-2 embeddings)(visual semantic pretraining, Used-for, next-word prediction)(CLIP, Hyponym-Of, multi-modal neural architecture)(neural language models, Compare, multi-modal neural architecture)(language models, Compare, neural language models)(BERT, Used-for, next-word prediction)(word embeddings, Conjunction, Poincaré embeddings)(artificial languages, Evaluate-for, structural knowledge transferability)(neural network encoders, hyponym-of, pre-trained language models)
(word embeddings, Part-of, contextual subword embeddings)(FastText, Compare, BPEmb)(BERT, Compare, BPEmb)(BERT, Compare, FastText)(multilingual named entity recognition, Used-for, contextual subword embeddings)(part-of-speech tagging, Used-for, contextual subword embeddings)(multilingual named entity recognition, Used-for, BPEmb)(part-of-speech tagging, Used-for, BPEmb)(multilingual named entity recognition, Used-for, BERT)(part-of-speech tagging, Used-for, BERT)(pretrained contextual subword embeddings, Conjunction, pretrained non-contextual subword embeddings)(ELMo, Conjunction, contextual word embeddings)(CLE models, Is-a-Prerequisite-of, cross-lingual transfer of NLP models)(cross-lingual word embeddings, Part-of, cross-lingual transfer).
(lexical relations, Compare, lexical expectations)(text classification, Used-for, dialogue state tracking)(semantic dependency parsing, Used-for, semantic relationships)(pre-trained language models, Compare, fine-tuned models)(transformer-based contextual word representations, Hyponym-Of, pre-trained language models)(syntactic sketches, Part-of, paraphrase generation)(hierarchical representations, Part-of, neural models)(complaints, Is-a-Prerequisite-of, sentiment analysis)(pragmatic interpretation, Evaluate-for, human communication)(BERT, Used-for, lexical relation classification)(pre-trained language models, Hyponym-Of, transformer-based contextual word representations)(word position deviation, Used-for, paraphrase generation)(HRQ-VAE, Used-for, syntactic sketches)(SphereRE, Used-for, lexical relation classification)(Layer-specific attention weights, Part-of, hierarchical representations)(automatic distinction, Used-for, lexical relations)(distillation vector embeddings, Used-for, lexical relation classification).
(morphological family, Part-of, morphologically rich languages)(morphological family, Evaluate-for, low-frequency word forms)(morphological family, Used-for, improving distributional vector spaces)(morphological family, Used-for, neural model for morphological inflection generation)(morphological family, Used-for, morphological disambiguation)(morphological family, Used-for, cross-lingual training)(morphological family, Used-for, Universal Dependencies corpora)(morphological family, Used-for, predicting syntactic traits of a word)(morphological family, Used-for, cross-lingual morphological tagging)(morphological family, Compare, character-level models)(morphological family, Used-for, semi-supervised learning of inflection generation)(morphological family, Used-for, analogical reasoning)
(vision language pre training, Part-of, cross-modal downstream tasks)(cross-modal downstream tasks, Used-for, vision language pre training)(vision language pre training, Conjunction, image recognition)(vision language pre training, Used-for, visual learning)(pre-trained language models, Evaluate-for, vision language pre training)(transformer, Used-for, vision language pre training)(E2E-VLP, Hyponym-Of, vision language pre training)(E2E-VLP, Evaluate-for, vision language pre training)(multimodal techniques, Evaluate-for, cross-modal downstream tasks)(unified transformer encoder-decoder architecture, Used-for, vision language pre training)(object detection, Evaluate-for, vision language pre training).
(action recognition, Used-for, image annotation)(action recognition, Used-for, scene understanding)(action recognition, Used-for, image retrieval)(question understanding, Evaluate-for, question answering)(semantic augmentation, Evaluate-for, question summarization)(pointer-generator networks, Compare, sequence-to-sequence attentional models)(REAT method, Compare, vanilla Seq2Seq model)(RankQA, Part-of, question answering)(REAT method, Used-for, neural response generation)(conventional paradigm, Part-of, question answering)(phrase retrieval, Hyponym-Of, sparse retrieval)(CuratedTREC, Hyponym-Of, sparse retrieval)(SQuAD-Open, Hyponym-Of, sparse retrieval)(semantic role labeling, Used-for, fact checking)(dual encoders, Used-for, answer retrieval).
(dialogue agents, Part-of, oriented dialogue system)(external database, Part-of, oriented dialogue system)(reinforcement learner, Part-of, oriented dialogue system)(task success prediction, Evaluate-for, oriented dialogue system)(encode-attend-decode paradigm, Used-for, oriented dialogue system)(encode-attend-decode paradigm, Part-of, dialogue systems)(query attention model, Part-of, oriented dialogue system)(intelligent assistants, Hyponym-Of, oriented dialogue system)(two-stage copynet, Part-of, oriented dialogue system)(dialogue state tracking, Part-of, oriented dialogue system)(Mem2Seq, Used-for, oriented dialogue system)(belief tracking, Part-of, oriented dialogue system)(response selection, Part-of, oriented dialogue system)(photo book dataset, Part-of, oriented dialogue system)(domain adaptation, Part-of, oriented dialogue system).
(neural semantic parser, Part-of, sequence-to-sequence model)(neural semantic parser, Used-for, semantic parsing)(neural semantic parser, Used-for, generate formal meaning representations)(neural semantic parser, Evaluate-for, performance on multilingual datasets)(neural semantic parser, Compare, grammar-based parser)(neural semantic parser, Evaluate-for, multilingual context)(neural semantic parser, Compare, monolingual semantic parser)(multilingual GeoQuery, Is-a-Prerequisite-of, semantic parser evaluation)(maximum marginal likelihood, Conjunction, reinforcement learning)(semantic graph, Part-of, semantic representation)(UCCA parsing, Used-for, semantic parsing improvement)(confidence modeling, Used-for, estimating confidence scores)(sequence-to-tree model, Is-a-Prerequisite-of, multi-task learning framework)(logical form, Part-of, semantic parsing)(Discourse Representation Theory, Used-for, generate formal meaning representations)(SMATCH score, Evaluate-for, MRS parser evaluation)(CNL
(automatic evaluation metrics, Compare, human judgements of response quality)(automatic evaluation procedure, Used-for, dialogue research)(ADEM, Used-for, automatic dialogue evaluation)(ADEM, Part-of, automatic evaluation procedure)(domain adaptability, Evaluate-for, LSTM neural network model)(syntactic information, Used-for, improving domain adaptability)(syntactic features, Part-of, syntactic information)(explicit syntactic features, Used-for, improving domain adaptability)(syntactic constraints, Part-of, Integer Linear Programming)(cross-domain setting, Evaluate-for, proposed model)(BLEU, Compare, ADEM)(semantic representations, Evaluate-for, crosslingual word similarity)(semantic representations, Evaluate-for, sentiment analysis)(concept-based multilingual embedding learning, Used-for, semantic representations)(concept-based multilingual embedding learning, Compare, previous approaches)(embedding learning by concept induction, Used-for, semantic representations)(concept-based multilingual embedding learning, Compare, previous approaches)(embedding learning by concept induction, Evaluate-for, crosslingual word similarity
(data text generation, Used-for, document summarization)(document summarization, Hyponym-Of, data text generation)(data text generation, Used-for, automatic question generation)(data text generation, Used-for, neural language model)(neural language model, Used-for, data text generation)(document summarization, Is-a-Prerequisite-of, abstractive summarization)(abstractive summarization, Part-of, document summarization)(video captioning, Used-for, data text generation)(sequence-to-sequence model, Used-for, data text generation)(data text generation, Used-for, text similarity measure)(text similarity measure, Part-of, natural language processing)(data text generation, Used-for, neural dialogue generation)(neural dialogue generation, Is-a-Prerequisite-of, end-to-end neural dialogue generation)(machine-generated poems, Evaluate-for, human evaluation)(multi-task learning, Used-for, data text generation)(adversarial methods, Used-for, text generation)
(discourse mode, Part-of, writing composition)(discourse mode, Used-for, automatic essay scoring)(discourse mode, Evaluate-for, sentences)(discourse mode, Hyponym-Of, narrative essays)(discourse mode, Hyponym-Of, Rhetorical Structure Theory)(discourse mode, Compare, stancetaking)(discourse mode, Used-for, essay scoring)(stancetaking, Hyponym-Of, politeness)(stancetaking, Hyponym-Of, affect)(stancetaking, Hyponym-Of, subjectivity)(discourse coherence, Part-of, human scoring rubrics for spoken language assessment)(Rhetorical Structure Theory, Part-of, discourse structure)(Rhetorical Structure Theory, Hyponym-Of, text categorization)(ArgRewrite, Used-for, revision analysis)(discourse segmenters, Used-for, discourse segmentation)(discourse segments,
(hate speech detection, Used-for, detecting online hate)(hate speech detection, Evaluate-for, model weaknesses)(hate speech detection, Evaluate-for, racial bias)(hate speech detection, Evaluate-for, effectiveness of bias reduction methods)(GCN, Used-for, social media user geolocation)(Gated Convolutional Bidirectional Attention-based Model, Used-for, off-topic spoken response detection)(bias, Evaluate-for, automatic hate speech detection models)(Gated neural network, Used-for, conversational-context aware end-to-end speech recognition)(hate speech detection models, Evaluate-for, performance)(HateCheck, Used-for, evaluating hate speech detection models)(VoxPopuli, Used-for, unsupervised representation learning)(Semi-supervIsed GeNerative Active Learning model, Used-for, Chinese text spam detection)(GCN, Is-a-Prerequisite-of, Gated Convolutional Bidirectional Attention-based Model)(parameter sharing and initialization strategy, Used-for, improving
(aspect category opinion sentiment, Hyponym-Of, aspect based sentiment analysis)(aspect category opinion sentiment, Is-a-Prerequisite-of, aspect-category sentiment analysis)(aspect category opinion sentiment, Is-a-Prerequisite-of, aspect-term sentiment analysis)(aspect-category sentiment analysis, Part-of, aspect based sentiment analysis)(aspect-term sentiment analysis, Part-of, aspect based sentiment analysis)(Gated Tanh-ReLU Units, Evaluate-for, aspect category opinion sentiment)(convolutional neural networks, Used-for, aspect category opinion sentiment)(product reviews, Evaluate-for, implicit aspects)(product reviews, Evaluate-for, implicit opinions)(aspect-category sentiment analysis, Compare, aspect-term sentiment analysis)(aspect category opinion sentiment, Hyponym-Of, Aspect-Category-Opinion-Sentiment Quadruple Extraction)(Restaurant-ACOS, Part-of, aspect category opinion sentiment)(Laptop-ACOS, Part-of, aspect category opinion sentiment)(aspect category opinion sentiment
(reading comprehension task, Is-a-Prerequisite-of, natural-language understanding systems)(gated self-matching networks, Used-for, reading comprehension task)(SQuAD dataset, Evaluate-for, reading comprehension task)(Cloze-style reading comprehension, Hyponym-Of, reading comprehension task)(GuessTwo, Used-for, reading comprehension task)(WikiReading dataset, Evaluate-for, reading comprehension task)(TriviaQA, Evaluate-for, reading comprehension task)(DuoRC, Evaluate-for, reading comprehension task)(MCTest, Evaluate-for, reading comprehension task)(attention-over-attention reader, Used-for, reading comprehension task).
(approach, Used-for, news recommendation)(news recommendation, Evaluate-for, alleviating information overload)(news recommendation, Part-of, user representation learning)(topic information, Part-of, news)(news encoder, Part-of, topic-aware news encoder)(user encoder, Part-of, topic-aware news encoder)(news encoder, Used-for, learning news representations)(news encoder, Used-for, learning from titles via CNN networks)(news encoder, Used-for, selecting important words via attention networks)(topic classification task, Conjunction, news encoder)(topic classification task, Conjunction, learning topic-aware news representations)(user encoder, Used-for, learning user representations from browsed news)(user encoder, Used-for, selecting informative news via attention networks)
(reasoning ability, Part-of, human intelligence)(reasoning ability, Part-of, artificial intelligence)(reasoning ability, Evaluate-for, neural network based inference models)(neural network based inference models, Used-for, Stanford Natural Language Inference Dataset)(reasoning ability, Evaluate-for, Dynamic Spatial Memory Network)(reasoning ability, Part-of, pragmatic reasoning)(pragmatic reasoning, Used-for, interpreting language in context)(reasoning ability, Evaluate-for, conversational reasoning model)(conversational reasoning model, Used-for, dialog systems).
(argument extraction, Part-of, information extraction)(argument extraction, Used-for, supporting argument detection)(supporting argument detection, Evaluate-for, claims)(argument extraction, Used-for, event extraction)(supporting arguments, Is-a-Prerequisite-of, LambdaMART ranker)(supporting arguments, Hyponym-Of, arguments)(supporting arguments, Compare, non-supporting arguments)(argument extraction, Part-of, NLP tasks)(information extraction, Part-of, NLP tasks)(event extraction, Part-of, NLP tasks).
(transition based parser, Used-for, parsing rhetorical structures)(transition based parser, Used-for, parsing the naked discourse tree)(transition based parser, Used-for, parsing agglutinative languages)(transition based parser, Used-for, semantic graph parsing for MRS)(transition based parser, Part-of, NLP)(transition based parser, Hyponym-Of, dependency parser)(dependency parser, Used-for, parsing)(arc-swift, Hyponym-Of, transition based parser)(MRS, Hyponym-Of, semantic representation)(AMR, Hyponym-Of, semantic representation)(policy gradient training, Used-for, IPS model training)(soft-attention based NMT, Compare, hard-attention based NMT)(memory networks, Used-for, discourse parsing)(Stack-LSTM, Used-for, transition-based AMR parsing)(word omission errors, Evaluate-for, NMT)(emotion-cause pair extraction, Used-for, parsing
(identifiability attention weight, Evaluate-for, trustworthiness of a model’s predictions)(ReCoSa, Used-for, identifying relevant contexts)(word level LSTM encoder, Part-of, ReCoSa)(self-attention mechanism, Part-of, ReCoSa)(attention weights, Used-for, further decoding process)(self-attention mechanism, Compare, traditional attention mechanism)(self-attention mechanism, Compare, cosine similarity)(attention weights, Compare, non-identifiable attention weights)(non-identifiability of attention weights, Evaluate-for, interpretability issues)(key vector, Part-of, attention mechanism)(value vector, Part-of, attention mechanism)(non-identifiable attention weights, Part-of, attention mechanism)(identifiable attention weights, Conjunction, hidden role of key vector)(decoupling key and value vector, Used-for, identifiable attention weights)
(syntactic generalization performance, Evaluate-for, neural network models)(syntactic generalization performance, Evaluate-for, model architecture)(syntactic generalization performance, Evaluate-for, training dataset size)(syntactic generalization performance, Used-for, evaluating human-like syntactic knowledge)(syntactic generalization performance, Compare, perplexity)(syntactic generalization performance, Compare, synthetic knowledge)(syntactic generalization performance, Part-of, neural network evaluation)(neural language models, Evaluate-for, syntactic generalization performance)(neural network models, Used-for, syntactic generalization performance)(model architecture, Evaluate-for, syntactic generalization performance)(syntactic knowledge, Evaluate-for, syntactic generalization performance)(training dataset size, Evaluate-for, syntactic generalization performance).
(discord relation, Hyponym-Of, discourse treebank)(RST annotation, Part-of, discourse treebank)(implicit discourse relation, Hyponym-Of, discourse treebank)(discourse structure, Part-of, discourse treebank)(SciDTB, Hyponym-Of, discourse treebank)(PDTB, Hyponym-Of, discourse treebank)(discourse relation identification, Part-of, discourse treebank)(Chinese Discourse Treebank, Hyponym-Of, discourse treebank)(discourse representation, Part-of, discourse treebank).
(distant supervision, Used-for, relation extraction)(distant supervision, Part-of, level distant relation extraction)(noise, Evaluate-for, model performance)(training data, Used-for, relation extraction)(dynamic transition matrix, Part-of, noise characterization)(curriculum learning based method, Hyponym-Of, learning method)(class ties, Part-of, distantly supervised relation extraction)(convolutional neural network, Used-for, joint relation extraction)(class imbalance problem, Evaluate-for, model training)(featux extraction, Part-of, statistical NLP)(bi-directional long short-term memory, Used-for, relation extraction)(Pocket Knowledge Base Population, Used-for, dynamic KB construction)(Open Information Extraction, Used-for, finding trigger words)(adversarial learning framework, Used-for, noise reduction)(Generative Adversarial Networks, Hyponym-Of, adversarial learning)(positive samples, Part-of, distant supervision)(GP-GNNs, Part-of, graph
(single document summarization, Hyponym-Of, document summarization)(single document summarization, Used-for, generating summaries)(single document summarization, Evaluate-for, informativeness)(single document summarization, Evaluate-for, fluency)(single document summarization, Evaluate-for, ROUGE metrics)(neural network models, Used-for, single document summarization)(abstractive summarization, Compare, extractive summarization)(sequence-to-sequence framework, Used-for, abstractive summarization)(abstractive summarization, Is-a-Prerequisite-of, email subject line generation)(graph-based attention mechanism, Used-for, abstractive summarization)(hierarchical encoder, Used-for, extractive document summarization)(attention-based extractor, Used-for, document modeling)(focus-attention mechanism, Used-for, Transformer-based encoder-decoder framework)(Bi-directional Selective Encoding with Template, Used-for, single document summarization).
(structured knowledge, Compare, unstructured text)(Neural Symbolic Machine, Part-of, structured knowledge)(Knowledge graphs, Used-for, structured knowledge)(EviNets, Used-for, structured knowledge)(Sequence-to-Dependency Neural Machine Translation, Used-for, structured knowledge)(neural dialogue generation, Used-for, structured knowledge)(Question Answering, Used-for, structured knowledge)(Open Information Extraction, Used-for, structured knowledge)(Knowledge base, Conjunction, structured knowledge)(EviNets, Conjunction, structured knowledge)(EviNets, Conjunction, unstructured text)(question-answering, Used-for, structured knowledge)
(entity linkage, Hyponym-Of, entity linking)(entity linkage, Part-of, NLP tasks)(entity linking, Part-of, knowledge base population)(entity linkage, Used-for, disambiguating entity mentions)(entity linking, Evaluate-for, mention clustering accuracy in coreference resolution)(entity linking, Used-for, aligning textual mentions to KB entries)(entity linking, Evaluate-for, effective linking methods)(entity linking, Evaluate-for, entity type prediction accuracy)(entity linking, Used-for, discovering knowledge in texts)(entity linking, Is-a-Prerequisite-of, knowledge enrichment in texts)(entity linking, Compare, coreference resolution)(entity linking, Used-for, state-of-the-art performance on benchmarks)(entity linking, Used-for, learning similarities between entities)(entity linking, Compare, entity embedding methods)(entity linkage, Used-for, pruning exclusive entities in KGs)(entity linkage, Compare, surface matching)(entity linkage, Hyponym-Of, zero-shot entity linking)(entity linkage, Evaluate-for
(knowledge graph completion, Part-of, Knowledge Graph Embeddings)(knowledge graph completion, Used-for, relation prediction)(knowledge graph completion, Used-for, link prediction)(knowledge graph completion, Evaluate-for, transductive learning)(KGC, Compare, holographic embeddings)(KGC, Compare, complex embeddings)(Feature-Rich Networks, Used-for, Knowledge Base Population)(Feature-Rich Networks, Part-of, knowledge graph completion)(KB completion, Part-of, knowledge graph completion)(transductive learning, Part-of, knowledge graph completion)(KG embeddings, Used-for, knowledge graph completion)(KG embeddings, Is-a-Prerequisite-of, link prediction)(adaptive sampler, Used-for, word embeddings)(adaptive sampler, Compare, negative sampler)(Freebase, Evaluate-for, Knowledge Base Completion)(attention-based feature embedding, Used-for, link prediction)(adversarially learned sampler, Hyponym-Of, adaptive sampler)
(fine grained entity typing, Hyponym-Of, entity typing)(fine grained sentiment analysis, Is-a-Prerequisite-of, sentiment intensity)(aspect extraction, Part-of, sentiment analysis)(CNN model, Used-for, aspect extraction)(BERT Masked Language Model, Used-for, generating hypernyms)(few-shot named entity recognition, Hyponym-Of, named entity recognition)(Few-NERD, Used-for, few-shot named entity recognition)(cross-lingual contrastive learning, Used-for, entity typing)(distant supervision, Used-for, ultra-fine entity typing)(supervised aspect extraction, Part-of, aspect-based sentiment analysis)(sentiment analyzer, Part-of, sentiment generator)(sentiment generator, Part-of, sentiment analysis)(entity heads, Hyponym-Of, named entity)(domain-specific embeddings, Hyponym-Of, embeddings)(vector-based model, Compare, box-based model)(entity mention detection, Used-for, entity typing)(E
(multilingual pre-trained, Used-for, multilingual embeddings)(multilingual pre-trained, Part-of, learning multilingual word representations)(multilingual pre-trained, Part-of, cross-lingual word embedding)(cross-lingual word embedding, Compare, multilingual embeddings)(cross-lingual word embedding, Part-of, multilingual word representations)(cross-lingual word embedding, Used-for, multilingual neural machine translation)(multilingual word representation, Used-for, multilingual neural machine translation)(word embedding models, Used-for, learning word vectors)(GloVe, Hyponym-Of, word embedding models)(multilingual pre-trained, Part-of, multilingual unsupervised NMT)(multilingual embeddings, Used-for, multilingual NER).
(stanford question answering dataset, Used-for, reading comprehension)(reading comprehension, Part-of, natural language processing)(constituent-centric neural architecture, Used-for, stanford question answering dataset)(constituents, Part-of, parse tree)(constituents, Part-of, constituent-centric neural architecture)(semantic similarity scoring, Used-for, EviNets)(knowledge bases, Conjunction, unstructured text documents)(question answering, Part-of, natural language processing)(sequential question answering, Is-a-Prerequisite-of, conversational QA)(parse tree, Used-for, constituent-centric neural architecture)(memory networks, Used-for, universal schema)(universal schema, Used-for, question answering)(semantic similarity scoring, Part-of, EviNets)(EviNets, Used-for, factoid question answering)(discourse relations, Used-for, question answering)(SciDTB, Evaluate-for, discourse dependency parsers)(question sequences, Part-of, conversational QA)(question and answer pairs,
(named entity recognition, Used-for, information extraction from biomedical abstracts)(named entity recognition, Used-for, news articles)(named entity recognition, Evaluate-for, generative model)(named entity recognition, Compare, traditional sequence labeling methods)(named entity recognition, Used-for, multilingual learning)(named entity recognition, Is-a-Prerequisite-of, cross-lingual NER)(named entity recognition, Evaluate-for, heuristic-based active learning)(named entity recognition, Evaluate-for, imitation learning)(named entity recognition, Compare, multilingual BERT)(named entity recognition, Used-for, lexical resources)(named entity recognition, Used-for, CoNLL 2003 NER task)(generative model, Evaluate-for, lexical resources)(traditional sequence labeling methods, Evaluate-for, sequential crowd labels)(long short term memory, Used-for, sequence prediction)(feedforward neural network, Used-for, local detection approach)(Bayesian annotation model, Used-for, markable identification)(Named-Entity Recognition, Hyponym-Of,
(neural language, Hyponym-Of, neural networks)(neural language, Part-of, Neural Symbolic Machine)(Neural Symbolic Machine, Part-of, sequence-to-sequence model)(Neural Symbolic Machine, Part-of, Lisp interpreter)(neural language, Used-for, language understanding)(language understanding, Evaluate-for, coherence and natural response)(program, Part-of, Neural Symbolic Machine)(sequence-to-sequence model, Part-of, Neural Symbolic Machine)(neural language, Hyponym-Of, REINFORCE)(REINFORCE, Used-for, optimize the task reward)(program execution, Part-of, Lisp interpreter)(Lisp interpreter, Used-for, perform program execution)(Neural Symbolic Machine, Evaluate-for, WebQuestionsSP dataset)(semantic parser, Used-for, convert natural language utterances)(semantic parser, Evaluate-for, SPADES and GRAPHQUESTIONS)(semantic parser, Evaluate-for, GEOQUERY and WEBQUESTIONS)(distributional vector
(chinese named entity, Part-of, named entity recognition)(named entity recognition, Part-of, natural language processing)(gazetteers, Used-for, named entity recognition)(chinese named entity, Compare, nested named entity)(graph neural networks, Used-for, named entity recognition)(there are challenges, Is-a-Prerequisite-of, chinese named entity recognition)(multilingual learning, Used-for, named entity recognition)(conditional softmax shared decoder architecture, Used-for, chinese named entity recognition)(dual adversarial transfer network, Used-for, chinese named entity recognition).
(political debate, Part-of, democratic political decision making)(political debate, Used-for, comparing candidates’ positions)(political debate, Used-for, Argument Mining)(political debate, Used-for, analyzing controversial topics)(Argument Mining, Used-for, identifying argumentative components)(argumentative components, Hyponym-of, premises)(argumentative components, Hyponym-of, claims)(legislators, Used-for, supporting bills)(active cosponsorship, Part-of, supporting bills)(passive cosponsorship, Part-of, supporting bills)(neural network model, Used-for, predicting clusters of activities)(political ideology prediction, Used-for, predicting out-of-distribution inputs)(graph models, Used-for, learning representation of political actors)(news outlets, Evaluate-for, political bias)(news outlets, Evaluate-for, factuality of reporting)(real-time detection, Used-for, profiling news outlets)
(sentence image, Used-for, visual reasoning)(sentence image, Part-of, visual reasoning language dataset)(sentence image, Hyponym-Of, visual data)(sentence image, Hyponym-Of, linguistic phenomena dataset)(sentence image, Is-a-Prerequisite-of, syntactic information)
(political bias, Part-of, media profiling)(factuality of reporting, Part-of, media profiling)(political bias, Compare, factuality of reporting)(political bias, Used-for, detecting likely fake news)(factuality of reporting, Used-for, detecting likely fake news)(media profiling, Used-for, detecting likely fake news)(what was written, Conjunction, who reads it)(who reads it, Conjunction, what was written about the target medium)(what was written, Evaluate-for, political bias)(what was written, Evaluate-for, factuality of reporting)(who reads it, Evaluate-for, political bias)(who reads it, Evaluate-for, factuality of reporting)(what was written about the target medium, Evaluate-for, political bias)(what was written about the target medium, Evaluate-for, factuality of reporting).
(KB-InfoBot, Part-of, Dialogue Agent)(Dialogue Agent, Used-for, Knowledge Bases)(Knowledge Bases, Compare, External Database)(Symbolic Query, Compare, Soft Posterior Distribution)(Soft Retrieval Process, Used-for, Reinforcement Learning)(Reinforcement Learning, Used-for, Task Success Rate)(End-to-End Training, Compare, Symbolic Operations)(Deep Dyna-Q, Used-for, Dialogue Policy Learning)(World Model, Part-of, Dialogue Agent)(World Model, Used-for, Simulated Experience)(Simulated Experience, Used-for, Dialogue Policy Learning)(Multimodal Language, Compare, Unimodal Sources)(CMU-MOSEI, Used-for, Sentiment Analysis)(CMU-MOSEI, Used-for, Emotion Recognition)(Dynamic Fusion Graph, Used-for, Multimodal Fusion)(Multimodal Dialogue Systems, Compare, Traditional Goal-oriented Dialogue Systems)(MMD, Part-of, Multimodal Dialogue Systems)(Visual Appearance, Part-of, Products)(Position
(factuality prediction, Part-of, NLU tasks)(factuality prediction, Used-for, event factuality prediction)(factuality prediction, Compare, previous models for factuality prediction)(factuality prediction, Is-a-Prerequisite-of, sentence assessment)(model distillation, Used-for, CLTC)(confidence modeling, Part-of, neural semantic parsers)(confidence modeling, Evaluate-for, quality of interpretation)(antisocial behavior prediction, Used-for, social systems)(sentence clustering, Used-for, thematic similarity metric)(table-to-text generation, Part-of, structured data transformation)(table-to-text generation, Evaluate-for, BLEU score)(review helpfulness modeling, Used-for, identifying quality content)
(knowledge graphs, Is-a-Prerequisite-of, link prediction)(knowledge graphs, Evaluate-for, entity linkage)(knowledge graphs, Used-for, KG embedding)(knowledge graphs, Part-of, knowledge base)(knowledge graph embeddings, Hyponym-Of, KG embedding)(KG embedding, Evaluate-for, link prediction)(KB-QA, Evaluate-for, question answering)(neural network model, Used-for, KB-QA)(CRF, Used-for, aspect extraction)(question representation, Part-of, neural network model)(dynamic memory-based network, Used-for, type description)(recurrent neural networks, Part-of, machine reading)(attention mechanism, Part-of, KBLSTM)(entity extraction, Part-of, machine reading)(event extraction, Part-of, machine reading)(KB, Part-of, question answering)(up-to-date entities, Evaluate-for, OpenDialKG)(narrative, Used-for, textworldsqa)(extraction model, Used-for
(human translator, Compare, neural machine translation)(neural machine translation, Part-of, natural language processing)(bi-directional LSTMs, Used-for, encoding source sentence)(convolutional layers, Compare, recurrent networks)(LSTM, Compare, LAU)(groundhog, Part-of, neural machine translation)(word reordering, Part-of, machine translation)(neural machine translation, Used-for, decoding)(neural machine translation, Evaluate-for, translation accuracy)(conjugate-to, Compare, recurrent neural networks)(deep neural networks, Used-for, modeling complex functions)(bi-directional LSTMs, Part-of, neural machine translation)(nested attention mechanism, Compare, traditional attention mechanism)(sequential encoder-decoder framework, Part-of, neural machine translation)(layer-wise relevance propagation, Used-for, interpreting machine translation)(posterior regularization, Used-for, integrating prior knowledge)(chunk-based decoders, Used-for, neural machine translation)(doubly-attentive decoder, Used
(book, Part-of, semantic dependency parsing)(TF-IDF based retrieval model, Used-for, retrieve similar conversation examples)(bookembedding framework, Evaluate-for, effectiveness)(grid-type recurrent neural networks, Used-for, model multi-predicate interactions)(continuous parser, Compare, policy gradient parser)(error-correcting codes, Used-for, robustness)(neural machine translation, Evaluate-for, BLEU scores)(recurrence neural network, Hyponym-Of, neural networks)(noncrossing graphs, Part-of, book)(Maximum Subgraph algorithm, Used-for, generate noncrossing graphs)(parsing, Evaluate-for, accuracy)(figure usage detection, Used-for, personal health mention detection)(multi-task learning, Used-for, train IPS model)(imitation learning, Hyponym-Of, unsupervised parsing)(personal health mention detection, Hyponym-Of, health information extraction).
(argument mining, Part-of, argument structure prediction)(argument mining, Part-of, stance polarity prediction)(argument mining, Part-of, stance intensity prediction)(argument mining, Part-of, proposition type prediction)(argument mining, Part-of, edge prediction between propositions)(argument mining, Part-of, claim extraction)(argument mining, Part-of, argument component identification)(argument mining, Part-of, argumentative relation prediction)(argument mining, Part-of, event argument extraction)(argument structure prediction, Used-for, non-tree-structured arguments)(stance polarity prediction, Part-of, stance detection)(stance intensity prediction, Part-of, stance detection)(stance polarity prediction, Part-of, polarity of a post’s stance)(stance intensity prediction, Part-of, intensity of a post’s stance)(proposition type prediction, Used-for, argument unit classification)(edge prediction between propositions, Used-for, discourse analysis)(event argument extraction, Part-of, argument mining)(claim extraction, Part-of, argument mining)(argumentative relation prediction, Used-for
(nlg model, Part-of, neural language model)(nlg model, Used-for, automatic generation of rhythmic poetry)(neural language model, Used-for, generating related sentences for a topic)(neural language model, Part-of, gated self-matching networks)(gated self-matching networks, Used-for, reading comprehension style question answering)(attention-over-attention reader, Evaluate-for, cloze-style reading comprehension)(sequence-to-sequence neural models, Hyponym-Of, nlg model)(memory augmented neural model, Hyponym-Of, nlg model)(nlg model, Used-for, automatic question generation)(conditional variational autoencoders, Hyponym-Of, nlg model)(conditional variational autoencoders, Used-for, generate diverse responses)
(neural network models, Used-for, multi-task learning)(shared layers, Part-of, multi-task learning)(task-invariant features, Part-of, multi-task learning)(adversarial multi-task learning framework, Is-a-Prerequisite-of, alleviating feature space interference)(extensive experiments, Used-for, demonstrate benefits)(shared knowledge, Used-for, transfer to new tasks)(sequence tagging, Compare, dependency parsing)(BiLSTMs, Compare, dependency parsing)(neural language model, Compare, constraint satisfaction problem)(phonetic encoding, Used-for, learn poetic devices)(generative neural language model, Used-for, learning representation of content)(discriminative weighted finite state machine, Used-for, constrain forms in poetry)(multi-space variational encoder-decoders, Hyponym-Of, labeled sequence transduction)(semi-supervised learning, Used-for, multi-space variational encoder-decoders)(supervised learning, Compare, semi-supervised learning)(Tree Kernels, Compare,
(natural language question, Used-for, natural response)(semantic parser, Used-for, natural language question)(natural language question, Is-a-Prerequisite-of, answer rationales)(COREQA, Used-for, natural language question)(natural language question, Evaluate-for, program learning)(natural language question, Part-of, question answering system)(natural language question, Used-for, emotion detection)(natural language question, Used-for, named entity recognition)(natural language question, Used-for, knowledge extraction)(natural language question, Evaluate-for, similarity measures)(semantic parsing, Part-of, question answering system)(answer rationales, Used-for, solving algebraic word problems)(emotion detection, Used-for, understanding individuals)(question answering system, Used-for, generating correct answers)(similarity measures, Used-for, plagiarism detection)(question answering system, Used-for, interpreting agents’ messages)(translation model, Used-for, interpreting agents’ messages).
(conversation model, Used-for, multi-turn conversation)(conversation model, Part-of, retrieval based chatbots)(response selection, Part-of, multi-turn conversation)(sequential matching network, Used-for, multi-turn conversation)(neural encoder-decoder models, Used-for, open-domain conversations)(conditional variational autoencoders, Used-for, conversation model)(latent variables, Used-for, conversation model)(Neural Belief Tracking, Used-for, conversation model)(dynamic neural semantic parsing framework, Used-for, conversation model)(generative models, Used-for, conversation model)(context-aware neural machine translation model, Used-for, conversation model)(Exemplar Encoder-Decoder network, Used-for, conversation model)(sentence compression models, Part-of, conversation model)(Neural Belief Tracking, Used-for, belief tracker)(PSO, Evaluate-for, task success)(RP, Evaluate-for, task success)(Sequential matching network, Part-of, conversation model).
(word-embedding models, Part-of, nlu task)(word analogy questions, Part-of, nlu task)(caption generation, Part-of, nlu task)(event coreference, Part-of, nlu task)(trigger detection, Part-of, nlu task)(aspect extraction, Part-of, nlu task)(cloze-style reading comprehension, Part-of, nlu task)(neural machine translation, Part-of, nlu task)(review spam detection, Part-of, nlu task)(time expressions recognition, Part-of, nlu task)(knowledge base completion, Part-of, nlu task)(semantic parsing, Part-of, nlu task)(video captioning, Part-of, nlu task).
(synthetic data, Part-of, deployment shift)(live user data, Part-of, deployment shift)(stale data, Part-of, temporal shift)(current data, Part-of, temporal shift)(training dataset, Part-of, source domain)(evaluation dataset, Part-of, target domain)(adversarial training, Used-for, domain adaptation)(domain separation network, Used-for, domain adaptation)(reading comprehension, Evaluate-for, Stanford Question Answering Dataset)(constituent-centric neural architecture, Used-for, reading comprehension)(parse tree, Used-for, generate candidate answers)(word sememe information, Used-for, word representation learning)(distributional word vectors, Used-for, retrofitting)(retrofitting, Hyponym-Of, semantic specialization)(recurrent neural networks, Conjunction, convolutional neural networks)(recurrent neural networks, Used-for, encoding natural language utterances)(SoPa, Used-for, combining neural representation learning with WFSAs)(WFSAs
(neural ranking, Used-for, ranking noun phrases)(PageRank, Hyponym-Of, neural ranking)(Salience Rank, Compare, Topical PageRank)(Topical PageRank, Compare, Salience Rank)(Salience Rank, Used-for, extracting keyphrases)(Topical PageRank, Used-for, ranking of noun phrases)
(machine translation model, Hyponym-Of, Neural Machine Translation)(Neural Machine Translation, Part-of, Deep Neural Networks)(Deep Neural Networks, Used-for, modeling complex functions)(bi-directional LSTM, Compare, convolutional layers)(convolutional layers, Used-for, encode the source sentence)(machine translation model, Evaluate-for, accuracy)(machine translation model, Evaluate-for, decoding speed)(linear associative units, Compare, LSTM unit)(linear associative units, Compare, GRU)(source syntax, Part-of, Neural Machine Translation)(source syntax, Used-for, improve translation accuracy)(target word sequence, Part-of, Sequence-to-Dependency Neural Machine Translation)(Sequence-to-Dependency Neural Machine Translation, Hyponym-Of, Neural Machine Translation)(target dependency structure, Part-of, Sequence-to-Dependency Neural Machine Translation)(hybrid neural model, Used-for, Grammatical Error Correction)(nested attention layers, Part-of, hybrid neural model)
(retrieval model, Used-for, response selection)(sequential matching network, Hyponym-Of, retrieval model)(response selection, Part-of, retrieval based chatbots)(response selection, Used-for, multi-turn conversation)(multi-turn conversation, Part-of, retrieval based chatbots)(retrieval model, Used-for, document retrieval)(retrieval model, Used-for, information retrieval)(retrieval model, Used-for, semantic parsing)(retrieval model, Used-for, cross-lingual OpenQA)(context-aware encoder-decoder model, Hyponym-Of, retrieval model)(retrieved datapoints, Part-of, retrieval model)(meta-learner, Conjunction, retrieval model)(meta-learner, Used-for, fast adaptation)
(nlp community, Part-of, natural language processing)(deep learning architecture, Used-for, knowledge base population)(Relation extraction, Used-for, knowledge base population)(QCN, Used-for, community question answering)(Incremental use, Used-for, nlp community)(text classification, Used-for, Text Deconvolution Saliency)(graph convolutional network, Used-for, classifying a user's occupational group)(Relative Negative Sentiment Bias, Evaluate-for, demographic bias)(human verbal communication, Part-of, natural language processing)(LSTM language model, Used-for, conversation text generation)(Text Deconvolution Saliency, Used-for, text analysis)(DNN comparison, Evaluate-for, nlp community).
(summarization model, Compare, encode-attend-decode paradigm)(summarization model, Hyponym-Of, abstractive summarization)(summarization model, Hyponym-Of, extractive summarization)(summarization model, Evaluate-for, ROUGE scores)(summarization model, Used-for, query-based summarization task)(summarization model, Used-for, abstractive sentence summarization)(summarization model, Used-for, multi-document summarization)(summarization model, Used-for, content selection)(abstractive summarization, Compare, extractive summarization)(query-based summarization, Hyponym-Of, abstractive summarization)(encode-attend-decode paradigm, Used-for, document attention model)(encode-attend-decode paradigm, Used-for, query attention model)(encode-attend-decode paradigm, Used-for, diversity based attention model)(neural sequence-to-sequence model, Used-for, abstractive text summarization)(ne
(bias, Part-of, pretrained language model)(bias, Used-for, relation extraction)(pretrained language model, Hyponym-Of, language model)(language model, Used-for, text generation)(language model, Used-for, humor recognition)(language model, Used-for, low-resource named entity recognition)(pretrained language model, Part-of, language representation model)(ERNIE, Used-for, capturing external knowledge)(ERNIE, Compare, BERT)(BERT, Compare, OpenAI Generative Pre-trained Transformer GPT)(distantly supervised relation extraction, Used-for, extracting relational facts)(multi-instance learning, Used-for, relation extraction).
(clinical event extraction, Is-a-Prerequisite-of, clinical documents)(clinical event extraction, Evaluate-for, performance on clinical datasets)(clinical event extraction, Part-of, DICE model)(clinical event extraction, Hyponym-Of, information extraction in the clinical domain)(DICE model, Evaluate-for, performance on clinical event extraction)(DICE model, Used-for, framing event extraction as a conditional generation problem)(DICE model, Conjunction, contrastive learning objective)(contrastive learning objective, Used-for, accurately deciding boundaries of biomedical mentions)(DICE model, Used-for, identifying entity mention boundaries)(DICE model, Conjunction, special markers and auxiliary mention identification task)(event extraction, Hyponym-Of, information extraction)(event extraction, Hyponym-Of, clinical event extraction)
(deep convolutional neural networks, Used-for, sentiment polarity classification)(multimodal sentiment detection, Used-for, image-text sentiment detection)(sentiment analysis, Compare, emotion analysis)(SentiBERT, Used-for, phrase-level sentiment classification)(phrase-level sentiment classification, Part-of, sentiment analysis)(sentiment-to-sentiment “translation”, Used-for, changing underlying sentiment of a sentence)(sarcasm, Related-to, sentiment)(sentiment lexicons, Used-for, advanced sentiment analysis)(sentiment embeddings, Used-for, capturing sentiment information)(sentiment detection, Part-of, sentiment analysis)(sentiment labeling, Part-of, sentiment analysis)(contrastive relation, Related-to, sentiment composition)(BLEU score, Evaluate-for, content preservation performance)(cross-domain transfer learning, Used-for, improving detection performance)(emotion lexicons, Related-to, sentiment lexicons)(parallel data, Used-for, training models)
(cross lingual word embeddings, Used-for, bilingual lexicon induction)(cross lingual word embeddings, Evaluate-for, bilingual classification)(cross lingual word embeddings, Evaluate-for, cross-lingual sentiment classification)(word embeddings, Part-of, cross lingual word embeddings)(parallel corpora, Used-for, cross-lingual word embeddings)(adversarial training, Used-for, cross lingual word embeddings)(bilingual lexicon induction, Evaluate-for, domain adaptation)(cross lingual word embeddings, Used-for, machine translation)(word similarity, Hyponym-Of, word embeddings)(neural word embeddings, Hyponym-Of, word embeddings)(domain adaptation, Used-for, statistical machine translation)(cross-lingual sentiment classification, Is-a-Prerequisite-of, cross lingual word embeddings)(bilingual tasks, Used-for, overcoming data sparsity)(bilingual tasks, Conjunction, cross-lingual classification)(monolingual embeddings, Part-of, cross lingual word
(semantic parsing datasets, Evaluate-for, semantic parsing)(Hearthstone dataset, Hyponym-Of, semantic parsing datasets)(Atis dataset, Hyponym-Of, semantic parsing datasets)(Jobs dataset, Hyponym-Of, semantic parsing datasets)(Geo dataset, Hyponym-Of, semantic parsing datasets)(multilingual GeoQuery corpus, Hyponym-Of, semantic parsing datasets)(multilingual version of the ATIS corpus, Hyponym-Of, semantic parsing datasets)(Overnight dataset, Hyponym-Of, semantic parsing datasets)(WikiSQL dataset, Hyponym-Of, semantic parsing datasets)(semantic parsing datasets, Used-for, evaluating semantic parsers)(Hearthstone dataset, Evaluate-for, code generation models)(Atis dataset, Evaluate-for, semantic parsing models)(Jobs dataset, Evaluate-for, semantic parsing models)(Geo dataset, Evaluate-for, semantic parsing models)(Overnight dataset, Evaluate-for, semantic parsing models)(WikiSQL dataset, Evaluate
(pre trained language model, Part-of, Universal Language Model Fine-tuning)(pre trained language model, Part-of, BERT)(pre trained language model, Part-of, neural language model)(pre trained language model, Used-for, sentiment classification)(Universal Language Model Fine-tuning, Used-for, transfer learning)(BERT, Used-for, NLP tasks)(Universal Language Model Fine-tuning, Evaluate-for, text classification tasks)(BERT, Evaluate-for, machine reading comprehension)(Universal Language Model Fine-tuning, Compare, BERT)(neural language model, Evaluate-for, query auto completion)(neural language model, Evaluate-for, rhythmic poetry generation)(BERT, Compare, ERNIE)(ERNIE, Hyponym-Of, pre trained language model)(pre trained language model, Used-for, language understanding)(pre trained language model, Used-for, reading comprehension)
(knowledge-driven conversational systems, Compare, retrieval-based models)(retrieval-based models, Compare, pretrained conversational agents)(diverse conversational corpus, Part-of, DialoGPT)(diverse conversational corpus, Used-for, knowledge-driven conversational systems)(diverse conversational corpus, Used-for, task-oriented dialogue systems)(KdConv, Part-of, diverse conversational corpus)(diverse conversational corpus, Used-for, DiscProReco)(DialoGPT, Used-for, dialogue annotation)(DiscProReco, Used-for, conversational discourse parsing)(Direct Learning, Part-of, diverse conversational corpus)(dropped pronoun recovery, Part-of, DiscProReco)(conversational discourse parsing, Part-of, DiscProReco)(dropped pronoun recovery, Used-for, conversational discourse parsing)(Style Transfer, Part-of, diverse conversational corpus)(open-domain dialogues, Compare, task-oriented dialogue systems)(response selection, Part-of, task-oriented dialogue systems)(Human Feedback, Part-of, diverse conversational
(reasoning datasets, Part-of, language and vision tasks)(reasoning datasets, Used-for, action recognition)(reasoning datasets, Used-for, visual reasoning language dataset)(reasoning datasets, Used-for, semantic analysis)(reasoning datasets, Used-for, crowdsourcing linguistically-diverse data)(reasoning datasets, Used-for, RC datasets)(reasoning datasets, Used-for, multi-hop QA)(RC datasets, Conjunction, DuoRC)(RC datasets, Evaluate-for, neural approaches)(DuoRC, Compare, SQuAD dataset)(semantic parsing, Hyponym-Of, reasoning datasets)(semantic parsing, Used-for, logical forms)(language model, Compare, LDA topic model)(language model, Evaluate-for, perplexity)(language model, Evaluate-for, coherence).
(knowledge base, Used-for, event knowledge)(KBLSTM, Part-of, neural model)(knowledge base, Part-of, KBLSTM)(knowledge base, Used-for, machine reading)(neural network model, Used-for, question representation)(cross-attention mechanism, Used-for, question representation)(event knowledge, Used-for, machine reading)(entity extraction, Part-of, machine reading)(event extraction, Part-of, machine reading)(knowlwege base, Used-for, machine reading)(neural model, Used-for, machine reading)(context, Part-of, sequence-to-dependency NMT)(dependency structure, Part-of, sequence-to-dependency NMT)(temporal information, Used-for, video captioning)(logically directed tasks, Used-for, video captioning)(background knowledge, Used-for, attention mechanism)(adversarial multi-criteria learning, Compare, single-criterion learning)(shared knowledge, Used-for, CWS)(sentence segmentation, Part-of
(neural machine translation, Hyponym-Of, machine translation)(bi-directional LSTM, Part-of, neural machine translation)(convolutional layers, Part-of, encoder)(gradient diffusion, Evaluate-for, deep architecture RNN)(linear associative units, Compare, LSTM unit)(linear associative units, Compare, GRU)(linear associative units, Used-for, reduce gradient propagation path)(parallel RNN encoder, Part-of, encoder)(hierarchical RNN encoder, Part-of, encoder)(mixed RNN encoder, Part-of, encoder)(sequence-to-dependency neural machine translation, Hyponym-Of, neural machine translation)(nested attention layers, Part-of, hybrid neural model)(layer-wise relevance propagation, Used-for, compute contribution of contextual word)(posterior regularization, Used-for, integrating prior knowledge into NMT)(chunk-based decoder, Hyponym-Of, word-level decoder)(multi-modal neural machine translation, Hyponym-Of, neural machine translation)
(word embeddings, Used-for, word analogy questions)(word embeddings, Used-for, caption generation)(word embeddings, Used-for, vector calculus)(Skip-Gram model, Used-for, learning word embeddings)(word vectors, Part-of, word embeddings)(word vectors, Exhibits, compositionality)(Skip-Gram model, part-of, word embeddings)(Skip-Gram model, connect-to, Sufficient Dimensionality Reduction)(word co-occurrences, Used-for, neural word embeddings)(bilingual word embeddings, part-of, word embeddings)(neural word segmentation, Used-for, word embeddings)(joint models, Used-for, Chinese word segmentation)(parsing, Used-for, dependency parsing)(semantic representations, Used-for, statistical parsing)(bilingual dictionaries, Used-for, bilingual word embeddings)(POS tagging, Part-of, dependency parsing)(Multi-Prototype Mention Embedding model, part-of, word embeddings)(Gaussian mixtures, form, multimodal word distributions)(energy-based max-margin objective, Used-for,
(task oriented dialogue system, Used-for, collaborative task success prediction in dialogues)(task oriented dialogue system, Used-for, handling open-domain user utterances)(task oriented dialogue system, Used-for, automatic diagnosis)(task oriented dialogue system, Used-for, document grounded conversations)(task oriented dialogue system, Evaluate-for, incorporating knowledge bases)(task oriented dialogue system, Evaluate-for, expressing specific emotions)(task oriented dialogue system, Evaluate-for, clarifying user needs)(task oriented dialogue system, Part-of, dialogue systems)(dialogue state tracking, Part-of, task oriented dialogue system)(Global-Locally Self-Attentive Dialogue State Tracker, Used-for, dialogue state tracking)(belief spans, Used-for, track dialogue beliefs)(memories, Part-of, Mem2Seq)(working memory, Part-of, WMM2Seq)(episodic memory, Part-of, WMM2Seq)(semantic memory, Part-of, WMM2Seq)(emotional words, Used-for
(natural language generation task, Hyponym-Of, Opinionated Natural Language Generation)(natural language generation task, Used-for, generating subjective responses triggered by users' agendas)(natural language generation task, Part-of, natural language processing)(Opinionated Natural Language Generation, Conjunction, generating subjective responses)(ONLG, Used-for, generating subjective responses)(Relational-Realizational grammar, Evaluate-for, Opinionated Natural Language Generation)(ONLG, Evaluate-for, human-like subjective responses)(sentence-level features, Compare, user- and county-level features)(formal semantic representations, Hyponym-Of, natural language generation task)(neural semantic parser, Evaluate-for, generating complex programs from natural language descriptions)(semantic-driven models, Compare, neural models)(semantic parsing, Part-of, natural language generation task)(multilingual model, Conjunction, semantic parsing)(parallel corpora, Used-for, Machine Translation)(parallel sentences, Used-for, generating high-quality
(information retrieval, Hyponym-Of, natural language processing)(Information Retrieval, Used-for, volatility prediction)(volatility prediction, Compare, factual market data)(word embeddings, Part-of, sentiment analysis)(word embeddings, Used-for, enhancing IR term weighting models)(factual market data, Conjunction, text resources)(TutorialBank, Used-for, facilitating NLP education)(Knowledge graph, Used-for, improving neural ranking models)(knowledge graph, Part-of, Entity-Duet Neural Ranking Model (EDRM))(neural search systems, Part-of, EDRM)(sentiment analysis, Used-for, volatility prediction)(neural generative models, Part-of, conversation models)(sentence functions, Part-of, sentence function classification models)(sentence function classification models, Evaluate-for, short-text conversations)(fuzzy TM matches, Used-for, augmenting NMT training data)(deep bilingual query-document representations, Used-for, boosting cross-lingual document retrieval)(semantic hashing, Used-for, fast similarity search
(morphologically rich languages, Hyponym-Of, languages)(distributional vector space models, Used-for, inducing accurate representations)(semantic lexicons, Part-of, improving distributional vector spaces)(language-specific rules, Used-for, generating morphological constraints)(inflectional forms, Part-of, the same word)(dialogue state tracking, Used-for, morph-fitted vectors)(neural language model, Hyponym-Of, large language model)(phonetic encoding, Part-of, neural language model)(rhyme, Part-of, common poetic devices)(rhythm, Part-of, common poetic devices)(alliteration, Part-of, common poetic devices)(constraint satisfaction problem, Is-a-Prerequisite-of, generative neural language model)(stochastic optimization, Compare, stochastic gradient Markov Chain Monte Carlo)(Bayesian learning algorithm, Used-for, learning weight uncertainty)(document context, Part-of, neural language model)(topic model-like architecture, Is-a-Prerequisite-of, neural language
(sentiment classifier, Part-of, sentiment classification)(sentiment classifier, Used-for, multimodal sentiment analysis)(sentiment classifier, Evaluate-for, performance improvement)(sentiment classifier, Evaluate-for, robustness to generalizability)(sentiment classifier, Is-a-Prerequisite-of, target-oriented sentiment classification)(sentiment classifier, Evaluate-for, aspect sentiment classification)(sentiment classifier, Evaluate-for, cross-domain sentiment classification)(sentiment classifier, Evaluate-for, sentence-level cross-lingual sentiment classification)(sentiment classifier, Evaluate-for, target-sensitive sentiment)(sentiment classifier, Evaluate-for, domain adaptation in sentiment analysis)(sentiment classifier, Used-for, detecting contractual obligations)(sentiment classifier, Used-for, detecting prohibitions)(sentiment classifier, Evaluate-for, aspect-level sentiment classification performance)(multimodal sentiment analysis, Is-a-Prerequisite-of, target-oriented sentiment classification)(aspect sentiment classification, Part-of, sentiment classification)(target-oriented sentiment classification, Part-of, sentiment classification)(sentence
(bilingual lexicon induction, Part-of, bilingual tasks)(bilingual lexicon induction, Used-for, word translations)(bilingual lexicon induction, Used-for, cross-lingual word embeddings)(cross-lingual embeddings, Part-of, bilingual lexicon induction)(cross-lingual embeddings, Used-for, bilingual dictionary induction)(bilingual dictionary induction, Part-of, unsupervised machine translation)(unsupervised machine translation, Used-for, bilingual lexicon induction)(adversarial training, Used-for, unsupervised cross-lingual word embeddings)(bilingual lexicon induction, Used-for, bilingual lexicon)(cross-lingual word embeddings, Evaluate-for, performance on downstream tasks)(hubness, Evaluate-for, accuracy of nearest neighbor)(bilingual lexicon induction, Part-of, machine translation system)(bilingual dictionary, Hyponym-Of, bilingual lexicon)(semi-supervision, Used-for, bilingual lexicon induction)(cross-lingual word embeddings, Hypon
(relation learning, Used-for, multi-lingual neural relation extraction framework)(relation learning, Compare, joint extraction of entities and relations)(relation learning, Used-for, hierarchical recurrent neural network)(relation learning, Part-of, neural network models)(relation learning, Is-a-Prerequisite-of, knowledge base completion)(deep neural networks, Compare, expressive kernels)(bandit structured prediction, Evaluate-for, variance reduction)(extractive multi-document summarization, Used-for, joint optimization)(emotion detection, Used-for, building emotional chatbots)(sequence-to-sequence models, Used-for, video captioning)(sequence-to-sequence models, Used-for, neural machine translation)(adversarial multi-task learning, Used-for, extensive experiments)(token-based sequence tagging, Compare, token-based dependency parsing)(dependency parsing, Compare, token-based sequence tagging)
(relation extraction task, Used-for, finding unknown relational facts)(relation extraction task, Part-of, information extraction)(relation extraction task, Used-for, Knowledge Base Population)(multi-lingual neural relation extraction framework, Used-for, relation extraction task)(mono-lingual attention, Part-of, multi-lingual neural relation extraction framework)(cross-lingual attention, Part-of, multi-lingual neural relation extraction framework)(distant supervision, Used-for, relation extraction task)(distant supervision, Evaluate-for, model performance)(dynamic transition matrix, Used-for, characterize noise)(hierarchical recurrent neural network, Used-for, relation extraction task)(Joint extraction of entities and relations, Part-of, information extraction)(bootstrapping, Used-for, learn dependency patterns)(distant supervision, Used-for, label data)(class ties, Used-for, improve relation extraction)
(event ontology, Part-of, event extraction)(event extraction, Used-for, extracting events)(zero-shot event extraction, Hyponym-Of, event extraction)(zero-shot event extraction, Part-of, textual entailment)(zero-shot event extraction, Part-of, question answering)(textual entailment, Hyponym-Of, zero-shot event extraction)(question answering, Hyponym-Of, zero-shot event extraction).
(research, Conjunction, language)(time token, Part-of, time expressions)(argument mining, Used-for, argumentative relation prediction)(software system, Used-for, implementing tasks)(KB-InfoBot, Used-for, retrieving information from knowledge bases)(paraphrase generation, Used-for, expanding natural language datasets)(Seq2seq models, Used-for, machine translation)(RNMT+ model, Part-of, neural networks)(SynTime, Is-a-Prerequisite-of, time expressions recognition)(Convolutional seq2seq model, Part-of, sequence-to-sequence modeling)(datasets, Conjunction, evaluating machine learning methods)(Mem2Seq, Used-for, task-oriented dialog systems)(crowdsourcing, Used-for, data collection)(action recognition, Used-for, image annotation)(factor graph model, Used-for, argument mining)(text-based user geolocation, Used-for, detecting dialectal terms)(fluency boost learning, Used-for, grammatical error correction)(modifiers, Part-of, time
(hypernyms, Part-of, taxonomy learning)(readability, Evaluate-for, RC datasets)(reading comprehension (RC), Hyponym-Of, language understanding task)(morph-fitting procedure, Evaluate-for, distributional vector space models)(REINFORCE, Used-for, structured prediction problem)(knowledge bases, Part-of, natural language processing tasks)(question-answer pairs, Evaluate-for, WebQuestionsSP dataset)(dialogue state tracking, Hyponym-Of, language understanding task)(parsing natural language descriptions, Used-for, program generation)(pre-trained word vectors, Used-for, Neural Belief Tracking framework)(inflectional forms, Part-of, morphological constraints)(semantic quality, Evaluate-for, word vector collection)(Utterance, Part-of, dialogue context)(character n-grams, Part-of, subword units)(morphological regularities, Part-of, language modeling task)(character trigram representations, Evaluate-for, bi-LSTMs performance)(log
(multilingual translation, Used-for, zero-shot translation)(multilingual translation, Used-for, simultaneous translation)(multilingual translation, Compare, bilingual translation)(neural machine translation, Evaluate-for, BLEU)(statistical machine translation, Evaluate-for, BLEU)(BLEU, Evaluate-for, translation quality)(neural machine translation, Compare, statistical machine translation)(simultaneous translation, Used-for, low-latency translation)(style transfer, Used-for, rephrasing text)(semantic parsing, Used-for, understanding sentence structure)(cross-lingual transfer learning, Used-for, training NLP models for low-resource languages)(cross-lingual word embeddings, Used-for, multilingual NLP systems)(multilingual connotation frames, Used-for, analyzing sentiments across languages)
(novel hierarchical attention network, Used-for, reading comprehension style question answering)(attention, Part-of, hierarchical attention network)(fusion, Part-of, hierarchical attention network)(DAG automaton, Hyponym-Of, formal device for manipulating graphs)(DAG transducer, Hyponym-Of, DAG automaton)(bleu score, Evaluate-for, NLG system)(fact verification, Used-for, verifying claims)(fact verification, Hyponym-Of, NLP tasks)(GEAR framework, Used-for, fact verification)(GEAR framework, Part-of, graph-based evidence aggregating and reasoning)(evidence graph, Part-of, GEAR framework)(evidence, Part-of, evidence graph)(BERT, Used-for, improving performance)(BERT, Hyponym-Of, pre-trained language models)(ERNIE, Compare, BERT)(ERNIE, Used-for, improving performance)(pre-trained language models, Used-for, capturing semantic patterns)(pre-trained language models, Evaluate
(neural summarization model, Part-of, deep neural networks)(neural summarization model, Part-of, encoder-decoder framework)(neural summarization model, Used-for, extractive summarization)(neural summarization model, Used-for, abstractive summarization)(neural summarization model, Used-for, Bi-directional Selective Encoding with Template)(neural summarization model, Used-for, Embednet)(encoder-decoder framework, Part-of, deep neural networks)(encoder-decoder framework, Hyponym-Of, neurual summarization model)(extractive summarization, Hyponym-Of, neurual summarization model)(abstractive summarization, Hyponym-Of, neurual summarization model)(deep neural networks, Part-of, encoder-decoder framework)
(word embeddings, Used-for, linguistic regularities)(linguistic regularities, Part-of, language)(linguistic regularities, Transfer, languages)(monolingual word embeddings, Requires, cross-lingual signals)(cross-lingual signals, Form-of, parallel corpus)(cross-lingual signals, Form-of, seed lexicon)(cross-lingual connection, Possible-without, supervision)(natural adversarial game, Used-for, training cross-lingual connection)(evaluation, Conjunction, unsupervised bilingual lexicon induction)(unsupervised bilingual lexicon induction, Used-for, word translations)(unsupervised bilingual lexicon induction, Used-for, bilingual dictionary induction)(unsupervised machine translation, Requires, adversarial cross-lingual word embedding)(bilingual dictionary induction, Facilitated-by, UBWE)(UBWE, Evaluate-for, adversarial word embedding technique)(UBWE, Part-of, UNMT)(bilingual tasks, Used-for, overcoming data sparsity
(cross lingual semantic parsing, Part-of, semantic parsing)(semantic parsing, Used-for, question answering)(semantic parsing, Part-of, natural language processing)(magic parsing, Conjunction, code generation)(code generation, Evaluate-for, BLEU score)(Bilingual Sentiment Embeddings, Hyponym-Of, cross lingual sentiment approaches)(Object-oriented Neural Programming, Used-for, document parsing)(DialSQL, Used-for, query generation)(DRT-based semantic parsing, Used-for, discourse representation)(SParC, Used-for, cross-domain semantic parsing)
(semantic parser, Used-for, converting natural language utterances to intermediate representations)(intermediate representations, Part-of, semantic parser)(predicate-argument structures, Part-of, intermediate representations)(predicate-argument structures, Used-for, mapping to target domains)(semantic parser, Trained-using, annotated logical forms)(semantic parser, Evaluate-for, SPADES)(semantic parser, Evaluate-for, GRAPHQUESTIONS)(semantic parser, Evaluate-for, GEOQUERY)(semantic parser, Evaluate-for, WEBQUESTIONS)(semantic parser, Component, deep learning model)(deep highway BiLSTM, Part-of, semantic role labeling)(semantic role labeling, Part-of, semantic parser)(LSTM, Is-a-Prerequisite-of, deep highway BiLSTM)(semantic embedding, Part-of, speech perception model)(reinforcement learning, Used-for, learning semantic representations)(semantic representation, Part-of, grounded verb semantics)(dependency graph, Used-for, noncrossing graphs)(noncrossing graphs, Part-of, semantic dependency
(parallel corpora, Used-for, machine translation)(parallel corpora, Used-for, cross-lingual word sense disambiguation)(EuroSense, Part-of, Europarl parallel corpus)(morphological analysis, Used-for, predicting syntactic traits)(multilingual BERT, Is-a-Prerequisite-of, zero-shot cross lingual model transfer)(language-invariant features, Conjunction, language-specific features)(language-specific features, Conjunction, mixture-of-experts models)(multilingual embeddings, Hyponym-Of, zero-resource setting)(cross-lingual transfer, Used-for, building syntactic analysis tools)(multilingual representations, Used-for, cross-lingual transfer)(cross-lingual transfer, Evaluate-for, syntactic analysis tools)(multilingual BERT, Part-of, multilingual masked language models)(multilingual BERT, Part-of, multilingual representations)(morphological analysis, Evaluate-for, POS tagging)(dependency parsing, Evaluate-for,
(multimodal machine translation, Hyponym-Of, neural machine translation)(neural machine translation, Used-for, translation tasks)(multimodal machine translation, Used-for, translation tasks)(doubly-attentive decoder, Part-of, multimodal machine translation)(spatial visual features, Part-of, multimodal machine translation)(convolutional neural networks, Used-for, spatial visual features)(source-language words, Part-of, multimodal machine translation)(target language, Part-of, multimodal machine translation)(back-translated in-domain multi-modal data, Used-for, multimodal machine translation)(sequencial encoder-decoder framework, Compare, multimodal machine translation)(parse trees, Compare, multimodal machine translation) (sentence-alignment, Part-of, multimodal machine translation)
(query-based summarization, Is-a-Prerequisite-of, summarization dataset)(encode-attend-decode paradigm, Used-for, query-based summarization)(query attention model, Part-of, query-based summarization)(document attention model, Part-of, encode-attend-decode paradigm)(diversity based attention model, Used-for, query-based summarization)(hybrid pointer-generator network, Used-for, query-based summarization)(coverage mechanism, Used-for, query-based summarization)(selective encoding model, Used-for, abstractive sentence summarization)(sentence encoder, Part-of, selective encoding model)(selective gate network, Part-of, selective encoding model)(attention equipped decoder, Part-of, selective encoding model)(hierarchical encoder, Used-for, document summarization)(SWAP-NET, Used-for, extractive summarization)(abstractive summarization, Is-a-Prerequisite-of, summarization dataset)(extractive summarization, Is-a-Prerequisite-of, summarization dataset
(dependency parse, Used-for, computational argumentation mining)(dependency parse, Compare, sequence tagging)(dependency parse, Part-of, multi-task learning)(dependency parse, Used-for, natural language processing)(BiLSTM, Used-for, sequence tagging)(neural network, Used-for, parsing)(joint learning, Evaluate-for, performance)(dependency parsing, Compare, sequence tagging)(dependency parser, Used-for, semantic dependency parsing)(algorithm, Used-for, deep dependency parsing)(Maximum Subgraph algorithms, Used-for, semantic dependency parsing)(LSTM, Used-for, joint extraction)(neural network, Used-for, joint extraction)(Covington parser, Hyponym-Of, arc-eager dependency parser)(dynamic oracle, Compare, monotonic version)(dynamic oracle, Used-for, non-monotonic transition system)(AM tasks, Part-of, CoNLL-X shared tasks)(AM tasks, Part-of, CoNLL-XI shared
(image text retrieval, Used-for, cross-modal information retrieval)(image text retrieval, Evaluate-for, VisualSparta)(VisualSparta, Hyponym-Of, image text retrieval)(VisualSparta, Evaluate-for, accuracy)(VisualSparta, Evaluate-for, efficiency)(Intra-modal Self-attention Distance, Evaluate-for, relation consistency)(Inter-modal Alignment on Intra-modal Self-attentions, Used-for, optimizing Intra-modal Self-attention Distance)(Simplified LXMERT, Hyponym-Of, multimodal pre-training models)(Simplified LXMERT, Evaluate-for, Image-Text Retrieval task)(multilingual moment retrieval, Hyponym-Of, cross-modal retrieval).
(modified natural question dataset, Part-of, Natural Questions dataset)(RikiNet, Used-for, reading Wikipedia pages for natural question answering)(dynamic paragraph dual-attention reader, Part-of, RikiNet)(multi-level cascaded answer predictor, Part-of, RikiNet)(dynamic paragraph dual-attention reader, Used-for, representing document and question)(multi-level cascaded answer predictor, Used-for, obtaining answer spans)(Natural Questions dataset, Used-for, evaluating RikiNet)(F1 score, Evaluate-for, long-answer tasks)(F1 score, Evaluate-for, short-answer tasks)(ensemble RikiNet, Compare, single RikiNet)(QuASE, Used-for, sentence encoding)(BERT, Used-for, QuASE)(QuASE, Evaluate-for, downstream tasks)(model-based retrieval, Conjunction, text retrieval)(model-based retrieval, Compare, index-based retrieval)(TOME, Used-for, model-based retrieval)(tokenized URLs, Part-of, TOME)(two-stage
(political debate, Conjunction, argument components)(political debate, Part-of, political campaigns)(argument components, Hyponym-Of, premises and claims)(Argument Mining, Used-for, political debates)(Neural Network architectures, Evaluate-for, Argument Mining)(feature-rich SVM learners, Evaluate-for, Argument Mining)(lexical features of tweets, Part-of, global probabilistic models)(network-based behavioral features, Part-of, global probabilistic models)(political framing, Compare, argument components).
(prompt based model, Compare, traditional techniques)(prompt based model, Used-for, question answering)(prompt based model, Used-for, text generation)(prompt based model, Part-of, natural language processing)(neural network, Part-of, attention-over-attention reader)(attention-over-attention reader, Used-for, reading comprehension)(attention mechanism, Part-of, decoder)(attention mechanism, Used-for, information representation)(geolocation prediction, Hyponym-Of, neural network)(attention mechanism, Used-for, word alignment)(NMT, Hyponym-Of, neural machine translation)(NMT, Part-of, neural network)(event detection, Part-of, natural language processing)(event detection, Used-for, identifying events)(question classification, Part-of, natural language processing)(group sparse autoencoders, Part-of, neural network)(group sparse CNNs, Part-of, neural network)(bi-directional LSTMs, Part-of, neural network)(neural machine translation, Hyponym-
(sentence representation learning, Used-for, sentence summarization)(sentence representation learning, Used-for, question answering)(sentence representation learning, Used-for, rumor detection)(sentence representation learning, Is-a-Prerequisite-of, neural machine translation)(sentence representation learning, Compare, word representation learning)(sentence representation learning, Is-a-Prerequisite-of, ABSTractive text summarization)(sentence representation learning, Used-for, encoding natural language utterances)(sentence representation learning, Used-for, interpretable response generation)(neural machine translation, Used-for, sentence representation learning)(word representation learning, Used-for, encoding natural language utterances)(question answering, Used-for, passage representation)(rumor detection, Evaluate-for, rumor identification)(ABSTractive text summarization, Compare, extractive text summarization)(word representation learning, Used-for, word similarity detection)(word representation learning, Evaluate-for, word analogy tasks)(attention scheme, Part-of, sememe-encoded models)(selective gate network, Part
(relation extraction, Compare, shot relation extraction)(relation extraction, Part-of, knowledge base)(relation extraction, Compare, distant supervision)(relation extraction, Compare, dependency parsing)(dependency parsing, Hyponym-Of, natural language processing)(distant supervision, Part-of, relation extraction)(neural network model, Part-of, relation extraction)(embedding sequences, Part-of, relation extraction)(experimental results, Evaluate-for, relation extraction)(PKB, Used-for, knowledge base)
(named entity relation, Part-of, Named Entity Recognition)(named entity relation, Hyponym-Of, Entity Recognition)(named entity relation, Used-for, Named Entity Extraction)(named entity relation, Used-for, Mention Detection)(named entity relation, Used-for, Multimodal Named Entity Disambiguation)(named entity relation, Compare, named entity disambiguation)(named entity recognition, Part-of, Knowledge Graph)(named entity recognition, Used-for, Cross-lingual NER)(named entity recognition, Used-for, Weakly Supervised Approaches)(named entity recognition, Evaluate-for, Pretty Strong Performance)(named entity recognition, Evaluate-for, State-of-the-art Results)(named entity recognition, Hyponym-Of, Entity Recognition)(named entity disambiguation, Part-of, Named Entity Recognition)(named entity disambiguation, Used-for, Knowledge Base completion)(named Entity, Part-of, Text)(recurrent neural network, Compare, bidirectional LSTM)(recurrent neural network, Compare
(Reading Comprehension MRC Model, Used-for, Zero Pronoun Resolution)(Reading Comprehension MRC Model, Used-for, Reading Comprehension)(Cloze-style Reading Comprehension, Hyponym-Of, Reading Comprehension)(Stochastic Answer Network, Is-a-Prerequisite-of, Reading Comprehension MRC Model)(Hierarchical Attention Network, Is-a-Prerequisite-of, Reading Comprehension MRC Model)(Distantly Supervised Open-Domain Question Answering, Is-a-Prerequisite-of, Reading Comprehension MRC Model)(Attention-based Sequence Learning Model, Is-a-Prerequisite-of, Reading Comprehension MRC Model)(Question Answering, Part-of, Reading Comprehension)(TriviaQA, Used-for, Reading Comprehension)(SQuAD, Used-for, Reading Comprehension)(MCTest, Used-for, Reading Comprehension)(Stanford Question Answering Dataset, Hyponym-Of, Reading
(reading comprehension, Is-a-Prerequisite-of, machine reading comprehension)(reading comprehension, Used-for, question answering)(gated self-matching networks, Used-for, reading comprehension)(pointer networks, Used-for, locating answers)(SQuAD, Evaluate-for, reading comprehension)(RNNs, Used-for, reading comprehension)(attention-over-attention reader, Used-for, cloze-style reading comprehension)(prerequisite skills, Evaluate-for, readability)(Common Nouns dataset, Evaluate-for, neural reading comprehension model)(TriviaQA, Evaluate-for, reading comprehension)(DuoRC, Evaluate-for, reading comprehension)(stochastic answer network, Used-for, multi-step reasoning)(hierarchical attention network, Used-for, question answering)(open-domain question answering, Used-for, finding answers)(distantly supervised open-domain question answering, Used-for, question answering)
(concept, relation, concept)(complex named entity, Is-a-Prerequisite-of, named entity recognition)(fixed-size ordinally forgetting encoding, Used-for, local detection approach)(fixed-size ordinally forgetting encoding, Used-for, named entity recognition)(feedforward neural network, Used-for, named entity recognition)(feedforward neural network, Used-for, local detection approach)(local detection approach, Compare, sequence labeling methods)(sequence labeling framework, Used-for, named entity recognition)(sequence labeling framework, Used-for, mention detection)(language modeling objective, Used-for, sequence labeling framework)(heuristic-based active learning, Compare, IL-based active learning)(imitation learning, Used-for, active learning policy)(multimodal named entity disambiguation, Hyponym-Of, named entity recognition)(multimodal network, Used-for, multimodal named entity disambiguation)(graph neural networks, Used-for, named entity recognition)(gazetteers, Used-for, named entity recognition)(cross-domain LM
(recognition, Part-of, relation extraction)(mono-lingual attention, Used-for, relation extraction)(cross-lingual attention, Used-for, relation extraction)(distant supervision, Evaluate-for, relation extraction)(dynamic transition matrix, Used-for, relation extraction)(curriculum learning based method, Used-for, dynamic transition matrix)(class ties, Used-for, relation extraction)(convolutional neural network, Used-for, joint relation extraction)(feature extraction, Part-of, natural language processing)(temporal relation classification, Evaluate-for, relation extraction)(Bi-LSTM, Used-for, relation extraction)(dependency paths, Used-for, relation extraction)(Pocket Knowledge Base Population, Used-for, relation extraction)(Open Information Extraction, Used-for, Pocket Knowledge Base Population)(reasoning, Used-for, Open Information Extraction)(crowdsourcing, Used-for, relation extraction)(DocRED, Evaluate-for, document-level relation extraction)(graph neural network, Used-for, relation extraction)(multi-hop reasoning, Used-for, relation
(fact check, Used-for, evaluating factual correctness)(evaluating factual correctness, Part-of, claim verification)(claim verification, Used-for, veracity prediction)(veracity prediction, Used-for, generating justifications for verdicts)(generating justifications for verdicts, Part-of, fact checking system)(fact checking system, Evaluate-for,fact correctness)(claim verification, Part-of, LogicalFactChecker)(LogicalFactChecker, Evaluate-for, verifying textual statements)(human fact-checkers, Compare, automated fact-checking systems)
(network rntn, Is-a-Prerequisite-of, gated self-matching networks)(network rntn, Compare, bi-directional LSTMs)(network rntn, Is-a-Prerequisite-of, Hybrid Code Networks)(network rntn, Used-for, named entity recognition)(network rntn, Compare, deep convolutional neural network)(gated self-matching networks, Used-for, reading comprehension)(Hybrid Code Networks, Used-for, dialog systems)(deep convolutional neural network, Used-for, text categorization)(bi-directional LSTMs, Used-for, neural machine translation)
(context word vector, Used-for, capturing semantic regularities)(Neural ASC models, Used-for, capturing semantic regularities)(context word vectors, Used-for, attention mechanism)(Cold-Start Aware Attention, Part-of, Hybrid Contextualized Sentiment Classifier)(Distributed word representations, Conjunction, domain-specific embeddings)(distributed word representations, Part-of, word embedding models)(Neural Belief Tracking, Used-for, distributed representations of user utterances)(Hybrid Contextualized Sentiment Classifier, Used-for, sentiment analysis)(Local Coordinates Coding, Used-for, obtaining global context vectors)(Neural ASC models, Used-for, aspect-level sentiment classification)(Canonical Correlation Analysis, Used-for, aligning word vectors)(word vectors, Part-of, language model)(PMI weighted co-occurrence vectors, Used-for, training word vectors)(GloVe, Used-for, learning vector representations)(word co-occurrence statistic, Used-for, training word embeddings)(self-attention mechanism, Part-of,
(word embeddings, Conjunction, different language)(cross-lingual word embeddings, Hyponym-Of, word embeddings)(bilingual word embeddings, Hyponym-Of, word embeddings)(multilingual word embeddings, Hyponym-Of, word embeddings)(cross-lingual text classification, Evaluate-for, word embeddings)(noise contrastive estimation, Used-for, word embeddings)(contrastive learning, Used-for, word embeddings)(word similarity, Evaluate-for, word embeddings)(semantic representations of words, Part-of, word embeddings)(word alignment tasks, Evaluate-for, word embeddings)(cross-lingual machine translation, Conjunction, word embeddings)(synthetic data, Used-for, word embeddings)(vector space representations of words, Part-of, word embeddings)(parallel corpora, Evaluate-for, word embeddings)(linear maps, Used-for, word embeddings)(synthetic parallel corpus, Used-for, word embeddings).
(dialog generation, Used-for, response generation)(encoder-decoder dialog model, Is-a-Prerequisite-of, dialog generation)(neural knowledge diffusion, Used-for, dialog generation)(DI-VAE, Used-for, dialog generation)(automatic translation, Used-for, response generation)(knowledge diffusion, Part-of, neural dialogue generation)(variational autoencoders, Used-for, interpretable generation)(knowledge, Evaluate-for, dialog generation)(neural knowledge diffusion model, Compare, encoder-decoder dialog model)(deep latent variable models, Evaluate-for, response generation).
(conditional text generation, Is-a-Prerequisite-of, natural language generation)(conditional text generation, Used-for, controlling properties of generated contents)(natural language generation, Used-for, text generation tasks)(text generation, Part-of, natural language processing)(natural language generation, Part-of, natural language processing)(sequence-to-sequence models, Used-for, text generation tasks)(sequence-to-sequence models, Part-of, text generation tasks)(sequence-to-sequence models, Compare, conditional text generation)(pre-train and plug-in variational auto-encoder, Part-of, conditional text generation)(conditional text generation, Compare, grammatical error correction)(conditional text generation, Evaluate-for, emerging conditions)(text generation, Conjunction, machine translation)(text generation, Conjunction, formality style transfer)(text generation, Conjunction, sentence compression)(text generation, Conjunction, sentence simplification)
(unsupervised cross lingual, Hyponym-Of, unsupervised machine translation)(unsupervised cross lingual, Part-of, cross-lingual NLP tasks)(unsupervised cross lingual, Used-for, bilingual word embeddings)(unsupervised cross lingual, Part-of, mining parallel sentences)(bilingual word embeddings, Hyponym-Of, unsupervised bilingual word embedding)(unsupervised bilingual word embeddings, Hyponym-Of, bilingual word embeddings)(bilingual word embeddings, Used-for, mining parallel sentences)(bilingual word embeddings, Used-for, cross-lingual NLP tasks)(unsupervised machine translation, Hyponym-Of, cross-lingual NLP tasks)(multilingual BERT, Hyponym-Of, cross-lingual NLP tasks).
(race nlp, Evaluate-for, racial bias)(race nlp, Evaluate-for, social bias)(NLP model development, Part-of, race nlp)(race, Hyponym-Of, social constructs)(NLP systems, Used-for, reinforcing racial hierarchies)(gender bias, Compare, racial bias)(pre-trained word embeddings, Conjunction, gender bias)(race-related bias, Part-of, NLP model development)(NLP research, Part-of, race nlp)(historical racism, Used-for, operationalizing race as a fixed single-dimensional variable)(inclusion, Used-for, racial justice in NLP research practices)
(summarization datasets, Part-of, abstractive summarization)(summarization datasets, Part-of, extractive summarization)(summarization datasets, Evaluate-for, summarization model)(query-based summarization dataset, Hyponym-Of, summarization datasets)(English Gigaword, Hyponym-Of, summarization datasets)(DUC 2004, Hyponym-Of, summarization datasets)(MSR abstractive sentence summarization dataset, Hyponym-Of, summarization datasets)(CNN/Daily Mail dataset, Hyponym-Of, summarization datasets)(social media corpus, Hyponym-Of, summarization datasets)(AMI corpus, Hyponym-Of, summarization datasets)(ICSI corpus, Hyponym-Of, summarization datasets)(DUC dataset, Hyponym-Of, summarization datasets)(WikiQA, Hyponym-Of, summarization datasets)(NewsQA, Hyponym-Of, summarization datasets
(adversarial training, Used-for, improving robustness of NMT models)(adversarial training, Used-for, sentence-level true-positive generator)(adversarial training, Used-for, style transfer)(adversarial training, Used-for, disentangling latent representations of style and content)(adversarial training, Used-for, multi-dimensional emotion regression)(adversarial training, Used-for, mitigating negative transfer in transfer learning)(adversarial training, Part-of, neural networks)(Japanese PAS analysis, Used-for, zero anaphora resolution)(neural models, Hyponym-Of, adversarial training)(QA models, Sensitive-to, adversarial inputs)(distant supervision, Used-for, relation extraction)(NMT models, Used-for, translation quality)(style transfer, Used-for, rephrasing text)(adversarial perturbation, Used-for, improving text classification robustness)(NMT models, Hyponym-Of, neural machine translation)(multi-dimensional emotion regression, Hypon
(multimodal language, Part-of, multimodal social media posts)(multimodal language, Part-of, human multimodal language)(human multimodal language, Evaluate-for, sentiment analysis)(human multimodal language, Evaluate-for, emotion recognition)(human multimodal language, Used-for, Dynamic Fusion Graph)(multimodal language, Compare, text-only NED models)(multimodal language, Used-for, zero-shot multimodal network)(multimodal social media posts, Used-for, Multimodal Named Entity Recognition)(multimodal social media posts, Used-for, Multimodal Named Entity Disambiguation)(sentiment analysis, Conjunction, emotion recognition)(Emotion analysis, Used-for, Dynamic Fusion Graph)(sentiment analysis, Used-for, CMU Multimodal Opinion Sentiment and Emotion Intensity)(emotion recognition, Used-for, CMU Multimodal Opinion Sentiment and Emotion Intensity)(multimodal language, Used-for, machine translation)(multi-modal neural machine translation,
(morphological analyzer, Used-for, tokenization)(morphological analyzer, Used-for, segmentation)(morphological analyzer, Evaluate-for, text classification)(morphological analyzer, Evaluate-for, morphological analysis)(morphological analyzer, Evaluate-for, morphological inflection generation)(morphological supervision, Enhance, character language models)(character language models, Part-of, neural networks)(inflectional lexica, Enhance, morphological tagging)(cross-lingual training, Enhance, morphological tagging)(knowledge-transfer scheme, Enhance, cross-lingual morphological tagging)(Universal Dependencies, Part-of, syntactic tree processing)(syntactic tree processing, Enhance, machine translation)(syntactic tree processing, Enhance, cross-lingual sentence similarity)(analogical reasoning, Enhance, linguistic regularities)(morphological variation, Hinders, UBLI task)(morphology-aware alignment model, Alleviate, morphological variation)(morphological constraints, Enhance, language understanding systems)(morphological constraints, Improve
(long form question answering, Is-a-Prerequisite-of, question representation)(long form question answering, Compare, neural network-based methods)(long form question answering, Used-for, accessing knowledge bases)(question answering, Conjunction, reading comprehension)(question answering, Hyponym-Of, long form question answering)(knowledge base, Conjunction, unstructured text)(long form question answering, Compare, simple questions)(neural network-based methods, Compare, rule-based methods).
(abstract syntax networks, Used-for, code generation)(abstract syntax networks, Used-for, semantic parsing)(abstract syntax trees, Part-of, abstract syntax networks)(attention-based sequence learning model, Used-for, automatic question generation)(group sparse autoencoders, Part-of, group sparse CNNs)(deep latent variable models, Used-for, response generation)(deep latent variable models, Evaluate-for, dialog systems)(DI-VAE, Part-of, unsupervised discrete sentence representation learning method)(DI-VST, Part-of, unsupervised discrete sentence representation learning method)(sequence to sequence models, Used-for, sentence generation)(neural network models, Used-for, pun generation)(conditional neural language model, Used-for, pun generation)(neural machine translation, Used-for, sentence generation)(hierarchical document encoder, Part-of, extractive document summarization)(attention-based extractor, Part-of, extractive document summarization)(generative neural network model,
(dialogue generation, Part-of, conversational systems)(dialogue generation, Evaluate-for, conversational systems)(Seq2Seq, Used-for, dialogue generation model)(dialogue generation model, Used-for, response generation)(neural knowledge diffusion model, Used-for, dialogue generation model)(multi-turn dialogue generation, Hyponym-Of, dialogue generation)(ReCoSa model, Used-for, multi-turn dialogue generation)(open-domain dialogue, Hyponym-Of, dialogue generation)(meta-words, Used-for, open-domain dialogue generation)(meta-words, Part-of, goal-tracking memory network)(response generation, Part-of, multi-turn dialogue generation).
(discourse coherence, Evaluate-for, discourse segmentation)(discourse coherence, Evaluate-for, text quality)(discourse coherence, Evaluate-for, summarization)(discourse coherence, Evaluate-for, language assessment)(discourse segmentation, Used-for, discourse parsers)(discourse segmentation, Used-for, coherence assessment)(disclosure representation structures, Part-of, discourse coherence)(disclosure coherence, Compare, dialogue coherence)(discourse coherence, Evaluate-for, multi-task learning)(text-level DRS parsing, Part-of, discourse coherence)(sentence-level discourse analysis, Used-for, discourse segmentation)(disclosure coherence, Part-of, neural language models)(discourse relationships, Hyponym-Of, discourse coherence)(discourse coherence, Evaluate-for, coherence scores)(discourse coherence, Hyponym-Of, monologue coherence)(discourse coherence, Compare, binary evaluation of coherence tasks)
(underrepresented language, Part-of, linguistic diversity)(Indonesia, Compare, Africa)(Heterogeneous Linguistics Graph, Used-for, enhance Chinese pre-trained language models)(pre-trained language models, Used-for, sequence generation task)(sentiment classification, Evaluate-for, cross-domain adaptation)(machine translation, Used-for, script normalization)(language model, Used-for, question generation)(question generation, Part-of, online education systems)(online education systems, Is-a-Prerequisite-of, high-quality education)(cross-domain adaptation, Evaluate-for, sentiment classification)(synthetic data, Used-for, script normalization)(low-resource languages, Compare, underrepresented language)(underrepresented language, Compare, low-resource languages)(Indonesian NLP, Is-a-Prerequisite-of, performance of current NLP systems)(natural language processing, Part-of, NLP research).
(neural retrieval model, Evaluate-for, text generation models)(neural retrieval model, Used-for, context-dependent semantic parsing)(neural retrieval model, Used-for, conversation models)(neural retrieval model, Compare, traditional TF-IDF retrieval model)(retrieved datapoints, Used-for, context-dependent semantic parsing)(retrieval model, Part-of, neural coreference models)(retrieval model, Part-of, context-aware encoder-decoder model)(context-aware retriever, Used-for, meta-learner)(meta-learner, Evaluate-for, context-aware retriever)(neural network model, Hyponym-Of, neural retrieval model)(retrieval model, Part-of, coreference resolution)
(zero shot text classification, Is-a-Prerequisite-of, text classification)(text classification, Hyponym-Of, text classification tasks)(zero shot text classification, Evaluate-for, sentiment classification)(zero shot text classification, Evaluate-for, argument mining)(zero shot text classification, Evaluate-for, named entity recognition)(zero shot text classification, Evaluate-for, aspect sentiment classification)(zero shot text classification, Evaluate-for, document-level sentiment classification)(zero shot text classification, Evaluate-for, automatic essay scoring)(zero shot text classification, Used-for, multi-task learning)(zero shot text classification, Compare, traditional text classification)(zero shot text classification, Evaluate-for, argument mining)(zero shot text classification, Compare, off-the-shelf knowledge transfer)(zero shot text classification, Compare, domain specific word embeddings)(zero shot text classification, Compare, universal language model fine-tuning)(zero shot text classification, Compare, sentiment resource enhanced attention network)
(entity recognition NER, Hyponym-Of, Named Entity Recognition (NER))(entity recognition NER, Compare, sequence labeling)(entity recognition NER, Used-for, named entity recognition tasks)(entity recognition NER, Used-for, mention detection (MD))(entity recognition NER, Hyponym-Of, ordinally forgetting encoding (FOFE))(entity recognition NER, Evaluate-for, CoNLL 2003 NER task)(entity recognition NER, Evaluate-for, TAC-KBP2015 Tri-lingual Entity Discovery and Linking (EDL) tasks)(entity recognition NER, Evaluate-for, TAC-KBP2016 Tri-lingual Entity Discovery and Linking (EDL) tasks)(entity recognition NER, Hyponym-Of, supervised machine learning models)(co-decoding schemes, Conjunction, projection-based approaches)(named entity recognition, Is-a-Prerequisite-of, large amounts of manually annotated data)(named entity recognition, Used-for, low-resource named
(relation extraction model, Used-for, finding relational facts from text)(relation extraction model, Used-for, entity and relationship extraction)(monolingual attention, Part-of, multi-lingual neural relation extraction framework)(cross-lingual attention, Part-of, multi-lingual neural relation extraction framework)(dynamic transition matrix, Used-for, noise characterization in training data)(dynamic transition matrix, Used-for, distant supervision)(joint extraction, Hyponym-Of, information extraction)(joint extraction, Used-for, entity and relationship extraction)(tagging scheme, Part-of, joint extraction)(tagging scheme, Used-for, converting extraction task to tagging problem)(geolocation prediction model, Used-for, predicting geolocation in social media)(dependency parsing, Used-for, dependency treebank construction)(neural network model, Used-for, dependency parsing)(neural stacking, Used-for, improving cross-lingual dependency parsing)(class ties, Part-of, relation extraction model)(convolutional neural network, Part
(pre trained model, Compare, neural encoder-decoder models)(pre trained model, Used-for, various NLP tasks)(pre trained model, Compare, PositionRank)(neural encoder-decoder models, Used-for, modeling open-domain conversations)(PositionRank, Used-for, keyphrase extraction)(PositionRank, Compare, PageRank)(PositionRank, Evaluate-for, keyphrase extraction from scholarly documents)(Bayesian model, Used-for, text segmentation)(Bayesian model, Compare, baseline models)(neural network based detection models, Hyponym-Of, detection models)
(morphological inflection, Used-for, language understanding)(morphological inflection, Evaluate-for, improving distributional vector spaces)(morphological inflection, Evaluate-for, word vector collection quality)(morphological inflection, Used-for, dialogue state tracking)(morphological inflection, Used-for, paradigm completion)(morphological inflection, Used-for, cross-lingual transfer)(morphological inflection, Used-for, knowledge sharing between languages)(morphological inflection, Evaluate-for, neural sequence-to-sequence models)(morphological inflection, Used-for, improving cross-lingual parser transfer)(morphological inflection, Evaluate-for, morphological tagging)(morphological inflection, Compare, morphological analysis)(morphological inflection, Evaluate-for, syntactic analogy tasks)(morphological inflection, Evaluate-for, text classification)(neural encoder-decoder model, Hyponym-Of, neural sequence-to-sequence models)(neural encoder
(social bias frame, Part-of, pragmatic frames)(pragmatic frames, Part-of, social biases)(social biases, Part-of, stereotypes)(social bias frame, Used-for, modeling pragmatic frames)(social bias frame, Used-for, understanding social biases)(Social Bias Inference Corpus, Used-for, modeling large scale social biases)(Social Bias Frames, Evaluate-for, NLP models)(pragmatic implications, Part-of, social biases)(baseline approaches, Evaluate-for, Social Bias Frames)(deep learning models, Evaluate-for, Social Bias Frames)
(translation model, Used-for, neural machine translation)(neural machine translation, Part-of, machine translation)(neural machine translation, Used-for, WMT'16 English-Romanian translation)(neural machine translation, Used-for, WMT'15 English-German translation)(neural machine translation, Used-for, WMT'14 English-French translation)(neural machine translation, Used-for, Chinese-to-English translation)(neural machine translation, Used-for, Japanese-English translation)(neural machine translation, Used-for, multi-modal translation)(neural machine translation, Used-for, low-resource translation)(neural machine translation, Used-for, zero-resource translation)(neural machine translation, Compare, statistical machine translation)(bi-directional LSTM, Part-of, neural machine translation)(convolutional layers, Part-of, neural machine translation)(deep neural networks, Used-for, modeling complex functions)(linear associative units, Compare, LSTM unit)(linear associative units,
(adversarial robustness, Is-a-Prerequisite-of, adversarial training)(adversarial robustness, Evaluate-for, language models)(adversarial robustness, Evaluate-for, dialogue generation models)(adversarial robustness, Evaluate-for, text classifiers)(adversarial robustness, Evaluates-for, RSMI)(adversarial robustness, Evaluates-for, BERT)(BERT, Used-for, textual adversarial attacks)(Flooding method, Used-for, defending adversarial attacks)(flooding method, Evaluate-for, generalization)(DGSlow, Evaluate-for, state-of-the-art dialogue generation models)(DGSlow, Used-for, crafting adversarial samples)(language models, Part-of, style transfer)(style transfer, Used-for, non-parallel corpora)(text classifiers, Used-for, text classification)(NMT-Adapt, Part-of, machine translation systems)(hybrid retrieval, Used-for, information retrieval tasks)(RoC, Used-for, improving hybrid retrieval performance).
(word embeddings, Used-for, word analogy questions)(word embeddings, Used-for, caption generation)(word vectors, Part-of, word embeddings)(additive compositionality, Part-of, word vectors)(Skip-Gram model, Used-for, learning word embeddings)(Skip-Gram model, Related-to, Sufficient Dimensionality Reduction framework)(word embeddings, Part-of, aspect extraction models)(neural word embeddings, Used-for, improving coherence)(Chinese word segmentation, Part-of, neural network-based joint models)(POS tagging, Part-of, neural network-based joint models)(dependency parsing, Part-of, neural network-based joint models)(character embeddings, Part-of, neural network-based joint models)(word embeddings, Used-for, dependency parsing)(complex networks, Enriched-by, word embeddings)(CNE, Part-of, word embeddings)(synonymy dictionaries, Part-of, graph-based approach)(word embeddings, Used-for, building weighted graph of synonyms)(word sense induction, Used-for, dealing with ambiguous
(question answering dataset squad, Used-for, reading comprehension)(reading comprehension, Evaluate-for, reading texts)(reading comprehension, Used-for, answering questions)(question answering, Hyponym-Of, reading comprehension)(knowledge base, Used-for, answering questions)(semantic units, Part-of, natural answer)(parse tree, Used-for, guiding answer generation)(latent variable, Used-for, sentence selection)(rc problem, Evaluate-for, Stanford Question Answering Dataset)(qa models, Evaluate-for, question answering)(qa models, Compare, existing models)(qa models, Evaluate-for, state of the art performance)(stochastic answer network, Evaluate-for, robustness)(parse tree, Used-for, generating candidate answers).
(answering dataset, Hyponym-Of, WikiReading dataset)(answering dataset, Hyponym-Of, SQuAD)(answering dataset, Hyponym-Of, TriviaQA)(answering dataset, Hyponym-Of, Stanford Natural Language Inference Dataset)(answering dataset, Hyponym-Of, Yahoo! Answers dataset)(answering dataset, Hyponym-Of, Multirelational QA dataset)(WikiReading dataset, Part-of, question answering)(SQuAD, Part-of, question answering)(TriviaQA, Part-of, question answering)(Stanford Natural Language Inference Dataset, Part-of, question answering)(Yahoo! Answers dataset, Part-of, question answering)(Multirelational QA dataset, Part-of, question answering)
(representation morphological, Used-for, improving the accuracy of Arabic NLP applications)(data-driven sub-word units, Evaluate-for, representation morphological)(characters as a unit of learning, Evaluate-for, representation morphological)(word embeddings learned using a character CNN, Evaluate-for, representation morphological)(representation morphological, Part-of, neural machine translation)(neural machine translation, Evaluate-for, ratio of source and target tokens)(neural machine translation, Used-for, perform translation at the sub-lexical level)(representation morphological, Part-of, morphological disambiguation)(deep learning-based approach, Used-for, morphological disambiguation)(representation morphological, Part-of, semantic parsing)(cross-lingual distributed logical representations, Is-a-Prerequisite-of, improving monolingual semantic parser)(learning distributed representations of logical forms, Used-for, improving semantic parsing results)(representation morphological, Part-of, neural network-based models)(neural network-based models, Hyponym-Of, event detection)(representation morphological, Part-of, B
(multi task learning, Used-for, learning shared layers)(multi task learning, Evaluate-for, extracting common features)(multi task learning, Evaluate-for, reducing noise)(multi task learning, Used-for, computational argumentation mining)(multi task learning, Used-for, parsing sentences into semantic dependency graphs)(multi task learning, Used-for, parsing sentences in multiple languages)(multi task learning, Used-for, training multilingual models)(multi task learning, Used-for, question generation)(multi task learning, Used-for, entailment generation)(question generation, Evaluate-for, improving abstractive summarization)(entailment generation, Evaluate-for, improving abstractive summarization)(multi task learning, Used-for, rumor detection)(multi task learning, Used-for, stance classification)(rumor detection, Conjunction, stance classification)(multi task learning, Evaluate-for, morphological tagging in morphologically rich languages)(multi task learning, Evaluate-for, entity recognition)(Bi
(detect hate speech, Used-for, mitigating bias)(detect hate speech, Evaluate-for, accuracy)(AAE, Part-of, dialects)(AAE, Compare, African American English)(African American English, Compare, other dialects)(AAE, Hyponym-Of, dialects)(CDS, Hyponym-Of, registers)(ADS, Hyponym-Of, registers)(child-directed speech, Compare, adult-directed speech)(counter-speech, Used-for, combating hate speech)(intent-conditioned counterspeech, Hyponym-Of, counter-speech)(QUARC, Used-for, intent-conditioned counterspeech)(HateCheck, Used-for, detecting model weaknesses)(HateCheck, Evaluate-for, hate speech detection models)(cross-lingual transfer learning, Used-for, learning to detect hate speech in multiple languages)(sentiment knowledge sharing, Used-for, hate speech detection)(text preprocessing, Is-a-Prerequisite-of, hate speech detection)(syntactic representations,
(learning morphological inflection, Used-for, labeled sequence transduction)(multi-space variational encoder-decoders, Used-for, labeled sequence transduction)(learning morphological inflection, Evaluate-for, neural sequence-to-sequence models)(neural model, Used-for, morphological inflection generation)(hard attention mechanism, Part-of, neural model)(Bahdanau attention model, Compare, hard attention mechanism)(word embedding approaches, Compare, morphology-based models)(Latent Meaning Models, Hyponym-Of, morphology-based models)(learning morphological inflection, Evaluate-for, statistical morphological inflectors)(wake-sleep algorithm, Used-for, variational inference)(morphological supervision, Part-of, character language models)(inflected words, Evaluate-for, modeling morphology)(morphological tagging, Hyponym-Of, learning morphological inflection)(multitask learning, Used-for, morphological tagging)(adversarial training, Used-for, morphological tagging)(deep learning sequence models, Used-for, morphological
(unsupervised selective rationalization, Is-a-Prerequisite-of, rationale generator)(unsupervised selective rationalization, Is-a-Prerequisite-of, predictor)(unsupervised selective rationalization, Evaluate-for, rationale plausibility)(unsupervised selective rationalization, Evaluate-for, task accuracy)(unsupervised selective rationalization, Evaluate-for, model faithfulness)(rationale generator, Part-of, unsupervised selective rationalization)(predictor, Part-of, unsupervised selective rationalization)(rationale generator, Used-for, generating rationales)(predictor, Used-for, making predictions)(rationale generator, Compare, predictor)(noise injection, Used-for, limiting implausible rationales)(unsupervised selective rationalization, Used-for, producing rationales alongside predictions)(unsupervised selective rationalization, Conjunction, deep learning models).
(dynamic self-attention network, Used-for, multi-passage reading comprehension)(multi-passage reading comprehension, Hyponym-Of, machine reading comprehension)(cross-passage information, Part-of, multi-passage reading comprehension)(cross-passage answer verification, Part-of, multi-passage reading comprehension)(multi-hop reading comprehension, Hyponym-Of, multi-passage reading comprehension)(dataset, Used-for, multi-passage reading comprehension)(neural model, Hyponym-Of, multi-passage reading comprehension)(final answer, Evaluate-for, multi-passage reading comprehension)(state-of-the-art performance, Evaluate-for, multi-passage reading comprehension)
(query concept, relation, extracted concept)(KB-InfoBot, Used-for, Knowledge Bases)(KB-InfoBot, Used-for, multi-turn dialogue)(KB-InfoBot, Part-of, goal-oriented dialogue agents)(goal-oriented dialogue agents, Used-for, search Knowledge Bases)(goal-oriented dialogue agents, Compare, task-oriented dialogue systems)(symbolic query, Part-of, KB-InfoBot)(symbolic query, Compare, induced “soft” posterior distribution)(induced “soft” posterior distribution, Is-a-Prerequisite-of, neural dialogue agents)(reinforcement learner, Used-for, soft retrieval process)(dialogue systems, Used-for, collaborative tasks)(SVM, Evaluate-for, task success prediction models)(tweets, Used-for, handling open-domain user utterances)(Web search queries, Used-for, handling open-domain user utterances)(psycholinguistic metrics, Evaluate-for, dialogues in movies)(dynamic knowledge graph embeddings, Used-for, model structured knowledge)(semantic parsing, Used-for
(chinese spelling correction csc, Part-of, spelling error correction)(spelling error correction, Is-a-Prerequisite-of, language understanding)(phonological similarity knowledge, Used-for, chinese spelling correction csc)(visual similarity knowledge, Used-for, chinese spelling correction csc)(graph convolutional network, Used-for, chinese spelling correction csc)(SpellGCN, Hyponym-Of, graph convolutional network)(BERT, Part-of, soft-masked bert)(soft-masked bert, Hyponym-Of, BERT)(pre-trained masked language model, Compare, BERT)(pre-trained masked language model, Part-of, PLOME)(PLOME, Hyponym-Of, pre-trained masked language model)(GRU networks, Used-for, PLOME)(pinyin representations, Part-of, chinese spelling correction csc)(glyph representations, Part-of, chinese spelling correction csc)(UMRSpell, Used-for, chinese spelling correction csc)(multiple
(recurrent network, Part-of, recurrent neural network)(recurrent network, Used-for, language modeling)(recurrent network, Used-for, dialog systems)(recurrent network, Used-for, reading comprehension)(recurrent network, Compare, convolutional layers)(recurrent network, Hyponym-Of, LSTM)(recurrent network, Part-of, Hybrid Code Networks)(recurrent network, Compare, feedforward neural network)(recurrent neural network, Used-for, entity extraction)(recurrent neural network, Used-for, relation detection)(recurrent neural network, Used-for, machine translation)(recurrent neural network, Used-for, machine reading)(bi-directional LSTM, Hyponym-Of, recurrent neural network)(deep residual bidirectional LSTM, Used-for, relation detection)(deep residual bidirectional LSTM, Is-a-Prerequisite-of, relation detector)(gated attention-based recurrent network, Used-for, question-answering)(attention-based recurrent neural network, Used-for, entity
(language generation task, Part-of, Natural Language Processing)(Neural Symbolic Machine, Used-for, language generation task)(sequence-to-sequence models, Used-for, language generation task)(semantic parsing, Is-a-Prerequisite-of, language generation task)(Neural Symbolic Machine, Conjunction, Neural “programmer”)(sequence-to-sequence model, Used-for, Natural Language Processing)(Neural Symbolic Machine, Compare, other code generation and semantic parsing approaches)(text similarity measures, Used-for, Natural Language Processing)(new text similarity measure, Used-for, similarity value computation)(Neural Symbolic Machine, Evaluate-for, parsing natural language descriptions into source code)(language generation task, Part-of, natural language generation)(Neural Symbolic Machine, Used-for, mapping language utterances to programs)(sequence-to-sequence model, Used-for, mapping language utterances to programs)(knowledge bases, Used-for, language generation task)(video captioning, Used-for, language generation task)
(social bias encoded, Compare, explicit social bias)(social bias encoded, Is-a-Prerequisite-of, social implications)(word embeddings, Part-of, social bias encoded)(word embeddings, Used-for, studying society)(word embeddings, Used-for, downstream applications)(word embeddings, Evaluate-for, accuracy of beliefs)(word embeddings, Evaluate-for, beliefs about people)(survey methods, Evaluate-for, beliefs about people)(biases in word embeddings, Compare, survey data across dimensions of social meaning)(gender bias, Compare, racial bias)(Social Bias Frames, Used-for, modeling pragmatic frames)(Social Bias Inference Corpus, Used-for, modeling and evaluation)(Social Bias Inference Corpus, Part-of, social media posts)(sentence-level representations, Compare, word-level embeddings)(ELMo, Is-a-Prerequisite-of, BERT)(Sent-Debias, Used-for, removing biases)(Sent-Debias, Used-for, sentiment analysis)(S
(direct speech-to-speech translation, Compare, cascaded speech translation)(direct speech-to-speech translation, Used-for, translating speech from one language to another)(direct speech-to-speech translation, Part-of, speech-to-speech translation)(direct speech-to-speech translation, Evaluate-for, inference speed)(direct speech-to-speech translation, Evaluate-for, robustness in noisy environments)(direct speech-to-speech translation, Compare, AV-S2ST)(direct speech-to-speech translation, Evaluate-for, BLEU score)(AV-S2ST, Used-for, dictation)(AV-S2ST, Used-for, dubbing films)(AV-S2ST, Conjunction, audio-visual speech-to-speech translation)(AV-S2ST, Part-of, speech-to-speech translation)(AV-S2ST, Evaluate-for, performance in noisy environments)(Stacked Acoustic-and-Textual Encoding, Hyponym-Of, encoding methods)(multi-teacher knowledge distillation, Part-of,
(contextualized embeddings, Is-a-Prerequisite-of, DEEB-RNN)(contextualized embeddings, Used-for, event detection)(contextualized embeddings, Compare, non-contextual subword embeddings)(contextualized embeddings, Hyponym-Of, BERT)(contextualized embeddings, Evaluate-for, affect dimensions)(contextualized embeddings, Evaluate-for, word sense disambiguation)(period embeddings, Conjunction, contextual embeddings)(context embeddings, Conjunction, contextual embeddings)(contextualized embeddings, Used-for, word sense disambiguation)(contextualized embeddings, Used-for, lexical substitution)(contextualized embeddings, Used-for, machine translation evaluation)(DEEB-RNN, Used-for, event detection)(BERT, Hyponym-Of, contextualized embeddings)(FastText, Hyponym-Of, non-contextual subword embeddings)(BPEmb, Hyponym-Of, non-contextual subword embeddings)(BERT, Compare, non-contextual
(retrieval-based models, Compare, task-oriented dialogue systems)(task-oriented dialogue systems, Part-of, intelligent assistants)Siri, Conjunction, Alexa)intelligent assistants, Conjunction, home electronics)intelligent assistants, Compare, non-task-oriented dialogue systems)hybrid dialogue systems, Part-of, intelligent assistants)user, Evaluate-for, hybrid dialogue systems)benchmark datasets, Part-of, hybrid dialogue systems)supervised methods, Conjunction, reinforcement learning)sequence-to-sequence (seq2seq) model, Used-for, tracking dialogue beliefs)belief spans, Part-of, task-oriented dialogue systems)Two Stage CopyNet, Compare, pipeline-based methods)dialogue state tracking, Part-of, task-oriented dialogue systems)Global-Locally Self-Attentive Dialogue State Tracker (GLAD), Used-for, tracking of rare states)multi-hop attention, Compare, pipeline designs)discourse relation identification, Evaluate-for, open-domain dialogue system)complaints, Evaluate-for, customer
(emotion cause pair extraction, Compare, emotion cause extraction)(emotion cause pair extraction, Used-for, extracting emotion-cause pairs)(emotion cause extraction, Part-of, emotion analysis)(emotion cause pair extraction, Part-of, emotion analysis)(emotion cause pair extraction, Used-for, emotion extraction)(emotion cause pair extraction, Used-for, cause extraction)(emotion extraction, Part-of, emotion cause pair extraction)(cause extraction, Part-of, emotion cause pair extraction)(emotion cause pair extraction, Evaluate-for, emotion-cause interaction modeling)(emotion cause pair extraction, Evaluate-for, emotion-cause ranking)(emotion cause pair extraction, Evaluate-for, emotion-cause graph modeling)(emotion cause pair extraction, Compare,  two-step approach)(emotion cause pair extraction, Compare,  one-step neural approach)(emotion cause pair extraction, Compare, transition-based model)(emotion cause pair extraction, Compare, Graph Convolutional Networks based methods)(emotion cause
(contextual embeddings, Compare, monolingual embeddings)(contextual embeddings, Part-of, neural word embeddings)(neural word embeddings, Compare, topic models)(neural word embeddings, Part-of, coherence improvement)(holographic embeddings, Conjunction, complex embeddings)(attention mechanism, Used-for, coherence improvement)(Deep PCCA, Hyponym-Of, PCCA)(cross-lingual word embeddings, Hyponym-Of, bilingual word embeddings)(NMT, Hyponym-Of, machine translation)(DEEB-RNN, Hyponym-Of, bidirectional RNN)(BLEU points, Evaluate-for, translation performance)(domain adaptation, Used-for, machine translation)(low-resource languages, Evaluate-for, sentiment analysis)(semi-supervised classification, Used-for, domain adaptation).
(training data, Part-of, Knowledge Base Population)(World knowledge, Used-for, Automatically label training data)(Linguistic knowledge, Used-for, Automatically label training data)(Automatically labeled data, Compare, Human-labeled data)(Distant supervision, Used-for, Reducing human effort in building training data)(Dynamic transition matrix, Evaluate-for, Noise characterization in training data)(Curriculum learning, Evaluate-for, Effective training of transition matrix)(Word-level deep convolutional neural network, Used-for, Text categorization)(Deep pyramid CNN, Hyponym-Of, Word-level deep convolutional neural network)(Sentiment classification, Part-of, Text categorization)(Topic categorization, Part-of, Text categorization)(Belief tracker, Part-of, Modern spoken dialogue systems)(Spoken Language Understanding models, Is-a-Prerequisite-of, Belief tracker)(Neural Belief Tracking, Hyponym-Of, Belief tracker)(Pre-trained word vectors, Used-for, Neural Belief Tracking)(
(relation linking, Is-a-Prerequisite-of, Knowledge Base Question Answering)(relation linking, Evaluate-for, AMR semantic parse)(AMR semantic parse, Part-of, sentence)(relation linking, Used-for, improving accuracy on benchmark datasets)(argumentation mining, Used-for, extracting argumentation structures)(argumentation structures, Part-of, argumentation mining)(argumentation graph, Part-of, argumentation mining)(token-based dependency parsing, Compare, token-based sequence tagging)(token-based sequence tagging, Used-for, improving classification scenarios)(BiLSTMs, Used-for, catching long-range dependencies)(multi-task learning, Evaluate-for, performance improvement)(BiLSTMs, Compare, token-based dependency parsing)(neural techniques, Used-for, end-to-end computational argumentation mining)(sentence-level relation classifier, Used-for, extracting commonsense knowledge)(implicit discourse relations, Compare, explicit discourse relations)(discourse relation identification, Used-for, identifying implicit relations
(Reading Comprehension, Compare, Cloze-Style Reading Comprehension)(Reading Comprehension, Is-a-Prerequisite-of, Natural-Language Understanding)(Reading Comprehension, Conjunction, Question Answering)(Reading Comprehension, Used-for, Evaluating RC Datasets)(Reading Comprehension, Used-for, GuessTwo Task)(Reading Comprehension, Conjunction, question generation)(Reading Comprehension, Conjunction, TriviaQA)(Reading Comprehension, Conjunction, DuoRC)(Reading Comprehension, Conjunction, automatic question generation)(Reading Comprehension, Part-of, DS-QA)(Reading Comprehension, Used-for, Story Comprehension)(Reading Comprehension, Used-for, Evaluating SCT)(TriviaQA, Part-of, Reading Comprehension Datasets)(DuoRC, Part-of, Reading Comprehension Datasets)(SQuAD, Part-of,
(model, Used-for, knowledge bases)(model, Used-for, semantic parsing)(neural architecture, Used-for, semantic role labeling)(semantic role labeling, Evaluate-for, state of the art)(deep highway BiLSTM architecture, Used-for, constrained decoding)(8-layer ensemble model, Evaluate-for, CoNLL 2005 test set)(8-layer ensemble model, Evaluate-for, CoNLL 2012)(semantic role labeling, Part-of, parsing natural language descriptions)(semantic parsing, Used-for, mapping natural language utterances)(semantic parsing, Used-for, generating formal semantic representations)(semantic parsing, Used-for, building domain-specific sentiment lexicons)(encoder-decoder framework, Used-for, keyphrase prediction)(deep highway BiLSTM architecture, Used-for, semantic role labeling)(sequence-to-tree model, Is-a-Prerequisite-of, multi-task learning framework)(variational autoencoding framework, Used-for, joint probabilistic model)(semantic graph, Used-for, representing meaning of a sentence
(professional fact checker, Used-for, predicting claim veracity from premise articles)(professional fact checker, Evaluate-for, effectiveness of passage retrieval models)(automated fact checking, Part-of, fact verification)(dense passage retrieval model, Used-for, improving retrieval quality)(claims, Hyponym-Of, information-seeking questions)(FAVIQ dataset, Part-of, fact verification)(claims, Used-for, verifying veracity)(professional fact checker, Compare, crowdworkers)(financial signals, Part-of, narrative financial reports)(compare-and-contrast pipeline, Used-for, locating relevant rationales in financial reports)(financial signals, Evaluate-for, narrative financial report effectiveness).
(summarization system, Hyponym-Of, extractive summarization system)(summarization system, Hyponym-Of, abstractive summarization system)(summarization system, Part-of, document summarization)(extractive summarization system, Used-for, extracting key information)(abstractive summarization system, Used-for, generating new text summary)(encode-attend-decode paradigm, Used-for, machine translation)(encode-attend-decode paradigm, Used-for, extractive summarization)(encode-attend-decode paradigm, Used-for, dialog systems)(query-based summarization, Compare, abstractive summarization)(query-based summarization, Part-of, summarization system)(Selective Encoding Model, Hyponym-Of, sequence-to-sequence model)(sequence-to-sequence model, Part-of, abstractive summarization system)(graph-based attention mechanism, Used-for, sequence-to-sequence model)(Hybrid pointer-generator network, Part-of, abstractive summarization system)(Sentences
(adversarial purification, Compare, adversarial training)(adversarial purification, Used-for, defending against textual adversarial attacks)(adversarial purification, Part-of, defense strategy against adversarial attacks)(adversarial purification, Part-of, NLP systems)(adversarial purification, Part-of, adversarial examples detection)(adversarial attack, Used-for, causing incorrect predictions)(adversarial purification, Used-for, removing adversarial perturbations)(adversarial purification, Used-for, recovering clean samples)(adversarial purification, Part-of, robust AI systems)
(Dialogue state tracking, Part-of, task-oriented dialogue systems)(Global-Locally Self-Attentive Dialogue State Tracker, Is-a-Prerequisite-of, self attentive parser)(self-attentive architecture, Used-for, self attentive parser)(attention, Used-for, self-attentive architecture)(LSTM encoder, Compare, self attentive architecture)(attention, Used-for, parsing accuracy)(Self Attentive Revision Encoder, Hyponym-Of, self attentive parser)(contextualized word embeddings, Used-for, self attentive parser)(BERT, Used-for, self attentive parser)(state-of-the-art results, Evaluate-for, self attentive parser)(composite attention, Used-for, BERT)(relative position embeddings, Compare, dynamic lightweight convolutions)(Succinct Document Representation, Is-a-Prerequisite-of, self attentive parser)(sequence parallelism, Compare, tensor parallelism)(Ring Self-Attention, Part-of, sequence parallelism)(Transformer, Part-of, self attentive parser
(language model lm, Used-for, language modeling)(language model lm, Part-of, NLP)(language model lm, Compare, neural language model)(neural language model, Part-of, language model lm)(neural language model, Used-for, sentence-level language modeling)(neural language model, Is-a-Prerequisite-of, incorporating document context)(integrating state-of-the-art neural language models, Part-of, Affect-LM)(Affect-LM, Hyponym-Of, neural language model)(Affect-LM, Used-for, generation of conversational text)(Affect-LM, Compare, traditional training of RNNs)(Recurrent neural networks, Compare, traditional training of RNNs)(traditional training of RNNs, Evaluate-for, overfitting)(stochastic optimization, Used-for, large training sets)(stochastic gradient Markov Chain Monte Carlo, Evaluate-for, RNNs weight uncertainty)(RNNs, Part-of, language models)(inducing
(hate speech detection model, Evaluate-for, held-out test data)(hate speech detection model, Evaluate-for, accuracy)(hate speech detection model, Evaluate-for, F1 score)(hate speech detection model, Evaluate-for, specific model weak points)(HateCheck, Evaluate-for, hate speech detection model)(bias in hate speech datasets, Used-for, evaluating hate speech detection model)(syntopical graph, Used-for, stance detection)(syntopical graph, Used-for, aspect detection)(syntopical reading, Is-a-Prerequisite-of, syntopical graph)(AAE tweets, Compare, tweets by self-identified African Americans)(hate speech detection model, Part-of, online hate detection)(speech recognition, Part-of, spoken language understanding)(HateCheck, Part-of, hate detection model evaluation models)(fine-grained labels, Hyponym-Of, challenging perturbations)(neural network, Part-of, detection models)(end-to-end framework, evaluate-for, word error rate
(argument, Part-of, argument generation)(neural sequence labeling model, Used-for, identification)(discourse modes, Used-for, automatic essay scoring)(approach, Used-for, automatic generation of rhythmic poetry)(generative neural language model, Used-for, poetry generation)(weighted finite state machine, Used-for, generating coherent poetry)(neural architecture, Used-for, code generation)(NLG system, Used-for, generating corrective REs)(text similarity measures, Used-for, recognition of paraphrases)(ONLG, Used-for, generating subjective responses)(crowdsourcing, Used-for, expanding natural language datasets)(ranker, Used-for, supporting argument detection)(latent variable models, Used-for, response generation)(ParaNMT-50M, Used-for, paraphrase generation)(BLEU, Used-for, evaluating generation systems)(multi-task learning, Used-for, abstractive summarization)(encoder-decoder dialog model, Used-for, response generation)(discriminators, Used-for, guiding RNN
(graph embedding, Compare, network embedding)(graph embedding, Used-for, node classification)(graph embedding, Used-for, link prediction)(graph embedding, Part-of, network analysis)(network embedding, Compare, Context-Aware Network Embedding)(Context-Aware Network Embedding, Part-of, network analysis)(network embedding, Used-for, vertex classification)(Context-Aware Network Embedding, Used-for, vertex classification)(Context-Aware Network Embedding, Used-for, link prediction)(Context-Aware Network Embedding, Used-for, node classification)(vertex, Part-of, network)(node, Part-of, network)(link prediction, Evaluate-for, network quality)(node classification, Evaluate-for, network quality)(vertex classification, Evaluate-for, network quality)(node classification, Hyponym-Of, classification)(vertex classification, Hyponym-Of, classification)(network, Used-for, communication)
(reinforcement learning, Used-for, coreference resolution)(HMM, Used-for, machine translation)(neural network, Used-for, lexicon models)(Baum-Welch algorithm, Used-for, training HMM)(phrase-based translation system, Compare, neural machine translation system)(self-attention networks, Used-for, machine translation)(position embedding, Part-of, self-attention networks)(shared encoder, Part-of, hierarchical encoder-decoder model)(conditions softmax shared decoder, Part-of, hierarchical encoder-decoder model)(LIMSSE, Used-for, post hoc explanation methods)(LIMSSE, Compare, LRP)(LIMSSE, Compare, DeepLIFT)(Pocket Knowledge Base Population, Hyponym-Of, Knowledge Base Population)(Commonsense Auto-Generated Explanation, Hyponym-Of, explainable systems)(BabbleLabble, Used-for, training classifiers)(noun-compound paraphrasing, Hyponym-Of, paraphr
(relation extraction, Used-for, finding unknown relational facts)(relation extraction, Used-for, neural relation extraction framework)(neural relation extraction framework, Used-for, multi-lingual texts)(multi-lingual neural relation extraction framework, Employs, mono-lingual attention)(mono-lingual attention, Utilizes, information within mono-lingual texts)(cross-lingual attention, Consider, information consistency and complementarity among cross-lingual texts)(multi-lingual texts, Used-for, relation extraction)(deep multiagent policies, Effective for, many tasks)(deep multiagent policies, Challenge, interpretation of their induced communication strategies)(translation model, Based-on, same belief about the world in a listener)(parsing natural language descriptions, Into, source code)(source code, Written-in, general-purpose programming language)(neural architecture, Used-for, capture the target syntax)(target syntax, Used-for, generation of complex programs)(semantic parsing, Related to, code generation)(first
(dependency parser, Used-for, dependency parsing)(dependency parser, Used-for, syntactic analysis)(dependency parser, Evaluate-for, grammatical error repair)(dependency parser, Evaluate-for, cross-lingual parsing)(dependency parser, Compare, transition-based parser)(dependency parser, Part-of, syntactic analysis)(dependency parser, Conjunction, POS tagging)(dependency parser, Conjunction, word segmentation)(BiLSTM, Used-for, dependency parsing)(non-monotonic transition system, Part-of, dependency parser)(Covington algorithm, Part-of, dependency parser)(semantic dependency parsing, Evaluate-for, dependency parser)(neural stacking, Is-a-Prerequisite-of, cross-lingual dependency parsing)(attention-based recurrent neural network, Used-for, relation extraction)(active sentiment domain adaptation, Used-for, sentiment analysis)(character composition model, Used-for, dependency parsing)(arc-swift, Part-of, transition-based dependency parser)(Jackknifing taggers, Is-a-Prerequisite-of, cross
(conversational corpus, Part-of, dialogue systems)(conversational corpus, Used-for, training conversational models)(task-oriented dialogue, Compare, open-domain conversational systems)(power spectrum overlap, Part-of, lexical entropy series)(relative phase, Part-of, lexical entropy series)(task-oriented dialogue, Used-for, automatic diagnosis)(interrogatives, Part-of, good questions)(multimodal baselines, Used-for, emotion recognitionin conversations)(emotion recognition in conversations, Part-of, dialogue systems)(Baum-Welch algorithm, Used-for, training HMM models)(recurrent seq2seq models, Used-for, conversational context processing)(transformer-based seq2seq models, Used-for, conversational context processing)(context-query relevance, Used-for, weighting context vectors)(SVM, Used-for, prediction models)(DialKG Walker model, Used-for, dialog context traversal)(hierarchical span-attribute tagging model, Used-for, symptom extraction)(sequence-to-sequence model, Hyponym-
(sentence, Part-of, machine reading)(discrete indicator features, Part-of, machine reading)(learning morphology, Used-for, machine reading)(neural networks, Used-for, machine reading)(attention mechanism, Used-for, machine reading)(entity extraction, Used-for, machine reading)(recurrent neural network, Part-of, machine reading)(event extraction, Used-for, machine reading)(contextual word, Part-of, machine reading)(external knowledge bases (KBs), Used-for, machine reading)(Layer-wise relevance propagation (LRP), Used-for, machine reading)(deep neural networks, Used-for, machine reading)(novel neural model, Hyponym-Of, deep neural networks)(multitask learning, Used-for, machine reading)(KBLSTM, Hyponym-Of, recurrent neural network)(interpret internal workings, Hyponym-Of, Layer-wise relevance propagation (LRP))(Wikipedia, Used-for, external knowledge bases (KBs))(Deep Neural Networks (DNNs
(contemporary language model, Used-for, language modeling)(generative model, Used-for, language modeling)(discriminative model, Used-for, language modeling)(neural language model, Part-of, contemporary language model)(context-phrase alignment, Part-of, contemporary language model)(phrase embeddings, Part-of, contemporary language model)(affective messages, Part-of, human verbal communication)(Affect-LM, Used-for, generation of conversational text)(Affect-LM, Part-of, contemporary language model)(character representations, Evaluate-for, morphological typologies)(LSTM language model, Part-of, sentence compression)(LSTM Noisy Channel Model, Used-for, disfluency detection)(ULMFiT, Used-for, sentiment classification)(LSTM language model, Used-for, neural language modeling)(phrase embeddings, Part-of, neural language modeling)(syntactic neural language model, Part-of, sentence compression)(language modeling, Used-for, parsing and language modeling)(self-attention mechanism, Part
(aspect extraction, Is-a-Prerequisite-of, predict sentiment)(predict sentiment, Part-of, aspect-based sentiment analysis)(neural word embeddings, Used-for, predict sentiment)(attention mechanism, Used-for, predict sentiment)(aspect-category sentiment analysis, Hyponym-Of, predict sentiment)(aspect-term sentiment analysis, Hyponym-Of, predict sentiment)(sentiment classifier, Used-for, predict sentiment)(target-sensitive memory networks, Is-a-Prerequisite-of, predict sentiment)(word embeddings, Used-for, predict sentiment)(Aspect-Level Sentiment Classification, Hyponym-Of, predict sentiment)(cold-start aware attention, Used-for, predict sentiment)(opinion targets, Part-of, predict sentiment)(sentiment polarities, Part-of, predict sentiment).
(task sentiment classification, Used-for, classifying sentiment polarities)(classifying sentiment polarities, Hyponym-Of, text classification)(text classification, Evaluate-for, accuracy)(domain-sensitive word embeddings, Used-for, task sentiment classification)(sentiment-aware word embeddings, Used-for, task sentiment classification)(social interaction features, Evaluate-for, task sentiment classification)(sentiment context, Part-of, sentiment-aware word embeddings)(target-oriented sentiment classification, Hyponym-Of, sentiment classification)(sentence level, Part-of, sentiment classification)(lexicon term level, Part-of, sentiment classification)(sentiment words, Hyponym-Of, sentiment classification)
(news summarization, Hyponym-Of, text summarization)(news summarization, Used-for, extracting key information)(abstractive summarization, Part-of, text summarization)(extractive summarization, Part-of, text summarization)(neural sequence-to-sequence models, Used-for, abstractive text summarization)(query-based summarization, Evaluate-for, news summarization)(encode-attend-decode paradigm, Used-for, news summarization)(coverage mechanism, Used-for, summarization)(query attention model, Used-for, summarization)(diversity based attention model, Used-for, summarization)(dataset building, Used-for, testing summarization models)(ROUGE-L scores, Evaluate-for, summarization performance)(pointer-generator network, Used-for, abstractive summarization)(CNN/Daily Mail summarization task, Evaluate-for, summarization models)(selective encoding model, Used-for, abstractive summarization)(recurrent neural networks, Used-for, sentence encoding and decoding
(parser trained, Evaluate-for, domain adaptation)(parser trained, Evaluate-for, sequence labeling tasks)(parser trained, Evaluate-for, out-of-domain treebanks)(parser trained, Used-for, constituent parsing)(non-monotonic transition system, Hyponym-Of, transition system)(dynamic oracle, Used-for, Covington parser)(domain adaptation, Used-for, handle domain dependence)(domain-adapted word embeddings, Used-for, sentiment classification tasks)(generic word embeddings, Conjunction, domain specific embeddings)(pre-trained word embeddings, Part-of, neural network architectures for NLP tasks)(name tagging and linking, Part-of, cross-lingual name tagging and linking framework)
(adversarial sample, Hyponym-Of, adversarial example)(adversarial sample, Evaluate-for, neural machine translation)(adversarial sample, Evaluate-for, text classification)(adversarial sample, Evaluate-for, style transfer)(adversarial sample, Evaluate-for, question answering)(adversarial sample, Used-for, perturb texts)(adversarial sample, Used-for, adversarial training)(adversarial sample, Used-for, attack neural networks)(adversarial sample, Used-for, evaluate robustness)(adversarial sample, Used-for, improve model robustness)(adversarial sample, Part-of, adversarial attack)(adversarial sample, Part-of, adversarial domain adaptation)(adversarial sample, Part-of, adversarial stability training)(adversarial training, Used-for, improve model robustness)(adversarial training, Part-of, adversarial stability training)(adversarial training, Used-for, style transfer)(adversarial training, Used-for,
(better language, Part-of, translation)(better language, Evaluate-for, translators)(better language, Evaluate-for, neural language model)(better language, Evaluate-for, natural language interface)(better language, Used-for, second language acquisition)(neural semantic parser, Part-of, better language)(predicate-argument structures, Part-of, neural semantic parser)(training, Used-for, neural semantic parser)(SPADES, Evaluate-for, neural semantic parser)(GRAPHQUESTIONS, Evaluate-for, neural semantic parser)(GEOQUERY, Evaluate-for, neural semantic parser)(WEBQUESTIONS, Evaluate-for, neural semantic parser)(poetry generation, Used-for, neural language model)(rhyme, Part-of, poetry generation)(rhythm, Part-of, poetry generation)(alliteration, Part-of, poetry generation)(constraint satisfaction problem, Hyponym-Of, poetry generation)(weighted finite state machine, Used-for, constraint satisfaction problem)(Language models, Used
(machine translation nmt model, Used-for, translation)(machine translation nmt model, Compare, statistical machine translation)(machine translation nmt model, Hyponym-Of, neural machine translation)(deep neural networks, Part-of, neural machine translation)(bi-directional lstms, Used-for, encode source sentence)(linear associative units, Used-for, reduce gradient propagation path)(convolutional layers, Used-for, encode source sentence)(parse trees, Used-for, obtain structural label sequences)(Parallel RNN encoder, Hyponym-Of, encoder)(Hierarchical RNN encoder, Hyponym-Of, encoder)(Mixed RNN encoder, Hyponym-Of, encoder)(nested attention layers, Part-of, grammatical error correction systems)(error-correcting codes, Part-of, binary codes)(encoder, Part-of, neural machine translation)(decoder, Part-of, neural machine translation)(layer-wise relevance propagation, Used-for, interpret internal workings
(multilingual learning, Part-of, Neural Named Entity Recognition)(multilingual GeoQuery corpus, Used-for, evaluation results)(sequence-to-tree model, Part-of, multi-task learning framework)(Character-based sequence-to-sequence model, Evaluate-for, Language Identification)(multilingual connotation frames, Used-for, study of public sentiments)(multilingual dataset, Part-of, semantic parsing)(joint multilingual sentence embedding, Used-for, filter noisy parallel data)(multi-task architecture, Used-for, supervised models with minimal data)(Name Tagging, Used-for, target task)(multilingual distributed representations, Used-for, learning text representations)(multilingual embeddings, Part-of, word and sentence embeddings)(multi-lingual multi-task architecture, Part-of, supervised models)(parallel data mining, Used-for, improving machine translation)(Language Identification (LID), Used-for, processing multilingual text)(multilingual dataset, Part-of, large-scale multilingual corpus of images)(polyglot semantic dependency parser, Evaluate-for
(aspect level sentiment classification, Hyponym-Of, sentiment analysis)(aspect level sentiment classification, Used-for, classifying sentiment polarities over opinion targets in sentences)(attention mechanism, Used-for, detecting sentiment context in aspect level sentiment classification)(target sensitive memory networks, Used-for, handling target-sensitive sentiment in aspect level sentiment classification)(text-based transfer learning, Used-for, improving aspect level sentiment classification performance)(Reinforced Bidirectional Attention Network, Used-for, semantic matching in aspect level sentiment classification)(aspect routing approach, Part-of, Transfer Capsule Network)(Transfer Capsule Network, Used-for, transferring document-level knowledge to aspect level sentiment classification)(progressive self-supervised attention learning, Improve, attention mechanisms in aspect level sentiment classification)(document-level labeled data, Evaluate-for, aspect level sentiment classification)(Multi-sentiment-resource Enhanced Attention Network, Improve, aspect level sentiment classification)(sentence level, Compare, document level)
(neural programmer, Part-of, Neural Symbolic Machine)(symbolic computer, Part-of, Neural Symbolic Machine)(REINFORCE, Used-for, optimize task reward)(sequential models, Compare, n-grams)(sequential models, Compare, skip-grams)(morph-fitting procedure, Used-for, improving distributional vector spaces)(neural architecture, Part-of, code generation)(grammar model, Used-for, capturing syntax as prior knowledge)(NLG system, Used-for, generating referring expressions)(corrective REs, Compare, REs without contrast marking)(TextFlow, Used-for, text similarity measures)(readability, Evaluate-for, RC datasets)(prerequisite skills, Evaluate-for, RC datasets)(ONLG, Used-for, generating subjective responses)(Relational-Realizational grammar, Compare, lexicalized grammars)(logogram features, Part-of, radical features for event extraction)(syllable features, Part-of, radical features for event extraction)(B
(story ending generation, Evaluate-for, sentiment intensity)(story ending generation, Evaluate-for, coherent and fluent)(story ending generation, Used-for, controlling sentiment intensity)(story ending generation, Hyponym-Of, natural language generation)(sentiment analyzer, Used-for, acquiring sentiment intensities)(sentimental generator, Used-for, controlling sentiment of the output)(sentimental generator, Part-of, story ending generation)Gaussian Kernel Layer, Part-of, controlling sentiment intensity)(coherent and fluent, Evaluate-for, story ending generation)(natural language generation, Used-for, text generation)(natural language generation, Conjunction, story ending generation)(story ending generation, Conjunction, story dataset)(sentiment intensity, Compare, sentiment labels)
(dialogue generation, Part-of, multi-turn dialogue agent)(multi-turn dialogue agent, Used-for, KB-InfoBot)(KB-InfoBot, Compare, traditional systems)(symbolic queries, Compare, soft retrieval process)(reinforcement learner, Used-for, higher task success rate)(neural end-to-end agent, Hyponym-Of, dialogue agent)(Affect-LM, Used-for, generation of conversational text)(text similarity measures, Used-for, plagiarism detection)(TextFlow, Compare, existing similarity measures)(automatic evaluation metrics, Compare, human judgements)(ADEM, Hyponym-Of, automatic evaluation model)(ADEM, Used-for, predicting scores to input responses)(deep RL framework, Used-for, dialogue policy learning)(Deep Dyna-Q, Hyponym-Of, RL framework)(world model, Used-for, dialogue agent optimization)(adverbial presupposition triggers, Used-for, natural language generation tasks)(sequence-to-sequence models, Used-for,
(translation task, Part-of, neural machine translation)(neural machine translation, Used-for, text translation)(BLEU, Evaluate-for, translation performance)(input sequence, Used-for, generating target text)(convolutional neural network, Part-of, creating embeddings)(visual embeddings, Used-for, characters processing)(syntactic information, Used-for, improving NMT)(hard-attention, Compare, soft-attention)(sequence-level training, Is-a-Prerequisite-of, lowering error accumulation)(unrelated language, Evaluate-for, transfer learning techniques)(parameter freezing, Part-of, NMT transfer learning)(synthetic data, Used-for, NMT model improvement)(Poincaré embeddings, Compare, distributional semantic representations).
(solving cold-start problem, Used-for, review spam detection)(neural network model, Used-for, detecting review spam)(neural network model, Used-for, solving cold-start problem)(end-to-end learning, Used-for, dialog systems)(Hybrid Code Networks, Compare, end-to-end learning of RNNs)(Hybrid Code Networks, Used-for, dialog systems)(Hybrid Code Networks, Used-for, reducing training data)(Classification models, Hyponym-Of, supervised ner)(supervised ner, Used-for, named entity recognition)(supervised ner, Evaluate-for, manually annotated data)(manually annotated data, Hyponym-Of, annotated NER data)(automatically labeled data, Compare, human-labeled data)(question answering models, Used-for, semi-supervised question answering)(Generative Domain-Adaptive Nets, Used-for, semi-supervised question answering)(generative model, Hyponym-Of, Generative Domain-Adaptive Nets)(automatic Pyramid scores, Used-for,
(extractive multi-document summarization, Used-for, content selection)(multi-document summarization, Part-of, document summarization)(optimization-based extractive multi-document summarization, Hyponym-Of, multi-document summarization)(genetic algorithm, Used-for, automatic training data generation)(abstractive summarization, Compare, extractive summarization)(graph-based attention mechanism, Used-for, neural abstractive document summarization)(content selection, Used-for, maximizing user-desired content)(hierarchical encoder, Used-for, representation of sentences)(attention-based extractor, Used-for, document modeling)(email subject line generation, Compare, document summarization)(determinantal point processes, Used-for, multi-document summarization)(similarity measure, Used-for, measuring redundancy)(global optimization method under length constraint, Used-for, neural text summarization models)(in-length summaries, Used-for, post-editing time)(single document summarization (SDS), Compare, multi-document summarization (MDS))(focus
(bias mention, Part-of, mention)(bias mention, Part-of, entity linking)(bias mention, Used-for, disambiguation method)(mention, Part-of, textual context)(entity linking, Part-of, NLP applications)(entity linking, Used-for, aligning textual mentions)(entity linking, Used-for, knowledge base)(disambiguation method, Evaluated-for, entity linking)(mention detection, Used-for, coreference)(coreference, Used-for, entity linking)(coreference resolution, Used-for, entity linking)(words, Part-of, textual context)(multi-prototype mention embeddings, Used-for, mention)(language model, Used-for, disambiguation)(entity, Part-of, knowledge base)(knowledge base, Used-for, entity linking)(entity linking, Compare, coreference)(coreference resolution, Compare, entity linking)(mention detection, Part-of, coreference resolution)(mention detection, Used-for, coreference)(mention, Part-of
(state-of-the-art named entity recognition, Is-a-Prerequisite-of, zero shot cross lingual)(annotating NER data by human, Part-of, state-of-the-art named entity recognition)(annotation projection, Used-for, zero shot cross lingual)(distributed representations of words, Used-for, zero shot cross lingual)(co-decoding schemes, Used-for, zero shot cross lingual)(Machine Translation, Used-for, zero shot cross lingual)(word embeddings, Used-for, zero shot cross lingual)(source language, Is-a-Prerequisite-of, zero shot cross lingual)(target language, Is-a-Prerequisite-of, zero shot cross lingual)(cross-lingual NER, Hyponym-Of, zero shot cross lingual)(bilingual dictionary induction, Used-for, zero shot cross lingual)(parallel corpora, Hyponym-Of, cross-lingual language processing)(Universal Dependencies, Used-for, cross-lingual syntactic structure processing
(multimodal sarcasm detection, Used-for, sarcasm detection)(multimodal sarcasm detection, Evaluate-for, sarcastic tweets)(multimodal sarcasm detection, Evaluate-for, image-text sentiment detection)(multimodal sarcasm detection, Is-a-Prerequisite-of, cross-modal graph convolutional network)(multimodal sarcasm detection, Evaluate-for, multimodal fake news detection)(multimodal sarcasm detection, Evaluate-for, Multi-channel Graph Neural Networks)(multimodal sarcasm detection, Evaluate-for, Multi-modal Sarcasm Explanation)(multimodal sarcasm detection, Used-for, real-world benchmark datasets)(multimodal sarcasm detection, Used-for, sarcasm target identification)(sarcasm target identification, Part-of, multimodal sarcasm detection)(sarcasm detection, Used-for, sentiment analysis)(sarcasm detection, Part-of, multidisciplinary NLP systems)(sarcasm detection, Compare, sentiment analysis)(cross-modal graph convolution
(document level sentiment, Part-of, aspect-based sentiment analysis)(aspect-based sentiment analysis, Evaluate-for, aspect term extraction)(aspect-based sentiment analysis, Evaluate-for, opinion term extraction)(aspect-based sentiment analysis, Evaluate-for, aspect-level sentiment classification)(document level sentiment, Used-for, minimizing information deficiency)(document level sentiment, Enhance, aspect-level sentiment classification)(document level sentiment, Enhance, intra-aspect sentiment consistency)(document level sentiment, Enhance, inter-aspect sentiment tendency)(complex inter-sentence relations, Part-of, document level sentiment)(aspect term extraction, Hyponym-Of, aspect-based sentiment analysis)(opinion term extraction, Hyponym-Of, aspect-based sentiment analysis)(aspect-level sentiment classification, Hyponym-Of, aspect-based sentiment analysis)(SentiBERT, Used-for, phrase level sentiment classification)
(dialogue agent, Is-a-Prerequisite-of, task-oriented dialogue systems)(dialogue agent, Used-for, user interaction)(dialogue agent, Used-for, accessing real-world knowledge)(dialogue agent, Part-of, Knowledge Base search)(dialogue agent, Used-for, multi-turn dialogue)(symbolic query, Compare, induced “soft” posterior distribution)(reinforcement learner, Used-for, higher task success rate)(user feedback, Used-for, training end-to-end agent)(response selection, Used-for, task success)(automatic evaluation, Is-a-Prerequisite-of, dialogue research)(neural belief tracking, Used-for, estimating user's goal)(neural model, Used-for, evolving knowledge graph embeddings)(belief tracker, Used-for, estimating user’s goal)(dialogue act classification, Part-of, spoken dialogue systems)(alignment, Conjunction, logistic regression models)(Transformer, Used-for, machine translation)(sequence-to-sequence model, Used-for, task-oriented dialogue systems)(neural knowledge
(Event extraction, Used-for, Supervised learning)(Supervised learning, Is-a-Prerequisite-of, State-of-the-art named entity recognition)(Hybrid Code Networks, Used-for, Supervised learning)(Supervised attention mechanisms, Used-for, Event detection)(Relation extraction, Used-for, Supervised learning)(Classification, Used-for, Supervised relation).
(automatic dialogue evaluation, Used-for, evaluating dialogue models)(automatic dialogue evaluation, Compare, existing automatic evaluation metrics)(existing automatic evaluation metrics, Compare, human judgements of response quality)(ADEM model, Evaluate-for, automatic dialogue evaluation)(ADEM model, generalize to evaluating, unseen dialogue models)(dialogue response quality, Compare, word-overlap metrics like BLEU)(challenge, Part-of, automatic dialogue evaluation)(new dataset of human response scores, Used-for, ADEM model predictions)(utterance-level predictions, Part-of, ADEM model)(system-level predictions, Part-of, ADEM model)(dynamic knowledge graph embeddings, Used-for, modeling structured and unstructured language)(dynamic knowledge graph embeddings, evolve as, dialogue progresses)(human evaluations, Evaluate-for, ADEM model)(goal achievement, Evaluate-for, ADEM model)(splitting algorithm, Used-for, sentence splitting)(sentence splitting, Compare, neural Machine Translation for simplification)(semantic parsing,
(aspect term extraction, Part-of, aspect-based sentiment analysis)(aspect term extraction, Evaluate-for, sentiment classification)(aspect term extraction, Hyponym-Of, opinion analysis)(aspect term extraction, Used-for, opinion summarization)(aspect term extraction, Is-a-Prerequisite-of, fine-grained sentiment analysis)(aspect-based sentiment analysis, Compare, fine-grained sentiment analysis)(fine-grained sentiment analysis, Part-of, sentiment analysis)(Conditional Random Fields, Evaluate-for, aspect term extraction)(word embeddings, Used-for, aspect term extraction)(dependency parsing, Used-for, aspect term extraction)(attention mechanism, Used-for, aspect term extraction)(recursive neural network, Used-for, aspect term extraction)(sequence labeling, Compare, sequence-to-sequence learning)(sequence-to-sequence learning, Evaluate-for, aspect term extraction)(dual recurrent neural network, Used-for, aspect term extraction)(cross-shared unit, Used-for, aspect term extraction)(referring expression generation, Compare, NeuralREG)(Ne
(fact checked claim, Part-of, fact-checking organization)(fact checked claim, Used-for, saving effort)(fact checked claim, Used-for, avoiding wasting time)(fact checked claim, Part-of, specialized dataset)(fact checked claim, Evaluate-for, detection)(fact checked claim, Part-of, FC-articles)(FC-articles, Is-a-Prerequisite-of, reranking)(retrieval approaches, Compare, MTM)(textual similarity approaches, Compare, MTM)(MTM, Used-for, rank FC-articles)(event information, Part-of, MTM)(pattern information, Part-of, MTM)(ROUGE-guided Transformer, Used-for, event information)(pattern vectors, Used-for, pattern information)(social media, Used-for, spreading false claims)
(domain sentiment lexicon, Used-for, sentiment classification)(SemAxis, Used-for, domain sentiment lexicon)(HCSC, Used-for, sentiment analysis)(CSAA, Part-of, HCSC)(MEAN, Used-for, sentiment prediction)(MEAN, Part-of, sentiment analysis)(MGNNS, Used-for, image-text sentiment detection)(Sentiment lexicons, Used-for, sentiment classification)(Negation words, Used-for, sentiment classification)(Intensity words, Used-for, sentiment classification)(KinGDOM, Used-for, domain-specific sentiment analysis)(Byte Pair Encodings, Used-for, multilingual embeddings)(Cross-domain sentiment classification, Used-for, unsupervised domain adaptation).
(explainability method, Compare, attribution methods)(explainability method, Used-for, interpreting how deep models predict)(attribution methods, Evaluate-for, accuracy of reflection of the reasoning process of the model)(attribution methods, Evaluate-for, model predictions)(LIMSSE, Part-of, explainability methods)(LRP, Part-of, explainability methods)(DeepLIFT, Part-of, explainability methods)(IP, Part-of, explainability methods)(mesh-based explanation, Compare, gradient-based explanation methods)(explainability method, Evaluate-for, interpretability)(evidence extraction, Is-a-Prerequisite-of, explainability methods)(accurate claim verification, Used-for, fact verification)(evidence extraction method, Used-for, explainability methods)(automatic evaluation paradigm, Used-for, explainability methods)(state-of-the-art model, Evaluate-for, quality of explainations)(fidelity-based metrics, Compare, Word Alignment Error Rate)(textual entailment task, Evaluate
(question, Used-for, reading comprehension)(reading comprehension, Evaluate-for, SQuAD dataset)(generative model, Used-for, generating questions)(experiments, Evaluate-for, WikiReading dataset)(experiments, Evaluate-for, question answering models)(cross-attention mechanism, Used-for, representing questions and scores)(unlabeled text, Evaluate-for, performance boost)(neural network model, Evaluate-for, question answering system)(neural network, Is-a-Prerequisite-of, KB-QA)(neural network, Used-for, relation detection)(KB-QA, Conjunction, entity linking)(sequence learning, Hyponym-Of, question answering)(long documents, Evaluate-for, model performance)(sentence selection, Part-of, reading comprehension)(experimental results, Evaluate-for, KB-QA system)(reading comprehension, Evaluate-for, deep neural networks)(neural architecture, Part-of, question answering system).
(joint entity relation extraction, Used-for, identifying entity mention spans)(joint entity relation extraction, Used-for, identifying relations between pairs of the entity mentions)(joint entity relation extraction, Part-of, relation extraction)(relation extraction, Used-for, Knowledge Base Population)(Knowledge Base Population, Compare, end-to-end relation extraction model for KB enrichment)(end-to-end relation extraction model for KB enrichment, Hyponym-Of, relation extraction)(GraphRel, Hyponym-Of, relation extraction)(GraphRel, Used-for, extracting relations)(GraphRel, Used-for, learning named entities and relations)(GraphRel, Used-for, predicting overlapping relations)(distant supervision, Used-for, collecting training data)(feature extraction, Part-of, statistical NLP)(relation extraction, Evaluate-for, accurate classifiers)(accurate classifiers, Is-a-Prerequisite-of, relation extraction)(curriculum learning based method, Used-for, training transition matrix)
None
(generating natural language, Used-for, question answering systems)(generating natural language, Used-for, generating natural answers)(sequence-to-sequence learning, Part-of, generating natural language)(COREQA, Used-for, generating natural language)(semantic units, Part-of, generating natural language)(copying mechanism, Part-of, generating natural language)(retrieving mechanism, Part-of, generating natural language)(encoder-decoder framework, Part-of, generating natural language)(natural language utterances, Evaluate-for, semantic parser)(natural language and mathematical expressions, Part-of, answer rationales)(learning decentralized multiagent policies, Evaluate-for, coordination via communication channel)(natural language strings, Compare, agent messages)(neural semantic parser, Evaluate-for, converting natural language to domain-general representations)(neural language model, Used-for, generating poetry)(natural language text, Source-of, commonsense knowledge)(learning algorithm, Evaluate-for, semantic parser)
(question answering system, Used-for, reading comprehension)(question answering system, Part-of, NLP applications)(question answering system, Evaluate-for, SQuAD dataset)(COREQA, Is-a-Prerequisite-of, question answering system)(question answering, Conjunction, machine comprehension)(gated attention-based recurrent networks, Used-for, question answering system)(pointer networks, Used-for, locating answer positions)(KB-QA, Part-of, question answering system)(EviNets, Is-a-Prerequisite-of, factoid question answering)(reading comprehension, Hyponym-Of, question answering)(dynamic neural semantic parsing framework, Used-for, sequential question answering)(end-to-end neural network model, Used-for, KB-QA)(relation detection, Part-of, KB-QA)(state-of-the-art models, Evaluate-for, reading comprehension)(multitask learning, Evaluate-for, machine reading at scale)(KB-QA system, Use-for, question answering system)
(open domain question answering, Part-of, question answering)(EviNets, Used-for, factoid question answering)(reading comprehension, Used-for, open domain question answering)(Universal schema, Used-for, open domain question answering)(Knowledge Base, Compare, raw text)(jumping LSTM, Evaluate-for, performance)(COREQA, Implement, sequence-to-sequence learning)(pointer networks, Used-for, locating answers)(generative domain-adaptive nets, Used-for, semi-supervised question answering)(constituent-centric neural architecture, Used-for, reading comprehension)(reinforcement learning, Used-for, training models)(sentence selector, Used-for, minimal context selection)(WikiReading dataset, Used-for, evaluating QA models)(SQuAD dataset, Used-for, evaluating QA models)(BLEU, Compare, human evaluation)(parsing framework, Used-for, semantic parsing)(TF-IDF matching, Used-for, search component)(representation learning, Used-for, representing answers)(evaluation metrics, Evaluate-for,
(image text pair, Part-of, multimodal post)(image text pair, Evaluate-for, multimodal sentiment detection)(image text pair, Used-for, VLP)(VLP, Part-of, vision-and-language tasks)(sentiment detection, Is-a-Prerequisite-of, multimodal sentiment detection)(ISD, Evaluate-for, IAIS)(RELIT, Used-for, pre-training)(ReMuQ, Used-for, knowledge retrieval)
(question answering dataset, Part-of, SQuAD)(question answering dataset, Part-of, WebQuestions)(question answering dataset, Part-of, SimpleQuestions)(question answering dataset, Part-of, WikiReading)(question answering dataset, Part-of, Spades)(SQuAD, Compare, WebQuestions)(SQuAD, Part-of, reading comprehension)(WebQuestions, Part-of, KB-QA)(SimpleQuestions, Part-of, KB-QA)(WikiReading, Part-of, reading comprehension)(Spades, Evaluate-for, universal schema).
(pre trained language, Hyponym-Of, fixed-vocabulary language models)(pre trained language, Used-for, named entity recognition)(neural semantic parser, Used-for, natural language utterances)(neural semantic parser, Used-for, predicate-argument structures)(predicate-argument structures, Part-of, domain-general natural language representations)(transition system, Used-for, mapping to target domains)(semantic parser, Used-for, converting natural language utterances)(annotated logical forms, Used-for, training semantic parser)(annotated logical forms, Part-of, denotations)(semantic parsing, Hyponym-Of, language modeling)(relative physical knowledge, Used-for, reasoning about actions and objects)(relative physical knowledge, Part-of, knowledge acquisition)(state-of-the-art, Evaluate-for, SPADES)(state-of-the-art, Evaluate-for, GRAPHQUESTIONS)(phylogenetic language trees, Used-for, uncovering history of source language)(source language, Compare, target
(aspect level sentiment, Part-of, aspect-based sentiment analysis)(aspect level sentiment, Used-for, determine sentiment polarity)(aspect level sentiment, Part-of, aspect sentiment classification)(aspect level sentiment, Used-for, predict sentiment polarities)(aspect level sentiment, Evaluate-for, aspect term extraction)(aspect level sentiment, Evaluate-for, aspect term-polarity co-extraction)(aspect level sentiment, Compare, document-level sentiment classification)(aspect level sentiment, Used-for, sentiment predictions)(aspect level sentiment, Used-for, determining sentiment context)(aspect level sentiment, Part-of, sequence prediction)(aspect level sentiment, Evaluate-for, attention mechanism)(aspect level sentiment, Evaluate-for, neural network models)(aspect level sentiment, Evaluate-for, recurrent neural networks)(aspect level sentiment, Evaluate-for, convolutional neural networks)(aspect level sentiment, Evaluate-for, transfer learning)
(controllable text generation, Part-of, Conditional Text Generation)(controllable text generation, Part-of, Neural Text Generation)(controllable text generation, Used-for, response generation)(variational model, Evaluate-for, controllable text generation)(PPVAE, Evaluate-for, controllable text generation)(CDL, Evaluate-for, controllable text generation)(textual entailment measures, Evaluate-for, faithfulness)(faithfulness, Compare, ROUGE)(Summarization model, Evaluate-for, abstractiveness scores)(lattice transformer, Evaluate-for, machine translation)(lattice transformer, Evaluate-for, speech translation)(SAMA, Evaluate-for, Job Posting Generation)(style transfer, Hyponym-Of, Conditional Text Generation)(personalized dialogue systems, Hyponym-Of, Conditional Text Generation).
(speech text translation, Part-of, Neural Machine Translation)(Neural Machine Translation, Hyponym-Of, Statistical Machine Translation)(Neural Machine Translation, Evaluate-for, translation quality)(translation system, Evaluate-for, translation quality)(translation system, Hyponym-Of, Neural Machine Translation)(adversarial stability training, Used-for, improve translation quality)(unsupervised neural machine translation, Hyponym-Of, Neural Machine Translation)(word alignment, Part-of, translation system)(word alignment, Evaluate-for, translation quality)(bidirectional tree encoder, Is-a-Prerequisite-of, incorporate source-side syntactic trees)(attention-based Neural Machine Translation, Part-of, Neural Machine Translation)(Neural Machine Translation, Compare, phrase-based systems)(syntactic information, Used-for, improve word alignment quality)(NMT models, Hyponym-Of, translation systems)(deep learning, Used-for, improve translation accuracy).
(language generation, Compare, language understanding)(language generation, Part-of, neural symbolic machine)(neural symbolic machine, Part-of, neural programmer)(neural symbolic machine, Part-of, symbolic computer)(neural programmer, Used-for, mapping language utterances to programs)(text similarity measures, Used-for, plagiarism detection)(text similarity measures, Used-for, information ranking)(text similarity measures, Used-for, recognition of paraphrases and textual entailment)(ONLG, Hyponym-Of, language generation)(NBT framework, Hyponym-Of, belief tracker)(generation systems, Evaluate-for, BLEU)(generation systems, Evaluate-for, human evaluation)(style transfer, Hyponym-Of, language generation)(relational-realizational grammar, Hyponym-Of, generative grammars)(lexicalized grammars, Hyponym-Of, generative grammars)
(improve translation, Evaluate-for, Neural Machine Translation)(Neural Machine Translation, Used-for, machine translation)(machine translation, Conjunction, statistical machine translation)(Neural Machine Translation, Evaluate-for, Chinese-English translation task)(Neural Machine Translation, Used-for, incorporating syntactic information)(translate, Used-for, agent messages)(improve translation, Evaluate-for, hierarchical representations)(Neural Machine Translation, Evaluate-for, target language)(Neural Machine Translation, Evaluate-for, English-French translation)(Neural Machine Translation, Evaluate-for, Sequence-to-Dependency Neural Machine Translation)(Neural Machine Translation, Evaluate-for, attention-based Neural Machine Translation)(Neural Machine Translation, Evaluate-for, posterior regularization)
(dialogue data, Used-for, training dialogue systems)(KB-InfoBot, Part-of, task-oriented dialogue systems)(task-oriented dialogue systems, Conjunction, open-domain dialogue systems)(belief tracker, Part-of, modern spoken dialogue systems)(Neural Belief Tracking, Hyponym-Of, belief tracker)(user utterance, Part-of, dialogue state tracking)(context modelling, Part-of, Neural Belief Tracking)(dialogue dynamics, Used-for, modelling task completion)(task success prediction, Evaluate-for, dialogue features)(utterance rewriting, Used-for, multi-turn dialogue modelling)(response selection, Used-for, identifying next utterance)(spatio-temporal features, Used-for, extracting matching information)(belief spans, Used-for, tracking dialogue beliefs).
(word embedding models, Used-for, learns vector representations)(GloVe, Used-for, learns vector representations)(word vectors, Part-of, GloVe)(co-occurrence statistics, Used-for, learns vector representations)(co-occurrence statistics, Used-for, GloVe)(relation vectors, Part-of, vector space)(Latent Meaning Models, Part-of, word embeddings)(morphemes, Part-of, words)(documents, Part-of, entity records)(sentences, Part-of, documents)(statistical event distribution, Part-of, corpus of novels)(story generation, Used-for, generating long and coherent text)(capsule networks, Used-for, multi-document summarization)(latent structure induction, Used-for, event causality reasoning)(Descriptors/scoring features, Part-of, entity matching rules)(Key Attribute Tree, Part-of, entity matching rules)(entity matching, Used-for, recognizing entity records)(typed entailment graphs, Used-for, modeling entailment relations).
(semantic dependency parsing, Compare, syntactic dependency parsing)(semantic dependency parsing, Evaluate-for, relationship between words)(semantic dependency parsing, Part-of, natural language processing)(semantic dependency parsing, Evaluate-for, meaning of a sentence)(semantic dependency parsing, Used-for, capturing word relationships)(semantic dependency parsing, Evaluate-for, directed acyclic graphs)
(discord relation, Part-of, stancetaking)(discourse relation, Used-for, signal relationships)(multidimensional analysis, Used-for, identify stance dimensions)(joint modeling approach, Used-for, label discourse relations)(implicit discourse relation classification, Used-for, recognizing implicit discourse relations)(Rhetorical Structure Theory, Used-for, discourse analysis)(discourse coherence, Evaluate-for, text quality)(discourse coherence, Used-for, language assessment)(joint inference framework, Used-for, extracting relations)(sciDTB, Conjunction, PDTB)(sciDTB, Conjunction, RST-DT)(attention-based Bi-LSTM, Used-for, classifying implicit discourse relations)(SciDTB, Used-for, evaluating discourse dependency parsers)(DSWE, Used-for, recognizing implicit discourse relations)(evaluation features, Evaluate-for, discourse structure)(relation schema induction, Used-for, identifying type signatures of relations)(neural networks, Used-for, predicting discourse coherence)(Chinese Discourse Tree
(End-to-end learning, Used-for, dialog systems)(Hybrid Code Networks, Compare, end-to-end approaches)(Hybrid Code Networks, Part-of, dialog systems)(Hybrid Code Networks, Used-for, supervised learning)(Hybrid Code Networks, Used-for, reinforcement learning)(Hybrid Code Networks, Is-a-Prerequisite-of, state-of-the-art performance)(bAbI dialog dataset, Used-for, dialog systems)(Open Information Extraction, Used-for, question-answering)(Open Information Extraction, Used-for, generating semi-structured knowledge)(retrofitting, Used-for, fine-tuning word vectors)(explicit retrofitting model, Used-for, semantic specialization)(neural reading comprehension model, Part-of, natural language understanding)(neural reading comprehension model, Used-for, improving machine comprehension)(commonsense knowledge, Part-of, neural reading comprehension model)(naive physical action-effect prediction, Part-of, artificial agents)(naive physical action-effect prediction, Used-for
(data text generation model, Used-for, generating summaries)(data text generation model, Used-for, text generation)(data text generation model, Part-of, Natural Language Generation)(data text generation model, Part-of, neural network architectures)(data text generation model, Evaluate-for, baseball domain)(data text generation model, Evaluate-for, RotoWire benchmark)(entity-centric neural architecture, Used-for, data text generation)(entity memory representations, Part-of, entity-centric neural architecture)(semantic parsing, Evaluate-for, Atis)(semantic parsing, Evaluate-for, Jobs)(semantic parsing, Evaluate-for, Geo)(abstract syntax networks, Used-for, code generation)(code generation, Used-for, generating executable outputs)(benchmark Hearthstone dataset, Evaluate-for, code generation)(multimodal hierarchical attention, Part-of, abstractive meeting summarizer)(multimodal hierarchical attention, Used-for, focusing on topically-relevant segments)(abstract syntax trees, Part-of, abstract syntax networks)(dec
None
(morphological paradigm, Is-a-Prerequisite-of, morphological analysis)(morphological typology, Part-of, morphological paradigm)(lexical relations, Hyponym-Of, morphological paradigm)(syntactic traits, Part-of, morphological analysis)(morphological regularities, Part-of, morphological analysis)(inflectional forms, Part-of, morphological analysis)(derivational antonyms, Part-of, morphological analysis)(morphological constraints, Part-of, morphological paradigm)(morphological disambiguation, Used-for, morphological analysis)(morphological segmentation, Part-of, morphological analysis)(cross-lingual morphological tagging, Used-for, morphological analysis)(morphologically rich languages, Compare, low-resource languages)(semantic quality, Evaluate-for, word vector collection)(character representations, Evaluate-for, morphological typologies)(cross-lingual transfer, Used-for, resource scarcity)(machine translation, Evaluate-for, syntactic structure compatibility)(text classification, Evaluate-for, tokenization)(neural
(dialogue state tracking, Hyponym-Of, task-oriented dialogue systems)(dialogue state tracking, Used-for, tracking unknown slot values)(dialogue state tracking, Part-of, end-to-end (E2E) systems)(dialogue state tracking, Used-for, handling multi-domain dialogues)(dialogue state tracking, Used-for, estimating user goals)(dialogue state tracking, Part-of, Neural Belief Tracking)(dialogue state tracking, Part-of, TRADE)(dialogue state tracking, Part-of, GLAD)(dialogue state tracking, Part-of, SOM-DST)(dialogue state tracking, Used-for, handling zero-shot and few-shot cases)(dialogue state tracking, Used-for, efficient DST)(dialogue state tracking, Part-of, DST-SC)(dialogue state tracking, Part-of, PhotoBook dataset)(dialogue state tracking, Part-of, Meta-Reinforced Multi-Domain State Generator (MERET))(dialogue state tracking, Compare, traditional ontology
(event relation, Evaluate-for, event extraction)(event relation, Compare, semantic relation)(event extraction, Part-of, event relation)(semantic dependency parsing, Part-of, event relation)(argumentative relations, Part-of, event relation)(event relation, Used-for, stancetaking)(dependency graph, Used-for, event relation)
(non contextual subword embeddings, Compare, contextual representation method)(non contextual subword embeddings, Evaluate-for, part-of-speech tagging)(non contextual subword embeddings, Evaluate-for, named entity recognition)(FastText, Hyponym-Of, non contextual subword embeddings)(BPEmb, Hyponym-Of, non contextual subword embeddings)(BERT, Evaluate-for, part-of-speech tagging)(BERT, Evaluate-for, named entity recognition)(BERT, Part-of, contextual representation method)(character representations, Conjunction, non contextual subword embeddings)(character representations, Conjunction, BERT)(argument clustering, Used-for, argument classification)(contextualized word embedding methods, Compare, non contextual subword embeddings).
(sentence-level sentiment classification, Hyponym-Of, sentiment classification)(sentence-level sentiment classification, Used-for, capturing sentiment in a sentence)(sentence-level sentiment classification, Used-for, text classification)(neural network models, Used-for, multi-task learning)(shared layers, Part-of, neural network models)(shared layers, Used-for, extracting common features)(task-specific features, Compare, noise)(adversarial multi-task learning framework, Used-for, reducing interference between shared and private feature spaces)(extensive experiments, Used-for, validating model approaches)(text classification tasks, Part-of, extensive experiments)(transferable knowledge, Used-for, new tasks)(cognitive NLP systems, Used-for, extracting cognitive features)(cognitive features, Part-of, cognitive NLP systems)(gaze data, Used-for, sentiment polarity detection)(gaze data, Used-for, sarcasm detection)(Convolutional Neural Network, Used-for, learning features from text and gaze data)(Convolutional Neural Network, Compare,
(natural language generation, Used-for, generating natural answers)(natural language generation, Used-for, generating rhythmic poetry)(generating rhythmic poetry, Used-for, learning poetic devices)(neural language model, Used-for, generating rhythmic poetry)(question answering systems, Used-for, generating natural answers)(generating natural answers, Part-of, question answering systems)(encoder-decoder framework, Used-for, generating natural answers)(natural language interfaces, Used-for, analyzing data)(natural language interfaces, Used-for, manipulating text)(natural language interfaces, Used-for, querying databases)(COREQA, Part-of, question answering systems)(COREQA, Used-for, generating natural answers)(semantic units, Used-for, generating natural answers)(copying and retrieving mechanisms, Used-for, generating natural answers)(sequence-to-sequence learning, Used-for, generating natural answers)(semantic parser, Used-for, converting natural language utterances)(semantic parser, Used-for, mapping to executable programs)
(phonetic encoding, Used-for, language model)(neural network architectures, Is-a-Prerequisite-of, language model)(pretrained word embeddings, Part-of, language model)(sequence labeling tasks, Evaluate-for, language model)(LSTM language model, Part-of, Noisy Channel Model)(diachronic accuracy, Evaluate-for, language model)(query auto-completion, Used-for, language model)(sentence embeddings, Part-of, language model)(binarized sentence embeddings, Compare, continuous sentence embeddings)(Cross-lingual word embeddings, Evaluate-for, cross-lingual NLP)(Coherent poetry, Compare, arbitrary forms and themes)(reranking, Used-for, entailment models)(abstractive summarization, Compare, extractive summarization)(probabilistic finite automata, Used-for, compute word prediction)(prism module, Used-for, disentangle semantics of words)(named entity recognition, Used-for, evaluate language models)(Winograd Schema Challenge, Evaluate
(distributional semantics, Evaluate-for, understanding language)(distributional semantics, Part-of, word representation learning)(distributional semantics, Used-for, semantic textual similarity)(distributional semantics, Used-for, lexical entailment)(distributional semantics, Part-of, compositional distributional semantics)(distributional semantics, Part-of, distributional vector spaces)(distributional semantics, Compare, pattern-based models)(distributional vector spaces, Used-for, improving word estimates)(distributional vector spaces, Evaluate-for, semantic quality)(distributional vector spaces, Used-for, dialogue state tracking)(morph-fitting procedure, Used-for, distributional vector spaces)(morph-fitting procedure, Used-for, improving word representations)(co-occurrences, Part-of, distributional semantics)(word similarity, Evaluate-for, distributional semantics)(word analogy, Evaluate-for, distributional semantics)(word sememe information, Used-for, improving word representation learning)(semantic textual similarity, Evaluate-for,
(neural text generation, Used-for, abstractive summarization)(neural text generation, Used-for, video captioning)(neural text generation, Used-for, dialog systems)(neural text generation, Used-for, pun generation)(neural text generation, Used-for, derived word generation)(neural text generation, Used-for, multi-turn information-seeking conversation systems)(neural text generation, Used-for, data-to-text generation)(neural text generation, Used-for, table-to-text generation)(neural text generation, Used-for, SQUASH)(abstractive summarization, Hyponym-Of, document summarization)(graph-based attention mechanism, Part-of, sequence-to-sequence framework)(sequence-to-sequence framework, Used-for, video captioning)(DI-VAE, Part-of, variational autoencoders)(DI-VST, Part-of, variational autoencoders)(DI-VAE, Used-for, interpretable response generation)(DI-VST, Used-for
(aspect-based sentiment analysis, Part-of, aspect sentiment)(aspect extraction, Part-of, aspect-based sentiment analysis)(sentiment polarity, Part-of, aspect sentiment)(memory networks, Used-for, aspect sentiment classification)(attention mechanism, Used-for, aspect sentiment classification)(target-sensitive sentiment, Compare, context-only sentiment)(attention mechanism, Used-for, memory networks)(Dual crOss-sharEd RNN, Used-for, aspect term-polarity co-extraction)(interactive multi-task learning network, Used-for, aspect term-polarity co-extraction)(sarcasm interpretation, Used-for, sentiment polarity)(sarcasm interpretation, Evaluate-for, sentiment words)(ASPECT SENTIMENT CLASSIFICATION TOWARDS QUESTION-ANSWERING, Used-for, sentiment classification)(senti, Evaluate-for, sentiment polarity)(target aspect, Part-of, aspect-oriented dependency tree)(aspect-oriented dependency tree, Used-for, sentiment prediction)(relational graph attention network, Used-for, sentiment prediction)(syntactic relative
(speech translation e2e st, Part-of, Neural Machine Translation)(Neural Machine Translation, Part-of, sequence modeling)(sequence modeling, Compare, recurrent neural networks(RNN))(sequence modeling, Compare, Self-attention networks(SAN))(speech translation e2e st, Is-a-Prerequisite-of, curriculum pre-training method)(curriculum pre-training method, Used-for, improving speech translation performance)(speech translation e2e st, Part-of, multitask learning)(multitask learning, Used-for, improving speech translation performance)(speech translation, Compare, cascaded models)(speech translation e2e st, Used-for, performing speech recognition and machine translation)(word embedding, Used-for, improving multitask ST model)(self-attention models, Evaluate-for, character-level neural machine translation)(self-attention models, Used-for, machine translation tasks)(self-attention models, Compare, recurrence structure)(transformer architecture, Evaluate-for, state-of-the-art machine translation results)(transformer
(word embeddings, Hyponym-Of, point representations)(word embeddings, Used-for, aspect-based sentiment analysis)(word embeddings, Used-for, bilingual word embeddings)(word embeddings, Used-for, neural word segmentation)(word embeddings, Used-for, dependency parsing)(word embeddings, Used-for, aspect extraction)(word embeddings, Used-for, named entity recognition)(word embeddings, Used-for, document clustering)(word embeddings, Used-for, pretraining character and word embeddings)(word embeddings, Used-for, pretraining context embeddings)(word embeddings, Hyponym-Of, SGNS)(word embeddings, Used-for, clustering empirical distributions)(word embeddings, Used-for, detecting MCI)(word embeddings, Used-for, forecasting market volatility)(word embeddings, Part-of, CNE)(word embeddings, Hyponym-Of, DSWE)(semantic concepts, Hyponym-Of, WordNet)(Word2vec, Hyponym-Of, SGNS)(attention mechanism, Used-for, coherence improvement
(deep neural network architecture, Used-for, context sensitive lemmatization)(context sensitive lemmatization, Part-of, language independent lemmatization)(edit tree, Part-of, transformation between a word-lemma pair)(bidirectional gated recurrent structures, Used-for, extracting character level dependencies)(bidirectional gated recurrent structures, Used-for, capturing contextual information)(Language Models, Hyponym-Of, bidirectional language models)(context embeddings, Used-for, sequence labeling tasks)(names entity recognition, Part-of, sequence labeling tasks)(chunking, Part-of, sequence labeling tasks)(unlabeled text, Used-for, pre-trained word embeddings)(pre-trained word embeddings, Used-for, context-sensitive representations)(synsets, Used-for, context sensitive embeddings)(prepositional phrase attachments, Evaluate-for, context-sensitive embeddings)(sentiment classification, Used-for, domain-sensitive and sentiment-aware embeddings)(style-sensitive word vectors, Part-of, continuous bag of words embedding model)(multilingual neural machine
(nli model, Hyponym-Of, compositional distributional semantics models)(nli model, Used-for, question answering)(question answering, Evaluate-for, labeled sequence transduction)(variational encoder-decoders, Part-of, nli model)(attention mechanism, Part-of, nli model)(deep neural networks, Part-of, nli model)(knowledge bases, Evaluate-for, nli model)(interactive topic models, Evaluate-for, nli model)(semantic role labeling, Evaluate-for, nli model)(deep highway BiLSTM architecture, Part-of, nli model)(self-matching attention mechanism, Part-of, nli model)(deep learning, Used-for, semantic role labeling)(LaVi models, Evaluate-for, nli model)(rumor detection models, Evaluate-for, nli model)(knowledge base completion, Evaluate-for, nli model)(neural encoder-decoder models, Evaluate-for, nli model)
(SeqVAT, Used-for, sequence labeling tasks)(SeqVAT, Part-of, virtual adversarial training)(conditional random field, Used-for, sequence models)(text-to-SQL models, Evaluate-for, lexical matching)(Spider-Syn, Used-for, evaluating text-to-SQL robustness)(LOVE, Used-for, OOV words)(LOVE, Conjunction, contrastive learning framework)(mimick-like models, Part-of, LOVE)(RSMI, Conjunction, randomized smoothing)(RSMI, Conjunction, masked inference)(RSMI, Used-for, improving adversarial robustness)(NLI, Used-for, alignment rationale explanations)(co-attention based models, Used-for, NLI)(alignment rationale explanations, Part-of, AREC)(adversarial training, Conjunction, additional synonym annotations)(ATINTER, Evaluate-for, adversarial inputs)(ATINTER, Improves, adversarial robustness).
(domain learning, Part-of, multi-task learning)(domain learning, Used-for, extracting common features)(domain learning, Used-for, task-invariant features)(domain learning, Evaluate-for, text classification)(domain learning, Evaluate-for, question answering)(domain learning, Compare, end-to-end learning)(domain learning, Compare, supervised learning)(domain learning, Compare, reinforcement learning)(domain learning, Is-a-Prerequisite-of, knowledge representation)(domain learning, Part-of, transfer learning)(domain learning, Used-for, domain adaptation)(domain learning, Used-for, feature learning)(domain learning, Hyponym-Of, adversarial multi-task learning)(task-invariant features, Part-of, multi-task learning)(adversarial multi-task learning, Compare, traditional multi-task learning)(end-to-end learning, Hyponym-Of, recurrent neural networks)(supervised learning, Conjunction, reinforcement learning)(recurrent neural networks, Used-for, dialog systems)(Hybrid Code Networks, Compare, end-to
(nested named entity, Part-of, Named Entity Recognition)(nested named entity, Compare, flat named entity)(nested named entity, Is-a-Prerequisite-of, Overlapping Named Entity Recognition)(nested named entity, Used-for, improving accuracy on sequence labeling tasks)(nested named entity, Evaluate-for, performance on datasets like ACE-2004 and GENIA)(Named Entity Recognition, Used-for, identifying entities in text)(nested named entity, Hyponym-Of, Named Entity Recognition)(nested named entity, Evaluate-for, datasets like ACE-2004 and GENIA)(flat named entity, Compare, nested named entity)(Overlapping Named Entity Recognition, Part-of, Named Entity Recognition)(Flat Named Entity Recognition, Hyponym-Of, Named Entity Recognition)(machine reading comprehension, Used-for, Named Entity Recognition)
(approach visual question answering, Part-of, Visual question answering)(Visual question answering, Evaluate-for, answering questions based on images)(Visual question answering, Used-for, multi-modal question answering)(multi-modal question answering, Conjunction, textual and visual inputs)(textual and visual inputs, Used-for, Visual and Textual Question Answering model)(Paragraph-style image captions, Used-for, visual question answering)(Visual Genome dataset, Evaluate-for, visual question answering models)MC², Evaluate-for, Conversational Machine Reading Comprehension(convolutional neural networks, Part-of, Multi-perspective Convolutional Cube)(Multi-perspective Convolutional Cube, Is-a-Prerequisite-of, effective conversation understanding)(GTK, Evaluate-for, conversational question answering)(Conversational Question Answering, Evaluate-for, follow-up question identification)(follow-up question identification, Used-for, effective answer finding)(Bridging Anaphora Resolution, Evaluate-for, question
(automatic argument generation, Compare, automatic question generation)(automatic argument generation, Compare, automatic rumour verification)(automatic argument generation, Compare, automatic impression generation)(automatic argument generation, Compare, automatic pun generation)(retrieval system, Used-for, automatic argument generation)(counter-argument generation, Part-of, automatic argument generation)(question answering, Evaluate-for, question generation)(argument generation, Compare, debate)(stance detection, Is-a-Prerequisite-of, automatic argument generation)(argument specificity assessment, Is-a-Prerequisite-of, automatic argument generation)(Wikipedia, Used-for, automatic argument generation)(sequence-to-sequence learning, Used-for, automatic question generation)(attention-based recurrent neural network, Hyponym-Of, neural network)(attention mechanism, Used-for, neural network)(LSTM, Hyponym-Of, neural network)(question generation, Part-of, reading comprehension)(retrieval system, Is-a-Prerequisite-of, counter-argument generation)(encoder-decoder neural network, Compare
(neural machine translation model, Part-of, neural machine translation)(neural machine translation, Used-for, machine translation)(neural machine translation model, Used-for, translating)(neural machine translation model, Used-for, source sentence)(neural machine translation model, Used-for, target language)(deep neural networks, Evaluate-for, improving neural machine translation)(bi-directional LSTM, Used-for, encoding the source sentence)(convolutional layers, Part-of, neural machine translation model)(linear associative units, Part-of, neural machine translation model)(neural machine translation model, Part-of, encoder-decoder framework)(neural machine translation model, Hyponym-Of, sequence-to-sequence model)(syntactic information, Part-of, neural machine translation model)(recurrent neural networks, Is-a-Prerequisite-of, neural machine translation model)(attention-based neural machine translation, Part-of, neural machine translation model)(syntactic trees, Used-for, neural machine translation model)(error propagation,
(human attention keyphrase extraction, Hyponym-Of, keyphrase extraction)(keyphrase extraction, Used-for, understanding text content)(keyphrase extraction, Used-for, organizing text content)(keyphrase extraction, Used-for, retrieving text content)(automated keyphrase extraction, Hyponym-Of, keyphrase extraction)(human reading behavior, Part-of, human attention keyphrase extraction)(reading duration, Used-for, representing human attention)(eye-tracking corpus, Used-for, estimating reading duration)(neural network models, Part-of, human attention keyphrase extraction)(attention mechanism, Used-for, merging human attention with neural network models)(unsupervised models, Conjunction, neural network models)(encoder-decoder framework, Part-of, generative model)(generative model, Hyponym-Of, keyphrase extraction)(human attention, Part-of, human reading behavior)(keyphrase, Part-of, text content)
(Grammatical error correction (GEC), Compare, neural machine translation)(Grammatical error correction (GEC), Used-for, correcting errors)(nested attention layers, Used-for, GEC)(neural machine translation, Part-of, encode-attend-decode paradigm)(encode-attend-decode paradigm, Compare, phrase-based systems)(query-based summarization, Compare, abstractive summarization)(direct HMM, Used-for, rerank n-best list)(compound splitters, Used-for, statistical machine translation)(nested attention mechanism, Used-for, correcting local errors)(nested attention mechanism, Used-for, neural models for GEC)(neural machine translation, Part-of, machine translation)(machine translation, Hyponym-Of, statistical machine translation)(machine translation, Hyponym-Of, neural machine translation)(conventional concepts machine translation, Is-a-Prerequisite-of, neural models)(Machine translation, Part-of, sentence translation)(sentence
(Neural network models, Used-for, multi-task learning)(multi-task learning, Part-of, shared layers)(adversarial multi-task learning framework, Is-a-Prerequisite-of, alleviating shared and private latent feature spaces)(shared knowledge, Used-for, new tasks)(Labeled sequence transduction, Used-for, transforming one sequence into another sequence)(multi-space variational encoder-decoders, Used-for, labeled sequence transduction)(neural networks, Used-for, handling both discrete and continuous latent variables)(video captioning, Used-for, describing the content of a video)(sequence-to-sequence models, Used-for, video captioning)(many-to-many multi-task learning model, Part-of, video captioning)(reading comprehension, Evaluate-for, understanding natural texts)(Stanford Question Answering Dataset (SQuAD), Used-for, reading comprehension)(constituent-centric neural architecture, Part-of, reading comprehension)(constituent-centric neural architecture, Used-for, generation of candidate answers
(aspect-based sentiment analysis, Part-of, sentiment)(aspect-based sentiment analysis, Part-of, implicit aspects)(aspect-based sentiment analysis, Part-of, implicit opinions)(ACOS Quadruple Extraction, Part-of, aspect-based sentiment analysis)(ACOS Quadruple Extraction, Part-of, implicit aspects)(ACOS Quadruple Extraction, Part-of, implicit opinions)(ACOS Quadruple Extraction, Used-for, aspect-category-opinion-sentiment quadruples)(Restaurant-ACOS, Part-of, ACOS Quadruple Extraction)(Laptop-ACOS, Part-of, ACOS Quadruple Extraction)(Restaurant-ACOS, Compare, Laptop-ACOS)(SemEval Restaurant dataset, Is-a-Prerequisite-of, Restaurant-ACOS)(aspect, Conjunction, category)(category, Conjunction, opinion)(opinion, Conjunction, sentiment)(category opinion sentiment, Used-for, ACOS Quadruple Extraction)
(languages, Compare, bilingual word embeddings)(monolingual word embeddings, Compare, bilingual word embeddings)(bilingual word embeddings, Used-for, cross-lingual Word Sense Disambiguation)(bilingual word embeddings, Used-for, Machine Translation)(bilingual word embeddings, Evaluate-for, bilingual lexicon induction)(bilingual word embeddings, Evaluate-for, unsupervised learning techniques)(bilingual dictionaries, Hyponym-Of, bilingual word embeddings)(document-aligned corpora, Compare, bilingual corpora)(document-aligned corpora, Part-of, bilingual word embeddings)(self-learning approach, Part-of, bilingual word embeddings)(vocabulary alignment, Part-of, bilingual word embeddings)(embedding spaces, Part-of, bilingual word embeddings)(translation model, Used-for, bilingual word embeddings)(dictionary-based mapping technique, Used-for, bilingual word embeddings)(parallel corpora, Part-of, bilingual word embeddings)(bilingual lexicon, Part-of, bilingual word embeddings).
(training data, Compare, human-labeled data)(supervised methods, Used-for, knowledge base population)(key arguments, Part-of, event type)(trigger words, Part-of, event type)(key arguments, Used-for, label events)(trigger words, Used-for, label events)(event extraction, Used-for, knowledge base population)(fixed-vocabulary language models, Compare, character-level language models)(character-level language models, Compare, hierarchical LSTM language model with caching)(feature extraction, Part-of, statistical NLP)(feature extraction, Compare, preprocessing step)(feature extraction, Part-of, text chunking)(feature extraction, Part-of, relation extraction)(feature extraction, Used-for, restructuring feature templates)(self-attention, Part-of, Transformer)(response selection, Used-for, attention mechanism)(text geolocation, Used-for, geographic information)(geographic metadata, Part-of, text geolocation)(Map Vector, Used-for, geolocation)(relation schema induction, Part-of, knowledge base)(relation
(Textual Similarity, Compare, Semantic Textual Similarity)(Textual Similarity, Used-for, Plagiarism Detection)(Textual Similarity, Used-for, Information Ranking)(Textual Similarity, Used-for, Paraphrase Detection)(Textual Similarity, Used-for, Textual Entailment Recognition)(Semantic Textual Similarity, Used-for, Sentence Representation Learning)(Sentence Embedding, Evaluate-for, Semantic Textual Similarity)(Contrastive Learning, Used-for, Sentence Embedding)(BERT, Used-for, Sentence Embedding)(ConSERT, Used-for, Sentence Embedding)(DefSent, Used-for, Sentence Embedding)(NLI Datasets, Used-for, Sentence Embedding)(ArcCSE, Used-for, Sentence Embedding)(RCMD, Used-for, Sentence Embedding)(WhitenedCSE, Used-for, Sentence Embedding)(Cross-lingual Word Embeddings, Evaluate-for, Semantic
(Deep Neural Networks, Used-for, Neural Machine Translation)(Neural Machine Translation, Compare, Statistical Machine Translation)(Recurrent Neural Networks, Part-of, Neural Machine Translation)(LSTM unit, Compare, GRU)(LAUs, Compare, LSTM unit)(LAUs, Compare, GRU)(LAUs, Part-of, Recurrent Neural Networks)(Sequence-to-Dependency Neural Machine Translation, Is-a-Prerequisite-of, Neural Machine Translation)(Sequence-to-Dependency Neural Machine Translation, Used-for, word generation)(query attention model, Part-of, query-based summarization)(query-based summarization, Compare, extractive summarization)(designation-based summarization, Evaluate-for, text summarization quality)(Binary code prediction, Used-for, reducing computation time and memory)(Syntax-aware system, Compare, syntax-agnostic NMT)(Multi-domain NMT, Conjunction, fine tuning)(Neural Machine Translation, Part-of, Continuous-space sentence
(semantic role labeling srl, Part-of, natural language processing)(semantic role labeling srl, Evaluate-for, CoNLL 2005)(semantic role labeling srl, Evaluate-for, CoNLL 2012)(semantic role labeling srl, Used-for, dependency parsing)(semantic role labeling srl, Part-of, fact checking)(semantic role labeling srl, Evaluate-for, FEVER dataset)(semantic role labeling srl, Part-of, Question-Answer driven Semantic Role Labeling QA-SRL)(semantic role labeling srl, Used-for, syntax-aware methods)(semantic role labeling srl, Evaluate-for, CPB 1.0)(semantic role labeling srl, Evaluate-for, Universal Proposition Bank)(semantic role labeling srl, Used-for, cross-lingual model transferring)(deep highway BiLSTM, Used-for, semantic role labeling srl)(character-level models, Used-for, semantic role labeling srl
(chain thought prompting, Part-of, Pre-trained contextual representations)(chain thought prompting, Used-for, multi-step reasoning tasks)(few-shot chain-of-thought prompting, Hyponym-Of, chain thought prompting)(zero-shot-CoT, Hyponym-Of, chain thought prompting)(symbolic chain-of-thought distillation, Used-for, improving smaller models)(symbolic chain-of-thought distillation, Hyponym-Of, chain thought prompting)(SCOTT, Part-of, chain thought prompting)(SCOTT, Is-a-Prerequisite-of, faithful small models)(Verify-and-Edit framework, Used-for, chain thought prompting)(backward chaining algorithm, Hyponym-Of, chain thought prompting)(MultiTool-CoT, Used-for, incorporating multiple external tools)(MultiTool-CoT, Hyponym-Of, chain thought prompting).
(unsupervised constituency parsing, Compare, supervised models)(unsupervised constituency parsing, Evaluate-for, accuracy)(unsupervised constituency parsing, Evaluate-for, experimental settings)(unsupervised constituency parsing, Evaluate-for, language branching tendencies)(unsupervised constituency parsing, Part-of, natural language processing)(unsupervised constituency parsing, Compare, statistical methods)(unsupervised constituency parsing, Compare, neural methods)(conventional methods, Compare, generative neural dependency model)(generative neural dependency model, Is-a-Prerequisite-of, continuous latent representation)(continuous latent representation, Part-of, global contextual information)(dynamic oracles, Compare, policy gradient method)(policy gradient method, Evaluate-for, tree-level metric)(syntactic structures, Compare, tree structures in PTB)(sequence-to-sequence models, Evaluate-for, constituency parsing)(statistical methods, Compare, neural methods).
(Generating answer with natural language sentence, Part-of, question answering systems)(COREQA, Used-for, domain question answering qa)(sequence-to-sequence learning, Used-for, domain question answering qa)(copying mechanism, Part-of, COREQA)(retrieving mechanism, Part-of, COREQA)(encoder-decoder framework, Used-for, domain question answering qa)(question representation, Used-for, domain question answering qa)(neural network model, Used-for, domain question answering qa)(cross-attention mechanism, Part-of, neural network model)(semantic parsing, Used-for, domain question answering qa)(dynamic neural semantic parsing framework, Used-for, conversational QA)(open-domain question answering, Hyponym-Of, domain question answering qa)(machine reading, Used-for, domain question answering qa)(factoid question answering, Hyponym-Of, domain question answering qa)(distantly supervised open-domain question answering, Hyponym-Of, domain question answering qa)(derived word generation
(cross lingual transfer, Used-for, multilingual embeddings)(cross lingual transfer, Used-for, cross-lingual word embedding technique)(cross lingual transfer, Used-for, multilingual text classification)(cross lingual transfer, Used-for, cross-lingual sentence similarity)(cross lingual transfer, Used-for, machine translation)(cross lingual transfer, Used-for, cross-lingual coreference resolution)(cross lingual transfer, Used-for, cross-lingual parsing)(cross lingual transfer, Used-for, bilingual lexicon induction)(cross lingual transfer, Used-for, cross-lingual classification)(parallel corpora, Part-of, cross lingual transfer)(unsupervised machine translation, Is-a-Prerequisite-of, cross lingual transfer)(bilingual tasks, Is-a-Prerequisite-of, cross lingual transfer)(domain adaptation, Is-a-Prerequisite-of, cross lingual transfer)(neural encoder-decoder model
(word embeddings, Is-a-Prerequisite-of, sentiment analysis)(machine translation, Compare, bilingual word embeddings)(Bilingual Sentiment Embeddings, Used-for, cross-lingual sentiment classification)(high-performing models, Part-of, sentiment analysis)(parallel data, Used-for, machine translation)(rank-based measures, Compare, vector cosine)(SynGCN, Hyponym-Of, Graph Convolution method)(SynGCN, Used-for, learning word embeddings)(SemGCN, Used-for, incorporating diverse semantic knowledge)(manifold based geometric approach, Used-for, unsupervised alignment of word embeddings)(contextual embeddings, Compare, traditional word embeddings)(sense-level embeddings, Compare, word embeddings)(sense representation, Evaluate-for, word change detection)(faithfulness property, Part-of, contextual embeddings)(faithful embeddings, Used-for, predicting causal links)(dynamic contextualized word embeddings, Hyponym-Of, contextual embeddings)(dynamic contextualized word embeddings, Used-for, capturing semantic variability)(
(summary generation, Used-for, natural language generation)(natural language understanding, Conjunction, natural language generation)(natural language generation, Used-for, text summarization)(Scratchpad Mechanism, Used-for, summary generation)(hierarchical LSTM model, Used-for, summary generation)(tracking module, Part-of, summary generation)(text generation module, Part-of, summary generation)(content structure, Used-for, summary generation)(structured convolutional decoder, Used-for, summary generation)(automatic evaluation metrics, Evaluate-for, summary generation).
(text generation, Hyponym-Of, natural language processing)(text generation, Used-for, document summarization)(text generation, Used-for, question generation)(text generation, Used-for, poetry generation)(text generation, Used-for, pun generation)(text generation, Used-for, derived word generation)(text style, Part-of, text generation)(homographic puns, Hyponym-Of, text generation)(derived word generation, Hyponym-Of, text generation)(document summarization, Is-a-Prerequisite-of, abstractive summarization)(document summarization, Is-a-Prerequisite-of, extractive summarization)(sequence-to-sequence models, Used-for, text generation)(graph-based attention mechanism, Used-for, neural networks)(conditional neural language model, Used-for, pun generation)(neural graph-to-sequence model, Used-for, AMR-to-text generation)(hierarchical attention, Used-for, data-to-text generation)(variational autoencoders, Used-for,
(relation extraction, Compare, named entity recognition)(relation extraction, Part-of, natural language processing)(named entity recognition, Part-of, natural language processing)(relation extraction, Used-for, identifying entity mention spans in raw text)(named entity recognition, Used-for, mention detection)(named entity recognition, Used-for, entity recognition)(relation extraction, Evaluate-for, performance)(named entity recognition, Evaluate-for, performance)(cross-lingual attention, Part-of, multi-lingual neural relation extraction framework)(mono-lingual attention, Part-of, multi-lingual neural relation extraction framework)(local detection, Compare, sequence labeling)(fixed-size ordinally forgetting encoding, Used-for, encoding sentence fragments)(state-of-the-art, Compare, existing methods)(temporal relation classification, Part-of, natural language processing)(bidirectional long short-term memory, Used-for, relation extraction)(class ties, Part-of, joint relation extraction)(convolutional neural network, Used-for, joint relation extraction)(feedforward
(dialog datasets, Used-for, training task-oriented dialog systems)(dialog datasets, Used-for, training end-to-end task-oriented dialog systems)(SQuAD, Hyponym-Of, dialog datasets)(DuoRC, Hyponym-Of, dialog datasets)(SQuADRUn, Hyponym-Of, dialog datasets)(SQuADRUn, Used-for, testing natural language understanding tasks)(Mem2Seq, Used-for, training task-oriented dialog systems)(KB-InfoBot, Used-for, training task-oriented dialog systems).
(Visually grounded language, Part-of, Language-vision tasks)(Visually grounded language, Used-for, Communicate with and learn from humans)(Visually grounded language, Used-for, Novel knowledge acquisition)(Visually grounded language, Used-for, Multi-hop reasoning)(Visually grounded language, Used-for, Navigation)(Visually grounded language, Evaluate-for, Effectiveness in Vision and Language Navigation (VLN))(Visually grounded language, Evaluate-for, Performance on vision-and-language tasks)(Joint imitation and reinforcement approach, Used-for, Grounded language learning)(Visual features, Used-for, Learning bootstrap textual models)(Concreteness of constituents, Used-for, Guide parsing of text)(Pre-trained visually grounded language models, Used-for, Vision-and-language tasks)(Knowledge-based visual question answering, Used-for, Multi-hop reasoning)(Hypergraph Transformer, Used-for, Knowledge-based visual QA)(Visual reasoning language dataset, Part-of, Vis
(unintended bias text, Part-of, text classification datasets)(text classification datasets, Evaluate-for, demographic identity-terms)(unintended bias text, Compare, traditional factual inference)(text classifiers, Used-for, text classification)(counterfactual inference, Used-for, mitigating biases)(counterfactual inference, Compare, traditional factual inference)(Corsair, Is-a-Prerequisite-of, unbiased decisions)(demographic identity-terms, Part-of, text classification)(abusive language detection, Evaluate-for, demographic identity-terms)(text classifiers, Part-of, machine learning models)(text classifiers, Compare, models trained with biased datasets)(factual input document, Compare, counterfactual counterparts)(counterfactual counterparts, Compare, factual input document)(Corsair, Used-for, text classification debiasing)(text classifiers, Conjunction, text classification datasets)(text classification debiasing framework, Used-for, mitigating biases)(text classification debiasing framework, Evaluate-for, unbiased decisions
(continual relation extraction, Part-of, relation extraction)(continual relation extraction, Compare, multi-lingual neural relation extraction framework)(multi-lingual neural relation extraction framework, Used-for, relation extraction)(distant supervision, Part-of, relation extraction)(class ties, Part-of, relation extraction)(entity tuple, Evaluate-for, class ties)(CNN, Used-for, joint relation extraction)(Bio tag embeddings, Used-for, relation identification)(multi-task architecture, Part-of, joint relation extraction)(PKB, Used-for, relation extraction)(feature extraction, Used-for, relation extraction)(Bi-LSTM, Used-for, relation extraction)(DSGAN, Used-for, filtering distant supervision training dataset)(GP-GNNs, Used-for, relation extraction)(deep learning architecture, Used-for, inferring missing entities)(GraphRel, Used-for, relation extraction)(DIAG-NRE, Used-for, summarizing and refining relational patterns).
(neural semantic, Used-for, semantic parsing)(neural semantic parsing, Hyponym-Of, semantic parsing)(neural semantic, Used-for, converting natural language utterances to intermediate representations)(neural semantic parser, Used-for, annotating logical forms)(transition system, Part-of, neural semantic parser)(predicate-argument structures, Used-for, semantic parsing)(predicate-argument structures, Part-of, intermediate representations)(neural semantic parser, Used-for, mapping to target domains)(neural semantic parser, Conjunction, AMR parser)(AMR graphs, Used-for, representing semantic content)(recurrent neural networks, Part-of, AMR parser)(recurrent neural networks, Used-for, inferring AMR graphs)(natural language generation system, Used-for, producing referring expressions)(WRL, Used-for, word representation learning)(seq2seq models, Hyponym-Of, neural sequence-to-sequence models)(semantic parser, Used-for, human-robot
(generated summary, Part-of, summarization)(generated summary, Evaluate-for, coherence)(generated summary, Evaluate-for, informativeness)(abstractive summarization, Part-of, summarization)(query-based summarization, Part-of, summarization)(encode-attend-decode paradigm, Used-for, machine translation)(encode-attend-decode paradigm, Used-for, extractive summarization)(encode-attend-decode paradigm, Used-for, dialog systems)(query attention model, Part-of, encode-attend-decode paradigm)(diversity based attention model, Part-of, encode-attend-decode paradigm)(Chinese poem generation, Used-for, creativity)(memory augmented neural model, Part-of, Chinese poem generation)(market comments, Evaluate-for, fluency)(market comments, Evaluate-for, informativeness)(oracle summary, Part-of, compressive summarization)(SWAP-NET, Part-of, extractive summarization)(soft templates, Used
(document-level event argument extraction, Hyponym-Of, event argument extraction)(document-level event argument extraction, Hyponym-Of, document-level event extraction)(document-level event argument extraction, Used-for, recognizing event arguments across sentences)(document-level event argument extraction, Part-of, document-level multi-event extraction)(document-level event argument extraction, Used-for, capturing long-range dependencies)(document-level event argument extraction, Conjunction, sentence-level event argument extraction)(document-level event argument extraction, Evaluate-for, effectiveness in identifying arguments over multiple sentences)(document-level event argument extraction, Evaluate-for, relationship between context length and model performance)
(aspect based sentiment analysis, Part-of, aspect extraction)(aspect based sentiment analysis, Part-of, aspect sentiment classification)(aspect extraction, Used-for, detecting aspect boundaries)(aspect extraction, Used-for, identifying aspects)(aspect sentiment classification, Used-for, determining sentiment polarity)(Gated Tanh-ReLU Units, Used-for, selectively outputting sentiment features)(convolutional neural networks, Compare, long short-term memory)(word embeddings, Used-for, vector space representations of words)(neural attention mechanism, Used-for, de-emphasizing irrelevant words)(Cold-Start Aware Attention, Used-for, pooling encoded word vectors)(IMN, Used-for, learning multiple related tasks)(distributed word representations, Used-for, Korean linguistic structure analysis)(dependency parsing, Used-for, syntactic analysis)(convolutional layers, Compare, LSTM layers).
(neural dialogue generation, Is-a-Prerequisite-of, neural knowledge diffusion)(neural knowledge diffusion, Used-for, introduce knowledge into dialogue generation)(facts matching, Part-of, neural knowledge diffusion)(entity diffusion, Part-of, neural knowledge diffusion)(neural dialogue generation, Evaluate-for, generating meaningful responses)(neural dialogue generation, Evaluate-for, generating diverse responses)(neural dialogue generation, Evaluate-for, generating natural responses)(TextFlow, Compare, n-grams overlap)(TextFlow, Compare, skip-grams overlap)(sequential models, Part-of, deep learning)(text similarity measures, Used-for, plagiarism detection)(text similarity measures, Used-for, information ranking)(text similarity measures, Used-for, recognition of paraphrases)(text similarity measures, Used-for, textual entailment)(DNA sequence alignment, Is-a-Prerequisite-of, TextFlow)(aspect-aware coarse-to-fine generation process, Used-for, review generation)(aspect-aware decoder, Part-of, aspect
(entity relation extraction, Used-for, extracting entities and their relationships)(entity relation extraction, Evaluate-for, improving model performance)(feature extraction, Is-a-Prerequisite-of, entity relation extraction)(dynamic transition matrix, Used-for, characterizing noise in training data)(temporal relation classification, Compare, entity relation extraction)(reducing redundant computation, Evaluate-for, feature extraction)(class ties, Used-for, leveraging relations in distantly supervised data)(convolutional neural network, Used-for, joint relation extraction)(bidirectional long short-term memory, Used-for, relation extraction)(Open Information Extraction, Used-for, generating semi-structured knowledge)(Knowledge Base Population, Used-for, extracting relations and mapping to a KB)(Pocket Knowledge Base, Is-a-Prerequisite-of, dynamically constructing a KB)(co-reference resolution, Part-of, distant supervision)(deep learning architecture, Used-for, learning thousands of relations)(HITS algorithm, Used-for, ranking relation instances and patterns)
(linguistically diverse conversational corpus, Part-of, computational linguistics)(linguistically diverse conversational corpus, Part-of, language technology)(language documentation movement, Used-for, linguistically diverse conversational corpus)(linguistically diverse conversational corpus, Used-for, studying turn-taking)(linguistically diverse conversational corpus, Used-for, studying timing)(linguistically diverse conversational corpus, Used-for, studying sequential structure)(linguistically diverse conversational corpus, Used-for, studying social action)(computational framework, Used-for, studying conversational behavior)(linguistically diverse conversational corpus, Hyponym-Of, conversational corpora)(paraphrase generation, Used-for, creating semantically similar but diverse sentences)(paraphrase generation, Evaluate-for, maintaining original meaning)(paraphrase generation, Evaluate-for, achieving linguistic diversity)(QCPG, Compare, traditional paraphrase generation methods)(QCPG, Used-for, controlling quality dimensions of paraphrases)(flexible language
(political debate 50 year, Part-of, democratic political decision making)(political debate 50 year, Used-for, compare the candidates’ positions)(political debate 50 year, Used-for, Argument Mining)(political debate 50 year, Used-for, creating a new corpus of 29k argument components)(Argument Mining, Part-of, typology of argument components)(Argument Mining, Part-of, political debates)(argument components, Hyponym-Of, premises)(argument components, Hyponym-Of, claims)(SVM learners, Compare, Neural Network architectures)(SVM learners, Evaluate-for, Argument Mining)(Neural Network architectures, Evaluate-for, Argument Mining)(USElecDeb60To16, Conjunction, accompanying software)
(natural language explanation prediction, Used-for, generating natural language explanations)(natural language, Part-of, natural language explanation prediction)(training classifiers, Used-for, natural language explanation prediction)(end-to-end system, Used-for, natural language explanation prediction)(neural models, Evaluate-for, generating natural language explanations)(conversational QA, Is-a-Prerequisite-of, natural language explanation prediction)(neural sequence generative models, Used-for, text infilling tasks)(sentence embeddings, Evaluate-for, sentence relation tasks)(dependency parsing, Used-for, learning sentence embeddings)(TextFlow, Used-for, text similarity)(sequence-to-sequence model, Used-for, generating intermediate forms)(iterative labeling framework, Used-for, learning intermediate forms)(aspect-aware decoder, Is-a-Prerequisite-of, review generation model)(SPADES, Evaluate-for, neural semantic parser)(GEOQUERY, Evaluate-for, neural semantic parser)(intermediate meaning representation scheme, Used-for, solving math word problems
(text generation model, Part-of, question answering models)(Generative Domain-Adaptive Nets, Used-for, text generation model)(Generative Domain-Adaptive Nets, Is-a-Prerequisite-of, question answering models)(abstract syntax networks, Part-of, text generation model)(abstract syntax networks, Part-of, syntax parsing)(abstract syntax networks, Evaluate-for, code generation)(Generative Domain-Adaptive Nets, Hyponym-Of, domain adaptation algorithms)(domain adaptation algorithms, Used-for, question answering models)(sequence-to-sequence framework, Used-for, text generation model)(graph-based attention mechanism, Part-of, sequence-to-sequence framework)(sentence summarization, Part-of, text generation model)(document summarization, Part-of, text generation model)(neural models, Part-of, text generation model)(error detection, Used-for, text generation model)(synchronous node replacement grammar, Used-for, text generation model)(graph transducer, Used-for, text generation model)(neural knowledge diffusion
(response generation, Used-for, multi-turn conversation)(response generation, Used-for, response selection)(response generation, Hyponym-Of, dialog response generation)(dialog response generation, Part-of, encoder-decoder dialog model)(deep latent variable models, Used-for, response generation)(conditionally generated responses, Is-a-Prerequisite-of, response generation)(Seq2Seq models, Used-for, response generation)(matching model, Used-for, response selection)(Spatio-Temporal Matching network, Used-for, response selection)(iterative training process, Used-for, response generation method)(unsupervised discrete sentence representation, Used-for, interpretable response generation)(sentence function, Used-for, response generation).
(trained language model, Used-for, poetry generation)(trained language model, Used-for, conversational text generation)(trained language model, Part-of, LSTM Noisy Channel Model)(trained language model, Part-of, hierarchical LSTM language model)(trained language model, Part-of, Universal Language Model Fine-tuning (ULMFiT))(trained language model, Part-of, RNNGs)(LSTM language model, Used-for, conversational text generation)(LSTM language model, Used-for, disfluency detection)(Multilingual Wikipedia Corpus, Used-for, language modeling)(Affect-LM, Evaluate-for, emotional content generation)(emotional content, Used-for, conversational text)(neural language model, Used-for, poetry generation)(neural language model, Used-for, conversational text)(neural language model, Used-for, language modeling)(word segments, Hyponym-Of, subword units)(characters, Hyponym-Of, subword
(binomial neural topic model, Compare, traditional topic models)(binomial neural topic model, Part-of, neural topic models)(binomial neural topic model, Evaluate-for, automatic topic extraction)(neural topic models, Compare, traditional topic models)(neural topic models, Is-a-Prerequisite-of, binomial neural topic model)(neural topic models, Used-for, automatic topic extraction)(neural topic models, Hyponym-Of, deep learning models)(deep learning models, Used-for, automatic readability assessment)(deep learning models, Evaluate-for, automatic readability assessment)(deep learning models, Compare, traditional machine learning models)
(question answering model, Used-for, reading comprehension)(question answering model, Used-for, question representation)(question answering model, Used-for, relation detection)(question answering model, Used-for, semi-supervised learning)(question answering model, Used-for, answer selection)(question answering model, Part-of, end-to-end system)(question answering model, Compare, human performance)(question answering model, Evaluate-for, exact match metrics)(question answering model, Is-a-Prerequisite-of, state-of-the-art performance)(question answering model, Used-for, generating natural answers)(question answering model, Used-for, parsing for question answering)(question answering model, Used-for, open-domain question answering)(question answering model, Used-for, reinforcement learning)(question answering model, Used-for, transfer learning)(question answering model, Used-for, Universal schema)(reading comprehension, Evaluate-for, SQuAD dataset)(relation detection, Part-of, Knowledge Base Question Answering)(open-domain question answering, Used-for, Wikipedia
(pretrained multilingual, Used-for, Named Entity Recognition)(pretrained multilingual, Used-for, Part-of-Speech Tagging)(pretrained multilingual, Is-a-Prerequisite-of, Neural language representation models)(multilingual BERT, Compare, non-contextual subword embeddings)(multilingual neural machine translation, Evaluate-for, translation quality)(Zeroshot translation, Compare, pivot-based approach)(ERNIE, Compare, BERT)(ERNIE, Used-for, Knowledge-driven tasks)(multilingual neural machine translation, Conjunction, language-sensitive method)(multi-lingual neural relation extraction framework, Part-of, relation extraction)(Language Identification, Is-a-Prerequisite-of, multilingual text processing)(cross-lingual word embeddings, Used-for, multilingual natural language processing)(multilingual word representations, Used-for, multilingual embeddings)(multilingual connotation frames, Used-for, sentiment analysis)(multilingual multi-task architecture, Used-for, sequence labeling)(Bilingual Lexicon Induction with
(translation quality, Evaluate-for, bi-directional LSTMs)(translation quality, Evaluate-for, convolutional layers)(translation quality, Evaluate-for, word reordering knowledge)(translation quality, Evaluate-for, chunk-based decoders)(translation quality, Evaluate-for, Multi-modal Neural Machine Translation)(translation quality, Evaluate-for, empirical adaptation results)(translation quality, Evaluate-for, low-resource languages)(translation quality, Evaluate-for, novel data augmentation approach)(translation quality, Evaluate-for, triangular training architecture)(translation quality, Evaluate-for, sentiment-to-sentiment translation)(translation quality, Evaluate-for, deep generative model)(translation quality, Evaluate-for, adversarial stability training)(translation quality, Evaluate-for, word translation via images)(translation quality, Evaluate-for, unsupervised pivot translation)(neural machine translation, Part-of, translation quality)(bi-directional LSTMs, Compare, convolutional layers)(neural machine translation, Is-a-Prerequisite-of, state-of-the-art translation)(word
(relation extraction, Used-for, relation prediction)(multi-lingual neural relation extraction framework, Used-for, relation prediction)(relation detection, Part-of, NLP applications)(KBQA, Part-of, NLP applications)(deep residual bidirectional LSTMs, Used-for, relation prediction)(relation detector, Used-for, KBQA system)(entity linking, Conjunction, relation detector)(unary relations, Used-for, relation prediction)(distant supervision, Hyponym-Of, relation extraction)(deep reinforcement learning strategy, Used-for, false-positive indicator)(triplet network, Used-for, thematic similarity prediction)(n-gram based attention model, Used-for, multi-word entity names capture)(triples extraction, Part-of, relation prediction).
(event argument extraction, Evaluate-for, RAMS dataset)(event argument extraction, Used-for, implicit arguments)(event argument extraction, Is-a-Prerequisite-of, information extraction)(event argument extraction, Part-of, document-level event extraction)(Siamese neural network, Evaluate-for, convincingness data set)(PRGC, Used-for, joint relational triple extraction)(FewDocAE, Part-of, Few-Shot Document-Level Event Argument Extraction)(BERE, Used-for, event relation extraction)(GNEVA, Evaluate-for, EAE models)(Multi-tier Knowledge Projection Network, Hyponym-Of, event relation extraction)(sentence-level extractors, Compare, document-level event extractors)(Frame-aware Event Argument Extraction, Used-for, reasoning in event frame-level scope)(PAIE, Used-for, prompt tuning for extractive objectives)(PAIE, Evaluate-for, sentence-level EAE)(PAIE, Evaluate-for, document-level EAE)(Seq2Seq-like learning, Used-for, event argument
(knowledge base, Part-of, natural language processing)(natural language processing, Used-for, reading comprehension)(reading comprehension, Hyponym-Of, comprehension)(reasoning, Hyponym-Of, intelligence)(ITransF, Used-for, knowledge base completion)(distributional vector spaces, Used-for, word representations)(morph-fitting procedure, Used-for, improving distributional vector spaces)(paraphrase generation, Used-for, expanding natural language datasets)(neural models, Is-a-Prerequisite-of, semantic-driven approach)(morph-fitting procedure, Part-of, distributional vector spaces)(attention mechanism, Conjunction, word embeddings)(attention-over-attention reader, Hyponym-Of, reading comprehension)(Clozure, Hyponym-Of, reading comprehension)(semantic-driven approach, Is-a-Prerequisite-of, neural models)(sentence compression, Part-of, language generation)(syntactic parsing, Conjunction, recursive architectures)(morph-fitting procedure, Conjunction, distributional
(neural semantic parser, Used-for, converting natural language utterances to intermediate representations)(natural language utterances, Part-of, natural language explanation)(answer rationales, Part-of, natural language explanation)(question answering systems, Used-for, generating natural language explanations)(COREQA, Used-for, generating natural answers)(natural language generation, Hyponym-Of, natural language explanation)(Recurrent Neural Networks, Used-for, reading text)(confidence, Evaluate-for, natural language explanation)
(recurrent neural networks, Part-of, recurrent neural tensor network)(recurrent neural networks, Used-for, reading comprehension)(recurrent neural networks, Used-for, knowledge base question answering)(recurrent neural networks, Used-for, sentence parsing)(recurrent neural networks, Used-for, dialog systems)(recurrent neural networks, Used-for, entity relation extraction)(recurrent neural networks, Used-for, machine translation)(recurrent neural networks, Used-for, sequence-to-sequence learning)(recurrent neural networks, Used-for, sentence compression)(recurrent neural networks, Used-for, dialog act classification).
(machine translation paper, Used-for, neural machine translation bibliography)(neural machine translation, Part-of, neural machine translation paper)(bi-directional LSTMs, Compare, convolutional layers)(source language, Part-of, translation product)(Deep Neural Networks, Part-of, Neural Machine Translation)(LSTM unit, Compare, LAUs)(recurrent networks, Compare, convolutional layers)(source sentence, Part-of, neural machine translation)(state-of-the-art, Evaluate-for, English-Romanian translation)(neural machine translation, Used-for, word-based representations)(word-based representations, Part-of, neural machine translation)(neural machine translation, Evaluate-for, BLEU score)(syntactic information, Part-of, source syntax)(source syntax, Used-for, neural machine translation)(translations, Compare, original text)(jumping LSTM, Compare, standard sequential LSTM).
(crossmodal attention, Part-of, multimodal pre-training approaches)(crossmodal attention, Part-of, image-text retrieval)(crossmodal attention, Used-for, bridging semantic gap)(crossmodal attention, Is-a-Prerequisite-of, word-region alignment)(crossmodal attention, Used-for, inter-modal alignment)(crossmodal attention, Used-for, cross-modal retrieval).
(contrastive visual semantic, Part-of, CLIP)(contrastive visual semantic, Used-for, visual semantic pretraining)(contrastive visual semantic, Used-for, word embeddings)(word embeddings, Compare, word2vec skip-grams)(word embeddings, Compare, Gaussian embeddings)(word embeddings, Used-for, representations of words)(multimodal word distributions, Used-for, multiple word meanings)(multimodal word distributions, Used-for, entailment)(multimodal word distributions, Used-for, rich uncertainty information)(energy-based max-margin objective, Used-for, learning distributions)(CLIP, Is-a-Prerequisite-of, contrastive visual semantic)(GPT-2, Compare, CLIP)(CLIP word embeddings, Compare, GPT-2)(CLIP word embeddings, Used-for, word-level semantic intrinsic evaluation tasks)(CLIP word embeddings, Evaluate-for, RG65 evaluation)(CLIP, Used-for, fine-grained semantic representations of sentences)(interactive semantic parsing, Hyponym-
(Content, Used-for, text classification task)(multi-task learning, Used-for, text classification task)(BiLSTM, Used-for, text classification task)(CNN, Evaluate-for, text classification task)(argument mining, Evaluate-for, text classification task)(cross-lingual text classification, Hyponym-Of, text classification task)(question classification, Hyponym-Of, text classification task)(Neural network models, Evaluate-for, text classification task)(implicit discourse relation classification, Evaluate-for, text classification task)(automatic speech recognition, Evaluate-for, text classification task)(copula-based topic model, Evaluate-for, text classification task)(character embeddings, Evaluate-for, text classification task).
(neural language model, Used-for, text generation)(neural language model, Used-for, poetry generation)(neural language model, Used-for, conversational text generation)(neural language model, Evaluate-for, language model perplexity)(neural language model, Compare, sentence-based model)(neural language model, Part-of, automatic generation of rhythmic poetry)(neural language model, Part-of, question answering system)(transformer model, Hyponym-Of, neural language model)(RNN, Hyponym-Of, neural language model)(LSTM, Hyponym-Of, RNN)(COREQA, Part-of, question answering system)(COREQA, Used-for, copying and retrieving mechanisms)(affect, Part-of, conversational text generation)(affect-discriminative word representations, Used-for, Affect-LM)(topic model-like architecture, Used-for, document context)(document context, Part-of, neural language model)(document context, Evaluate-for, language model performance)(
(biomedical entity linking, Hyponym-Of, entity linking)(biomedical entity linking, Is-a-Prerequisite-of, cross-lingual biomedical entity linking)(cross-lingual biomedical entity linking, Used-for, resource-poor languages)(knowledge-enhanced multilingual LMs, Used-for, cross-lingual biomedical entity linking)(domain-specific transfer methods, Used-for, cross-lingual biomedical entity linking)(domain-specific transfer methods, Evaluate-for, transferring domain-specific knowledge)(pretrained language models, Used-for, biomedical entity linking)(cross-lingual transfer methods, Evaluate-for, cross-lingual biomedical entity linking)(biomedical entity linking, Evaluate-for, capability to handle specialised in-domain tasks)(domain-specific transfer methods, Hyponym-Of, cross-lingual transfer methods)(expert knowledge, Used-for, biomedical entity linking)(resource-rich languages, Compare, resource-poor languages)(knowledge-enhanced multilingual LMs, Compare, knowledge
(document level event extraction, Part-of, information extraction)(document level event extraction, Is-a-Prerequisite-of, event argument extraction)(document level event extraction, Compare, sentence level extraction)(document level event extraction, Part-of, end-to-end model)(document level event extraction, Evaluate-for, event argument extraction)(document level event extraction, Evaluate-for, document level relation extraction)(document level event extraction, Evaluate-for, RE models)(document level event extraction, Evaluate-for, IE models)(document level event extraction, Evaluate-for, NMT)(document level event extraction, Used-for, event argument extraction)(document level event extraction, Used-for, inter-sentence relations)(event extraction, Hyponym-Of, information extraction)(event argument extraction, Used-for, recognizing event arguments)(information extraction, Used-for, creating datasets)(end-to-end model, Hyponym-Of, neural sequence models)(document level relation extraction, Compare, sentence RE)(document level relation extraction, Evaluate-for, heuristic
(sarcasm detection, Conjunction, sentiment polarity)(sarcasm detection, Conjunction, sentiment classification)(sentiment polarity, Hyponym-Of, sentiment classification)(convolutional neural network, Used-for, sentiment polarity)(text classification, Part-of, natural language processing)(domain adaptation, Used-for, sentiment classification)(sentiment lexicons, Used-for, sentiment classification)(negation words, Used-for, sentiment classification)(intensity words, Used-for, sentiment classification)(sentence-level annotation, Is-a-Prerequisite-of, sentiment classification)(sentiment classifier, Part-of, sentiment classification)(recurrent neural networks, Used-for, sentiment classification)(domain dependence, Used-for, sentiment classification)(semi-supervised learning, Used-for, sentiment classification)(bilingual tasks, Used-for, sentiment classification)(cross-lingual classification, Conjunction, sentiment classification)(picturebook, Used-for, sentiment classification)(target-oriented sentiment classification, Hyponym-Of,
(latent topic, Compare, non-latent topic)(latent topic, Hyponym-Of, latent variable model)(latent topic, Part-of, LDA-based model)(latent topic, Part-of, latent document-topic vector)(latent topic, Used-for, text segmentation)(latent topic, Used-for, summarization)(latent topic, Used-for, multilingual embeddings)(latent topic, Evaluate-for, topic coherence)(latent topic, Used-for, user preferences mapping)(user preference, Evaluate-for, latent topic)(latent variable, Part-of, latent topic)(topic distribution, Part-of, latent topic)(neural topic model, Evaluate-for, latent topic)(latent topic, Used-for, stance detection).
(None)
(generate sentence, Used-for, natural language response)(generate sentence, Evaluate-for, question answering systems)(question answering systems, Compare, dialog systems)(COREQA, Used-for, generate sentence)(encoder-decoder framework, Part-of, COREQA)(dependency graph, Used-for, semantic dependency parsing)(semantic dependency parsing, Compare, abstractive summarization)(selective encoding model, Evaluate-for, sentence summarization)(sentence encoder, Part-of, selective encoding model)(attention equipped decoder, Part-of, selective encoding model)(Noisy Channel Model, Compare, LSTM language model)(distributed word representations, Compare, topic-sensitive representations)(parallel sentences, Part-of, parallel corpora)(generative models, Compare, discriminative models)(deep network, Hyponym-Of, neural models)(query-based summarization, Compare, query classification)(ROUGE-L scores, Evaluate-for, summarization performance).
(compositional distributional semantics, Used-for, semantic composition)(compositional distributional semantics, Used-for, predicting compositionality)(compositional distributional semantics, Used-for, evaluating multiword expressions)(compositional distributional semantics, Evaluate-for, Polish evaluation dataset)(compositional distributional semantics, Compare, traditional approaches in an unsupervised setting)(compositional distributional semantics, Part-of, distributional semantic models)(distributional semantic models, Part-of, distributional semantics)(Poincaré embeddings, Used-for, predicting compositionality)(Poincaré embeddings, Used-for, taxonomy induction)(APTs, Used-for, grammatical type consideration)(APTs, Is-a-Prerequisite-of, distributional inference)(multi-task learning, Used-for, improving accuracy of neural semantic parser)(BERT embeddings, Used-for, improving accuracy of semantic parser)(SentiBERT, Used-for, capturing compositional sentiment semantics)(Pixie Autoencoder
(named entity, Part-of, natural language processing)(named entity, Used-for, named entity recognition)(named entity, Part-of, metonymy resolution)(named entity, Part-of, lexical resources)(named entity, Used-for, geographical parsing)(lexical resources, Used-for, named entity recognition)(lexical resources, Used-for, part-of-speech induction)(named entity recognition, Part-of, natural language processing)(named entity recognition, Hyponym-Of, named entity)(named entity recognition, Used-for, entity linking)(named entity recognition, Used-for, metonymy resolution)(geographical parsing, Used-for, named entity recognition)(geographical parsing, Hyponym-Of, named entity)(entity linking, Used-for, multimodal named entity disambiguation)(entity linking, Used-for, knowledge base)(lexical resources, Hyponym-Of, dictionaries)(lexical resources, Hyponym-Of, gazetteers)(dictionaries
(task oriented dialogue, Compare, pipeline designs)(task oriented dialogue, Evaluate-for, task success prediction)(task oriented dialogue, Hyponym-Of, goal-oriented dialogue agents)(task oriented dialogue, Evaluate-for, disease identification)(task oriented dialogue, Part-of, task-oriented dialogue system framework)(task oriented dialogue, Part-of, two separated long-term memories)(task oriented dialogue, Part-of, task-oriented dialogue datasets)(task oriented dialogue, Used-for, document knowledge)(NLIs, Used-for, consistency of dialogue agents)(retrieval-based models, Compare, task-oriented dialogue systems)(retrieval-based models, Evaluate-for, response selection)(model parameters, Evaluate-for, dialogue ontology)(model parameters, Used-for, tracking of rare states)(model parameters, Part-of, end-to-end models)(seq2seq model, Part-of, end-to-end model)(seq2seq model, Used-for, tracking dialogue believes)(Global-Locally Self-Attentive Dialogue State Tracker, Evaluate
 (spoken dialogue system, Part-of, intelligent assistant)(spoken dialogue system, Hyponym-Of, task-oriented dialogue system)(spoken dialogue system, Hyponym-Of, non-task-oriented dialogue system)(spoken dialogue system, Used-for, chat detection)(spoken dialogue system, Used-for, disease diagnosis)(spoken dialogue system, Part-of, end-to-end learning framework)(spoken dialogue system, Part-of, dialogue state tracking)(spoken dialogue system, Part-of, adverbial presupposition trigger prediction)(spoken dialogue system, Part-of, user sentiment analysis)(spoken dialogue system, Part-of, discourse relation identification)(spoken dialogue system, Part-of, belief tracker)(spoken dialogue system, Used-for, handling unknown slot values)(Neural Belief Tracking, Part-of, belief tracker)(belief tracker, Part-of, spoken dialogue system)(task-oriented dialogue system, Hyponym-Of, spoken dialogue system)(non-task-oriented dialogue system, Hyponym-Of, spoken dialogue system)(open
(document modeling, Compare, word modeling)(document modeling, Used-for, sentence extraction)(document modeling, Used-for, event detection)(document modeling, Part-of, document summarization)(document modeling, Evaluate-for, text classification)(document modeling, Evaluate-for, document retrieval)(document summarization, Hyponym-Of, document modeling)(document embedding, Part-of, document modeling)(covariance matrix, Used-for, document modeling)(lexical distribution, Used-for, document modeling)(metadata, Used-for, document modeling)(hierarchical models, Compare, non-hierarchical models)(variational inference methods, Hyponym-Of, document modeling).
(multi party dialogue, Hyponym-Of, dialogue)(transformers, Used-for, hierarchical representations)(token- and utterance-level language modeling, Part-of, hierarchical representations)(utterance order prediction, Part-of, hierarchical representations)(transformers, Evaluate-for, multiparty dialogue)(transformers, Used-for, hierarchical representations in multiparty dialogue)(utterance embeddings, Part-of, hierarchical representations)(token embeddings, Part-of, hierarchical representations)(multi-task learning, Used-for, span-based question answering)(span-based question answering, Hyponym-Of, question answering)(utterance prediction, Conjunction, token span prediction)(BERT, Compare, RoBERTa)
(domain sentiment, Compare, domain-specific information)(domain sentiment, Compare, domain-invariant representation)(domain sentiment, Compare, user/product information)(domain sentiment, Compare, sentiment analysis)(domain sentiment, Used-for, volatility prediction)(domain sentiment, Used-for, target-oriented sentiment classification)(domain sentiment, Used-for, aspect-based sentiment analysis)(domain sentiment, Used-for, sentiment transfer)(domain sentiment, Used-for, humor recognition).
(abstractive summarization, Part-of, document summarization)(neural models, Used-for, abstractive sentence summarization)(graph-based attention mechanism, Used-for, sequence-to-sequence framework)(GLAD, Used-for, dialogue state tracking)(Global modules, Part-of, GLAD)(local modules, Part-of, GLAD)(hierarchical document encoder, Part-of, document modeling)(attention-based extractor, Part-of, document modeling)(task-oriented dialogue system, Used-for, medical diagnosis)(email subject line generation, Hyponym-Of, abstractive summarization)(GOLC, Part-of, neural text summarization models)(neural text summarization models, Used-for, document summarization)(Transformer-based encoder-decoder framework, Part-of, abstractive document summarization)(focus-attention mechanism, Part-of, Transformer-based encoder-decoder framework)(automatic evaluation metrics, Used-for, evaluating abstractive summarization)(response selection, Used-for, task-oriented dialogue)(reward learning,
(recurrent neural, Hyponym-Of, recurrent neural network)(recurrent neural network, Compare, traditional training of RNNs)(stochastic optimization, Compare, stochastic gradient Markov Chain Monte Carlo)(FOIL-COCO, Part-of, MS-COCO dataset)(LaVi models, Evaluate-for, caption classification)(LaVi models, Evaluate-for, foil word detection)(LaVi models, Evaluate-for, foil word correction)(RNN, Used-for, language modeling)(Recurrent neural networks, Compare, traditional methods)(KBQA, Used-for, relation detection)(recurrent neural networks, Part-of, Hybrid Code Networks)(discourse structure, Evaluate-for, text categorization)(KBs, Evaluate-for, machine reading)(jumping modified LSTM, Compare, standard sequential LSTM)(morphological inflection generation, Evaluate-for, neural model)(paraphrastic sentence embeddings, Used-for, sentence pairs)(containment relation, Evaluate-for, medical events)(sentence compression
(coreference resolution, Part-of, text-to-text generation)(machine translation, Part-of, text-to-text generation)(Penn Treebank, Used-for, constituency parsing)(French Treebank, Used-for, constituency parsing)(genetic algorithm, Used-for, automatic Pyramid scores)(attention-based extractor, Is-a-Prerequisite-of, answer selection)(encoder-decoders, Used-for, text style transfer)(Bag-of-Super-Word-Embeddings, Conjunction, string kernels)(Sino-Tibetan languages, Hyponym-Of, Chinese language)(CNN regression, Used-for, petition popularity prediction)(support vector machine, Compare, LSTM-based recurrent neural network)(metaphor corpora, Part-of, analyzing emotion in language)(hybrid pointer-generator network, Compare, standard sequence-to-sequence attentional model)(automatic training data generation, Evaluate-for, automatic Pyramid scores)(coverage, Part-of, pointer-generator network)(Naïve approaches, Compare, proposed unsupervised system)(
(None)
(multilingual representation, Part-of, multilingual connotation frames)(multilingual representation, Evaluate-for, targeted public sentiments)(multilingual representation, Used-for, analyzing public sentiments)(multilingual representation, Evaluate-for, semantic textual similarity tasks)(multilingual representation, Part-of, language modeling)(multilingual representation, Evaluate-for, dialogue state tracking)(multilingual representation, Used-for, downstream tasks in NLP)(multilingual connotation frames, Evaluate-for, targeted public sentiments)(multilingual connotation frames, Conjunction, English connotation frames)(dialog state tracking, Used-for, improving language understanding)(dialog state tracking, Evaluate-for, end-to-end learning of RNNs)(distributional vector space models, Used-for, representing low-frequency word forms)(distributional vector space models, Evaluate-for, semantic quality of word vectors)
(Implicit discourse relation classification, Used-for, text classification tasks)(Feature imitation framework, Used-for, implicit discourse relation classification)(Word embeddings, Part-of, cross-lingual word embeddings)(Greedy algorithm, Hyponym-Of, PWWS)(GAN, Hyponym-Of, local GAN)(GAN, Hyponym-Of, global GAN)(Unsupervised neural machine translation, Used-for, cross-language translation)(Neural models, Used-for, question answering)(Neural models, Evaluate-for, SQuAD)(PWW, Compare, existing techniques for text adversarial attack)(Adversarial attacks, Part-of, text classification)(Synonyms substitution strategy, Used-for, generating adversarial examples)(Question answering, Used-for, analyzing minimal context)(Multilingual transfer learning, Used-for, low-resource target language models)(Neural response generation, Used-for, dialogue systems)(Adversarial Filtering, Part-of
(text classification, Used-for, multi-task learning)(text classification, Evaluate-for, neural network models)(text classification, Evaluate-for, BiLSTMs)(text classification, Evaluate-for, CNN)(text classification, Evaluate-for, implicit discourse relation classification)(text classification, Evaluate-for, adversarial feature adaptation technique)(text classification, Evaluate-for, CNE)(text classification, Evaluate-for, complex networks enriched with word embedding)(text classification, Evaluate-for, Support Vector Machine)(text classification, Evaluate-for, linguistic features)(text classification, Evaluate-for, cross-lingual text classification)(text classification, Evaluate-for, LDA-based model)(text classification, Evaluate-for, visual character embedding)(text classification, Evaluate-for, sequence to sequence neural networks)(text classification, Evaluate-for, NTS systems)(text classification, Evaluate-for, group sparse CNNs)(text classification, Evaluate-for, neural models)(text classification, Evaluate-for, generative neural network architecture)(text classification, Evaluate-for, novel attentional
(neural network, Part-of, diagnosis system)(Bayesian Network, Part-of, diagnosis system)(deep neural networks, Used-for, high accuracy)(Bayesian Networks, Used-for, interpretability)(QAGS, Used-for, evaluating factual consistency)(QAGS, Used-for, identifying factual inconsistencies)(QAGS, Evaluate-for, factual consistency)(Example-Based GEC, Used-for, improving interpretability)(Example-Based GEC, Used-for, supporting language learners)(Entity-Aware Convolutional Neural Networks, Part-of, diagnosis system)(Bayesian Network Ensembles, Part-of, diagnosis system)(Bayesian Network, Compare, deep neural networks)(deep neural networks, Compare, Bayesian Network)(GEC, Compare, Example-Based GEC)(Example-Based GEC, Compare, GEC)(interpretability, Evaluate-for, diagnosis system accuracy)(interpretability, Evaluate-for, GEC)(interpretability, Evaluate-for, QAGS)(interpretability, Evaluate-for, Bayesian Network Ensembles)(Entity
(relation extraction, Used-for, information extraction)(relation extraction, Is-a-Prerequisite-of, knowledge base enrichment)(relation extraction, Evaluate-for, multi-lingual neural relation extraction framework)(relation extraction, Evaluate-for, distant supervision)(relation extraction, Evaluate-for, joint extraction of entities and relations)(relation extraction, Used-for, feature extraction)(relation extraction, Evaluate-for, adversarial learning framework)(relation extraction, Evaluate-for, context-aware neural network model)(relation extraction, Evaluate-for, multi-lingual multi-task architecture for sequence labeling)(relation extraction, Evaluate-for, Open Information Extraction)(relation extraction, Evaluate-for, neural encoder-decoder model)
(long short term memory, Is-a-Prerequisite-of, neural approach)(neural approach, Used-for, predicting sequences in unannotated text)(Hidden Markov Model, Part-of, aggregating sequential crowd labels)(Affect-LM, Part-of, generation of conversational text)(affect-discriminative word representations, Part-of, Affect-LM)(Long Short-Term Memory, Used-for, LSTM Noisy Channel Model)(noisy channel model, Used-for, disfluency detection)(recurrent neural networks, Part-of, Machine Translation)(long short term memory, Part-of, Machine Translation)(attention mechanisms, Part-of, neural models)(attention-based recurrent neural networks, Part-of, joint extraction of entity mentions and relations)(Bi-LSTM, Used-for, relation extraction)(aspect-level sentiment classification, Evaluate-for, attention-based long short term memory)(syntax-infused variational autoencoder, Part-of, variational autoencoder)(variational autoencoder, Used-for, generating sentences)(
(event commonsense evaluation, Used-for, evaluating commonsense in dialogue systems)(ACCENT, Used-for, event commonsense evaluation)(event commonsense evaluation, Part-of, commonsense reasoning)(commonsense knowledge bases (CSKBs), Used-for, event commonsense evaluation)(event-relations tuples, Part-of, event commonsense evaluation)(ACCENT, Evaluate-for, dialogue systems).
(concept, relation, concept)(text generation task, Used-for, video captioning)(text generation task, Used-for, summarization)(text generation task, Used-for, code generation)(text generation task, Used-for, question generation)(text generation task, Used-for, AMR-to-text generation)(text generation task, Used-for, paraphrase generation)(text generation task, Compare, machine translation task)(text generation task, Is-a-Prerequisite-of, neural machine translation systems)(text generation task, Used-for, sentence summarization)(text generation task, Used-for, morphologial inflection generation)(text generation task, Used-for, supporting argument detection)(text generation task, Used-for, chatbot responses generation)(neural machine translation systems, Part-of, machine translation task)(summarization, Conjunction, question generation)(summarization, Conjunction, AMR-to-text generation)(neural abstractive summarization, Part-of, summarization)(abstractive summarization,
(cross-lingual continual learning, Part-of, continual learning)(cross-lingual continual learning, Part-of, cross-lingual transfer learning)(continual learning, Evaluate-for, task difficulty)(continual learning, Evaluate-for, task order)(cross-lingual continual learning, Used-for, Visual Question Answering)(cross-lingual transfer learning, Used-for, structured prediction tasks)(cross-lingual transfer learning, Used-for, hate speech detection)(cross-lingual transfer learning, Used-for, named entity recognition)(cross-lingual transfer learning, Used-for, fine-grained entity typing)(cross-lingual transfer learning, Used-for, open-domain dialogue modeling)(cross-lingual transfer learning, Evaluate-for, language-specific taboo interjections)(cross-lingual transfer learning, Evaluate-for, lexical-phonetic distance)(prompt-learning, Hyponym-Of, cross-lingual transfer learning)(catastrophic forgetting, Is-a-Prerequisite-of, cross
(lexical sememe prediction, Evaluate-for, annotation efficiency)(lexical sememe prediction, Evaluate-for, annotation consistency)(lexical sememe prediction, Used-for, automatically recommending sememes for words)(lexical sememe prediction, Hyponym-Of, NLP tasks)(lexical sememe prediction, Used-for, improving annotation efficiency)(lexical sememe prediction, Used-for, improving annotation consistency)(lexical sememe prediction, Compare, existing methods of lexical sememe prediction)(existing methods of lexical sememe prediction, Used-for, relying on the external context of words)(existing methods of lexical sememe prediction, Evaluate-for, low-frequency words)(existing methods of lexical sememe prediction, Evaluate-for, out-of-vocabulary words)(low-frequency words, Part-of, issue for lexical sememe prediction)(out-of-vocabulary words, Part-of, issue for lexical sememe prediction)(issue for lexical sememe prediction, Evaluate-for, existing methods of lexical sem
(relation extraction, Hyponym-Of, sentence relation extraction)(multi-lingual neural relation extraction framework, Used-for, relation extraction)(mono-lingual attention, Part-of, multi-lingual neural relation extraction framework)(cross-lingual attention, Part-of, multi-lingual neural relation extraction framework)(distant supervision, Used-for, relation extraction)(dynamic transition matrix, Used-for, distant supervision)(class ties, Used-for, relation extraction)(convolutional neural network, Used-for, joint relation extraction)(Bi-LSTM, Used-for, relation extraction)(Pocket Knowledge Base Population, Used-for, relation extraction)(Open Information Extraction, Used-for, relation extraction)(BONIE, Used-for, relation extraction)(semantic specialization, Used-for, relation extraction)(automatic seed selection, Used-for, relation extraction)(noise reduction, Used-for, distant supervision)(Named Entity Disambiguation, Part-of, relation extraction model)(DocRED, Used-for, relation extraction)(graph neural network
(speech translation model, Part-of, neural machine translation)(neural machine translation, Used-for, machine translation)(neural machine translation, Is-a-Prerequisite-of, coreference resolution)(neural machine translation, Used-for, style transfer)(conversational-context, Part-of, speech translation model)(adversarial stability training, Used-for, neural machine translation)(beam search algorithm, Used-for, neural machine translation)(context-aware neural machine translation model, Compare, context-agnostic version)(context-aware neural machine translation model, Compare, concatenation of the context and source sentences)(tree-coverage model, Used-for, source-side syntax)(recurrent neural network grammar, Part-of, attention-based neural machine translation)(bidirectional tree encoder, Used-for, syntactic information)(reordering model, Used-for, neural machine translation)(self-attentive neural networks, Used-for, sentiment analysis)(self-attentive neural networks, Compare, recurrent neural networks).
(reinforcement learning, Evaluate-for, story quality)(visual captioning, Compare, generating abstract stories)(behavioral cloning algorithms, Evaluate-for, generating abstract stories)(pre-trained language models, Hyponym-Of, neural language representation models)(BERT, Compare, ERNIE)(ERNIE, Compare, BERT)(language models, Used-for, language representation)(pre-trained language models, Used-for, machine reading comprehension)(machine reading comprehension, Part-of, NLP)(BERT, Evaluate-for, MRC)(pre-trained language models, Evaluate-for, NLP tasks)(neural language representation models, Part-of, NLP)(embeddings, Hyponym-Of, input representations)(Cross-lingual word embedding, Compare, embedding-based methods)(neural machine translation, Evaluate-for, POS tagging)(POS tagging, Part-of, NLP)(NLP models, Evaluate-for, cross-lingual transfer)(transfer learning, Compare, multilingual model)(multilingual neural machine translation, Hypon
(adversarially trained, Part-of, semi-supervised adversarial training)(adversarially trained, Evaluate-for, performance improvement)(adversarially trained, Used-for, robustness enhancement)(adversarially trained, Used-for, zero anaphora resolution)(adversarially trained, Compare, supervised systems)(adversarially trained, Used-for, multi-dimensional emotion regression)(neural network, Used-for, zero anaphora resolution)(neural network, Used-for, multi-dimensional emotion regression)(neural network, Used-for, stance classification)(multi-task learning, Used-for, disentangling latent representations)(adversarial training, Used-for, disentangling latent representations)(NMT models, Part-of, neural network)(NMT models, Evaluate-for, translation quality)(NMT models, Part-of, stable translation under input perturbations)(multilingual unsupervised NMT, Hyponym-Of, NMT models)(adversarial stability training,
(factoid question answering, Is-a-Prerequisite-of, reading comprehension)(factoid question answering, Evaluate-for, BLEU)(factoid question answering, Used-for, WebQuestions)(factoid question answering, Used-for, WikiMovies)(factoid question answering, Used-for, Yahoo! Answers)(factoid question answering, Used-for, TREC QA)(factoid question answering, Used-for, universal schema)(factoid question answering, Used-for, stochastic answer network)(End-to-end neural dialogue generation, Compare, factoid question answering)(Question Condensing Networks, Evaluate-for, factoid question answering)(Open Information Extraction, Used-for, factoid question answering)(question answering, Part-of, factoid question answering)(question answering, Used-for, SQuAD)(question answering, Part-of, machine reading comprehension)(question answering, Part-of, open-domain question answering)(question answering, Used-for, Wikipedia)(generative model, Hypon
(named entity disambiguation, Part-of, knowledge base)(named entity disambiguation, Used-for, relation extraction)(named entity disambiguation, Used-for, cross-lingual word sense disambiguation)(named entity disambiguation, Evaluate-for, news articles)(named entity disambiguation, Evaluate-for, historical corpora)(named entity disambiguation, Evaluate-for, open-domain tasks)(named entity disambiguation, Evaluate-for, SnapCaptionsKB dataset)(named entity disambiguation, Used-for, end-to-end relation extraction)(named entity disambiguation, Used-for, entity retrieval)(named entity disambiguation, Used-for, machine translation)(named entity disambiguation, Used-for, multilingual social media posts)(sense representation, Is-a-Prerequisite-of, named entity disambiguation)(sense-level information, Is-a-Prerequisite-of, named entity disambiguation)(parallel corpora, Used-for, cross-lingual word sense disambiguation)(Euro
(topic distribution, Hyponym-Of, segment-specific topic distribution)(topic distribution, Hyponym-Of, document-specific topic distribution)(distributional vector spaces, Evaluate-for, topic distribution)(LDA, Evaluate-for, topic distribution)(segment-specific topic distribution, Part-of, LDA-based model)(document-specific topic distribution, Part-of, LDA-based model)(LDA, Used-for, topic distribution)(LDA-based model, Used-for, topic distribution).
(pretraining language, Used-for, neural word segmentation)(neural word segmentation, Part-of, neural NLP applications)(pretraining language, Evaluate-for, model improvement)(model improvement, Compare, state-of-the-art performance)(pretraining language, Used-for, neural network input layer)(input layer, Part-of, neural network)(neural network, Compare, traditional NLP techniques)(traditional NLP techniques, Evaluate-for, language understanding)
(object naming, Part-of, referring expression generation)(object recognition, Compare, object naming)(distributional word embeddings, Used-for, models of referential word meaning)(attention-based recurrent neural network, Used-for, entity mentions)(Automatic Content Extraction (ACE) corpora, Evaluate-for, entity mentions)(entity mentions, Part-of, named-entity recognition)(lexical resources, Used-for, part-of-speech induction)(lexical resources, Used-for, named-entity recognition)(proposed generative model, Evaluate-for, part-of-speech induction)(fixed-size ordinally forgetting encoding (FOFE) method, Used-for, named entity recognition)(named entity recognition, Part-of, natural language processing)(mention detection, Hyponym-Of, named entity recognition)(part-of-speech tagging, Used-for, language processing)(knowledge base completion (KBC) models, Evaluate-for, entity recognition)(tagging layer, Part-of, neural network)(emotion recognition in conversations, Compare, named entity
(translation task, Part-of, Neural Machine Translation)(translation task, Used-for, evaluating NMT performance)(NMT models, Conjunction, sequential encoder-decoder framework)(bi-directional LSTMs, Compare, convolutional layers)(DNNs, Used-for, modeling complex functions)(deep architecture, Compare, shallow architecture)(LSTM unit, Compare, GRU)(LSTM unit, Compare, LAUs)(neural word embeddings, Used-for, capturing word co-occurrence)(attention mechanism, Used-for, de-emphasizing irrelevant words)(phylogenetic language trees, Used-for, tracing language history)(source syntax, Used-for, improving NMT)(Sequential NMT models, Compare, source-side syntactic trees)(sequence-to-dependency NMT, Used-for, improving translation quality)(chunk-based decoders, Used-for, modeling global dependencies)(Multi-modal Neural Machine Translation, Conjunction, spatial visual features)(data scarcity problem, Evaluate-for
(automatic fake news, Part-of, deception detection)(LIAR, Used-for, automatic fake news)(hybrid convolutional neural network, Used-for, automatic fake news)(statistical approaches, Compare, hybrid convolutional neural network)(fake news datasets, Hyponym-Of, LIAR)(fact checking models, Head-of, automatic fake news)(political fact checking datasets, Used-for, automatic fake news detection)(automatic fact verification, Focus-on, estimating truthfulness)(aspect-based explainable system, Aid-in, human fact-checkers)(semi-automatically generated dataset, Part-of, FACTIFY-5WQA)(5W framework, Part-of, question-answer-based fact explainability)(baseline QA system, Used-for, locating answers from evidence documents)(automatic fake news detection, Evaluate-for, surface-level linguistic patterns)(claims, Evaluate-for, claim veracity)(evidence, Evaluate-for, claim veracity)(claims, Compare, evidence).
(aspect extraction, Hyponym-Of, sentiment analysis)(aspect extraction, Hyponym-Of, topic models)(aspect extraction, Hyponym-Of, neural word embeddings)(aspect extraction, Used-for, discovering coherent aspects)(sentiment analysis, Used-for, sentiment polarity detection)(sentiment analysis, Used-for, sarcasm detection)(sentiment analysis, Part-of, cognitive NLP systems)(sentiment analysis, Part-of, text classification tasks)(neural word embeddings, Used-for, word co-occurrence distribution)(neural word embeddings, Compare, topic models)(attention mechanism, Used-for, de-emphasizing irrelevant words)(CNN, Used-for, learning features from gaze and text)(CNN, Compare, handcrafted gaze and textual features)(CNN, Compare, text input alone)
(generated text, Part-of, Sequence-to-sequence models)(generated text, Compare, human-generated text)(Sequence-to-sequence models, Used-for, parsing)(Sequence-to-sequence models, Used-for, generating text)(AMR, Evaluate-for, generated text)(AMR, Part-of, Abstract Meaning Representation)(AMR parsing, Evaluate-for, Sequence-to-sequence models)(BLEU score, Evaluate-for, generated text)(neural model, Hyponym-Of, Sequence-to-sequence models)(LSTM, Used-for, sequence-based modeling)(new sentences, Evaluate-for, noisy sequences)(Chinese poem generation, Used-for, Sequence-to-sequence models)(attention mechanism, Hyponym-Of, neural model)(memory augmented neural model, Evaluate-for, Chinese poem generation)(disfluency detection, Evaluate-for, Sequence-to-sequence models)(Hierarchical Dirichlet Process, Part-of, neural models)(new sentence pairs, Used-for, improving quality)(social media summarization
(neural language model lm, Used-for, language modeling)(neural language model lm, Compare, traditional training)(neural language model lm, Compare, hierarchical LSTM language model)(neural language model lm, Compare, fixed-vocabulary language models)(neural language model lm, Compare, LSTM language model)(neural language model lm, Is-a-Prerequisite-of, poetry generation)(neural language model lm, Is-a-Prerequisite-of, conversational text generation)(neural language model lm, Part-of, recurrent neural networks)(recurrent neural networks, Used-for, machine translation)(recurrent neural networks, Compare, modified LSTM with jumping)(modified LSTM with jumping, Evaluate-for, sentiment analysis)(modified LSTM with jumping, Evaluate-for, news article classification)(modified LSTM with jumping, Evaluate-for, automatic question answering)(Affect-LM, Evaluate-for, generation of conversational text)(semantic-driven approach, Compare, neural models)(GuessTwo, Used-for,
(object naming, Hyponym-Of, referring expression generation)(object recognition, Compare, object naming)(visual information, Used-for, lexical information)(distributional word embeddings, Part-of, models of referential word meaning)(zero-shot learning, Evaluate-for, object naming)(topic models, Compare, neural word embeddings)(attention mechanism, Used-for, word embeddings)(bilingual word embeddings, Hyponym-Of, word embeddings)(document-aligned corpora, Used-for, bilingual word embeddings)(distributional space, Part-of, word embeddings)(entity categorization, Used-for, taxonomy learning)(hypernym prediction, Evaluate-for, hypernyms)(volatility prediction, Used-for, financial markets)(market data, Conjunction, textual information)(factual market data, Part-of, market data)(semantic relationships, Part-of, network analysis)(dialogue systems, Evaluate-for, collaborative dialogue)(knowledge graph embeddings, Part-of, neural model)(phrase-based machine translation, Compare, neural machine translation
(multilingual neural machine, Part-of, neural machine translation)(multilingual neural machine, Used-for, low-resource translation)(multilingual neural machine, Compare, individual bilingual models)(multilingual translation, Used-for, improving translation performance)(multilingual translation, Evaluate-for, parameter sharing)(multilingual translation, Compare, individual models)(neural machine translation, Part-of, machine translation)(neural machine translation, Hyponym-Of, multilingual neural machine)(neural machine translation, Evaluate-for, translation accuracy)(neural machine translation, Used-for, incorporating linguistic prior)(multi-lingual neural relation extraction, Evaluate-for, relation extraction)(multi-lingual neural relation extraction, Compare, mono-lingual methods)(cross-lingual attention, Conjunction, mono-lingual attention)(cross-lingual attention, Used-for, information consistency)(parse trees, Used-for, obtaining structural label sequences)(Hierarchical RNN encoder, Hyponym-Of, synt
(dialogue summarization, Conjunction, extractive summarization)(dialogue summarization, Conjunction, abstractive summarization)(dialogue summarization, Conjunction, multi-document summarization)(encode-attend-decode paradigm, Part-of, dialogue summarization)(repeated phrases, Compare, unique phrases)(query-based summarization, Compare, dialogue summarization)(ROUGE-L scores, Evaluate-for, dialogue summarization)(sequence-to-sequence framework, Used-for, dialogue summarization)(neural models, Used-for, abstractive summarization)(determinantal point processes (DPPs), Used-for, multi-document summarization)(content selection, Part-of, dialogue summarization)(Integer Linear Programming (ILP), Used-for, multi-document summarization)(encoder-decoder framework, Part-of, abstractive summarization)(semantic similarity, Evaluate-for, summarization models)(external summaries, Compare, automatic generation systems)(Sentence selection, Part-of, extractive document summarization)(Hierarchical encoder
(pretrained multilingual model, Compare, monolingual models)(pretrained multilingual model, Part-of, multilingual neural machine translation)(pretrained multilingual model, Used-for, low-resource Named Entity Recognition)(pretrained multilingual model, Evaluate-for, sequence labeling)(low-resource languages, Used-for, multilingual sequence labeling)(knowledge distillation, Used-for, pretrained multilingual model)(multiple languages, Part-of, multilingual neural machine translation)(language-independent representation, Used-for, pretrained multilingual model)(pretrained multilingual model, Is-a-Prerequisite-of, multilingual sequence labeling).
(entity recognition, Compare, part-of-speech induction)(entity recognition, Compare, named-entity recognition)(entity recognition, Used-for, CoNLL 2003 NER task)(entity recognition, Used-for, TAC-KBP2015 EDL tasks)(entity recognition, Used-for, TAC-KBP2016 EDL tasks)(entity recognition, Used-for, GermEval 2014)(entity recognition, Hyponym-Of, NER)(entity recognition, Hyponym-Of, MD)(entity recognition, Is-a-Prerequisite-of, multilingual learning for NER)(NER, Used-for, named entity detection)(NER, Used-for, CoNLL 2003 NER dataset)(NER, Used-for, GermEval 2014)(NER, Conjunction, MD)(NER, Compare, part-of-speech induction)(part-of-speech induction, Hyponym-Of, text classification)(part-of-speech induction, Compare, named-entity recognition)(named-
(neural text classifier, Used-for, text classification)(text classification, Part-of, natural language processing)(adversarial training, Used-for, neural text classifier)(atomic flip operation, Used-for, neural text classifier)(soft probabilistic predictions, Used-for, neural text classifier)(attention layers, Evaluate-for, neural text classifier)(visual character embedding, Used-for, neural text classifier)(character-level model, Hyponym-Of, neural text classifier)(attention mechanisms, Compare, visual character embedding)(capsule networks, Compare, neural text classifier)(deep neural network classifiers, Used-for, text classification)(unsupervised relation extraction, Used-for, text classification)(word tokenization, Evaluate-for, text classification)(semantic-preserving constraints, Used-for, neural text classifier)(cross-lingual text classification, Used-for, neural text classifier)
(coherent paragraph summary, Part-of, summarization dataset)(summarization dataset, Compare, existing datasets)(summarization dataset, Used-for, assessing the summarization output)(human evaluation, Evaluate-for, summarization dataset)(coherent paragraph summary, Compare, how-to articles)(summarization datasets, Compare, article summaries on the WikiHow website)(speech pre-training, Compare, GPT-2)(multi-stream transformer language model, Part-of, prosody-aware generative spoken language model)(HiFi-GAN model, Part-of, prosody-aware generative spoken language model)(prosody-aware generative spoken language model, Used-for, comprehension of speech with prosody).
(Commonsense knowledge, Part-of, Natural language text)(Reporting bias, Evaluate-for, Commonsense knowledge)(Relative physical knowledge, Evaluate-for, Actions)(Relative physical knowledge, Evaluate-for, Objects)(Long Short Term Memory, Used-for, Predict sequences)(Named-Entity Recognition, Used-for, News articles)(Information Extraction, Used-for, Biomedical abstracts)(Cognitive features, Part-of, Cognitive NLP systems)(Eye-movement patterns, Part-of, Cognitive features)(EEG signals, Part-of, Cognitive features)(Brain-imaging, Part-of, Cognitive features)(Convolutional Neural Network, Used-for, Sentiment polarity)(Convolutional Neural Network, Used-for, Sarcasm Detection)(Encoder-decoder framework, Used-for, Keyphrase prediction)(Named Entity Recognition, Is-a-Prerequisite-of, Information Extraction)(Multi-layer recurrent highway network, Used-for, Speech perception)(Framing, Used-for, Political strategy)(Neural sequence-to-sequence models, Used-for, Text summar
(multimodal datasets, Used-for, name tagging)(multimodal datasets, Used-for, sentiment analysis)(multimodal datasets, Used-for, emotion recognition)(multimodal datasets, Used-for, natural language understanding)(multimodal datasets, Part-of, multimodal sentiment analysis)(multimodal datasets, Part-of, multimodal social media posts)(multimodal datasets, Part-of, multimodal research)(multimodal datasets, Used-for, speaker trait analysis)(multimodal datasets, Evaluate-for, multimodal fusion)(multimodal datasets, Used-for, zero-shot disambiguation)(multimodal datasets, Used-for, natural language inference)(multimodal datasets, Used-for, semantic parsing)(multimodal datasets, Evaluate-for, robustness)(multimodal datasets, Evaluate-for, performance)
(topic discovery, Used-for, learning text representations)(topic discovery, Used-for, gaining insight into document corpora)(topic discovery, Hyponym-Of, topic models)(topic discovery, Used-for, discovering spatially distinct topics)(topic discovery, Part-of, natural language processing)(topic discovery, Used-for, rapid discovery of relevant spatial and temporal topics)(topic models, Part-of, topic discovery)(topic discovery, Used-for, document classification)(topic discovery, Used-for, topic guided keyphrase generation)(topic discovery, Used-for, low rank approximation of the original document-word matrix)(topic discovery, Used-for, discovering dispersed document topics)(topic discovery, Used-for, generating dispersed proportions of document topics)(topic discovery, Used-for, latent document-topic features)(word embeddings, Used-for, topic discovery)(keyphrases, Used-for, topic discovery)(mixed counting models, Used-for, topic discovery)(MOOCCube, Used-for, topic discovery)(Neural variational inference
(neural model, Used-for, semantic parsing)(neural model, Used-for, poetry generation)(neural model, Compare, discriminative models)(neural model, Used-for, language modeling)(neural model, Used-for, spoken language acquisition)(neural model, Used-for, text similarity measure)(neural model, Used-for, constituency parsing)(neural model, Used-for, word segmentation)(neural model, Used-for, Chinese poem generation)(neural model, Used-for, generating market comments)(neural model, Used-for, open-vocabulary language modeling)(neural model, Conjunction, back-propagation through time)(neural model, Used-for, machine translation)(neural model, Used-for, morphological inflection generation)(neural model, Evaluate-for, sequence labeling tasks)(neural model, Used-for, multilingual semantic parsing)(neural language model, Used-for, poetry generation)(neural language model, Compare, stochastic optimization)(neural language model
(deep syntactic, Compare, syntactic information)(deep syntactic, Is-a-Prerequisite-of, morpheme segmentation algorithm)(synTime, Compare, deep syntactic)(deep syntactic, Evaluate-for, NMT)(deep syntactic, Used-for, question answering)(deep syntactic, Used-for, semantic role labeling)(deep syntactic, Used-for, morphological analysis)(deep syntactic, Used-for, parsing detection)(deep syntactic, Used-for, semantic representation)(deep syntactic, Used-for, treebank conversion)
(word embeddings, Hyponym-Of, distributional word embeddings)(word embeddings, Used-for, capturing linguistic regularities)(word embeddings, Used-for, mapping words to vector spaces)(word embeddings, Used-for, forecasting market volatility)(word embeddings, Used-for, cross-lingual tasks)(word embeddings, Used-for, document clustering)(word embeddings, Used-for, dependency parsing)(word embeddings, Used-for, named entity recognition)(word embeddings, Used-for, transfer learning)(word embeddings, Used-for, building knowledge graph embeddings)(word embeddings, Compare, bag-of-words approaches)(distributional word embeddings, Used-for, linking visual to lexical information)(topic models, Compare, word embedding models)(cross-lingual signals, Is-a-Prerequisite-of, connecting monolingual word embeddings)(LSTM models, Used-for, context embeddings)(synonymy dictionaries, Conjunction, word embeddings)(traditional word analogies, Compare, temporal word analogies).
(citation text generation, Used-for, describing cited paper context)(citing paper, Part-of, scholarly papers)(cited paper, Part-of, scholarly papers)(implicit citation text, Evaluate-for, citation text generation)(BERT, Used-for, implicit citation extraction)(pointer-generator network, Used-for, citation text generation)(implicit aspects, Part-of, product reviews)(implicit opinions, Part-of, product reviews)(ACOS Quadruple Extraction, Used-for, aspect-based sentiment analysis)(aspect-category-opinion-sentiment quadruples, Part-of, ACOS Quadruple Extraction)(Restaurant-ACOS, Is-a-Prerequisite-of, ACOS Quadruple Extraction)(Laptop-ACOS, Is-a-Prerequisite-of, ACOS Quadruple Extraction)(Target-Stance Extraction, Used-for, stance detection)(target classification, Part-of, Target-Stance Extraction)(target generation, Part-of, Target-Stance Extraction)(multi-task approach
(dialogue state tracking, Used-for, predicting current dialogue state)(Global-Locally Self-Attentive Dialogue State Tracker, Hyponym-Of, dialogue state tracking)(slot connections, Part-of, Dialogue State Tracking with Slot Connections)(DST-SC, Hyponym-Of, Dialogue State Tracking with Slot Connections)(Dual Slot Selector, Part-of, DSS-DST)(DSS-DST, Hyponym-Of, dialogue state tracking)(Transferable Dialogue State Generator, Hyponym-Of, dialogue state tracking)(Selectively Overwriting Memory for Dialogue State Tracking, Hyponym-Of, dialogue state tracking)(Global modules, Part-of, Global-Locally Self-Attentive Dialogue State Tracker)(Local modules, Part-of, Global-Locally Self-Attentive Dialogue State Tracker)(Two-stage DSS-DST, Hyponym-Of, dialogue state tracking)(Slot Value Generator, Part-of, DSS-DST)(context
(dialogue state tracking, Part-of, Natural Language Processing)(unknown slot values, Part-of, dialogue state tracking)(spoken language understanding, Part-of, Natural Language Processing)(pointer network, Used-for, extract unknown slot values)(Stochastic Gradient Descent, Used-for, learn word representations)(AllVec, Used-for, generate word representations)(AllVec, Compare, Stochastic Gradient Descent)(batch gradient learning, Part-of, AllVec)(neural machine translation, Part-of, Natural Language Processing)(phrase-based statistical machine translation, Compare, neural machine translation)(low-resource NMT, Is-a-Prerequisite-of, high-resource settings)(curriculum learning, Part-of, training efficiency)(word embedding, Part-of, neural machine translation)(NMT, Compare, PBSMT)(plagdet, Evaluate-for, plagiarism detection)(paraphrased datasets, Part-of, training data)(RST discourse treebank, Evaluate-for, discourse parsing)(discourse cohesion, Part-of, discourse parsing)(
(Language model like BERT, Hyponym-Of, Language model)(Language model, Part-of, Natural Language Processing)(Neural language model, Hyponym-Of, Language model)(Language model, Used-for, Query auto-completion)(Language model, Evaluate-for, Sentiment classification)(Stochastic gradient Markov Chain Monte Carlo, Used-for, Training neural networks)(Recurrent neural networks, Hyponym-Of, Neural networks)(Affect-LM, Part-of, Language model)(LSTM, Hyponym-Of, Neural networks)(Universal Language Model Fine-tuning, Used-for, Text classification)(Hierarchical multi-scale language model, Hyponym-Of, Multi-scale representation model)(Attention mechanism, Part-of, Transformer)(Transformer, Hyponym-Of, Neural networks)(Disentangled latent representation learning, Used-for, Style transfer)(Conditional language model, Hyponym-Of, Language model)(Bayesian learning algorithm, Used-for, Training neural networks
(automatic fake news detection, Used-for, deception detection)(LIAR dataset, Used-for, automatic fake news detection)(meta-data, Part-of, hybrid convolutional neural network)(text, Part-of, hybrid convolutional neural network)(hybrid convolutional neural network, Hyponym-Of, automatic fake news detection)(hyperpartisan news, Compare, fake news)(stylometry, Evaluate-for, hyperpartisan news)(stylometry, Evaluate-for, fake news)(fact checking, Used-for, fake news detection)(knowledge base, Part-of, CompareNet)(external knowledge, Used-for, automatic fake news detection)(claim veracity, Evaluate-for, evidence)(social media, Used-for, misinformation propagation)(news environment, Part-of, News Environment Perception Framework)(popularity-oriented module, Part-of, News Environment Perception Framework)(novelty-oriented module, Part-of, News Environment Perception Framework)(multi-modal form, Hyponym-Of, fake news)(psycholingu
(named entity recognition ner, Part-of, natural language processing)(named entity recognition ner, Used-for, information extraction)(named entity recognition ner, Evaluate-for, improved performance)(named entity recognition ner, Part-of, multilingual learning)(named entity recognition ner, Used-for, cross-lingual tasks)(named entity recognition ner, Hyponym-Of, sequence labeling)(named entity recognition ner, Conjunction, mention detection)(named entity recognition ner, Compare, traditional sequence labeling methods)(named entity recognition ner, Is-a-Prerequisite-of, identifying entities in text)(named entity recognition ner, Compare, local detection approach)(named entity recognition ner, Evaluate-for, accuracy improvement)(named entity recognition ner, Used-for, topic modeling)(neural networks, Used-for, named entity recognition)(BiLSTM, Part-of, neural networks)(linear-chain CRFs, Compare, BiLSTM)(hidden markov model, Is-a-Prerequisite-of, aggregating sequential crowd labels)(hidden mark
(parsing, Part-of, natural language processing)(monolingual dependency parser, Used-for, parsing)(treebank, Used-for, monolingual dependency parser)(treebank embeddings, Used-for, parsing)(treebank embeddings, Compare, concatenating training sets)(fine-tuning, Used-for, treebank embeddings)(multiple treebanks, Used-for, training monolingual dependency parser)(treebank embedding vector, Hyponym-Of, treebank embeddings)(treebank embeddings, Compare, single treebanks)(treebank embeddings, Evaluate-for, improvements in parsing accuracy)(predefined treebank embedding vectors, Part-of, treebank embedding)(interpolated treebank vectors, Part-of, treebank embedding)(treebank vector prediction, Part-of, treebank embeddings)
(fact checking model, Used-for, predicting the veracity of a claim)(fact checking model, Used-for, verifying the truthfulness of a claim)(fact checking model, Used-for, reasoning about multiple retrievable evidence)(fact checking model, Evaluate-for, claim verification accuracy)(fact checking model, Evaluate-for, FEVER score)(fact checking model, Part-of, fake news detection)(fact checking model, Used-for, generating justifications for verdicts)(fact checking model, Is-a-Prerequisite-of, fake news detection)(fake news detection, Is-a-Prerequisite-of, fact checking model)(fact checking, Hyponym-Of, fact checking model)(matching model, Used-for, response selection)(matching model, Part-of, retrieval-based dialogue systems)(co-teaching framework, Part-of, matching model)(semantic role labeling, Used-for, obtaining rich semantic structures of evidence)(pre-trained models, Used-for, improving fact checking)(pre-trained models, Hyponym-Of
(generating summary, Used-for, multi-document summarization)(generating summary, Used-for, abstractive summarization)(generating summary, Used-for, extractive summarization)(generator module, Part-of, data-to-text generation model)(encoder-decoder framework, Part-of, neural text summarization models)(query auto-completion, Used-for, search engine feature)(Semantic Relevance Based neural model, Used-for, Chinese social media summarization)(reinforcement learning methods, Used-for, optimizing policy search)(GOLC, Used-for, neural text summarization models)(in-length summaries, Evaluate-for, improved post-editing time)(template-aware summary generation, Part-of, seq2seq framework)(Arel framework, Used-for, generating abstract stories from photo streams)(saliency-selection network, Part-of, Transformer-based encoder-decoder framework)(segment, Part-of, meeting recordings)(utterance, Part-of, meeting recordings)(multi-modal hierarchical
(language technology, Part-of, natural language processing)(language technology, Is-a-Prerequisite-of, multilingualism)(language technology, Evaluate-for, gender bias)(language technology, Evaluate-for, linguistic diversity)(language technology, Used-for, machine translation)(language technology, Conjunction, language resources)(language technology, Evaluate-for, low-resource languages)(language technology, Used-for, Legal Artificial Intelligence)(language technology, Evaluate-for, language representation)(machine translation, Used-for, language technology)(Legal Artificial Intelligence, Used-for, legal tasks)(gender bias, Evaluate-for, speech translation)(gender bias, Evaluate-for, natural language processing)(language resources, Evaluate-for, language technology)(language resources, Is-a-Prerequisite-of, NLP systems)
(seq2seq text generation, Used-for, text generation)(seq2seq text generation, Compare, non-autoregressive models)(seq2seq text generation, Part-of, video captioning)(seq2seq text generation, Part-of, AliMe Chat)(seq2seq text generation, Used-for, automatic pun generation)(seq2seq text generation, Evaluate-for, grammatical error correction)(sequence-to-sequence models, Hyponym-Of, seq2seq text generation)(sequence-to-sequence models, Used-for, task-oriented text generation improvements)(deep encoder-decoder architecture, Used-for, abstractive summarization)(grammar of generated sentences, Part-of, text generation)(joint training, Used-for, encoding boards)(homographic pun generation, Evaluate-for, readability and quality)(knowledge attention module, Used-for, enhancing models with external knowledge)(temporal dynamics, Part-of, video captioning)(logically-directed entailment, Used-for, better caption decoder representations)(Bayesian unsupervised
(vision language model, Used-for, caption classification)(vision language model, Used-for, foil word detection)(vision language model, Used-for, foil word correction)(vision language model, Evaluate-for, FOIL-COCO)(FOIL-COCO, Part-of, MS-COCO dataset)(FOIL-COCO, Compare, LaVi models)(MS-COCO dataset, Hyponym-Of, FOIL-COCO)(FOIL-COCO, Evaluate-for, text-image relation).
(neural network rnns, Compare, lstm units)(neural network rnns, Compare, gru units)(neural network rnns, Part-of, question answering system)(neural network rnns, Part-of, abstractive summarization system)(neural network rnns, Part-of, named entity recognition)(neural network rnns, Part-of, geolocation prediction models)(neural network rnns, Part-of, local coherence models)(neural network rnns, Used-for, language modeling)(neural network rnns, Used-for, text categorization)(neural network rnns, Used-for, dependency parsing)(neural network rnns, Used-for, sentence parsing)(neural network rnns, Evaluate-for, weight uncertainty)(neural network rnns, Hyponym-Of, artificial neural networks)(deep neural network, Hyponym-Of, neural networks)(deep neural network, Evaluate-for, neural machine translation)(linear associative units, Compare, lstm
(cross domain sentiment, Used-for, sentiment classifier)(cross domain sentiment, Is-a-Prerequisite-of, cross-domain sentiment classification)(cross domain sentiment, Compare, traditional non-neural-network-based model)(syntactic information, Used-for, models robustness)(Integer Linear Programming, Used-for, introducing syntactic constraints)(manually labeled data, Evaluate-for, domain adaptability)(weighted ensemble of classifiers, Evaluate-for, cross-domain classification performance)(BERT, Used-for, unsupervised domain adaptation)(adversarial training, Used-for, enhanced domain-invariant features)(KinGDOM, Used-for, enriching document semantics)(ConceptNet knowledge graph, Part-of, KinGDOM)(χ2 test and cosine-similarity, Used-for, identifying polarity preserving significant words).
(attention, Part-of, Transformer)(attention, Used-for, interpretability)(attention, Used-for, model performance)(attention, Part-of, self-attention unit)(identifiability attention, Is-a-Prerequisite-of, interpretability)(attention, Used-for, polarity score)(attention, Used-for, attention score)(attention weights, Part-of, attention)(attention weights, Used-for, model's predictions)(attention weights, Used-for, interpretability)(attention weights, Hyponym-Of, probability distribution)(attention weights, Used-for, attention)(attention weights, Is-a-Prerequisite-of, identifiable weights)(attention weights, Part-of, head)(attention weights, Used-for, gradient update process)(gradient update process, Evaluate-for, local minimum)(key vector, Part-of, attention)(value vector, Part-of, attention)(attention, Conjunction, polarity score)(attention, Conjunction, attention score)(attention mechanism, Evaluate-for, interpretability)(attention mechanism, Part-of,
(disfluency detection, Part-of, dialogue model)(task-oriented dialogue system, Part-of, dialogue model)(open-domain dialogue system, Part-of, dialogue model)(deep latent variable model, Part-of, dialogue model)(dynamic knowledge graph embedding, Part-of, dialogue model)(KB-InfoBot, Part-of, dialogue model)(automatic dialogue evaluation, Part-of, dialogue model)(affect conditioning, Part-of, dialogue model)(Neural Belief Tracking, Part-of, dialogue model)(encoder-decoder framework, Part-of, dialogue model)(Affect-LM, Part-of, dialogue model)(belief tracker, Part-of, dialogue model)(sentence representation learning, Part-of, dialogue model)(reinforcement learning, Used-for, dialogue model)(sequence-to-sequence model, Used-for, dialogue model)(Transformer, Used-for, dialogue model)(LSTM language model, Used-for, dialogue model)(Noisy Channel Model, Used-for, dialogue model)(automatic retrieval, Used-for, dialogue model)(
(None)
(Skip-Gram, Part-of, embedding model)(ITransF, Part-of, embedding model)(SDR framework, Part-of, embedding model)(word2vec, Hyponym-Of, embedding model)(GA Reader, Part-of, embedding model)(CANE, Part-of, embedding model)(bidirectional language models, Part-of, embedding model)(Riemannian optimization, Used-for, embedding model)(Max-margin objective, Used-for, embedding model)(Multi-Prototype Mention Embedding, Part-of, embedding model).
(language understanding, Used-for, symbolic reasoning)(language understanding, Evaluate-for, reading comprehension)(language understanding, Evaluate-for, review helpfulness modeling)(language understanding, Used-for, action recognition)(language understanding, Used-for, semantic parsing)(language understanding, Used-for, machine translation)(language understanding, Used-for, dialogue state tracking)(language understanding, Used-for, document modeling)(language understanding, Evaluate-for, cross-cultural differences)(neural "programmer", Part-of, Neural Symbolic Machine)(symbolic "computer", Part-of, Neural Symbolic Machine)(key-variable memory, Part-of, neural "programmer")(Lisp interpreter, Part-of, symbolic "computer")(REINFORCE, Used-for, optimize task reward)(REINFORCE, Augmented-with, iterative maximum-likelihood training)(neural "programmer", Used-for, mapping language utterances to programs)(symbolic "computer", Used-for, program execution)(belief tracker, Part-of, spoken dialogue systems
(paraphrase generation, Part-of, natural language processing)(paraphrase generation, Used-for, expanding natural language datasets)(paraphrase generation, Evaluate-for, correctness)(paraphrase generation, Evaluate-for, grammaticality)(paraphrase generation, Evaluate-for, linguistic diversity)(paraphrase generation, Part-of, ParaNMT-50M)(paraphrase generation, Used-for, generating paraphrase pairs)(paraphrase generation, Evaluate-for, accuracy)(paraphrase generation, Evaluate-for, diversity)(paraphrase generation, Part-of, machine learning systems)(crowdsourcing, Used-for, expanding natural language datasets)(semantic parsing, Compare, paraphrase generation)(deep latent variable models, Used-for, response generation)(dataset, Part-of, paraphrase generation)(paraphrase generation, Is-a-Prerequisite-of, semantic textual similarity).
(unsupervised bilingual word embedding, Used-for, bilingual dictionary induction)(word embedding, Used-for, neural machine translation)(word embedding, Used-for, metaphor identification)(unsupervised machine translation, Used-for, machine translation)(unsupervised machine translation, Part-of, machine translation)(unsupervised machine translation, Used-for, mining parallel sentences)(unsupervised machine translation, Used-for, parallel sentence mining)(parallel sentences, Part-of, mining parallel sentences)(bilingual word embeddings, Used-for, cross-lingual word embeddings)(bilingual word embeddings, Part-of, word embeddings)(unsupervised cross-lingual word embeddings, Part-of, word embeddings)(unsupervised machine translation, Evaluate-for, translation systems)(OCR post-correction, Used-for, unsupervised training)(sequence-to-sequence model, Used-for, OCR post-correction)(attention mechanism, Part-of, sequence-to-sequence model)(multi-input attention averaging, Part-of, sequence-to-sequence model
(aspect based sentiment, Part-of, sentiment analysis)(aspect based sentiment, Part-of, aspect sentiment classification)(aspect based sentiment, Part-of, aspect term extraction)(aspect based sentiment, Compare, general sentiment analysis)(aspect based sentiment, Used-for, determine the sentiment polarity towards a specific aspect)(aspect sentiment classification, Part-of, aspect based sentiment)(aspect term extraction, Part-of, aspect based sentiment)(aspect term extraction, Used-for, extracting aspects of a product or service from an opinion document)(sentiment analysis, Used-for, determining sentiment polarity)(aspect based sentiment, Used-for, determining sentiment polarity towards individual aspects)
(bias nlp, Compare, error disparities)(bias nlp, Compare, outcome disparities)(bias nlp, Used-for, prejudice detection)(bias nlp, Evaluate-for, NMT quality)(bias nlp, Evaluate-for, hate speech detection)(bias nlp, Is-a-Prerequisite-of, corpus evaluation)(bias nlp, Is-a-Prerequisite-of, annotation workflow)(bias nlp, Compare, model overamplification)(bias nlp, Compare, label bias)(bias nlp, Compare, selection bias)(bias nlp, Compare, semantic bias)(predictive bias, Part-of, bias nlp)(racial bias, Part-of, bias nlp)(gender bias, Part-of, bias nlp)(error disparities, Part-of, predictive bias)(outcome disparities, Part-of, predictive bias)(model overamplification, Part-of, predictive bias)(label bias, Part-of, predictive bias)(selection bias, Part-of, predictive bias)(semantic bias
(classification task, Part-of, natural language processing)(distant supervision, Used-for, building training data)(distant supervision, Part-of, classification task)(dynamic transition matrix, Used-for, characterizing noise)(curriculum learning, Used-for, training transition matrix)(factor graph model, Used-for, argument mining)(argumentative relations, Part-of, factor graph model)(SVM, Supports, factor graph model)(RNN, Supports, factor graph model)(implicit discourse relation classification, Part-of, classification task)(annotated implicit connectives, Used-for, implicit discourse relation classification)(feature imitation framework, Used-for, learning from neural network)(adversarial model, Used-for, adaptive imitation scheme)(Cross-lingual text classification, Hyponym-Of, classification task)(model distillation, Used-for, Cross-lingual text classification)(sentence-level sentiment classification, Hyponym-Of, classification task)(sentiment lexicons, Part-of, sentence-level
(multilingual model, Used-for, second language acquisition)(multilingual model, Used-for, semantic parsing)(multilingual model, Used-for, language identification)(multilingual model, Used-for, name tagging)(multilingual model, Used-for, cross-lingual coreference)(multilingual model, Evaluate-for, low-resource settings)(multilingual model, Part-of, multilingual learning)(multilingual model, Used-for, multilingual named entity recognition)(multilingual model, Used-for, multilingual machine translation)(multilingual model, Used-for, multilingual distributed text representation)(multilingual model, Part-of, multi-task learning framework)(multilingual model, Compare, monolingual model)
(generation semantic parsing, Part-of, natural language descriptions to source code parsing)(generation semantic parsing, Evaluate-for, scale up to generation of complex programs)(semantic parser, Used-for, generation semantic parsing)(Structure-aware neural architecture, Used-for, semantic parsing)(abstract syntax trees, Used-for, structured meaning representations)(StructVAE, Used-for, semi-supervised semantic parsing)(Sequence-to-Action, Used-for, semantic parsing)(confidence modeling, Used-for, neural semantic parsers)(dual learning algorithm, Used-for, semantic parsing)(OONP, Used-for, document parsing)(semantic parser, Is-a-Prerequisite-of, mapping natural language to executable programs).
None
(chinese word segmentation cws, Part-of, neural word segmentation)(neural word segmentation, Used-for, chinese word segmentation cws)(chinese word segmentation cws, Evaluate-for, various segmentation criteria)(external training sources, Used-for, neural word segmentation)(adversarial multi-criteria learning, Used-for, chinese word segmentation cws)(chinese word segmentation cws, Is-a-Prerequisite-of, pos tagging)(chinese word segmentation cws, Compare, character-based models)(chinese word segmentation cws, Used-for, matching performance improvement)(chinese word segmentation cws, Evaluate-for, accuracy improvement on benchmarks)(chinese word segmentation cws, Evaluate-for, segmentation and classification of epistemic activities)(chinese word segmentation cws, Used-for, dependency parsing)(chinese word segmentation cws, Is-a-Prerequisite-of, language modeling)(chinese word segmentation cws, Is-a-Prerequisite-of, machine translation)(chinese word segmentation c
(labeled sequence transduction, Used-for, transforming one sequence into another)(multi-space variational encoder-decoders, Used-for, labeled sequence transduction)(discrete and continuous latent variables, Part-of, multi-space variational encoder-decoders)(neural networks, Used-for, handle discrete and continuous latent variables)(SIGMORPHON morphological inflection benchmark, Evaluate-for, multi-space variational encoder-decoders)(cold-start problem, Evaluate-for, novel neural network model)(review spam detection, Used-for, solving cold-start problem)(semantic knowledge, Used-for, event extraction)(unsupervised learning techniques, Used-for, obtaining meaningful representations of words)(unsupervised neural machine translation, Used-for, machine translation)(bilingual dictionary induction, Hyponym-Of, bilingual tasks)(domain adaptation, Used-for, bilingual tasks).
(morphologically rich languages, Compare, low-frequency word forms)(morphologically rich languages, Compare, distinct lexical relations)(distributional vector space models, Evaluate-for, low-frequency word forms)(distributional vector space models, Evaluate-for, distinct lexical relations)(distributional vector space models, Compare, curated semantic lexicons)(morph-fitting procedure, Used-for, improving distributional vector spaces)(morph-fitting procedure, Used-for, pulling inflectional forms together)(morph-fitting procedure, Used-for, pushing derivational antonyms apart)(morph-fitting procedure, Is-a-Prerequisite-of, improving low-frequency word estimates)(morph-fitting procedure, Is-a-Prerequisite-of, boosting semantic quality of word vectors)(morph-fitted vectors, Used-for, dialogue state tracking)(semantic representation, Is-a-Prerequisite-of, NLP tasks)(semantic schemes, Compare, syntactic schemes)(semantic schemes, Conjunction, AMR)(semantic schemes, Conjunction, UCCA)(
(argument extraction eae, Compare, feature extraction)(multi-lingual neural relation extraction framework, Part-of, relation extraction)(cognitive features, Part-of, Cognitive NLP systems)(eye-movement/gaze data, Used-for, cognitive features)(Convolutional Neural Network, Used-for, cognitive features extraction)(aspect extraction, Part-of, aspect-based sentiment analysis)(neural word embeddings, Used-for, aspect extraction)(attention mechanism, Used-for, aspect extraction)(distant supervision, Used-for, relation extraction)(dynamic transition matrix, Used-for, distant supervision)(attention-based recurrent neural network, Used-for, entity mentions extraction)(Automatic Content Extraction, Evaluate-for, attention-based recurrent neural network model)(Singlish, Evaluate-for, dependency parsing)(feature extraction, Used-for, statistical NLP)(message passing algorithm, Used-for, feature template restructuring)(Tensor Factorization with Back-off and Aggregation, Used-for, Higher-order Relation Schema Induction)(question-answer
(orthography, Hyponym-Of, dialectal content)(morphological feature, Part-of, Semitic languages)(lexicalized, Hyponym-Of, morphological feature)(non-lexicalized, Hyponym-Of, morphological feature)(lemmas, Hyponym-Of, lexicalized)(diacritized forms, Hyponym-Of, lexicalized)(gender, Hyponym-Of, non-lexicalized)(number, Hyponym-Of, non-lexicalized)(part-of-speech tags, Hyponym-Of, non-lexicalized)(joint modeling, Used-for, morphological patterns)(morphological patterns, Used-for, disambiguating lexical choices)(different modeling granularity, Evaluate-for, joint modeling)(Arabic, Evaluate-for, state-of-the-art results)(Modern Standard Arabic, Hyponym-Of, Arabic)(Egyptian Arabic, Hyponym-
(experiments, Evaluate-for, semantic parsing)(natural language utterances, Used-for, generating formal meaning representations)(predicate-argument structures, Used-for, semantic parsing)(semantic parser, Used-for, parsing natural language descriptions into source code)(annotation, Part-of, semantic parsing model training)(unsupervised semantic parsing, Compare, supervised semantic parsing)(latent representations, Used-for, unsupervised semantic parsing)(semantic parsing, Used-for, natural language understanding)(triclustering, Used-for, semantic frame induction)(FrameNet-derived dataset, Evaluate-for, semantic frame induction methods)(cross-lingual distributed logical representations, Used-for, improving monolingual semantic parser performance)(Auto-Encoding model, Hyponym-Of, unsupervised learning method)(unsupervised dependency parsing, Evaluate-for, accuracy)(Dependency triples, Part-of, frame induction)(sentence splitting, Used-for, text simplification)(semantic textual similarity, Evaluate-for, semantic parsing
(free text rationale, Hyponym-Of, rationales)(free text rationale, Used-for, explainable NLP)(free text rationale, Used-for, chain-of-thought prompting)(free text rationale, Compare, human rationales)(free text rationale, Evaluate-for, model reasoning and prediction processes)(REV, Used-for, evaluating free text rationales)(free text rationale, Evaluate-for, human judgment)(rationales, Part-of, Rationales-centric Double-robustness Learning)(inductive bias, Part-of, Rationales-centric Double-robustness Learning)(Counterfactual reasoning objective, Part-of, faithful knowledge distillation)(rationales, Part-of, large language models)(Gen-U, Evaluate-for, human utility)(machine generated rationales, Evaluate-for, human utility)
(video question answering, Compare, text question answering)(video question answering, Compare, image question answering)(video question answering, Part-of, question answering)(video question answering, Compare, table question answering)(question answering, Hyponym-Of, text question answering)(question answering, Hyponym-Of, image question answering)(question answering, Hyponym-Of, table question answering)(question answering, Hyponym-Of, visual question answering)(question answering, Evaluate-for, Stanford Question Answering Dataset)(question answering, Evaluate-for, WebQuestionsSP dataset)(question answering, Evaluate-for, TriviaQA).
(language model trained, Used-for, poetry generation)(language model trained, Used-for, incorporating document context)(neural language model, Hyponym-Of, language model trained)(phonetic encoding, Part-of, neural language model)(poetic devices, Part-of, English poetry)(constraint satisfaction problem, Used-for, poetry generation)(discriminative weighted finite state machine, Part-of, constraint satisfaction problem)(back-propagation through time, Used-for, training RNNs)(stochastic gradient Markov Chain Monte Carlo, Used-for, learning weight uncertainty in RNNs)(topic model-like architecture, Part-of, neural language model)(LSTM language model, Hyponym-Of, language model trained)(Affect-LM, Hyponym-Of, LSTM language model)(Affect-LM, Used-for, generation of conversational text)(emotionally colored words, Part-of, affective messages)(SICK corpus, Used-for, estimating compositional distributional semantics models)(
(feature extraction, Part-of, NLP task especially)(dialogue state tracking, Part-of, NLP task especially)(word representations, Part-of, NLP task especially)(discourse parsing, Part-of, NLP task especially)(relationship extraction, Part-of, NLP task especially)(data augmentation, Part-of, NLP task especially)(code-switching, Part-of, NLP task especially).
(contextualized word embeddings, Compare, non-contextual subword embeddings)(contextualized word embeddings, Used-for, multilingual NLP)(contextualized word embeddings, Used-for, open-domain argument search)(contextualized word embeddings, Compare, pretrained non-contextual subword embeddings)(contextualized word embeddings, Compare, pretrained subword embeddings)(contextualized word embeddings, Evaluate-for, argument classification)(contextualized word embeddings, Evaluate-for, argument clustering)(contextualized word embeddings, Used-for, cross-lingual model transfer)(contextualized word embeddings, Compare, ELMo)(contextualized word embeddings, Compare, BERT)(BERT, Part-of, contextualized word embeddings)(ELMo, Part-of, contextualized word embeddings)(BERT, Compare, BPEmb)(character representations, Compare, non-contextual subword embeddings)(BERT, Compare, FastText)
(labeled data, Used-for, increase performance on low-resource language)(cross-lingual attention, Used-for, consider information consistency)(language relatedness, Evaluate-for, ability to transfer morphological knowledge)(cross-lingual transfer, Part-of, cross-lingual neural relation extraction)(syntactic structure compatibility, Evaluate-for, cross-lingual transfer)(syntactic tree processing, Used-for, reduce anisomorphism)(cross-lingual transfer, Used-for, boost NLP for low-resource languages)(neural crosslingual coreference model, Used-for, improve entity linking accuracy)(cross-lingual word embeddings, Used-for, facilitate cross-lingual transfer)(cross-lingual embeddings, Evaluate-for, downstream performance)(unsupervised multilingual embedding method, Evaluate-for, low-resource scenarios)(cross-lingual transfer, Evaluate-for, poor transfer from distant languages).
(sentiment classification, Used-for, text classification)(sentiment classification, Used-for, sentiment polarity)(sentiment classification, Used-for, sarcasm detection)(sentiment classification, Part-of, domain adaptation)(sentiment classification, Used-for, automatic text analysis)(sentiment analysis, Hyponym-Of, sentiment classification)(sentiment classification, Is-a-Prerequisite-of, feature extraction)(sentiment classification, Evaluate-for, CNN)(sentiment classification, Evaluate-for, RNN)(sentiment polarity, Conjunction, sarcasm detection)(sarcasm detection, Compare, sentiment polarity)(text classification, Hyponym-Of, NLP tasks)(domain adaptation, Evaluate-for, sentiment classification)(domain adaptation, Used-for, transfer learning)(domain adaptation, Evaluate-for, sentiment classifier)(sarcasm detection, Evaluate-for, feature extraction)(text features, Part-of, sentiment classification)(domain-specific sentiment similarities, Used-for, sentiment classifier)
(chinese spelling correction, Hyponym-Of, CSC)(chinese spelling correction, Used-for, detecting spelling errors)(chinese spelling correction, Used-for, correcting spelling errors)(chinese spelling correction, Evaluate-for, language understanding ability)(chinese spelling correction, Conjunction, phonological knowledge)(chinese spelling correction, Conjunction, visual similarity knowledge)(Chinese Spelling Correction, Part-of, Chinese natural language processing)(MG lattice, Used-for, Chinese relation extraction)(phonological similarity knowledge, Used-for, Chinese Spelling Correction)(visual similarity knowledge, Used-for, Chinese Spelling Correction)(language models, Used-for, Chinese Spelling Correction)(BERT, Used-for, Chinese Spelling Correction)(SpellGCN, Used-for, Chinese Spelling Correction)(PLOME, Used-for, Chinese Spelling Correction)(PHMOSpell, Used-for, Chinese Spelling Correction)(UMRSpell, Used-for, Chinese Spelling Correction)(language models, Used-for, detecting errors)(
(task word segmentation, Part-of, text processing)(word segmentation models, Evaluate-for, task word segmentation)(speech register, Evaluate-for, task word segmentation)(prosody, Evaluate-for, task word segmentation)(prosodic boundaries, Evaluate-for, word segmentation models)(neural models, Evaluate-for, task word segmentation)(training, Used-for, neural models)(working procedures, Used-for, neural models)(character embedding inputs, Used-for, neural word segmenter)(subword units, Part-of, open vocabulary problems)(subword segmentation, Evaluate-for, subword units)(tokenization, Evaluate-for, text classification)(tokenization, Evaluate-for, task word segmentation)(language model, Part-of, tokenization)(sampling segmentation, Used-for, language model)(lattice-based encoders, Evaluate-for, neural machine translation)(lattice positional encoding, Part-of, lattice-based encoders)(lattice-aware self-attention, Part-of, lattice-based encoders)(Dynamic Programming Encoding (DPE),
(language vision, Compare, human vision)(language vision, Compare, Neural network based inference models)(language vision, Evaluate-for, caption classification)(language vision, Evaluate-for, foil word detection)(language vision, Evaluate-for, foil word correction)(language vision, Part-of, FOIL-COCO)(language vision, Evaluate-for, image annotation)(language vision, Evaluate-for, scene understanding)(language vision, Evaluate-for, image retrieval)(SEQ-TO-SEQ learning, Part-of, COREQA)(COPYING MECHANISM, Part-of, COREQA)(RETRIEVING MECHANISM, Part-of, COREQA)(Neural network based inference models, Used-for, modeling inference)(syntactic parsing information, Used-for, inference modeling)(Neural network based inference models, Evaluate-for, reasoning and inference)(semantic units, Used-for, generating natural answers)(DATA SCARCITY, Evaluate-for, neural machine translation)(pivot-to-target NMT model, Used-for,
(generative language model, Is-a-Prerequisite-of, poetry generation)(generative neural models, Used-for, constituency parsing)(neural language model, Compare, pure sentence-based model)(neural language model, Used-for, learning representation of content)(language models, Used-for, generating sentences for a topic)(hierarchical LSTM language model, Used-for, generating word tokens character by character)(generative language model, Part-of, constraint satisfaction problem)(generative neural language model, Used-for, learning representations of English poetry)(RNN, Used-for, language modeling)(RNN, Evaluate-for, overfitting)(RNN, Compare, r-RNTN)(r-RNTN, Compare, RNTN)(r-RNTN, Evaluate-for, language model performance)(stochastic gradient Markov Chain Monte Carlo, Evaluate-for, weight uncertainty in RNN)(topic model-like architecture, Used-for, incorporating document context)(neural language model, Used-for, including document context
(concept learning, Part-of, embedding learning)(word embeddings, Hyponym-Of, monolingual embeddings)(monolingual embeddings, Compare, cross-lingual word embeddings)(parallel corpus, Used-for, connecting monolingual embeddings)(bilingual word embeddings, Hyponym-Of, cross-lingual word embeddings)(monolingual embeddings, Part-of, monolingual data)(monolingual data, Used-for, training monolingual embeddings)(bilingual lexicon induction, Evaluate-for, cross-lingual word embeddings)(monolingual embeddings, Evaluate-for, learning word embeddings)(contrastive learning, Used-for, learning word embeddings)(adversarial training, Used-for, mapping monolingual embeddings)(PCCA, Used-for, text embedding induction)(multilingual word similarity, Evaluate-for, cross-lingual embedding methods)(bilingual text embeddings, Compare, monolingual embeddings).
(contrastive learning, Used-for, learning by contrasting positive and negative samples)(noise contrastive estimation, Hyponym-Of, contrastive learning)(contrastive learning, Part-of, augmenting the negative sampler into a mixture distribution)(generic word embeddings, Compare, domain specific embeddings)(domain specific embeddings, Hyponym-Of, embeddings)(domain adapted word embeddings, Hyponym-Of, embeddings)(domain adapted word embeddings, Used-for, aligning corresponding word vectors)(canonical correlation analysis, Used-for, aligning corresponding word vectors)(kernel CCA, Used-for, aligning corresponding word vectors)(domain adapted word embeddings, Evaluate-for, sentiment classification tasks)(CNN model, Used-for, aspect extraction)(aspect extraction, Part-of, fine-grained sentiment analysis)(aspect extraction, Part-of, sentiment classification tasks)(embeddings, Part-of, pre-trained embeddings for aspect extraction)(domain specific embeddings, Part-of, pre-trained embeddings for aspect extraction)(generic
(stanford question answering, Part-of, reading comprehension)(reading comprehension, Is-a-Prerequisite-of, reading comprehension style question answering)(gated self-matching networks, Used-for, reading comprehension style question answering)(pointer networks, Used-for, locating the positions of answers)(question-aware passage representation, Part-of, gated self-matching networks)(self-matching attention mechanism, Used-for, refining the representation)(SQuAD dataset, Evaluate-for, model)(constituent-centric neural architecture, Part-of, neural network models)(neural network models, Compare, LSTM-based baseline models)(semantic parsing, Used-for, question answering)(open-domain question answering, Used-for, answering complex questions)(Wikipedia, Evaluate-for, open-domain question answering)(memory networks, Used-for, aligning structured KBs and unstructured text)(factoid question answering, Compare, complex question answering)(EviNets, Used-for, evaluating candidate answer entities)(BLEU, Evaluate-for, generation systems)(control
(domain question answering, Part-of, question answering)(domain question answering, Used-for, providing factoid answers)(domain question answering, Used-for, providing complex answers)(domain question answering, Evaluate-for, reading comprehension)(domain question answering, Compare, open-domain question answering)(domain question answering, Compare, closed-domain question answering)(domain question answering, Is-a-Prerequisite-of, KB-QA)(COREQA, Used-for, domain question answering)(neural network-based methods, Used-for, domain question answering)(KB-QA, Part-of, domain question answering)(KB-QA, Used-for, retrieving correct answers)(neural network, Part-of, neural network-based methods)(residual learning, Part-of, hierarchical recurrent neural network)(bidirectional LSTMs, Part-of, hierarchical recurrent neural network)(jumping technique, Used-for, improving model speed)(EviNets, Part-of, domain question answering)(Open Information Extraction, Used-for, generating semi-structured knowledge).
(labeled sequence transduction, Is-a-Prerequisite-of, learning morphological)(multi-space variational encoder-decoders, Used-for, learning morphological)(morphological inflection generation, Used-for, learning morphological)(morphological analysis, Part-of, learning morphological)(morphological tagging, Part-of, learning morphological)(semi-supervised learning, Used-for, learning morphological)(SIGMORPHON morphological inflection benchmark, Evaluate-for, learning morphological)(character-level models, Evaluate-for, learning morphological)(cross-lingual parser transfer, Used-for, learning morphological)(synonym conjunction process, Part-of, learning morphological)(factorial conditional random fields, Used-for, learning morphological)(analogical reasoning, Evaluate-for, learning morphological)(neural machine translation, Used-for, learning morphological)(gender stereotypes, Evaluate-for, learning morphological)(multitask learning, Used-for, learning morphological)(adversarial training, Used-for, learning morphological)(low-resource languages, Evaluate-for, learning morphological
(generalization, Compare, specialization)(semantic parsing, Part-of, natural language processing)(zero-shot setting, Evaluate-for, generalization)(neural parsers, Used-for, generalization)(structured output prediction, Used-for, generalization)(decomposition-integration, Is-a-Prerequisite-of, Hierarchical Semantic Parsing)(question decomposer, Used-for, Hierarchical Semantic Parsing)(neural language models, Evaluate-for, generalization)(contextual embeddings, Evaluate-for, generalization)(semantic representations, Used-for, semantic parsing)(natural language syntax, Hyponym-Of, semantic parsing).
(improve summarization system, Used-for, state-of-the-art integer linear programming framework)(improve summarization system, Part-of, neural abstractive summarization)(improve summarization system, Part-of, neural extractive summarization)(improve summarization system, Used-for, factual correctness reward)(neural extractive summarization, Is-a-Prerequisite-of, improve summarization system)(neural abstractive summarization, Is-a-Prerequisite-of, improve summarization system)(state-of-the-art integer linear programming framework, Used-for, multi-document summarization)(factual correctness reward, Used-for, improve summarization system)(neural abstractive summarization, Compare, neural extractive summarization)(document summarization, Hyponym-Of, summarization)(abstractive summarization, Hyponym-Of, summarization)(extractive summarization, Hyponym-Of, summarization)(email subject line generation, Compare, news headline generation)(sequence-to-se
(neural techniques, Used-for, end-to-end computational argumentation mining)(token-based dependency parsing, Hyponym-Of, computational argumentation mining)(token-based sequence tagging, Hyponym-Of, computational argumentation mining)(multi-task learning setup, Part-of, computational argumentation mining)(BiLSTMs, Used-for, token-based sequence tagging)(long-range dependencies, Part-of, argumentation mining problem)(word embeddings, Used-for, dependency parsing)(character strings embeddings, Used-for, joint task)(ensemble of multiple models, Used-for, search-based structured prediction tasks)(dependency parsing, Hyponym-Of, search-based structured prediction tasks)(neural machine translation, Hyponym-Of, search-based structured prediction tasks)(Universal Dependencies 2.0 framework, Used-for, English dependency parsing)(syntactic relations, Part-of, dependency parsing)(semantic parser, Used-for, Abstract Meaning Representations)(tree representations, Part-of, AMR graph)(dependency tree parsing,
(outperform strong baseline dialogue, Used-for, Metaphoric expressions)(outperform strong baseline dialogue, Used-for, Multi-label few-shot learning)(outperform strong baseline dialogue, Used-for, Adversarial attack)(outperform strong baseline dialogue, Used-for, Neural architecture search)(outperform strong baseline dialogue, Used-for, Abstract Meaning Representation)(Adversarial attack, Used-for, NLP tasks)(Neural architecture search, Used-for, learning architectures)(Multi-label few-shot learning, Used-for, sentiment analysis)(outperform strong baseline dialogue, Used-for, Machine translation)(outperform strong baseline dialogue, Used-for, Image captioning)(outperform strong baseline dialogue, Used-for, cross-lingual language model pre-training)(outperform strong baseline dialogue, Used-for, parallel text generation)(outperform strong baseline dialogue, Used-for, long textual document classification).
(adversarial multi-task learning, Improves, text classification tasks)(adversarial multi-criteria learning, Improves, Chinese word segmentation)(adversarial generation techniques, Used-for, style transfer)(adversarial stability training, Improves, NMT models)(adversarial examples, Used-for, image captioning)(white-box adversarial examples, Used-for, character-level neural classifier)(adversarial training, Improves, stance classification)(adversarial examples, Used-for, text classification)(adversarial training, Improves, text classification robustness)(adversarial input perturbations, Evaluates-for, self-attentive neural networks)(negative transfer, Mitigated-by, adversarial training)(Dual Adversarial Transfer Network, Used-for, low-resource NER)(Generalized Resource-Adversarial Discriminator, Used-for, low-resource NER).
(textual adversarial attack, Used-for, revealing the vulnerability of deep neural networks)(synonyms substitution strategy, Used-for, generating adversarial examples)(Probability Weighted Word Saliency (PWWS), Is-a-Prerequisite-of, textual adversarial attack)(Word Saliency speedup Local Search (WSLS), Is-a-Prerequisite-of, textual adversarial attack)(adversarial training, Used-for, improving model robustness)(semantic similarity, Evaluate-for, adversarial examples)(Convolutional and LSTM models, Evaluate-for, PWWS)(BiLSTM and BERT, Evaluate-for, novel attack model)(Dirichlet Neighborhood Ensemble (DNE), Is-a-Prerequisite-of, defense against synonym substitution-based attacks)(adversarial training, Part-of, textual adversarial attack)(MATE-KD, Used-for, improving knowledge distillation performance)(CLINE, Used-for, improving robustness under semantically adversarial attacking)(reliability testing, Used-for, improving accountability of
(entity embeddings, Hyponym-Of, word embeddings)(entity embeddings, Used-for, query understanding)(entity embeddings, Used-for, fine-grained entity categorization)(entity embeddings, Part-of, taxonomy learning)(context embeddings, Used-for, entity embeddings)(pretrained word embeddings, Part-of, neural network architectures for NLP tasks)(pretrained word embeddings, Used-for, entity embeddings)(context embeddings, Is-a-Prerequisite-of, entity embeddings)(network embedding, Compare, entity embeddings)(holographic embeddings, Compare, complex embeddings)(visual character embedding, Part-of, character embeddings)(Latent Meaning Models, Used-for, word embeddings)(multi-prototype mention embeddings, Used-for, entity linking)(context-aware embeddings, Is-a-Prerequisite-of, network embedding)(attention mechanism, Part-of, recurrent neural network)(similarity scoring, Used-for, entity embeddings)(word embeddings, Part-of, entity embeddings).
(dialogue system, Part-of, task-oriented dialogue systems)(dialogue system, Part-of, non-task-oriented dialogue systems)(dialogue system, Used-for, query-based summarization)(dialogue system, Used-for, automatic diagnosis)(dialogue system, Part-of, encoder-decoder dialog model)(Hybrid Code Networks (HCNs), Used-for, dialogue system)(Neural Belief Tracking (NBT), Used-for, dialogue system)(dialogue system, Used-for, chat detection)(mem2seq, Used-for, dialogue system)( GLAD, Used-for, dialogue system)(encoder-decoder dialog model, Part-of, dialogue system)(belief tracker, Part-of, dialogue system)(spoken Language Understanding models, Used-for, dialogue system)(Neural Belief Tracking (NBT), Used-for, dialogue system)(ptrnet, Used-for, dialogue system)( automatic diagnosis, Used-for, dialogue system)(semantic decoding, Used-for, dialogue system)(attention mechanism, Part-of, encode-attend-decode
(multilingual masked language modeling, Used-for, cross-lingual transfer)(monolingual BERT models, Used-for, multilingual masked language modeling)(BERT, Compare, multilingual BERT)(BERT, Evaluate-for, machine reading comprehension)(mBERT, Hyponym-Of, pre-trained multilingual language models)(mBERT, Used-for, cross-lingual transfer)(XLM-R, Compare, multilingual BERT)(XLM-R, Used-for, cross-lingual transfer)(multilingual masked language models, Used-for, quality estimation for machine translation)(XCSQA, Hyponym-Of, multilingual masked language modeling)(XCODAH, Hyponym-Of, multilingual masked language modeling)(BERT, Conjunction, mBERT)(cross-lingual transfer, Used-for, quality estimation for machine translation).
(None
(semantic parser, Is-a-Prerequisite-of, translating high-level textual descriptions)(reinforcement learning, Conjunction, maximum marginal likelihood)(new neural semantic parser, Evaluate-for, context-dependent semantic parsing task)(code generation, Used-for, semantic parsing)(abstract syntax networks, Used-for, modeling unstructured inputs)(abstract syntax trees, Part-of, decoder with dynamically-determined modular structure)(new baseline results, Evaluate-for, sixteen novel datasets)(sentence splitting, Used-for, simplification operations)(machine translation, Used-for, simplification based on semantic parsing)(UCCA parsing, Compare, AMR)(UCCA parsing, Compare, SDP)(UCCA parsing, Compare, Universal Dependencies)(multitask learning, Conjunction, uniform transition-based system)(SHRG-based parser, Compare, previous data-driven models)(SHRG-based parser, Used-for, producing semantic graphs)(SHRG-based parser, Evaluate-for, elementary dependency match)(annotating NL utterances,
(machine reading comprehension, Hyponym-Of, reading comprehension)(machine reading comprehension, Used-for, question answering)(machine reading comprehension, Used-for, identifying answer spans)(machine reading comprehension, Compare, multi-passage reading comprehension)(question answering, Used-for, answering questions from a passage)(reinforcement learning, Used-for, sentence selection)(pointer networks, Used-for, locating answers in passages)(gated attention-based recurrent networks, Used-for, matching question and passage)(self-matching attention mechanism, Used-for, refining passage representation)(dynamic self-attention, Used-for, modeling long-range dependencies)(stochastic answer network, Used-for, multi-step reasoning)(state-of-the-art models, Compare, existing QA models)(world knowledge, Used-for, neural network architectures)(convolutional neural network, Used-for, text-based multiple choice question answering)(multi-turn question answering, Hyponym-Of, question answering).
(language model, Used-for, poetry generation)(neural language model, Part-of, first approach)(phonetic encoding, Used-for, learn representation of English poetry)(generative neural language model, Hyponym-Of, language model)(RNN, Hyponym-Of, neural language model)(stochastic gradient Markov Chain Monte Carlo, Used-for, learn weight uncertainty in RNNs)(Affect-LM, Hyponym-Of, neural language model)(LSTM, Hyponym-Of, neural language model)(MaxEnt reranker, Part-of, disfluency detection)(sequence-level loss smoothing, Used-for, train language models)(top-down construction, Evaluate-for, number agreement)(syntactic information, Is-a-Prerequisite-of, number agreement)(syntactic language model, Hyponym-Of, language model)(knowledge distillation, Used-for, improve LSTM performance)(diverse decoding strategies, Evaluate-for, generate diverse natural language)(COMET,
(discourse structure, Part-of, text categorization)(discourse structure, Used-for, understanding a text)(discourse structure, Evaluate-for, text classification)(discourse structure, Evaluate-for, sentence ordering)(discourse structure, Evaluate-for, predicting the outcome of a story)(discourse structure, Is-a-Prerequisite-of, news content structures)(Discourse Representation Theory, Hyponym-Of, discourse structure)(Rhetorical Structure Theory, Hyponym-Of, discourse structure)(representations of discourse, Hyponym-Of, discourse structure)(data programming, Evaluate-for, learning discourse structure)(RST discourse tree, Hyponym-Of, discourse structure)(language model, Evaluate-for, capturing discourse structure)(abstractive summaries, Evaluate-for, capturing discourse structure)(parsing techniques, Evaluate-for, UCCA)(parsing techniques, Evaluate-for, syntactic dependency parsing)(parsing techniques, Evaluate-for, semantic parsing)
(natural language generation nlg, Part-of, neural semantic parser)(neural semantic parser, Used-for, natural language utterances)(natural language utterances, Part-of, predicate-argument structures)(predicate-argument structures, Used-for, interpretation)(COREQA, Used-for, question answering system)(question answering system, Used-for, generate natural answers)(COREQA, Part-of, encoder-decoder frameworks)(encoding-decoding framework, Part-of, neural networks)(text similarity measures, Used-for, plagiarism detection)(text similarity measures, Used-for, information ranking)(text similarity measures, Use-for, recognition of paraphrases)(prerequisite skills, Evaluate-for, readability)(reading comprehension rc datasets, Evaluate-for, readability)(guessing task, Used-for, GuessTwo task)(GuessTwo, Conjunction, deep language understanding)(text similarity measures, Part-of, natural language understanding systems)(semantic-driven approaches, Compare, neural models)(naturalization, Is-a-Prerequisite-of, complex actions)(
(conversational machine reading, Compare, traditional single-turn machine reading comprehension)(conversational machine reading, Evaluate-for, extending MRC by multi-turn interactions)(conversational machine reading, Is-a-Prerequisite-of, comprehending context profoundly and efficiently)(conversational machine reading, Used-for, answering high-level user questions)(conversational machine reading, Part-of, conversational QA)(novel neural network model, Is-a-Prerequisite-of, comprehending conversation from different perspectives)(conversational machine reading, Compare, single-turn machine reading comprehension)(conversational machine reading, Used-for, learning conversational history)(conversational machine reading, Evaluate-for, efficiency from different perspectives).
(neural semantic parser, Evaluate-for, domain-general natural language representations)(neural semantic parser, Used-for, converting natural language utterances)(semantic parser, Used-for, mapping to target domains)(predicate-argument structures, Used-for, semantic parsing)(semantic parser, Trained-using, annotated logical forms)(semantic parser, Trained-using, denotations)(morph-fitting procedure, Used-for, improving distributional vector spaces)(morph-fitting procedure, Injects, morphological constraints)(downstream task, Evaluate-for, morph-fitted vectors)(neural machine translation, Evaluate-for, source languages)(neural machine translation, Evaluate-for, target languages)(discourse structure, Used-for, text categorization)(attention mechanism, Used-for, computing representation of text)(UCCA parser, Handles, general graph structures)(sequence-to-sequence models, Trained-using, AMR graphs)(geolocation prediction, Uses, complex neural network)(local coherence model, Uses, convolutional neural network
(coherent summary, Part-of, summarization)(summarization, Hyponym-Of, extractive summarization)(summarization, Hyponym-Of, abstractive summarization)(summarization, Used-for, generating summaries)(coherent summary, Is-a-Prerequisite-of, detecting salient information)(detecting salient information, Part-of, multi-document summarization)(multi-document summarization, Used-for, leveraging graph representations)(graph representations, Part-of, multi-document summarization)(coherent summary, Evaluate-for, maximum likelihood estimation)(coherent summary, Evaluate-for, neural abstractive multi-document summarization)(maximum likelihood estimation, Used-for, training summarization models)(Humor, Hyponym-Of, stylistic headline)(Romance, Hyponym-Of, stylistic headline)(Clickbait, Hyponym-Of, stylistic headline)(stylistic headline, Part-of, Stylistic Headline Generation (
(neural semantic parser, Part-of, natural language generation)(TASK-FLOW, Hyponym-Of, natural language generation)(COREQA, Part-of, natural language generation)(text similarity measures, Used-for, natural language generation)(Opinionated Natural Language Generation, Hyponym-Of, natural language generation)(Referring Expressions, Part-of, natural language generation)(reading comprehension (RC), Evaluate-for, natural language generation)(multilingual GeoQuery corpus, Used-for, natural language generation)(universal schema, Used-for, natural language generation)(ParaNMT-50M, Evaluate-for, natural language generation)(neural knowledge diffusion (NKD), Part-of, natural language generation)(chess commentary, Hyponym-Of, natural language generation)(BabbleLabble, Part-of, natural language generation)(multimodal social media posts, Used-for, natural language generation)
(contrastive visual semantic pretraining, Used-for, mitigating anisotropy)(contrastive visual semantic pretraining, Used-for, improving zero-shot performance)(contrastive visual semantic pretraining, Part-of, multilingual alignment)(contrastive visual semantic pretraining, Part-of, contrastive alignment pretraining)(contrastive visual semantic pretraining, Evaluate-for, intralayer self-similarity)(GPT-2, Compare, CLIP)(GPT-2, Part-of, contextualized English language representations)(GPT-2, Evaluate-for, word-level semantic intrinsic evaluation tasks)(CLIP, Part-of, zero-shot multimodal image classifier)(CLIP, Used-for, encoding image captions)(CLIP, Evaluate-for, word-level semantic intrinsic evaluation tasks)(CLIP, Evaluate-for, Spearman’s Modern)(mBERT, Compare, XLM-R)(mBERT, Compare, mT5-Large)(Spanglish, Conjunction, Hinglish
(orthography, Compare, standard orthography)(morphological feature, Part-of, word)(morphological feature, Compare, lexicalized feature)(morphological feature, Compare, non-lexicalized feature)(lexicalized feature, Conjunction, non-lexicalized feature)(lexicalized feature, Hyponym-Of, morphological feature)(non-lexicalized feature, Hyponym-Of, morphological feature)(lexicalized feature, Is-a-Prerequisite-of, better context modeling)(non-lexicalized feature, Is-a-Prerequisite-of, better context modeling)(lemmas, Hyponym-Of, lexicalized feature)(diacritized forms, Hyponym-Of, lexicalized feature)(gender, Hyponym-Of, non-lexicalized feature)(number, Hyponym-Of, non-lexicalized feature)(part-of-speech tags, Hyponym-Of,
(generate coherent informative comment, Part-of, automatic article commenting)(generate coherent informative comment, Used-for, encouraging user engagement)(generate coherent informative comment, Part-of, graph-to-sequence model)(graph-to-sequence model, Used-for, generating coherent and informative comments)(graph-to-sequence model, Part-of, comment generation)(comment generation, Part-of, sentence generation)(coherence, Conjunction, informativeness)(coherence, Conjunction, relevance)(informativeness, Conjunction, relevance)(graph-to-sequence model, Part-of, topic interaction graph)(topic interaction graph, Part-of, organizing the article into graph structure)(topic interaction graph, Used-for, better understanding internal structure)(topic interaction graph, Used-for, generating coherent comments)(news document, Part-of, online news platforms)(news document, Part-of, Tencent Kuaibao)(Tencent Kuaibao, Hyponym-Of, Chinese online news platform)(online news platforms, Used-for, automatic article
(None)
(word embeddings, Part-of, neural network architecture)(word embeddings, Used-for, document analysis)(neural word segmentation, Use-for, pretraining character embeddings)(statistical segmentation, Use-for, leveraging external information)(embedding learning, Evaluate-for, cross-lingual word similarity evaluation)(word embeddings, Used-for, enhancing text classification)(pre-trained word embeddings, Used-for, neural network architectures for NLP tasks)(embedding spaces, Part-of, bilingual dictionaries)(Word segmentation, Used-for, modular segmentation model)(Statistical segmentation, Compare, Neural word segmentation)(bilingual word embeddings, Hyponym-Of, word embeddings)(text classification, Used-for, sentiment analysis)(word embeddings, Hyponym-Of, domain-specific embeddings)(embedding learning, Used-for, evaluating text cluster distributions)(recurrent neural networks, Used-for, part-of-speech tagging)(word embeddings, Used-for, entity recognition and chunking)(document clustering, Use-for, extracting pseudo-parallel sentences)(word representations, Used
(extractive summarization, Is-a-Prerequisite-of, query-based summarization)(extractive summarization, Part-of, summarization)(extractive summarization, Compare, abstractive summarization)(query-based summarization, Part-of, summarization)(query-based summarization, Used-for, extracting relevant points from document)(abstractive summarization, Hyponym-Of, summarization)(sequence-to-sequence model, Used-for, extractive summarization)(sequence-to-sequence model, Used-for, abstractive summarization)(sequence-to-sequence model, Part-of, neural sequence-to-sequence models)(neural sequence-to-sequence models, Hyponym-Of, neural models)(neural models, Hyponym-Of, machine learning)(sentence encoder, Part-of, neural sequence-to-sequence models)(summary, Used-for, evaluating summarization systems)
(distributional semantics model, Used-for, semantic composition models)(distributional semantics model, Used-for, hypernymy modeling)(distributional semantics model, Compare, neural network models)(distributional semantics model, Used-for, detecting compositionality)(distributional semantics model, Part-of, distributional vector space models)(distributional semantics model, Compare, Functional Distributional Semantics)(distributional semantics model, Used-for, semantic textual similarity tasks)(distributional semantics model, Used-for, downstream tasks)(distributional semantics model, Conjunction, distributional information)(distributional semantics model, Compare, pattern-based methods)(distributional semantics model, Compare, siamese network)(distributional semantics model, Used-for, decoding patterns of brain activity)(distributional semantics model, Compare, pretrained Transformers)(distributional vector space models, In-contrast, curated semantic lexicons)(peeking a curated semantic lexicon collects morphological constraints, pulling inf
(satire detection, Compare, sarcasm detection)(satire detection, Compare, irony detection)(satire detection, Used-for, detecting subtle forms of language)(satire detection, Part-of, NLP)(sarcasm detection, Part-of, satire detection)(irony detection, Part-of, satire detection)(sarcasm detection, Used-for, multi-modal approaches)(irony detection, Used-for, disambiguating linguistic information).
(trained language, Part-of, neural semantic parser)(neural semantic parser, Used-for, converting natural language utterances)(neural semantic parser, Used-for, generating intermediate representations)(intermediate representations, Hyponym-Of, predicate-argument structures)(predicate-argument structures, Used-for, mapping to target domains)(trained language, Part-of, neural sequence models)(neural sequence models, Used-for, mapping utterances to SQL)(neural sequence models, Used-for, semantic parsing)(trained language, Part-of, neural MT models)(neural MT models, Evaluate-for, learning morphology)(morphology, Part-of, source language)(trained language, Used-for, developing TextFlow)(TextFlow, Used-for, computing text similarity)(text similarity, Used-for, paraphrase detection)(trained language, Part-of, neural architecture)(neural architecture, Used-for, grammar model)(grammar model, Used-for, capturing target syntax)(trained language, Part-of, named entity recognition)(
(multilingualism, Part-of, multilingual learning)(multilingual learning, Part-of, multilingual named entity recognition)(multilingual semantic dependency parsing, Used-for, understanding similarities and differences between languages)(multilingual neural machine translation, Used-for, multilingual translation)(language identification, Is-a-Prerequisite-of, processing multilingual text)(multilingual embeddings, Used-for, learning multilingual distributed representations of text)(multilingual embeddings, Used-for, multilingual neural machine translation)(multilingual GeoQuery corpus, Used-for, evaluating multilingual semantic parsing)(ATIS corpus, Used-for, evaluating multilingual semantic parsing)(Twitter, Used-for, studying public sentiments)(multilingual connotation frames, Used-for, analyzing targeted public sentiments)(multilingual multi-task architecture, Is-a-Prerequisite-of, sequence labeling)(pretrained contextual embeddings, Part-of, multilingual pre-trained models)(pretrained non-contextual subword embeddings, Part-of, multilingual pre-trained models)(BERT, Compare, FastText)(BERT, Compare,
(text generation, Part-of, artificial intelligence)(neural network models, Used-for, text generation)(sequence-to-sequence models, Used-for, text generation)(abstractive summarization, Hyponym-Of, text generation)(video captioning, Hyponym-Of, text generation)(conversational text generation, Hyponym-Of, text generation)(automatic question generation, Hyponym-Of, text generation)(semantic parsing, Hyponym-Of, text generation)(AMR-to-text generation, Hyponym-Of, text generation)(text similarity measures, Evaluate-for, text generation)(sequence-to-sequence models, Part-of, neural network models)(abstractive summarization, Is-a-Prerequisite-of, improved summarization techniques)(affect-discriminative word representations, Used-for, conversational text generation)(Affect-LM, Used-for, conversational text generation)(Multi-task learning, Used-for, video captioning)(
(extractive summarization model, Used-for, sentence scoring)(extractive summarization model, Used-for, sentence selection)(extractive summarization model, Hyponym-Of, summarization)(extractive summarization model, Part-of, sequence-to-sequence model)(sentence scoring, Part-of, extractive summarization model)(sentence selection, Part-of, extractive summarization model)(extractive summarization model, Compare, abstractive summarization model)(extractive summarization, Hyponym-Of, summarization)(abstractive summarization, Hyponym-Of, summarization)(neural sequence-to-sequence model, Hyponym-Of, extractive summarization model)(extractive summarization system, Compare, supervised models)(extractive summarization, Part-of, summarization)(SWAP-NET, Hyponym-Of, extractive summarization model)
(treebank embeddings, Hyponym-Of, embeddings)(embeddings, Part-of, NLP tools)(treebank embeddings, Used-for, natural language processing tasks)(embeddings, Used-for, downstream NLP tasks)(NLP tasks, Evaluate-for, embeddings).
(multilingual neural machine translation, Conjunction, neural machine translation)(multilingual neural machine translation, Conjunction, machine translation)(neural machine translation, Part-of, neural models)(neural machine translation, Compare, statistical machine translation)(syntactic information, Is-a-Prerequisite-of, neural machine translation)(parallel corpus, Used-for, multilingual neural machine translation)(bidirectional translation, Used-for, multilingual neural machine translation)(target language, Used-for, multilingual neural machine translation)(neural network-based lexicon, Part-of, neural models)(posterior regularization, Used-for, neural machine translation)(error-correcting codes, Used-for, neural machine translation)(multi domain NMT, Compare, fine tuning)(multi domain NMT, Conjunction, fine tuning)(prior knowledge, Hyponym-Of, features in a log-linear model)(source sentences, Part-of, neural machine translation)(BLEU scores, Evaluate-for, neural machine translation)
(commonsense knowledge base, Compare, canonical templates)(commonsense knowledge base, Part-of, Skip-gram framework)(commonsense knowledge base, Used-for, word representation learning)(commonsense knowledge base, Used-for, commonsense knowledge extraction)(commonsense knowledge base, Used-for, natural language understanding)(commonsense knowledge base, Used-for, question answering)(word sememes, Part-of, commonsense knowledge base)(sememe information, Used-for, word representation learning)(lexical sememe prediction, Used-for, annotation efficiency)(commonsense knowledge, Part-of, commonsense knowledge base).
(adversarial attack, Used-for, generating adversarial examples)(adversarial examples, Part-of, neural machine translation)(adversarial examples, Used-for, text classification)(adversarial examples, Used-for, machine vision and perception)(adversarial examples, Part-of, adversarial training)(adversarial training, Is-a-Prerequisite-of, robustness)(neural machine translation, Used-for, language translation)(neural machine translation, Conjunction, text classification)(text classification, Used-for, emotion regression)(emotion regression, Part-of, natural language processing)(emotion regression, Used-for, rating emotion dimensions)(rating emotion dimensions, Part-of, input text)(rating emotion dimensions, Conjunction, word weights)(word weights, Part-of, attention layer)(attention layer, Part-of, neural networks).
(bias category, Part-of, CHBias)(CHBias, Used-for, bias evaluation)(CHBias, Used-for, bias mitigation) (CHBias, Evaluate-for, Chinese conversational language models)(Chinese conversational language models, Used-for, conversational capabilities)(pretrained Chinese conversational models, Evaluate-for, biases)(bias categories, Part-of, CHBias)(ageism, Hyponym-Of, bias category)(appearance biases, Hyponym-Of, bias category)(debiasing methods, Used-for, making response generation less biased)
(word embeddings, Compare, object recognition)(word embeddings, Part-of, machine translation methods)(word embeddings, Used-for, sentiment classification)(word embeddings, Used-for, document clustering)(word embeddings, Used-for, named entity recognition)(word embeddings, Used-for, bilingual lexicon induction)(object names, Part-of, referring expression generation)(object names, Compare, mutually exclusive labels)(referring word meaning, Hyponym-Of, distributional word embeddings)(context-sensitive embeddings, Part-of, neural network architectures)(context embeddings, Used-for, NLP tasks)(recurrent network, Used-for, word-level representations)(bilingual word embeddings, Use-for, cross-lingual sentiment approaches)(semantic concepts, Part-of, WordNet)(semantic concepts, Compare, synsets)(prediction, Part-of, prepositional phrase attachment)(latent meanings, Used-for, word embeddings)(bilingular dictionaries, Used-for, cross-lingual connections)(embed semantic concepts, Used-for,
(abstractive summarization, Compare, extractive summarization)(abstractive summarization, Hyponym-Of, text summarization)(abstractive summarization, Used-for, generating novel summaries)(abstractive summarization, Used-for, avoiding repetition)(encode-attend-decode paradigm, Used-for, abstractive summarization)(query attention model, Part-of, query-based summarization)(diversity based attention model, Part-of, query-based summarization)(hybrid pointer-generator network, Used-for, accurate reproduction of information)(coverage mechanism, Used-for, reducing repetition)(selective encoding model, Hyponym-Of, sequence-to-sequence framework)(sentence encoder, Part-of, selective encoding model)(selective gate network, Part-of, selective encoding model)(attention equipped decoder, Part-of, selective encoding model)(graph-based attention mechanism, Used-for, abstractive document summarization)(ROUGE-L scores, Evaluate-for, model performance)(question generation
(topic aware, Compare, single model)(topic aware, Compare, topic-specific model)(topic aware, Part-of, identification of good arguments)(stance detection, Part-of, topic aware)(SciTail, Part-of, topic aware)(DEISTE, Used-for, SciTail)(training factuality-aware models, Compare, model quality)(inter-topic preferences, Part-of, topic aware)(OpenBookQA, Part-of, topic aware).
(language model, Part-of, neural networks)(neural networks, Used-for, language understanding)(neural semantic parser, Used-for, semantic parsing)(neural language model, Part-of, question answering system)(CoreQA, Hyponym-Of, question answering system)(language model, Evaluate-for, sentence level)(FOIL-COCO, Evaluate-for, LaVi models)(word vector collection, Used-for, semantic quality improvement)(morph-fitting procedure, Used-for, improving distributional vector spaces)(language, Used-for, inducing accurate representations)(REINFORCE, Used-for, task reward)(language model, Evaluate-for, language model perplexity)(Deep neural networks, Compare, kernel methods)(morphologically rich languages, Compare, other languages)(neural language model, Compare, sentence-based model)(language model, Evaluate-for, broader document context)(syntactic parsers, Used-for, improving semantic role labeling).
(vision language navigation, Compare, BabyWalk)(vision language navigation, Used-for, instruction following)(vision language navigation, Evaluate-for, goal completion)(vision language navigation, Compare, ProbES)(vision language navigation, Evaluate-for, Coverage weighted by Length Score)(Coverage weighted by Length Score, Evaluate-for, Room-to-Room dataset)(coverage weighted by Length Score, Evaluate-for, Room-for-Room dataset)(vision language navigation, Part-of, multimodal learning)(multimodal learning, Compare, cross-modal language generation)(cross-modal language generation, Used-for, image captioning)(image captioning, Evaluate-for, non-English languages)(PLuGS, Evaluate-for, image captioning)(PLuGS, Part-of, cross-modal language generation)(vision language navigation, Used-for, autonomous agents)(autonomous agents, Evaluate-for, generalization ability)(BabyWalk, Evaluate-for, long instructions)(BabyWalk, Used-for, learning from short instructions)(Neural module networks, Used-for,
(fact-checking datasets, Hyponym-Of, datasets)(fact-checking datasets, Used-for, fact-checking)(fact-checking datasets, Evaluate-for, neural summarization models)(fact-checking datasets, Compare, premise articles)(fact-checking datasets, Used-for, factual error correction systems)(fact-checking datasets, Used-for, claim matching)(fact-checking datasets, Is-a-Prerequisite-of, automated fact-checking models)(fact-checking datasets, Part-Of, FAVIQ)(claim matching, Used-for, scale fact-checking)(factual error correction, Hyponym-Of, fact verification)(claim matching, Evaluate-for, embedding model)(evidence retrieval, Part-of, fact-checking in dialogue)(claim verification, Part-of, fact-checking in dialogue)(claim veracity, Evaluate-for, premise articles)(FAVIQ, Hyponym-Of, fact-checking datasets)
(grammatical error correction gec, Used-for, correcting both global errors and local errors)(grammatical error correction gec, Used-for, evaluating text-to-text generation)(grammatical error correction gec, Used-for, improving seq2seq text generation)(grammatical error correction gec, Evaluate-for, accuracy of corrections)(grammatical error correction gec, Part-of, neural machine translation)(nested attention layers, Used-for, grammatical error correction gec)(ERRANT, Used-for, grammatical error correction gec)(SEQ2SEQ models, Hyponym-Of, neural sequence-to-sequence models)(fluency boost learning, Used-for, seq2seq models)(fluency boost inference, Used-for, seq2seq models)(nested attention layers, Part-of, new hybrid neural model)(ERRANT, Evaluate-for, error type performance)(substitute, Part-of, new dependency parsing scheme)(DELETE, Part-of, new dependency parsing scheme)
(aspect-based sentiment analysis, Hyponym-Of, sentiment analysis)(sentiment analysis, Part-of, natural language processing)(aspect extraction, Part-of, aspect-based sentiment analysis)(sentiment polarity, Part-of, aspect-based sentiment analysis)(classification process, Used-for, aspect-based sentiment analysis)(attention mechanism, Used-for, improving coherence of aspects)(neural word embeddings, Used-for, improving coherence of aspects)(domain adaptation, Used-for, sentiment analysis)(sentiment feature distribution, Evaluate-for, performance of sentiment classifiers)(active learning mode, Used-for, selecting and annotating samples)(multimodal sentiment analysis, Hyponym-Of, sentiment analysis)(utterances, Part-of, videos in multimodal sentiment analysis)(LSTM-based model, Used-for, capturing contextual information from utterances)(cross-domain sentiment classification, Hyponym-Of, sentiment analysis)(polarity preserving significant words, Used-for, cross-domain sentiment classification)(χ2 test
(neural topic modeling, Used-for, topic coherence)(neural topic modeling, Used-for, document clustering)(neural topic modeling, Hyponym-Of, neural network models)(Wasserstein autoencoders, Hyponym-Of, neural topic modeling)(Wasserstein autoencoders, Used-for, Dirichlet prior on latent document-topic vectors)(Maximum Mean Discrepancy, Used-for, minimizing distribution matching in WAE)(Maximum Mean Discrepancy, Compare, Generative Adversarial Network)(topic uniqueness metric, Used-for, measuring diversity in topics)(coherence measure NPMI, Used-for, evaluating topic quality)(neural network models, Used-for, cross-lingual transfer learning)(neural network models, Is-a-Prerequisite-of, multilingual setup)(adversarial networks, Used-for, learning language-invariant features)(mixtures-of-experts models, Used-for, exploiting similarity between languages)
(relation extraction method, Part-of, information extraction)(relation extraction method, Conjunction, distant supervision)(relation extraction method, Conjunction, neural Open IE)(relation extraction method, Conjunction, PKBP)(relation extraction method, Conjunction, BONIE)(relation extraction method, Conjunction, DSGAN)(relation extraction method, Part-of, Knowledge Base Population)(PKBP, Used-for, knowledge base enrichment)(PKBP, Conjunction, Open Information Extraction)(distant supervision, Used-for, relation extraction)(distant supervision, Used-for, noise reduction)(distant supervision method, Evaluate-for, model performance)(relation extraction, Hyponym-Of, relation extraction method)(neural Open IE, Evaluate-for, benchmark dataset)(BONIE, Conjunction, bootstrapping)(graph-based neural network model, Evaluate-for, relation extraction)(Open Information Extraction, Used-for, relation extraction).
(task question answering, Used-for, reading comprehension)(task question answering, Is-a-Prerequisite-of, evaluating generation systems)(task question answering, Part-of, knowledge base question answering)(task question answering, Is-a-Prerequisite-of, question representation)(task question answering, Conjunction, retrieval component and comprehension component)(task question answering, Hyponym-Of, open-domain question answering)(task question answering, Compare, semantic parsing).
(deep learning based Chinese, Evaluate-for, Chinese NER)(deep learning based Chinese, Evaluate-for, Chinese Natural Language Processing)(deep learning based Chinese, Is-a-Prerequisite-of, neural char-based models)(deep learning based Chinese, Compare, neural word-based models)(deep learning based Chinese, Used-for, Chinese word segmentation)(deep learning based Chinese, Part-of, NLP)(deep learning-based Chinese, Compare, word-based models).
(neural word, Used-for, segmentation)(neural word, Used-for, embeddings)(neural word, Part-of, neural word segmentation)(neural word, Part-of, embeddings of character strings)(POS tagging, Part-of, embeddings of character strings)(dependency parsing, Part-of, embeddings of character strings)(joint models, Is-a-Prerequisite-of, preventing error propagation)(error propagation, Evaluate-for, pipeline models)(neural network-based joint models, Used-for, Chinese word segmentation)(neural network-based joint models, Used-for, POS tagging)(neural network-based joint models, Used-for, dependency parsing)(neural approaches, Compare, pipeline models).
(extractive qa, Compare, query-based summarization)(extractive qa, Compare, abstractive summarization)(extractive qa, Part-of, extractive summarization)(extractive summarization, Conjunction, abstractive summarization)(extractive summarization, Used-for, extractive qa)(extractive qa, Evaluate-for, ROUGE-L scores)(extractive qa, Is-a-Prerequisite-of, user feedback)(query-based summarization, Compare, extractive qa)(relation extraction, Used-for, extractive qa)(extractive qa, Compare, SQuADRUn)(aspect sentiment classification, Compare, extractive qa)(aspect term extraction, Compare, extractive qa)
(chinese named entity recognition, Part-of, named entity recognition)(named entity recognition, Used-for, information extraction)(chinese named entity recognition, Used-for, information extraction)(gazetteers, Used-for, chinese named entity recognition)(gazetteers, Used-for, named entity recognition)(graph neural networks, Used-for, chinese named entity recognition)(graph neural networks, Used-for, named entity recognition)(LSTM-CRF, Used-for, named entity recognition)(neural transfer method, Used-for, named entity recognition)(Dual Adversarial Transfer Network, Hyponym-Of, neural transfer method)(Dual Adversarial Transfer Network, Used-for, chinese named entity recognition)(Dual Adversarial Transfer Network, Used-for, information extraction)(named entity recognition, Is-a-Prerequisite-of, cross-lingual transfer)(cross-lingual transfer, Used-for, chinese named entity recognition).
(neural semantic parser, Part-of, semantic parsing)(predicate-argument structures, Used-for, natural language representations)(neural semantic parser, Evaluate-for, SPADES)(neural semantic parser, Evaluate-for, GRAPHQUESTIONS)(answer rationales, Used-for, generating natural answers)(semantic units, Part-of, natural answer)(semantic units, Used-for, copying)(semantic units, Used-for, retrieving)(COREQA, Used-for, question answering)(Text similarity measures, Used-for, plagiarism detection)(TextFlow, Is-a-Prerequisite-of, Text similarity measures)(GuessTwo, Used-for, evaluating common entities)(physical knowledge, Part-of, common sense knowledge)(ITransF, Used-for, knowledge base completion)(SQL, Part-of, database querying)(semantic parser, Used-for, converting natural language to programs)(reinforcement learning, Used-for, training semantic parsers)(maximum marginal likelihood, Used-for, training semantic parsers)(bilexical dependencies, Part-of, statistical
(explanation prediction, Used-for, common-sense question answering)(common-sense properties, Part-of, explanation prediction)(correct answer choice, Part-of, explanation prediction)(incorrect answer choices, Part-of, explanation prediction)(commonsense knowledge, Used-for, explanation prediction)(explanation prediction, Compare, question answering models).
(detecting, Used-for, emotion detection)(detecting, Used-for, detecting contractual obligations)(detecting, Used-for, detecting prohibitions)(detecting, Used-for, detecting critical plot twists)(detecting, Used-for, detecting protected health information)(detecting, Used-for, detecting spelling errors)(detecting, Used-for, detecting spoiler sentences)(detecting, Used-for, detecting training data quality)(detecting, Used-for, detecting language mistakes)(detecting, Used-for, detecting personal disclosures)(detecting, Used-for, detecting sentiment)(detecting, Used-for, detecting commonsense causal relations)(detecting, Used-for, detecting employment status)(correcting, Used-for, correcting spelling errors)(correcting, Used-for, correcting language mistakes)(emotion detection, Used-for, emotional chatbots)(emotion detection, Used-for, better understanding individuals)(deep learning models, Used-for, emotion detection)(h
(named entity recognition, Hyponym-Of, information extraction)(named entity recognition, Hyponym-Of, mention detection)(named entity recognition, Hyponym-Of, part-of-speech induction)(named entity recognition, Is-a-Prerequisite-of, cross-lingual NER)(named entity recognition, Is-a-Prerequisite-of, multilingual named entity recognition)(named entity recognition, Used-for, evaluating methods on news articles)(named entity recognition, Used-for, evaluating methods on biomedical abstracts)(named entity recognition, Used-for, big data scenarios)(named entity recognition, Used-for, small data scenarios)(named entity recognition, Compare, traditional sequence labeling)(named entity recognition, Compare, local detection approach)(named entity recognition, Compare, rule-based negation detection)(named entity recognition, Compare, multi-task problem)(named entity recognition, Part-of, embedding knowledge graphs)(named entity recognition, Part-of, neural named entity recognition)(named entity recognition, Part-of, CoNLL 
(datasets, Part-of, reading comprehension datasets)(reading comprehension datasets, Is-a-Prerequisite-of, natural-language understanding systems)(RC datasets, Evaluate-for, prerequisite skills)(RC datasets, Evaluate-for, readability)(MCTest, Hyponym-Of, RC datasets)(SQuAD, Hyponym-Of, RC datasets)(linguistically diverse datasets, Used-for, training and evaluating robust machine learning systems)(data collection, Used-for, dataset construction)(crowdsourcing, Used-for, paraphrase generation)(paraphrase generation, Evaluate-for, correctness)(paraphrase generation, Evaluate-for, grammaticality)(paraphrase generation, Evaluate-for, linguistic diversity)(paraphrase generation, Compare, accuracy and diversity in crowd responses)(scalar annotations, Used-for, dataset construction)(scalar annotations, Used-for, system quality estimation by human judgments)(direct assessment, Compare, online pairwise ranking aggregation)(online pairwise ranking aggregation, Compare, hybrid approach EASL)(Conceptual
(syntactic parsing, Part-of, Natural Language Processing)(syntactic parsing, Evaluate-for, Parsing sentences to linguistically-expressive semantic representations)(Natural Language Processing, Used-for, extracting time expressions from free text)(free text, Part-of, Natural Language Processing)(time expressions, Compare, compositional questions)(time token, Hyponym-Of, syntactic token types)(time expressions, Part-of, SynTime)(morpho-syntactic regularities, Hyponym-Of, syntactic parsing)(syntactic information, Used-for, neural machine translation)(semantic graph generation, Used-for, semantic parsing)(semantic parsing, Part-of, Natural Language Processing)(syntax-aware system, Compare, syntax-agnostic NMT baseline)
(word embeddings trained, Used-for, leveraging large-scale raw texts).(word embeddings trained, Used-for, bilingual word embeddings).(word embeddings trained, Used-for, pretraining character and word embeddings).(word embeddings trained, Used-for, capturing linguistic regularities).(word embeddings trained, Used-for, embedding semantic concepts).(word embeddings trained, Used-for, learning discourse-specific word embeddings).(word embeddings, Compare, Bag of Words).(word embeddings, Used-for, aspect extraction).(word embeddings, Used-for, predicting volatility).(word embeddings, Used-for, evaluating word similarity).(word embeddings, Used-for, dependency parsing).(word embeddings, Part-of, neural word segmentation).(word embeddings, Used-for, synonymy dictionaries).(word embeddings, Used-for, sentiment analysis).(word embeddings, Used-for, clustering empirical distributions).(word embeddings, Used-for, pseudo-parallel sentences).(word embeddings, Used-for, identifying dialectal terms).(word embeddings, Used-for, sequence labeling).(word embeddings, Used-for, prepositional phrase
(dialogue learning, Compare, end-to-end learning)(Hybrid Code Networks, Used-for, dialogue learning)(HCNs, Evaluate-for, dialog state inference)(recurrent neural networks, Used-for, dialog systems)(HCNs, Used-for, reducing training data requirements)(multi-turn response selection, Part-of, dialogue learning)(conversational interaction, Part-of, dialogue learning)(Deep Dyna-Q, Evaluate-for, dialogue policy learning)(Deep Dyna-Q, Compare, reinforcement learning)(dialogue learning, Compare, imitation learning)(dialogue learning, Hyponym-Of, language learning)(dialogue agent, Part-of, dialogue learning)(Rewriting, Used-for, multi-turn dialogue modeling)(Transformer, Used-for, attention mechanism)(semantic and functional dependencies, Part-of, dialogue learning)(semantic parsing, Used-for, dual learning algorithm)(semantic parsing, Evaluate-for, logical form generation).
(Automatic argument, Evaluate-for, argument mining)(argument mining, Used-for, argumentative relation prediction)(argument mining, Is-a-Prerequisite-of, elementary unit type classification)(Automatic argument, Conjunction, automatic question generation)(argumentative relation prediction, Hyponym-Of, event detection)(event detection, Used-for, identifying and categorizing events)(argument information, Part-of, event detection)(Automatic argument, Conjunction, automatic essay scoring)(essay scoring, Evaluate-for, discourse modes)(Automatic argument, Conjunction, supporting argument detection)(Automatic argument, Compare, opinionated natural language generation)(opinionated natural language generation, Used-for, generating subjective responses)(Automatic argument, Conjunction, good argument detection)(Automatic argument, Conjunction, argument validity)
(trained language model plms, Is-a-Prerequisite-of, poetry generation)(trained language model plms, Used-for, parse trees and sentences)(trained language model plms, Used-for, language modeling)(trained language model plms, Compare, discriminative models)(trained language model plms, Used-for, LSTM language modeling)(trained language model plms, Evaluate-for, previous context use)(trained language model plms, Hyponym-Of, neural language models)(trained language model plms, Used-for, hierarchical attention)(trained language model plms, Part-of, hierarchical multi-scale language model)(trained language model plms, Used-for, elastically consolidated weights)(trained language model plms, Is-a-Prerequisite-of, r-RNTNs)(trained language model plms, Used-for, query auto-completion)(trained language model plms, Used-for, text classification).
(encoder-decoder dialog model, Used-for, dialog systems)(unsupervised discrete sentence representation learning method, Integrates-with, encoder-decoder dialog models)(DI-VAE, Improves, variational autoencoders)(DI-VST, Improves, variational autoencoders)(sequence-to-sequence models, Used-for, NLP)(softmax transformation, Part-of, sequence-to-sequence models)(sparse sequence-to-sequence models, Compare, dense sequence-to-sequence models)(PhotoBook dataset, Used-for, visually-grounded task-oriented dialogues)(Model-Agnostic Meta-Learning, Used-for, personalized dialogue learning)(reference chain, Part-of, shared information)(dataset, Compare, baseline model)(neural networks, Improve, distributed representations of words)(rule-based models, Compare, neural models)(neural networks, Part-of, deep models)(state-of-the-art models, Compare, rule-based models)(statistical hypothesis testing, Used-for, multi-label classification)(Prolog
(current state nlp, Part-of, state-of-the-art)(state-of-the-art, Evaluate-for, AMR-to-text generation)(state-of-the-art, Evaluate-for, sequence-to-sequence model)(current state nlp, Compare, standard benchmark)(state-of-the-art, Compare, existing methods)(sequence-to-sequence model, Is-a-Prerequisite-of, sequence labeling)(sequence labeling, Used-for, named entity recognition)(sequence labeling, Used-for, chunking)(neural graph-to-sequence model, Compare, sequence-to-sequence model)(neural graph-to-sequence model, Evaluate-for, graph-level semantics)(neural architecture, Used-for, Word Sense Disambiguation)(neural architecture, Compare, statistical script learning)(sequence-to-sequence model, Part-of, abstractive text summarization)(state-of-the-art, Evaluate-for, multilingual WSD)(sequence labeling, Hyponym-Of, NLP tasks)(graph-to-text generation, Used-for, sentence generation)(
(neural networks, Used-for, language understanding)(neural networks, Used-for, symbolic reasoning)(neural programmer, Part-of, Neural Symbolic Machine)(symbolic computer, Part-of, Neural Symbolic Machine)(sequence-to-sequence model, Part-of, neural programmer)(Lisp interpreter, Part-of, symbolic computer)(REINFORCE, Used-for, optimize task reward)(iterative maximum-likelihood training, Used-for, improve stability of REINFORCE)(WebQuestionsSP dataset, Used-for, evaluation)(reading comprehension datasets, Used-for, development of natural-language understanding systems)(prerequisite skills, Evaluate-for, reading comprehension datasets)(readability, Evaluate-for, reading comprehension datasets)(belief tracker, Part-of, spoken dialogue systems)(Spoken Language Understanding models, Used-for, belief tracker)(hand-crafted lexicons, Used-for, belief tracker)(Neural Belief Tracking, Hyponym-Of, belief tracker)(pre-trained word vectors, Used-for, Neural Bel
(visual question answering, Is-a-Prerequisite-of, question answering)(question answering, Hyponym-Of, visual question answering)(visual question answering, Compare, tabular question answering)(visual question answering, Part-of, visual reasoning)(question answering, Part-of, machine comprehension)(dynamic neural semantic parsing, Evaluate-for, visual question answering)(derivational morphology, Used-for, visual question answering)(question representation, Used-for, neural network-based question answering)(neural network models, Evaluate-for, question answering)(distantly supervised question answering, Evaluate-for, question answering)(convolutional neural network, Used-for, visual question answering)(paragraph-level question answering, Compare, visual question answering)(EviNets, Evaluate-for, question answering)(KB-QA, Compare, visual question answering)
(relation extraction, Compare, chinese relation extraction)(chinese relation extraction, Conjunction, multi-lingual neural relation extraction framework)(multi-lingual neural relation extraction framework, Used-for, enhancing learning from texts in various languages)(mono-lingual attention, Used-for, utilizing information within mono-lingual texts)(cross-lingual attention, Used-for, considering information consistency among cross-lingual texts)(distant supervision, Used-for, building training data for classification tasks)(dynamic transition matrix, Used-for, characterizing noise in training data)(query-based summarization, Used-for, highlighting points relevant to a given query)(encode-attend-decode paradigm, Used-for, generating summaries and translations)(query attention model, Part-of, query-based summarization)(diversity based attention model, Part-of, query-based summarization)(external knowledge bases, Part-of, enhancing recurrent neural networks for machine reading)(KBLSTM, Used-for, integrating background knowledge in machine reading)(ne
(neural network models, Compare, recurrent neural networks)(neural network models, Compare, convolutional neural networks)(convolutional networks, Hyponym-Of, neural networks)(recurrent networks, Hyponym-Of, neural networks)(adversarial multi-task learning, Used-for, sharing latent feature spaces)(multi-task learning, Used-for, extracting common features)(ULMFiT, Used-for, domain sentiment classification)(domain adaptation, Used-for, domain sentiment classification)(hybrid contextualized sentiment classifier, Used-for, domain sentiment classification)(rumor detection, Compare, domain sentiment classification)(Picturebook embeddings, Used-for, semantic relatedness)(discourse-specific word embeddings, Used-for, discourse relation recognition)(string kernels, Used-for, essay grading)(domain specific word embeddings, Compare, generic word embeddings)(Multi-sentiment-resource Enhanced Attention Network, Used-for, sentiment classification)(cold-start aware attention, Used-for, handling limited data)(inductive transfer learning, Used-for
(AMR graphs, Part-of, visual semantic)(semantic role labeling, Evaluate-for, visual semantic)(semantic relevance, Evaluate-for, visual semantic)(semantic relatedness,Evaluate-for,visual semantic)(semantic parsing, Used-for,visual semantic)(semantic analysis, Part-of,visual semantic)(graph-to-sequence model, Used-for,visual semantic)(hierarchical attention network, Part-of,visual semantic)(semantic frame induction, Part-of,visual semantic)(dynamic spatial memory network, Evaluate-for,visual semantic).
(speech translation, Part-of, End-to-end automatic speech recognition)(End-to-end automatic speech recognition, Used-for, translating speech)(End-to-end automatic speech recognition, Compare, conventional DNN/HMM systems)(End-to-end automatic speech recognition, Part-of, neural machine translation)(neural machine translation, Compare, statistical machine translation)(connectionist temporal classification, Conjunction, attention-based methods)(connectionist temporal classification, Part-of, End-to-end automatic speech recognition)(attention-based methods, Conjunction, connectionist temporal classification)(hybrid CTC/attention architecture, Hyponym-Of, End-to-end automatic speech recognition)(hybrid CTC/attention architecture, Evaluate-for, automatic speech recognition benchmarks)(decode, Used-for, translating speech)(connectionist temporal classification, Used-for, automatic speech recognition)
(Neural Machine Translation, Compare, Phrase-based Machine Translation)(Neural Machine Translation, Used-for, Translation Tasks)(Bi-directional LSTMs, Used-for, Encoding Source Sentence)(Convolutional Layers, Compare, Recurrent Networks)(Convolutional Layers, Used-for, Encoding Source Sentence)(Chunk-based Decoders, Conjunction, Word-level Decoders)(Deep Neural Networks, Enhance, Neural Machine Translation)(Layer-wise Relevance Propagation, Used-for, Interpret NMT)(Posterior Regularization, Used-for, Integrating Prior Knowledge)(Hybrid Model NMT+RNNG, Used-for, Parsing and Translating)(Unsupervised NMT, Enhances, Cross-language Translation)(Subword Regularization, Improve, NMT Robustness)(Gated Graph Neural Networks, Compare, Standard Recurrent Networks)(Gated Graph Neural Networks, Used-for, Encoding Graph Structure)(Neural Machine Translation, Compare, Deep Generative Model
(attention mechanism, Is-a-Prerequisite-of, attention weight)(memory mechanism, Used-for, balancing linguistic accordance and aesthetic innovation)(memory mechanism, Used-for, generating poems with different styles)(neural model, Part-of, sequence-to-sequence neural models)(Chinese poem generation, Used-for, innovative generations that are still rule-compliant)(memory augmented neural model, Used-for, Chinese poem generation)(Multi-modal Neural Machine Translation, Part-of, neural models)(doubly-attentive decoder, Used-for, incorporating spatial visual features)(convolutional neural networks, Used-for, obtaining spatial visual features)(back-translated in-domain multi-modal data, Used-for, efficient exploitation by NMT)(back-translated in-domain multi-modal data, Evaluate-for, NMT model efficiency)(morphological inflection generation, Part-of, neural model)(monotonic alignment, Is-a-Prerequisite-of, morphological inflection generation)(generative conversational system, Part-of, natural language
(learning word embedding, Used-for, linking visual to lexical information)(learning word embedding, Used-for, capturing linguistic regularities)(learning word embedding, Used-for, investigating structural similarity of embedding spaces)(learning word embedding, Used-for, pretraining character and word embeddings)(learning word embedding, Used-for, representing short texts produced in neuropsychological assessments)(learning word embedding, Used-for, forecasting volatility in financial markets)(learning word embedding, Used-for, bilingual lexicon induction)(learning word embedding, Used-for, improving PP attachment model)(learning word embedding, Used-for, obtaining meaningful representations of words)(learning word embedding, Used-for, character composition in dependency parsing)(learning word embedding, Used-for, multiple word senses)(learning word embedding, Used-for, discerning different meanings)(learning word embedding, Used-for, measuring word similarity)(object naming, Part-of, referring expression generation)(object naming, Compare, object recognition)(object naming, Used-for, linking visual to lexical information)(distributional word
(spoken dialogue system, Hyponym-Of, dialogue system)(non-task-oriented dialogue system, Hyponym-Of, dialogue system)(task-oriented dialogue system, Hyponym-Of, dialogue system)(hybrid dialogue system, Hyponym-Of, dialogue system)(spoken dialogue system, Part-of, dialogue system)(hybrid dialogue system, Part-of, spoken dialogue system)(hybrid dialogue system, Part-of, non-task-oriented dialogue system)(hybrid dialogue system, Part-of, task-oriented dialogue system)(dialogue policy optimization, Used-for, task-oriented dialogue system)(user intent classification, Used-for, task-oriented dialogue system)(reward learning, Used-for, semi-supervised policy learning)(two stage CopyNet, Evaluate-for, scalability)(end-to-end neural architecture, Evaluate-for, dialogue system)(photoBook dataset, Used-for, visual dialog)(Recurrent Dual Attention Network, Hyponym-Of, visual dialog моделей)(believe spans, Used-for, seq2
(language processing, Part-of, Natural Language Processing)(Recurrent Neural Networks, Used-for, language modeling)(stochastic gradient Markov Chain Monte Carlo, Used-for, learning weight uncertainty in Recurrent Neural Networks)(Minimal Recursion Semantics, Used-for, semantic representation)(sequence labeling framework, Used-for, language modeling)(folk wisdom, Evaluate-for, language processing)(Natural Language Inference, Is-a-Prerequisite-of, Recognizing Textual Entailment)(text annotation, Part-of, language processing)(error detection, Used-for, creating high-quality language resources)(Fixed-size Ordinarily Forgetting Encoding, Used-for, named entity recognition)(ITransF, Used-for, knowledge base completion)(active learning, Used-for, error detection)(attention-based baselines, Compare, neural encoder-decoder transition-based parser)(feedforward neural network, Used-for, predicting entity label)(sentiment analysis, Part-of, language processing)(parse trees, Part-of, language processing)(word embeddings, Used-for
(parsing model, Part-of, A* CCG parsing model)(parsing model, Used-for, CCG parsing)(parsing model, Is-a-Prerequisite-of, sentence modeling)(parsing model, Compare, Generative model)(parsing model, Compare, Discriminative model)(parsing model, Used-for, question answering)(parsing model, Hyponym-Of, language modeling)(parsing model, Conjunction, language modeling)(parsing model, Part-of, encoder-decoder framework)(question answering, Used-for, detecting good arguments)(question answering, Is-a-Prerequisite-of, civics applications)(sentence modeling, Part-of, dependency parsing)(dependency parsing, Used-for, syntactic parsers)(syntactic parsers, Used-for, improving parsing results)(CCG parsing model, Compare, semantic role labeling)(semantic role labeling, Used-for, sentence understanding)(parsing model, Part-of, entity extraction)(entity extraction, Evaluate-for, named entity
(conjunction, Conjunction, conjunction-based models)(programs, Part-of, solving algebraic word problems)(rationales, Used-for, deriving final answer)(recurrent neural networks, Used-for, reading comprehension)(gated self-matching networks, Is-a-Prerequisite-of, answer questions from given passage)(pointer networks, Used-for, locating positions of answers)(gated attention-based recurrent networks, Used-for, obtaining question-aware passage representation)(self-matching attention mechanism, Used-for, refining passage representation)(SQuAD dataset, Evaluate-for, model performance)(reading comprehension, Part-of, question answering)(coarse model, Used-for, selecting relevant sentences)(RNN, Used-for, producing the answer)(sentence selection, Used-for, question answering)(reinforcement learning, Used-for, training)(cross-attention mechanism, Used-for, representing questions more precisely)(semi-supervised question answering, Evaluate-for, boosting model performance)(constituent-centric neural architecture, Used-for, candidate
(controllable summarization, Part-of, summarization)(controllable summarization, Used-for, providing user-specified aspects)(controllable summarization, Compare, standard summarization)(summarization, Hyponym-Of, summarization task)(summarization, Used-for, generating summaries)(Controllable summarization, Evaluate-for, better assisting information need)(EntSUM, Part-of, controllable summarization)(EntSUM, Used-for, controllable summarization)(EntSUM, Part-of, human-annotated data set)(human-annotated data set, Part-of, Controllable summarization)(EntSUM, Used-for, named entities as the aspects to control).
(inter sentence relation extraction, Part-of, document-level RE)(document-level RE, Used-for, extracting inter-sentence relations)(relation extraction, Used-for, finding relational facts)(DocRED, Part-of, document-level RE)(multi-instance learning, Used-for, addressing noisy labels in relation extraction)(labelled edge graph convolutional neural network, Used-for, inter-sentence relation extraction)(graph convolutional neural network, Used-for, relation extraction)(intra-sentence relations, Compare, inter-sentence relations)(multi-instance learning, Part-of, relation extraction).
(morphological task, Part-of, language understanding task)(morphological task, Used-for, word vector collection)(morphological task, Evaluate-for, dialogue state tracking)(morphological task, Evaluate-for, semantic quality)(morphological task, Is-a-Prerequisite-of, inflection generation)(morphological task, Used-for, paradigm completion)(morphological inflection, Is-a-Prerequisite-of, dialogue state tracking)(cross-lingual transfer, Part-of, paradigm completion)(morphological disambiguation, Evaluate-for, agglutinative languages)(cross-lingual transfer, Conjunction, multitask learning)(morphologically rich language, Part-of, morphological task)(morphological supervision, Used-for, character language models)(morphological supervision, Evaluate-for, bits-per-character performance)(morphological model, Used-for, morphological task)(morphology, Evaluate-for, semantic role labeling)(word segments, Conjunction, characters)(synt
(contextualized representation, Compare, context-invariant representations)(contextualized representation, Used-for, sequence labeling tasks)(contextualized representation, Used-for, capturing affect dimensions)(contextualized representation, Used-for, multilingual unsupervised NMT scheme)(contextualized representation, Used-for, encapsulating lexical and contextual information)(contextualized representation, Part-of, neural network architectures)(contextualized representation, Part-of, contextualized word embeddings)(contextualized representation, Part-of, BERT networks)(contextualized word embeddings, Used-for, capturing affect dimensions)(contextualized word embeddings, Used-for, domain-aware parsing)(contextualized word embeddings, Used-for, sequence labeling tasks)(neural network architectures, Used-for, generating contextualized representations)(BERT networks, Used-for, generating contextualized representations)(contextualized representation, Evaluate-for, text classification tasks)(contextualized representation, Evaluate-for, named entity recognition)(contextualized representation, Evaluate-for,
(extractive summary, Used-for, covering salient points)(extractive summary, Part-of, extractive summarization)(extractive summarization, Used-for, document summarization)(extractive summarization, Evaluate-for, ROUGE scores)(sentence scoring, Part-of, extractive summarization)(sentence selection, Part-of, extractive summarization)(extractive summarization, Hyponym-Of, document summarization)(sentence-level attention, Part-of, extractive summarization)(salient sentences, Part-of, extractive summary)(key words, Part-of, extractive summary)(optimization technique, Part-of, extractive summarizer)(query-based summarization, Compare, extractive summarization)(abstractive summarization, Compare, extractive summarization)(encode-attend-decode paradigm, Used-for, extractive summarization)(unsupervised extractive summarization, Hyponym-Of, extractive summarization)(supervised extractive summarization, Hyponym-Of,
(topic aware news representation, Used-for, news recommendation)(news recommendation, Part-of, information overload alleviation)(topic aware news encoder, Part-of, topic aware news representation)(news representation, Part-of, topic aware news representation)(attention networks, Used-for, selecting important words)(CNN networks, Used-for, learning news representations from titles)(user encoder, Part-of, topic aware news representation)(user representation learning, Used-for, representing users from browsed news).
(approach topic aware, Part-of, Question answering over knowledge base)(approach topic aware, Part-of, Cross-lingual text classification)(approach topic aware, Part-of, Neural machine translation)(Neural machine translation, Compare, Statistical machine translation)(statistical machine translation, Part-of, Machine translation)(Cross-lingual text classification, Evaluate-for, Documents written in different languages)(question answering over knowledge base, Used-for, Accessing substantial knowledge)(question answering over knowledge base, Evaluate-for, Out-of-vocabulary problem)(named entity recognition, Part-of, Natural language processing)(question representation, Part-of, Neural network model)(Neural network model, Used-for, Representing questions and scores dynamically)(Sentiment-to-sentiment translation, Hyponym-Of, Sentiment analysis)(document similarity, Evaluate-for, Comparing texts of non-comparable lengths)(Domain adaptation, Used-for, Generalizing to new domain with limited supervision)(T
(entity extraction, Part-of, information extraction)(entity extraction, Evaluate-for, relation extraction)(entity extraction, Compare, mention detection)(entity extraction, Used-for, knowledge base population)(relation extraction, Part-of, information extraction)(relation extraction, Evaluate-for, keyphrase extraction)(relation extraction, Evaluate-for, event extraction)(relation extraction, Compare, mention detection)(relation extraction, Compare, named entity recognition)(relation extraction, Compare, aspect extraction)(relation extraction, Used-for, knowledge base population)(aspect extraction, Part-of, aspect-based sentiment analysis)(distant supervision, Used-for, relation extraction)(distant supervision, Used-for, event extraction)(attention mechanism, Used-for, aspect extraction)(attention mechanism, Used-for, relation extraction)(word embeddings, Used-for, aspect extraction)(word embeddings, Used-for, named entity recognition)(generative model, Used-for, keyphrase extraction)(dependency parsing, Used-for, entity extraction)(named entity recognition, Part-of, entity extraction)(named entity recognition
(transformer language model, Part-of, neural machine translation)(transformer language model, Used-for, language representation)(transformer language model, Compare, LSTM-based model)(transformer language model, Compare, recurrent neural network)(neural machine translation, Part-of, natural language processing)(LSTM-based model, Compare, recurrent neural network)(neural language representation model, Used-for, language understanding)(BERT, Hyponym-Of, neural language representation model)(ERNIE, Hyponym-Of, neural language representation model)(knowledge graphs, Used-for, language representation)(knowledge graphs, Part-of, ERNIE)(language modeling, Used-for, natural language processing)(syntactic neural language model, Used-for, sentence compression)(RNN-based language model, Part-of, machine translation)(RNN-based language model, Is-a-Prerequisite-of, transformer language model)
(error correction, Used-for, correcting grammatical errors)(error correction, Used-for, correcting spelling errors)(error correction, Used-for, improving sentence fluency)(error correction, Use-of, neural machine translation)(error correction, Use-of, nested attention layers)(error correction, Evaluate-for, CoNLL-14 benchmark dataset)(error correction, Part-of, GEC system)(error correction, Use-of, ERRANT)(ERRANT, Use-of, rule-based framework)(ERRANT, Used-for, extracting edits)(ERRANT, Used-for, classifying edits)(ERRANT, Used-for, reducing annotator workload)(ERRANT, Used-for, standardizing GEC datasets)(error correction, Use-of, active learning)(error correction, Compare, human correction)(error correction, Use-of, seq2seq model)(seq2seq model, Has-limitations, error-corrected data)(seq2seq model, Has-limitations, multiple errors)(seq2seq model, Use-of, flu
(aspect extraction, Part-of, aspect-based sentiment analysis)(aspect-based sentiment analysis, Used-for, extracting sentiment polarity on targets)(aspect-based sentiment analysis, Used-for, aspect term extraction)(aspect-based sentiment analysis, Used-for, aspect term-polarity co-extraction)(aspect-based sentiment analysis, Compare, general sentiment analysis)(aspect, Part-of, aspect term extraction)(aspect, Part-of, aspect term-polarity co-extraction)(general sentiment analysis, Compare, aspect-based sentiment analysis)(LSTM-based model, Used-for, multimodal sentiment analysis)(domain adaptation, Used-for, sentiment analysis)(volatility prediction, Used-for, sentiment analysis)(Bilingual Sentiment Embeddings, Used-for, cross-lingual sentiment classification)(convolutional neural networks, Used-for, aspect-based sentiment analysis)(cold-start problem, Used-for, aspect-based sentiment analysis)(aspect extraction, Used-for, fine-grained sentiment analysis)(interactive multi-task
(aspect-level sentiment classification, Part-of, sentiment analysis)(target-oriented sentiment classification, Part-of, sentiment analysis)(target-oriented sentiment classification, Used-for, classifying sentiment polarities over individual opinion targets in a sentence)(RNN with attention, Used-for, target-oriented sentiment classification)(deep convolutional neural networks, Used-for, sentiment polarity classification)(sentiment polarity classification, Used-for, determining sentiment polarity of a sentence)(attention mechanism, Part-of, RNN with attention)(aspect sentiment classification, Part-of, sentiment analysis)(memory networks, Used-for, aspect sentiment classification)(word embeddings, Used-for, sentiment classification)(multi-sentiment-resource Enhanced Attention Network, Used-for, sentiment prediction)(document-level knowledge, Used-for, aspect-level sentiment classification)(aspect routing, Part-of, Transfer Capsule Network)(Transfer Capsule Network, Used-for, aspect-level sentiment classification)(attention-based LSTM networks, Used-for, aspect-level sentiment classification)(dual recurrent neural network, Part-of, Dual crOss
(goal oriented visual dialogue, Compare, non-goal oriented dialog agents)(goal oriented visual dialogue, Hyponym-Of, goal-oriented dialogue systems)(goal oriented visual dialogue, Used-for, automatically generate questions about an image)(goal oriented visual dialogue, Used-for, identify undisclosed objects in an image)(goal oriented visual dialogue, Part-of, multimodal dialogue systems)(goal oriented visual dialogue, Evaluate-for, contextual information in images)(goal oriented visual dialogue, Is-a-Prerequisite-of, end-to-end goal-oriented visual dialogue system)(goal oriented visual dialogue, Is-a-Prerequisite-of, reinforcement learning with regularized information gain)(goal oriented visual dialogue, Is-a-Prerequisite-of, Rational Speech Act framework)(goal oriented visual dialogue, Compare, state-of-the-art dialogue systems)(multi-turn dialogue agent, Used-for, search Knowledge Bases)(symbolic query, Compare, soft retrieval process)(reinforcement learner, Hyponym-Of, neural dialogue agents)(multimodal dialogue systems
(kernel methods, Used-for, language learning)(kernel methods, Used-for, inference tasks)(expressive kernels, Hyponym-Of, tree kernels)(expressive kernels, Used-for, improve NLP performance)(deep neural networks, Used-for, automatically learning feature representations)(deep neural networks, Used-for, training)(deep neural networks, Evaluate-for, tensor data)(structured information, Part-of, kernel methods)(structured information, Part-of, neural networks)(Nystrom low-rank approximation, Used-for, pre-training deep architecture input layer)(kernelized neural network, Evaluate-for, state-of-the-art accuracy)(kernelized neural network, Evaluate-for, different tasks)(discourse structure, Part-of, rhetorical structure theory)(discourse structure, Used-for, benefiting text categorization)(recursive neural network, Used-for, computing representation of text)(attention mechanism, Used-for, focusing on salient content)(structured prediction, Evaluate-for, output structure)(bandit structured prediction, Evaluate-for, output structure)(
(recurrent neural network, Compare, convolutional neural network)(recurrent neural network, Part-of, text categorization)(convolutional neural network, Part-of, text categorization)(disconnected recurrent neural network, Hyponym-Of, recurrent neural network)(disconnected recurrent neural network, Compare, recurrent neural network)(disconnected recurrent neural network, Compare, convolutional neural network)(convolutional neural network, Used-for, question answering)(question answering, Part-of, machine comprehension)(disconnected recurrent neural network, Used-for, text categorization)(convolutional neural network, Used-for, text-based multiple choice question answering)(text-based multiple choice question answering, Evaluate-for, multiple choice question)(text-based multiple choice question answering, Evaluate-for, question-option tuple)(syntactic parsing information, Used-for, solving relational tasks)(solving relational tasks, Used-for, question similarity)(structural representations, Used-for, neural networks)(tree kernels, Used-for, neural networks)(question similarity
(KB-InfoBot, Part-of, multi modal dialogue)(multimodal cue, Part-of, multi modal dialogue)(ReCoSa, Used-for, multi modal dialogue)(personalized dialogue agents, Used-for, multi modal dialogue)(reinforcement learning, Used-for, multi modal dialogue)(Deep Dyna-Q, Used-for, multi modal dialogue)(user simulator, Used-for, multi modal dialogue)(Tensor rank minimization, Used-for, multi modal dialogue)(CMU-MOSEI, Used-for, multi modal dialogue)(MNED, Used-for, multi modal dialogue)(ADEM, Evaluate-for, multi modal dialogue evaluation)(dynamic fusion graph, Used-for, multi modal dialogue)(self-attention mechanism, Used-for, multi modal dialogue)(LSTM, Used-for, multi modal dialogue)(snapchat, Used-for, multi modal named entity disambiguation)(Instagram, Used-for, multi modal named entity disambiguation)(Multi-modal hierarchical fusion, Used-for, multi modal sarcasm detection
(neural text generation, Part-of, text generation)(neural text generation, Used-for, text generation tasks)(neural text generation, Part-of, natural language generation)(neural text generation, Compare, traditional text generation)(neural text generation, Used-for, generating narratives)(neural text generation, Used-for, abstractive summarization)(neural text generation, Used-for, question generation)(neural text generation, Used-for, procedural text generation)(neural text generation, Used-for, multilingual speech-to-text translation)(neural text generation, Evaluate-for, hallucination in content)(neural text generation, Evaluate-for, faithfulness of summaries)(neural text generation, Evaluate-for, performance on benchmarks)(neural text generation, Evaluate-for, generating factual summaries)(neural text generation, Consists-of, self-attention mechanisms)(self-attention mechanisms, Part-of, transformers)(transformers, Evaluate-for, neural question generation)(convnets, Used-for, multi-turn conversation
(fact checking, Part-of, fake news detection)(fact checking, Used-for, verifying truthfulness of a claim)(fact checking, Evaluate-for, claim veracity)(neural retrieval, Evaluate-for, fact checking)(semantic role labeling, Used-for, fact checking)(graph convolutional network, Used-for, fact checking)(graph attention network, Used-for, fact checking)(media bias, Used-for, fact checking)(discrepancies of corpus-based and thesaurus-based word similarities, Used-for, improving fact checking)(manual fact-checking, Compare, automatic fact-checking)(claim matching, Used-for, fact checking)(retrieval, Used-for, fact checking)(XLNet, Used-for, fact checking)(training strategy with factual correctness reward, Used-for, improving fact checking systems)(Fact Extraction and VERification (FEVER) dataset, Evaluate-for, fact checking systems)(semantic-level structure of evidence, Used-for, fact checking)(justifications for verdicts, Part-of, fact checking
(recurrent neural network language models, Part-of, maximum likelihood estimation)(maximum likelihood estimation, Evaluate-for, effectiveness)(maximum likelihood estimation, Hyponym-Of, reward augmented maximum likelihood approach)(exposure bias, Part-of, maximum likelihood estimation)(maximum likelihood estimation, Compare, token-level loss smoothing)(token-level loss smoothing, Conjunction, sequence-level loss smoothing)(token-level loss smoothing, Used-for, overcoming limitations in maximum likelihood estimation)(sequence-level smoothing approach, Used-for, overcoming limitations in maximum likelihood estimation)(reward augmented maximum likelihood approach, Used-for, predict sentences close to ground truth)(sequence-level smoothing approach, Used-for, predict sentences close to ground truth)(token-level loss smoothing, Used-for, predict sentences close to ground truth)(neural models, Used-for, dialog response generation)(iterative training process, Used-for, address generic responses)(ensemble method based on boosting, Used-for, address generic responses)(mutual-information-based decoding, Conjunction,
(lingual continual learning, Compare, zero-shot cross-lingual models)(lingual continual learning, Part-of, cross-lingual transfer learning)(lingual continual learning, Evaluate-for, imbalanced training)(lingual continual learning, Used-for, building NLP models)(cross-lingual transfer learning, Conjunction, multilingual embeddings)(cross-lingual transfer learning, Hyponym-Of, cross-lingual generalisability)(catastrophic forgetting, Part-of, continual learning)(catastrophic forgetting, Evaluate-for, task difficulty)(nonlinear model refinement, Evaluate-for, boundary-agnostic distribution shift)(norm-based curriculum learning, Used-for, neural machine translation)(unsupervised embeddings, Compare, supervised embeddings)
(visual question answering vqa, Compare, tabular question answering)(visual question answering vqa, Compare, paragraph comprehension)(question answering, Part-of, visual question answering vqa)(question answering, Part-of, tabular question answering)(question answering, Part-of, paragraph comprehension)(deep learning models, Evaluate-for, visual question answering vqa)(adversarial examples, Evaluate-for, visual question answering vqa)(model accuracy, Evaluate-for, visual question answering vqa)
(question answer, Used-for, retrieving answers)(question answer, Evaluate-for, accuracy)(question answer, Part-of, reading comprehension)(word-embedding models, Used-for, word analogy questions)(word-embedding models, Used-for, caption generation)(word-embedding models, Part-of, natural language processing)(additive compositionality, Part-of, word vectors)(Skip-Gram model, Used-for, learning word vectors)(Sufficient Dimensionality Reduction, Related-to, Skip-Gram model)(question answering systems, Part-of, natural language processing)(COREQA, Part-of, question answering systems)(encoder-decoder framework, Used-for, generating answers)(attention-based recurrent networks, Part-of, question answering models)(SQuAD dataset, Used-for, evaluating question answering models)(ensemble model, Compare, single model)(reading comprehension, Part-of, natural language processing)(knowledge base, Used-for, question answering)(gaze fixations, Evaluate-for, native
(machine reading comprehension mrc, Used-for, question answering)(machine reading comprehension mrc, Part-of, natural language processing)(gated self-matching networks, Used-for, reading comprehension)(pointer networks, Used-for, locating positions of answers)(reinforcement learning, Used-for, multi-step reasoning)(answer boundary, Part-of, machine reading comprehension mrc)(answer content, Part-of, machine reading comprehension mrc)(cross-passage answer verification, Part-of, machine reading comprehension mrc)(coarse model, Hyponym-Of, machine reading comprehension mrc)(expensive RNN, Hyponym-Of, machine reading comprehension mrc)(Attention-over-Attention reader, Used-for, cloze-style reading comprehension)(N-best re-ranking strategy, Used-for, improving performance)(stochastic answer network, Hyponym-Of, machine reading comprehension mrc)(inferential machine comprehension, Hyponym-Of, machine reading comprehension mrc)(dynamic self
(sememes, Hyponym-Of, sentiment knowledge)(sememes, Part-of, word meanings)(word meanings, Part-of, sentences)(word representation learning, Evaluate-for, sentiment knowledge)(sentiment axis, Hyponym-Of, sentiment knowledge)(SemAxis, Evaluate-for, sentiment knowledge)(domain-specific semantics, Hyponym-Of, sentiment knowledge)(SemAxis, Used-for, sentiment lexicons).
(category opinion sentiment quadruple, Part-of, Aspect-Category-Opinion-Sentiment Quadruple Extraction)(Restaurant-ACOS, Part-of, aspect-based sentiment analysis)(Laptop-ACOS, Part-of, aspect-based sentiment analysis)(Restaurant-ACOS, Is-a-Prerequisite-of, SemEval Restaurant dataset)(Laptop-ACOS, Compare, SemEval Laptop dataset)(aspect-category-opinion-sentiment quadruples, Used-for, aspect-based sentiment analysis)(implicit aspects, Part-of, product reviews)(implicit opinions, Part-of, product reviews).
(conceptual complexity, Evaluate-for, text complexity)(conceptual complexity, Part-of, spreading activation)(spreading activation, Part-of, DBpedia knowledge graph)(semantic priming, Part-of, memory processes)(sentence wrap-up, Part-of, memory processes)(forgetting, Part-of, memory processes)(semantic priming, Used-for, spreading activation)(psycholinguistic theories, Part-of, models memory processes)(semantic priming, Part-of, conceptual complexity)(processing Chinese texts, Evaluate-for, computational linguistics)(text segmentation, Conjunction, word discovery)(TopWORDS-Seg, Used-for, text segmentation)(TopWORDS-Seg, Used-for, word discovery)(psycholinguistic knowledge, Part-of, personality detection)(natural language understanding, Conjunction, language technology)(turn-taking, Part-of, informal social interaction)(timing, Part-of, informal social interaction)(NLI datasets, Evaluate-for, NLU research)(parameter size, Evaluate-for, psychometric quality)(
(concept, relation, concept)(lingual cross modal, Part-of, cross-view language modeling)(lingual cross modal, Part-of, Premise-based Multi-modal Reasoning)(Premise-based Multi-modal Reasoning, Used-for, vision language cross-modal reasoning)(cross-view language modeling, Used-for, aligning two different views of the same object)(cross-view language modeling, Part-of, CCLM)(CCLM, Is-a-Prerequisite-of, multi-lingual image-text retrieval datasets)(CCLM, Evaluate-for, IGLUE)(multi-lingual image-text retrieval datasets, Evaluate-for, CCLM)(visual content, Used-for, multimodal MT)(multimodal back-translation, Used-for, unsupervised MMT)(pseudo visual pivoting, Part-of, visually-pivoted captioning)(unsupervised bitext mining, Part-of, bilingual lexicons)(bilingual lexicons, Used-for, word alignment)(multi-lingual image-text retrieval
(lattice LSTM, Compare, character-based LSTM)(lattice LSTM, Compare, word-based LSTM)(Gated recurrent cells, Part-of, lattice LSTM)(sentence-level planning, Used-for, generating natural language responses)(semantic parser, Used-for, semantic parsing framework)(semantic parser, Evaluate-for, effectiveness)(semantic parsing, Hyponym-Of, structured output modeling)(logical form, Part-of, semantic parsing framework)(Seq2Seq models, Used-for, sequence-to-sequence tasks)(graph-to-sequence model, Used-for, automatic article commenting)(dependency parsers, Is-a-Prerequisite-of, document-level relation extraction)(semantic representations, Used-for, modeling neural responses)(topic interaction graph, Hyponym-Of, structured topic model)(structured semantic representations, Is-a-Prerequisite-of, Seq2Seq models)(first-order models, Compare, higher-order models)(spatiotemporal quantity extraction, Used-for, analyzing news events)(document-level graphs
(language models, Part-of, character level language modeling)(phonetic encoding, Is-a-Prerequisite-of, character level language modeling)(Recurrent neural networks, Used-for, character level language modeling)(fixed-vocabulary language models, Compare, character level language modeling)(hierarchical LSTM language model, Compare, character level language modeling)(self-attention mechanism, Used-for, character level language modeling)(text8, Evaluate-for, character level language modeling)(Multilingual Wikipedia Corpus, Evaluate-for, character level language modeling)(context size, Evaluate-for, character level language modeling)(bits-per-character, Evaluate-for, character level language modeling)(morphological supervision, Evaluate-for, character level language modeling)(neural variational language model, Used-for, character level language modeling)(Chinese word segmentation, Compare, character level language modeling)(Penn Treebank Character dataset, Evaluate-for, character level language modeling)(WikiText-2 dataset, Evaluate-for, character level language modeling)
(multilingual pre training, Compare, bilingual corpus)(multilingual pre training, Compare, monolingual embeddings)(multilingual pre training, Compare, multilingual translation)(multilingual pre training, Compare, code-switching)(multilingual pre training, Used-for, cross-lingual transfer)(multilingual pre training, Used-for, low-resource translation)(multilingual pre training, Used-for, masked language modeling)(multilingual pre training, Used-for, language-sensitive embedding)(multilingual pre training, Used-for, multilingual neural machine translation)(multilingual pre training, Used-for, multilingual word representations)(multilingual pre training, Used-for, multilingual named entity recognition)(multilingual pre training, Part-of, multilingual learning)(multilingual pre training, Evaluate-for, word translation accuracy)(multilingual pre training, Evaluate-for, training loss of low-resource language)(multilingual pre training, Evaluate-for, translation quality improvement)(multilingual pre training, Evaluate-for, sentiment labeling accuracy)(multilingual pre training, Evaluate-for
(monolingual word embeddings, Used-for, natural language processing)(monolingual word embeddings, Used-for, document analysis)(monolingual word embeddings, Used-for, word similarity)(monolingual word embeddings, Compare, bilingual word embeddings)(monolingual word embeddings, Used-for, word embeddings)(word embeddings, Hyponym-Of, monolingual word embeddings)(word embeddings, Hyponym-Of, bilingual word embeddings)(word embeddings, Used-for, document clustering)(word embeddings, Part-of, natural language processing tasks)(word embeddings, Used-for, semantic relationship analysis)(word embeddings, Used-for, machine translation)(word embeddings, Used-for, word analogy detection)(word embeddings, Used-for, aspect-based sentiment analysis)(word embeddings, Hyponym-Of, embedding spaces)(embedding spaces, Used-for, bilingual dictionary induction)(embedding spaces, Used-for, semantic space transformation)(embedding spaces, Hyponym-Of, vector space representations)(vector space representations, Used-for
(extractive summarization, Compare, neural abstractive summarization)(abstractive summarization, Hyponym-Of, neural abstractive summarization)(sequence-to-sequence models, Part-of, neural abstractive summarization)(deep neural networks, Used-for, text summarization)(hybrid pointer-generator network, Used-for, neural abstractive summarization)(coverage, Used-for, neural abstractive summarization)(CNN/Daily Mail dataset, Evaluate-for, neural abstractive summarization)(global encoding framework, Used-for, neural abstractive summarization)(GOLC, Used-for, neural abstractive summarization)(Transformer-based encoder-decoder framework, Used-for, abstractive document summarization)(focus-attention mechanism, Part-of, Transformer-based encoder-decoder framework)(saliency-selection network, Part-of, Transformer-based encoder-decoder framework)(multi-task learning, Used-for, neural abstractive summarization)(adversarial domain adaptation, Used-for, neural abstractive summarization)(sequential
(gender bias, Is-a-Prerequisite-of, classification bias)(gender bias, Evaluate-for, machine translation)(gender bias, Part-of, natural languages)(gender bias, Compare, gender stereotypes)(gender bias, Is-a-Prerequisite-of, bias mitigation)(gender bias, Used-for, reducing translation quality)(gender bias, Used-for, unsupervised learning)(gender bias, Evaluate-for, word embeddings)(bias-specific dataset, Used-for, measuring classification bias)(bias-specific dataset, Evaluate-for, co-reference resolution system)(co-reference resolution system, Evaluate-for, gender-stereotypical sentences)(translator, Evaluate-for, gender identity)(conceptual gender stereotypes, Compare, real-world gender stereotypes)(language models, Evaluate-for, classification bias)(language models, Compare, word embeddings)(word embeddings, Used-for, debiasing techniques)(word embeddings, Compare, semantic subspace)(word embeddings, Evaluate-for, Double Hard Debias)
(labeled sequence transduction, Part-of, unsupervised learning)(multi-space variational encoder-decoders, Used-for, labeled sequence transduction)(neural networks, Used-for, discrete latent variables)(neural networks, Used-for, continuous latent variables)(RNNs, Used-for, dialog systems)(Hybrid Code Networks, Compare, end-to-end approaches)(Hybrid Code Networks, Used-for, inferring latent representation of dialog state)(unsupervised generative model, Used-for, error detection)(Bayesian unsupervised text segmentation, Part-of, text segmentation)(event detection, Used-for, categorizing events)(convulutional neural network, Is-a-Prerequisite-of, joint relation extraction)(convulutional neural network, Used-for, integrating with pairwise ranking framework)(generative adversarial networks, Used-for, enhancing cross-language translation)(graph-based framework, Used-for, abstractive meeting speech summarization)(StructVAE, Used
(recurrent neural network rnns, Hyponym-Of, neural network)(recurrent neural network rnns, Part-of, attention-based neural network)(recurrent neural network rnns, Part-of, Hybrid Code Networks)(recurrent neural network rnns, Evaluated-for, question answering)(recurrent neural network rnns, Used-for, language modeling)(recurrent neural network rnns, Used-for, machine translation)(recurrent neural network rnns, Used-for, document classification)(recurrent neural network rnns, Used-for, automatic question answering)(recurrent neural network rnns, Part-of, KBQA system)(recurrent neural network rnns, Hyponym-Of, grid-type recurrent neural network)(recurrent neural network rnns, Part-of, Gated-Attention Reader)(recurrent neural network rnns, Used-for, sentence compression)(recurrent neural network rnns, Is-a-Prerequisite-of, supervised learning)(recurrent neural network rnns, Is
(None)
(entity mention relation, Evaluate-for, attention-based recurrent neural network)(entity mention relation, Evaluate-for, long short term memory network)(entity mention relation, Compare, tree-based LSTM model)(entity mention relation, Conjunction, entity mention)(entity mention relation, Hyponym-Of, information extraction)(attention-based recurrent neural network, Used-for, semantic relations)(joint extraction, Used-for, entity mentions)(joint extraction, Used-for, relations)(tagging scheme, Used-for, joint extraction task)(end-to-end models, Used-for, entity mentions)(end-to-end models, Used-for, relations)(containment relation, Evaluate-for, medical events)(neural architecture, Used-for, containment relation)(entity typing task, Conjunction, entity mention)(Knowledge Graph embedding, Evaluate-for, link prediction)(paraphrasing, Hyponym-Of, semantic relation)(neural model, Used-for, paraphrasing)(dimension reduction technique, Used-for, relation compositions)(domain-sensitive embeddings
(cross lingual embeddings, Evaluate-for, bilingual lexicon induction)(cross lingual embeddings, Evaluate-for, cross-lingual classification)(cross lingual embeddings, Evaluate-for, cross-lingual word similarity)(cross lingual embeddings, Part-of, multilingual distributed representations of text)(multilingual distributed representations of text, Used-for, cross-lingual word similarity)(bilingual word embeddings, Is-a-Prerequisite-of, cross lingual embeddings)(bilingual word embeddings, Compare, monolingual embeddings)(bilingual sentiment embeddings, Hyponym-Of, cross lingual embeddings)(multi-task modeling approach, Is-a-Prerequisite-of, multilingual distributed representations of text)(multilingual skip-gram model, Used-for, cross-lingual sentence similarity)(cross-lingual sentence similarity, Is-a-Prerequisite-of, multilingual distributed representations of text)(multilingual embeddings, Part-of, cross lingual embeddings)(multi-task learning, Is-a-Prere
(graph neural, Used-for, semantic parsing)(graph neural, Used-for, relation extraction)(graph neural, Used-for, entity alignment)(graph neural, Part-of, neural techniques)(semantic parsing, Part-of, natural language processing)(relation extraction, Part-of, natural language processing)(entity alignment, Part-of, natural language processing)(semantic parsing, Used-for, logical forms generation)(relation extraction, Used-for, discovering relations)(entity alignment, Used-for, knowledge graph embedding)(knowledge graph embedding, Part-of, knowledge graph)(neural techniques, Used-for, end-to-end computational argumentation mining)(end-to-end computational argumentation mining, Part-of, natural language processing).
(morphologically rich languages, Part-of, language understanding systems)(neural language model, Used-for, generating coherent poetry)(Affect-LM, Used-for, customizing emotional content)(neural language model, Compare, standard LDA topic model)(neural encoder-decoder transition-based parser, Used-for, parsing sentences)(minimal recursion semantics, Evaluate-for, statistical parsing)(text similarity measures, Used-for, plagiarism detection)(text similarity measures, Used-for, information ranking)(text similarity measures, Used-for, recognition of paraphrases)(text similarity measures, Used-for, textual entailment)(TextFlow, Used-for, computing similarity value)(language interface, Conjunction, manipulating text)(language interface, Conjunction, querying databases)(language interface, Conjunction, analyzing data)(linguistic typology, Used-for, studying structures in human language)(stochastic point processes, Evaluate-for, vowel inventory)(reasoning and inference, Compare, modeling inference)(LSTM language model, Conjunction
(neural dialogue, Used-for, automatic diagnosis)(neural dialogue, Used-for, collaborative dialogue)(neural dialogue, Used-for, multi-turn dialogue)(neural dialogue, Evaluate-for, task success rate)(neural dialogue, Evaluate-for, human-like responses)(neural dialogue, Part-of, task-oriented dialogue systems)(poetic devices, Hyponym-Of, phonetic encoding)(neural language model, Part-of, poetry generation)(neural language model, Used-for, generating related sentences)(SVM, Used-for, task success prediction)(sentiment labels, Part-of, Multimodal EmotionLines Dataset)(task-oriented dialogue systems, Is-a-Prerequisite-of, automatic diagnosis)(conditional information density, Evaluate-for, task success)(KB-InfoBot, Used-for, searching Knowledge Bases)(adversarial examples, Used-for, testing robustness of image captioning)(GLAD, Used-for, dialogue state tracking)(Dialogue Act classification, Part-of, task-oriented
(prototype mention embeddings, Used-for, disambiguate each mention)(prototype mention embeddings, Hyponym-Of, embeddings)(Multi-Prototype Mention Embedding model, Used-for, disambiguate each mention)(Multi-Prototype Mention Embedding model, Part-of, prototype mention embeddings)(Multi-Prototype Mention Embedding model, Used-for, learn multiple sense embeddings)(knowledge base, Used-for, derive entities)(textual contexts, Used-for, modeling words)(word embeddings, Part-of, textual contexts)(entity embeddings, Part-of, knowledge base)(prototype mention embeddings, Evaluate-for, entity linking)(LASER sentence embeddings, Compare, dependency parser-based embeddings)(sentence embeddings, Part-of, vector representations of sentences)(sentence embeddings, Evaluate-for, probing and downstream experiments)
(semantic relatedness, Compare, analogies)(attention-based recurrent neural network, Used-for, joint extraction of entity mentions and relations)(LSTM network, Part-of, attention-based recurrent neural network)(semantic relations, Part-of, LSTM network)(dependency trees, Compare, LSTM network)(Automatic Content Extraction corpora, Used-for, experiments)(feature-based joint model, Compare, attention-based recurrent neural network)(end-to-end tree-based LSTM model, Compare, attention-based recurrent neural network)(entity mentions, Part-of, experiments)(relations, Part-of, experiments)(Agent-Artifact relations, Compare, Physical relations)(Agent-Artifact relations, Compare, Part-Whole relations)(transformer-based language models, Used-for, identifying analogies)(GPT-2, Part-of, transformer-based language models)(RoBERTa, Part-of, transformer-based language models)(BERT, Compare, GPT-2)(BERT, Compare, RoB
(morphological typology, Used-for, studying the range of structures present in human language)(phonological typology, Conjunction, morphological typology)(morphological typology, Used-for, modeling different morphological processes)(morphological typology, Evaluate-for, language modeling task)(character trigram representations, Part-of, language modeling task)(factorial conditional random fields, Used-for, cross-lingual morphological tagging)(deep learning-based approach, Used-for, morphological disambiguation)(Universal Dependencies corpora, Conjunction, simulated low-resource setting)(multitask learning, Used-for, addressing morphological richness and dialectal variations)(analogical reasoning, Used-for, capturing linguistic regularities)(Chinese lexical knowledge, Conjunction, analogical reasoning task)(unsupervised bilingual lexicon induction, Evaluate-for, word translations from monolingual corpora)(polysynthetic languages, Part-of, high complexity in vocabularies)(neurally-informed approaches, Used-for, bootstrapping morphological analyzer
(morphologically rich, Compare, low-frequency word forms)(morphologically rich, Compare, distributional vector space models)(distributional vector space models, Compare, curated semantic lexicons)(distributional vector space models, Part-of, language understanding systems)(morph-fitted vectors, Used-for, dialogue state tracking)(neural model, Used-for, morphological inflection generation)(morphologically rich, Part-of, morphological typologies)(character-level models, Evaluate-for, long range dependencies)(cross-lingual transfer, Is-a-Prerequisite-of, reducing anisomorphism)(statistical morphological inflectors, Compare, neural sequence-to-sequence models)(semi-supervised learning, Used-for, inflection generation)(character representations, Part-of, morphological typologies)(character representations, Compare, word segments)(morphological tagging, Used-for, predicting syntactic traits)
(specializing word embeddings according, Used-for, semantic similarity)(Pseudofit, Used-for, specializing word embeddings according)(Pseudofit, Used-for, acquiring synonyms)(pseudo-sense, Used-for, Pseudofit)(representations, Used-for, making the initial embeddings more generic)
(word embedding model, Part-of, word analogy questions)(word embedding model, Part-of, caption generation)(word embedding model, Part-of, Skip-Gram model)(Skip-Gram model, Compare, SDR framework)(word embedding model, Used-for, aspect extraction)(neural approach, Compare, topic models)(neural approach, Used-for, aspect extraction)(word embedding model, Part-of, bilingual word embeddings)(bilingual word embeddings, Compare, large parallel corpora)(word embedding model, Used-for, neural word segmentation)(word embedding model, Part-of, vector space representations)(vector space representations, Evaluate-for, word similarity)(word embedding model, Part-of, knowledge base completion)(word embedding model, Part-of, synsets)(word embedding model, Part-of, multimodal word distributions)(multimodal word distributions, Evaluate-for, word similarity)(word embedding model, Evaluate-for, volatility prediction)(volatility prediction, Used-for, forecasting market risk)(word embedding model, Part-of,
(Text similarity measures, Part-of, plagiarism detection)(Text similarity measures, Part-of, information ranking)(Text similarity measures, Part-of, recognition of paraphrases)(TextFlow, Hyponym-Of, text similarity measures)(TextFlow, Used-for, sequence matching)(TextFlow, Used-for, input text pairs)(TextFlow, Evaluate-for, paraphrase detection)(TextFlow, Evaluate-for, textual entailment recognition)(TextFlow, Evaluate-for, ranking relevance)(LSTM-based model, Used-for, utterances)(LSTM-based model, Evaluate-for, classification process)(LSTM-based model, Is-a-Prerequisite-of, contextual information)(Layer-wise relevance propagation, Used-for, neural machine translation)(Layer-wise relevance propagation, Used-for, interpret the internal workings)(Neural machine translation, Evaluate-for, contribution of each contextual word)(Context sensitive lemmatization, Compare, Lemming)(Context sensitive lemmatization, Compare, Morfette)(Context sensitive lemmatization, Evaluate
(knowledge graph completion, Hyponym-Of, knowledge base completion)(knowledge graph completion, Compare, link prediction/knowledge graph completion)(knowledge graph completion, Compare, holographic embeddings)(knowledge graph completion, Compare, complex embeddings)(knowledge graph completion, Evaluate-for, neighborhood attention)(knowledge graph completion, Evaluate-for, link prediction)(knowledge graph completion, Evaluate-for, entity relationship generalization)(knowledge graph completion, Evaluate-for, multi-hop reasoning)(link prediction, Evaluate-for, entailment score)(link prediction, Compare, entailment graph induction)(complex embeddings, Compare, holographic embeddings)(holographic embeddings, Compare, complex embeddings)(neighborhood attention, Evaluate-for, link prediction)(neighborhood attention, Used-for, knowledge graph completion)(entity relationship generalization, Part-of, knowledge graph completion).
(grammatical error correction, Part-of, neural machine translation)(grammatical error correction, Used-for, correcting grammatical errors)(grammatical error correction, Evaluate-for, CoNLL-14 benchmark dataset)(grammatical error correction, Used-for, reducing annotator workload)(grammatical error correction, Used-for, standardising GEC datasets)(grammatical error correction, Is-a-Prerequisite-of, evaluating text-to-text generation)(grammatical error correction, Used-for, improving seq2seq text generation)(grammatical error correction, Compare, humans in terms of performance)(grammatical error correction, Evaluate-for, grammaticality improvements)(grammatical error correction, Used-for, cross-sentence context correction)(grammatical error correction, Used-for, improving online inference efficiency)(grammatical error correction, Used-for, multilingual language models)(grammatical error correction, Used-for, interpretability for language learning)(neural machine translation, Used-for,
(offensive language classifier, Evaluate-for, robustness)(offensive language classifier, Used-for, combat hateful speech)(offensive language classifier, Used-for, combat racist speech)(offensive language classifier, Used-for, offensive speech classification)(TBO, Part-of, offensive language classifier)(TBO, Used-for, offensive language identification)(offensive language classifier, Used-for, detect offensive language)(offensive language classifier, Used-for, detect racist language)(new dataset, Used-for, offensive language identification)(offensive language classifier, Evaluate-for, accuracy)(adversarial attack, Evaluate-for, offensive language classifier)(greedy-based word selection, Used-for, adversarial attack)(attention-based word selection, Used-for, adversarial attack)(context-aware embeddings, Used-for, adversarial attack)(TBO, Compare, other offensive language identification datasets)(crafty adversarial attacks, Compare, primitive adversarial attacks)(adhoc decisions, Part-of, moderation decisions)(appropriate language,
(knowledge graph embedding, Used-for, learning low-rank representations)(knowledge graph embedding, Used-for, link prediction)(knowledge graph embedding, Used-for, constructing unified graphs)(knowledge graph embedding, Used-for, entity linkage)(knowledge graph embedding, Evaluate-for, interpretability)(knowledge graph embedding, Evaluate-for, scalability)(knowledge graph embedding, Evaluate-for, model effectiveness)(knowledge graph embedding, Hyponym-Of, deep relational learning)(knowledge graph embedding, Compare, holographic embeddings)(knowledge graph embedding, Compare, complex embeddings)(holographic embeddings, Part-of, spectral version)(complex embeddings, Part-of, frequency domain in Fourier transform)(deep relational learning, Conjunction, entity alignment)(automatic evaluation, Evaluate-for, model effectiveness)(human evaluation, Evaluate-for, model human-likeness)(model, Compare, baselines)(link prediction, Part-of, knowledge graph embedding)(entity linkage, Part-of, knowledge graph embedding
(explanation attention, Used-for, multi-turn response selection)(explanation attention, Used-for, representation of text segments)(explanation attention, Conjunction, stacked self-attention)(explanation attention, Used-for, response matching with context)(machine translation, Used-for, word reordering detection)(word order information, Used-for, sequence modeling)(word reordering detection, Evaluate-for, SAN and RNN)(machine translation, Is-a-Prerequisite-of, word reordering detection)(word position embedding, Used-for, sequence modeling)(machine translation, Part-of, SAN performance)(machine translation, Part-of, RNN performance)
(large language model llm, Part-of, neural language model)(neural language model, Compare, hierarchical LSTM language model)(neural language model, Compare, recurrent neural networks)(neural language model, Conjunction, recurrent neural tensor networks)(neural language model, Is-a-Prerequisite-of, language modeling)(language modeling, Used-for, poetry generation)(poetry generation, Part-of, natural language processing)(neural language model, Part-of, Affect-LM)(Affect-LM, Used-for, generation of conversational text)(character trigram representations, Part-of, hierarchical LSTM language model)(pre-trained word embeddings, Part-of, neural network architectures for NLP tasks)(context embeddings, Part-of, NLP systems)(sequence labeling tasks, Evaluate-for, NLP systems)(ULMFiT, Used-for, inductive transfer learning)(neural caching model, Conjunction, LSTM)(Taylor's law, Used-for, evaluating language models)(syntax, Part
(Constituency parsing, Is-a-Prerequisite-of, Syntax-aware system)(Constituency parsing, Part-of, Syntactic parsing)(Natural language processing, Hyponym-Of, Sequence-to-sequence models)(Constituency parsing, Evaluate-for, State-of-the-art performance)(Constituency parsing, Is-a-Prerequisite-of, Empty category detection)(Neural model, Used-for, Constituency parsing)(Greedy top-down inference, Used-for, Constituency parsing)(Joint modeling approach, Used-for, Salient discussion points identification)(Syntactic information, Used-for, Neural machine translation system)(Penn Treebank, Evaluate-for, Constituency parsing)(State-of-the-art models, Conjunction, Discriminative models)(Recurrent layers, Part-of, Neural network)(Neural models, Used-for, Constituency parsing)(Semantic Dependency Parsing, Conjunction, Constituency parsing)(Parse trees
(event causality identification, Used-for, identifying causal relations between events in unstructured texts)(knowledge bases, Used-for, generating well-formed new sentences)(data sparsity, Evaluated-for, challenging neural sequence labeling)(seed alignment, Part-of, multilingual KG completion)(SciREX, Conjunction, multiple IE tasks)(textual features, Conjunction, visual features)(data sparsity, Evaluate-for, performance of transition-based model)(document-level event causality identification, Hyponym-Of, event causality identification).
(morphological compositionality, Used-for, capturing morphological compositionality in BERT)(morphological compositionality, Used-for, expressing word-relative syntactic regularities)(morphological compositionality, Hyponym-Of, semantic compositionality)(semantic compositionality, Used-for, neural networks)(semantic compositionality, Used-for, understanding complex linguistic units)(semantic compositionality, Used-for, composing meanings of constituents)(BERT, Used-for, tackling natural language processing tasks)(pre-trained language models, Used-for, encoding semantic and syntactic content)(KinyaBERT, Hyponym-Of, BERT)(two-tier BERT architecture, Used-for, leveraging a morphological analyzer)(language modeling data, Evaluated-for, bits-per-character performance)(morphologically rich languages, Evaluate-for, morphological analyzers)(morphological compositionality, Evaluate-for, low-resource languages)(inductive biases, Used-for, compositionality in neural language
(embeddings, Part-of, neural network architectures)(recurrent network, Part-of, neural network architectures)(bidirectional language models, Used-for, context embeddings)(context embeddings, Used-for, NLP systems)(sequence labeling tasks, Evaluate-for, context embeddings)(named entity recognition, Evaluate-for, context embeddings)(chunking, Evaluate-for, context embeddings)(Gated-Attention Reader, Part-of, sequence labeling tasks)(multiplicative interactions, Part-of, attention mechanism)(attention mechanism, Part-of, Gated-Attention Reader)(recurrent neural network document reader, Part-of, Gated-Attention Reader)(subjectivity, Evaluate-for, word embeddings)(sentiment classification, Evaluate-for, word embeddings)(SentiVec, Used-for, sentiment classification)(context embeddings, Part-of, embeddings semantic)(context embeddings, Used-for, PP attachment model)(PP attachment model, Evaluate-for, embeddings semantic)(multilingual model, Is-a-Prerequisite-of, semantic
(image captioning, Compare, vision-and-language navigation)(image captioning, Part-of, vision languages)(neural image captioning, Hyponym-Of, image captioning)(Show-and-Fool, Used-for, neural image captioning)(Show-and-Fool, Evaluate-for, robustness of language grounding)(neural image captioning, Part-of, vision languages)(visual language grounding, Part-of, vision languages)(image feature extraction, Part-of, neural image captioning)(language caption generation, Part-of, neural image captioning)(CNN, Part-of, image feature extraction)(RNN, Part-of, language caption generation)(visual language grounding, Used-for, neural image captioning)
(shot continual relation extraction, Part-of, relation extraction)(relation extraction, Used-for, identifying entity mention spans)(relation extraction, Used-for, identifying relations between pairs of entity mentions)(neural relation extraction framework, Used-for, multi-lingual texts)(mono-lingual attention, Used-for, mono-lingual texts)(cross-lingual attention, Used-for, information consistency and complementarity among cross-lingual texts)(distant supervision, Used-for, building training data)(dynamic transition matrix, Evaluate-for, noise in the training data)(curriculum learning based method, Used-for, training the transition matrix)(feature extraction, Part-of, statistical NLP)(feature extraction, Hyponym-Of, preprocessing step)(Bi-LSTM, Used-for, relation extraction)(BONIE, Used-for, extracting numerical relations)(message passing algorithm, Used-for, feature templates restructuring)(bootstrapping, Used-for, learning specific dependency patterns)(seed selection,
(pretrained language model, Used-for, text classification)(pretrained language model, Part-of, transfer learning)(transfer learning, Hyponym-Of, Universal Language Model Fine-tuning)(Universal Language Model Fine-tuning, Used-for, task-specific fine-tuning)(neural language model, Compare, pretrained language model)(pretrained language model, Compare, non-syntactic neural language model)(pretrained language model, Used-for, evaluating sentence compression)(language model, Part-of, recurrent neural network)(pretrained language model, Evaluate-for, diachronic accuracy).
(compositional generalization semantic parsing, Compare, sequence-to-sequence models)(compositional generalization semantic parsing, Compare, convolutional networks)(compositional generalization semantic parsing, Compare, recurrent networks)(compositional generalization semantic parsing, Used-for, handling both natural language variation and compositional generalization)(compositional generalization semantic parsing, Evaluate-for, datasets like GeoQuery, SCAN, and CLOSURE)(compositional generalization semantic parsing, Evaluate-for, Compositional Freebase Queries (CFQ))(compositional generalization semantic parsing, Compare, neural machine translation)(compositional generalization semantic parsing, Compare, standard seq-to-seq models)(compositional generalization semantic parsing, Is-a-Prerequisite-of, diverse evaluations in semantic parsing)(compositional generalization semantic parsing, Hyponym-Of, semantic parsing)(semantic parsers, Compare, seq2seq models)(semantic parsers, Used-for, mapping sentences to
(explainable nlp, Used-for, understanding a narrative)(explainable nlp, Used-for, explaining naive psychology of story characters)(explainable nlp, Used-for, explainable multi-hop QA)(explainable nlp, Used-for, recognizing textual entailment)(explainable nlp, Used-for, explainable response generation)(explainable nlp, Used-for, generating fine-grained explanations)(explainable nlp, Used-for, explainable claim verification)(explainable nlp, Used-for, visual captioning evaluation)(explainable nlp, Used-for, Court Judgment Prediction and Explanation)(explainable nlp, Used-for, personalized recommendation generation)(explainable nlp, Used-for, math problem solving)(explainable nlp, Used-for, multimodal environment modeling)(explainable nlp, Used-for, enhancing response generation with implicit knowledge)(explainable multi-hop QA, Part-of, explainable nlp
(speech text translation st, Hyponym-Of, machine translation)(speech text translation st, Hyponym-Of, neural machine translation)(neural machine translation, Used-for, translation tasks)(speech text translation st, Hyponym-Of, end-to-end models)(end-to-end models, Compare, cascaded models)(end-to-end models, Used-for, joint training)(joint training, Used-for, reducing error propagation)(end-to-end models, Hyponym-Of, SimulSpeech)(speech encoder, Part-of, SimulSpeech)(speech segmenter, Part-of, SimulSpeech)(text decoder, Part-of, SimulSpeech)(speech text translation st, Hyponym-Of, curriculum pre-training method)(curriculum pre-training method, Used-for, transcription learning)(curriculum pre-training method, Used-for, understanding utterances)(curriculum pre-training method, Used-for, mapping words in different languages)(SATE method, Part-of, speech text
(multimodal embeddings, Part-of, multimodal sentiment analysis)(multimodal embeddings, Used-for, entity disambiguation)(word embeddings, Used-for, sentiment analysis)(neural word embeddings, Hyponym-Of, word embeddings)(bilingual word embeddings, Hyponym-Of, word embeddings)(multimodal word distributions, Hyponym-Of, word embeddings)(network embedding, Compare, multimodal embeddings)(Context-Aware Network Embedding, Hyponym-Of, network embedding)(CAN, Used-for, link prediction)(Aspect extraction, Part-of, aspect-based sentiment analysis)(Deep PCCA, Evaluate-for, cross-lingual image description retrieval).
(abstractive summarization, Conjunction, extractive summarization)(abstractive summarization, Compare, query-based summarization)(abstractive summarization, Is-a-Prerequisite-of, neural sequence-to-sequence models)(neural sequence-to-sequence models, Evaluate-for, abstractive text summarization)(query-based summarization, Part-of, summarization)(query-based summarization, Used-for, highlighting relevant points)(abstractive summarization, Compare, compressive summarization)(compressive summarization, Compare, extractive summarization)(query attention model, Part-of, encode-attend-decode paradigm)(document attention model, Part-of, encode-attend-decode paradigm)(encode-attend-decode paradigm, Compare, hybrid pointer-generator network)(hybrid pointer-generator network, Used-for, accurately reproducing information)(coverage mechanism, Used-for, tracking summarized content)(saliency, Evaluate-for, informativeness)(SEQUENCE-to-SEQUENCE framework, Compare, novel
(Automatic grammatical error correction, Used-for, error correction)(cross-sentence context, Hyponym-Of, context)(auxiliary encoder, Part-of, neural encoder-decoder models)(neural encoder-decoder models, Used-for, GEC)(predictive bias, Compare, label bias)(predictive bias, Compare, selection bias)(predictive bias, Compare, model overamplification)(predictive bias, Compare, semantic bias)(predictive bias, Part-of, bias)(label bias, Part-of, predictive bias)(selection bias, Part-of, predictive bias)(model overamplification, Part-of, predictive bias)(semantic bias, Part-of, predictive bias)(bias, Evaluate-for, model performance)(predictive bias, Evaluate-for, model performance)(predictive bias framework, Used-for, organizing bias mitigation efforts)(bias mitigation, Used-for, improving model fairness)(Stereotype Content Model, Used-for, debiasing)(debiasing, Evaluate-for, language
(language, Used-for, machine translation)(machine translation, Evaluate-for, language)(machine translation, Evaluate-for, target language)(cross-lingual transfer, Used-for, low-resource target language)(cross-lingual transfer, Evaluate-for, target language performance)(decoder, Used-for, target language)(multi-lingual transfer, Evaluate-for, target language)(unsupervised neural MT systems, Evaluate-for, target language)(seq-to-dependency NMT, Evaluate-for, target language)(cross-cultural differences, Evaluate-for, target languages)(cross-cultural similarities, Evaluate-for, target languages)(bilingual lexicon induction, Used-for, cross-lingual tasks)(cross-lingual classification, Used-for, cross-lingual tasks)(language agnostic representations, Evaluate-for, target languages)(cross-lingual word embeddings, Used-for, bilingual tasks)(cross-lingual twitter sentiment classification, Used-for, bilingual tasks)(medical bilingual lexicon induction, Used-for, bilingual tasks)(domain adaptation
(natural question dataset, Hyponym-Of, machine reading comprehension benchmark)(natural question dataset, Used-for, evaluating two-grained answers)(two-grained answers, Part-of, machine reading comprehension benchmark)(Natural Questions, Hyponym-Of, machine reading comprehension benchmark)(RikiNet, Used-for, natural question answering)(Natural Questions, Used-for, training and evaluating models)(machine reading comprehension framework, Used-for, modeling documents' hierarchical nature)(graph attention networks, Used-for, obtaining different levels of representations)(multi-grained machine reading comprehension framework, Used-for, modeling document granularity)
(transcribed speech, Part-of, ASR)(transcribed speech, Part-of, summary-dialogue chunk pairs)(transcribed speech, Part-of, QASR)(transcribed speech, Part-of, VoxPopuli)(transcribed speech, Used-for, acoustic-based Arabic dialect identification)(transcribed speech, Used-for, linguistics-based Arabic dialect identification)(transcribed speech, Used-for, punctuation restoration)(transcribed speech, Used-for, speaker identification)(transcribed speech, Used-for, speaker linking)(transcribed speech, Used-for, natural language processing)(transcribed speech, Used-for, Spoken Language Understanding)(transcribed speech, Used-for, downstream NLP tasks)(transcribed speech, Used-for, training ASR systems)(transcribed speech, Used-for, self-training)(QASR, Compare, MGB-2 corpus)(VoxPopuli, Compare, QASR)(language model, Used-for, transcription error mitigation)(Automatic Speech Recognizer (ASR), Used
(Stanford Question Answering Dataset, Part-of, reading comprehension datasets)(SQuAD, Hyponym-Of, reading comprehension datasets)(WikiReading, Hyponym-Of, reading comprehension datasets)(CNN, Hyponym-Of, reading comprehension datasets)(Children’s Book Test, Hyponym-Of, reading comprehension datasets)(MCTest, Hyponym-Of, reading comprehension datasets)(TriviaQA, Hyponym-Of, reading comprehension datasets)(DuoRC, Hyponym-Of, reading comprehension datasets)(SQuAD, Used-for, evaluating RC datasets)(TriviaQA, Used-for, evaluating RC datasets)(TriviaQA, Is-a-Prerequisite-of, neural approaches in language understanding)(DuoRC, Used-for, studying language understanding)(WikiReading, Evaluate-for, neural models)(WikiReading, Evaluate-for, semantic-driven approaches).
(machine translation, Compare, statistical machine translation)(neural machine translation, Part-of, machine translation)(bi-directional LSTM, Used-for, encoding source sentences)(convolutional layers, Used-for, encoding source sentences)(Deep Neural Networks, Part-of, machine translation)(linear associative units, Used-for, reducing gradient propagation path)(recurrent neural networks, Used-for, various NLP tasks)(syntactic structures, Part-of, neural machine translation)(neural machine translation, Hyponym-Of, machine translation)(Grammatical error correction, Used-for, correcting errors in text)(nested attention layers, Used-for, Grammatical error correction)(layer-wise relevance propagation, Used-for, interpreting neural machine translation)(posterior regularization, Used-for, integrating prior knowledge into neural machine translation)(chunk-based decoders, Used-for, neural machine translation)(convolutional neural networks, Used-for, extracting visual features)(in-domain data, Part-of, multi-modal neural machine translation)(zero-resource
(dialogue modeling, Compare, single-turn dialogue modeling)(dialogue modeling, Compare, multi-turn dialogue modeling)(dialogue modeling, Compare, retrieval-based dialogues)(dialogue modeling, Compare, generative conversational systems)(dialogue modeling, Compare, encoder-decoder dialog model)(dialogue modeling, Used-for, achieving a common goal)(dialogue modeling, Used-for, context-response matching)(dialogue modeling, Used-for, response generation)(dialogue modeling, Evaluate-for, quality of dialogue systems)(dialogue modeling, Evaluate-for, multi-domain dialogue tracking)(dialogue modeling, Evaluate-for, multi-turn response selection)(dialogue modeling, Evaluate-for, answer selection)(deep reinforcement learning, Used-for, zero pronoun resolution)(zero pronoun resolution, Evaluate-for, coreference links)(zero pronoun resolution, Evaluate-for, semantic information capturing)(transformer, Used-for, machine translation)(transformer, Compare, RNN-based approaches)(transformer, Compare, convolutional seq2seq model
(position encoding, Part-of, self-attention networks)(position encoding, Used-for, preserving word order information)(position encoding, Used-for, machine translation)(cross-lingual positional relationships, Part-of, cross-lingual scenarios)(cross-lingual positional relationships, Used-for, modeling positional relationships between languages)(cross-lingual positional relationships, Used-for, tackling word order divergences)(multilingual speech-to-text translation, Used-for, speech translation)(LayerNorm and Attention finetuning, Used-for, zero-shot crosslingual transfer)(LayerNorm and Attention finetuning, Used-for, zero-shot cross-modality transfer)(wav2vec 2.0, Used-for, acoustic modeling)(mBART, Used-for, multilingual text generation)(Cross-View Language Modeling, Used-for, aligning different views into a common semantic space)(Cross-View Language Modeling, Part-of, cross-lingual cross-modal pre-training)(conditional masked language modeling, Used-for, aligning views with
(neural topic model, Used-for, document context)(neural topic model, Compare, LDA topic model)(neural topic model, Used-for, coherent topics)(neural topic model, Is-a-Prerequisite-of, aspect extraction)(aspect extraction, Hyponym-Of, sentiment analysis)(aspect extraction, Used-for, coherent aspects)(word embeddings, Used-for, aspect extraction)(attention mechanism, Used-for, training)(syntactic information, Used-for, SRL performance)(interactive topic models, Used-for, understanding large collections of text)(Tandem Anchors, Compare, sampling-based interactive topic modeling) 
(word vector, Part-of, distributional vector space model)(word vector, Used-for, representing word semantics)(word vector, Used-for, semantic quality improvement)(morph-fitting procedure, Used-for, improving distributional vector spaces)(semantic lexicons, Compare, morphological constraints)(morphological constraints, Used-for, improving word vectors)(dialogue state tracking, Evaluate-for, morph-fitted vectors)(bridging models, Part-of, neural machine translation)(word vector, Used-for, analogy tasks)(word semantics, Evaluate-for, word vectors)(retrieved embeddings, Part-of, feed-forward networks)(neural network mappings, Part-of, cross-modal applications)(multilingual connotation frames, Used-for, public sentiment analysis).
(textual feature, Part-of, coreference resolver)(textual feature, Used-for, sentiment analysis)(textual feature, Used-for, sarcasm detection)(textual feature, Conjunction, cognitive feature)(cognitive NLP system, Used-for, sarcasm detection)(eye movement pattern, Part-of, cognitive feature)(EEG signal, Part-of, cognitive feature)(brain imaging, Part-of, cognitive feature)(lexical resource, Used-for, part-of-speech induction)(lexical resource, Used-for, named-entity recognition)(generative model, Evaluate-for, lexical resource)(domain adaptation, Evaluate-for, sentiment analysis)(sentiment classifier, Part-of, domain adaptation)(multi-modal neural machine translation model, Conjunction, spatial visual feature)(multi-modal neural machine translation model, Used-for, image description)(multi-modal neural machine translation model, Used-for, translation)(kernelized neural network, Evaluate-for, structured information)(kernelized neural network, Evaluate-for, feature representation)(Nyst
(language model, Part-of, neural language model)(neural language model, Evaluate-for, automatic generation of rhythmic poetry)(neural language model, Used-for, generating a representation of content)(language model, Evaluate-for, recurrent neural networks)(recurrent neural networks, Hyponym-Of, neural language model)(language model, Evaluate-for, LSTM)(LSTM, Hyponym-Of, neural language model)(recurrent neural networks, Is-a-Prerequisite-of, language model)(neural language model, Used-for, parsing)(neural language model, Used-for, language modeling)(language model, Used-for, disfluency detection)(neural language model, Evaluate-for, POS-tagging)(neural language model, Used-for, auto-completion).
(deep network, Part-of, hierarchical representations of a piece of text)(lower layers of the network, Used-for, distributing layer-specific attention weights to individual words)(higher layers of the network, Used-for, composing meaningful phrases and clauses)(distant supervision, Used-for, relation extraction)(distantly-supervised training samples, Part-of, distant supervision)(deep reinforcement learning strategy, Used-for, generating false-positive indicators for relation types)(sequence-to-sequence model, Used-for, single-input correction)(new decoder with multi-input attention averaging, Used-for, searching for consensus among multiple sequences)(attention mechanisms, Used-for, boosting performance on NLP tasks)(attention mechanisms, Used-for, identifying information that models found important)(attention weights, Part-of, attention mechanisms)(deep learning models, Used-for, solving math word problems)(group attention mechanism, Used-for, extracting global features in MWPs)(group attention mechanism, Used-for, extracting quantity-related features in MWPs)(group attention mechanism, Used-for
(aspect extraction, Part-of, aspect-based sentiment analysis)(neural word embeddings, Used-for, improving coherence of aspects)(attention mechanism, Used-for, de-emphasizing irrelevant words)(neural models, Used-for, minimum feature engineering)(neural word embeddings, Used-for, character and word embeddings)(POS tagging, Conjunction, dependency parsing)(joint Chinese analysis, Used-for, preventing error propagation)(speech register, Evaluate-for, word segmentation)(prosody, Evaluate-for, word segmentation)(Chinese word segmentation, Used-for, deep learning-based Chinese NLP)(Chinese NER, Used-for, lattice-structured LSTM)(open vocabulary problems, Part-of, neural machine translation)(lattice positional encoding, Used-for, improving NMT performance)(segmenting a chunk of text, Is-a-Prerequisite-of, processing Chinese text)(multi-grained lattice framework, Used-for, Chinese relation extraction)(lattice-based encoders, Used-for, word-level and subword-level representations)(Byte Pair
(nested named entity recognition, Hyponym-Of, named entity recognition)(named entity recognition, Is-a-Prerequisite-of, natural language processing)(nested named entity recognition, Evaluate-for, TAC-KBP2015)(nested named entity recognition, Evaluate-for, TAC-KBP2016)(nested named entity recognition, Evaluate-for, CoNLL 2003 NER task)(nested named entity recognition, Compare, flat annotation)(Long Short Term Memory, Used-for, sequence labeling)(Long Short Term Memory, Used-for, nested named entity recognition)(linear-chain CRFs, Used-for, nested named entity recognition)(BiLSTM, Used-for, nested named entity recognition)(multilingual named entity recognition, Part-of, nested named entity recognition)(BPEmb, Used-for, multilingual named entity recognition)(BPEmb, Compare, BERT)(BERT, Compare, FastText)(CoNLL 2003 NER task, Part-of, nested named entity recognition)(ACE-2004,
(grained sentiment, Hyponym-Of, sentiment analysis)(sentiment analysis, Used-for, sentiment classification)(sentiment analysis, Used-for, sentiment polarity)(CNN, Used-for, sentiment classification)(RNN, Used-for, sentiment classification)(multimodal sentiment analysis, Is-a-Prerequisite-of, LSTM-based model)(ONLG, Hyponym-Of, Opinionated Natural Language Generation)(ONLG, Used-for, subjective responses)(domain adaptation, Used-for, sentiment analysis)(cross-lingual sentiment analysis, Used-for, multilingual sentiment classification)(word embeddings, Evaluate-for, word similarity)(word embeddings, Evaluate-for, sentiment classification)(cross-domain sentiment classification, Used-for, transferring sentiment information)(BLSE, Hyponym-Of, Bilingual Sentiment Embeddings)(hypernym, Hyponym-Of, taxonomy learning)(multimodal word distributions, Hyponym-Of, word embeddings)(minimalist grammar, Used-for, parsing)(BLSE,
(neural text, Used-for, text classification)(neural text, Used-for, computational argumentation mining)(neural text, Used-for, neural symbolic machine)(neural text, Used-for, neural semantic parser)(neural text, Used-for, neural machine translation)(neural text, Used-for, learning commonsense knowledge)(neural text, Used-for, recurrent neural networks)(neural text, Used-for, convolutional neural network)(neural text, Used-for, dialog systems)(neural text, Used-for, constituency parsing)(neural text, Used-for, neural word segmentation)(neural text, Used-for, extraction of entity mentions and relations)(neural text, Used-for, text categorization)(neural text, Used-for, abstractive text summarization)(neural text, Used-for, Chinese word segmentation, POS tagging and dependency parsing)(multi-task learning, Compare, adversarial multi-task learning)(adversarial multi-task learning, Used-for, multi
(entity recognition ner, Is-a-Prerequisite-of, entity mentions)(entity recognition ner, Used-for, mention detection MD)(Attention-based recurrent neural network, Used-for, entity recognition ner)(long short term memory LSTM network, Used-for, semantic relations extraction)(Auto Content Extraction ACE corpora, Evaluate-for, entity recognition ner)(SPTree, Compare, Attention-based recurrent neural network)(lexical resources, Used-for, named-entity recognition)(named-entity recognition, Compare, part-of-speech induction)(named-entity recognition, Conjunction, mention detection MD)(named-entity recognition, Evaluate-for, CoNLL 2003 NER task)(named-entity recognition, Evaluate-for, TAC-KBP2015)(named-entity recognition, Evaluate-for, TAC-KBP2016)(feedforward neural network FFNN, Used-for, named-entity recognition)(annotation projection, Used-for, cross-lingual NER)(word embeddings, Used-for, cross
(neural language model, Used-for, perplexity)(perplexity, Evaluate-for, language model prediction)(neural language model, Used-for, sentence generation)(sequence-based model, Compare, document-based model)(LSTM, Hyponym-Of, neural language model)(Affect-LM, Used-for, conversational text generation)(token-level loss smoothing, Hyponym-Of, reward augmented maximum likelihood approach)(language model, Used-for, pun generation)(recurrent neural network grammar (RNNG), Is-a-Prerequisite-of, natural language comprehension)(recurrent neural network (RNN), Part-of, neural language model)(text classification, Used-for, mapping documents into categories)(semi-automated tool, Used-for, identifying trafficking ads)(query auto-completion, Used-for, search engine)(Transformer, Used-for, character level language modeling)(GPT, Hyponym-Of, pre-trained language model)(transformer, Hyponym-Of,
(link prediction, Used-for, knowledge graph completion)(link prediction, Used-for, holographic embeddings)(link prediction, Used-for, complex embeddings)(holographic embeddings, Compare, complex embeddings)(holographic embeddings, Part-of, knowledge graph completion)(complex embeddings, Part-of, knowledge graph completion)
(bias multilingual representation, Part-of, universal encoder)(bias multilingual representation, Used-for, multilingual unsupervised NMT scheme)(bias multilingual representation, Evaluate-for, monolingual counterparts)(monolingual models, Compare, multilingual models)(universal encoder, Used-for, inter-lingual representation)(inter-lingual representation, Part-of, multilingual unsupervised NMT)(multilingual unsupervised NMT, Used-for, back-translating)(universal encoder, Part-of, denoising autoencoding)(denoising autoencoding, Used-for, back-translating)(denoising autoencoding, Part-of, universal encoder)(bias multilingual representation, Evaluate-for, downstream task performance)(downstream task performance, Evaluate-for, pretraining data size)(multilingual models, Evaluate-for, typologically diverse languages)(pretraining data size, Compare, monolingual tokenizer).
(model generated summary, Used-for, sentence function)(model generated summary, Part-of, sequence-to-sequence network)(model generated summary, Evaluate-for, inter-annotator agreement)(model generated summary, Part-of, Non-autoregressive (NAR) models)(model generated summary, Compare, sentence function controlled response)(sentence function, Used-for, achieve the purpose of the speaker)(model generated summary, Evaluate-for, informativeness)(model generated summary, Evaluate-for, ROUGE metrics)(model generated summary, Evaluate-for, semantic relevance)(encoder-decoder model, Part-of, sequence-to-sequence network)(sequence-to-sequence network, Used-for, generate summaries)(focus-attention mechanism, Part-of, encoder)(transformer-based encoder-decoder framework, Part-of, sequence-to-sequence network)(model generated summary, Compare, summary retrieved from IR platform)(dependency structure, Part-of, target word sequence)(semantic relevance, Evaluate-for, summaries)(knowledge distillation, Evaluate-for, target-token
(information retrieval task, Part-of, natural language processing)(information retrieval task, Used-for, similarity search)(similarity search, Conjunction, information retrieval task)(information retrieval task, Evaluate-for, query-document relevance)(query-document relevance, Part-of, information retrieval task)(factual market data, Conjunction, information retrieval task)(cold-start problem, Evaluate-for, spam detection)(response selection, Used-for, retrieval-based chatbots)(sequential matching network, Used-for, response selection)(entity-oriented search, Hyponym-Of, information retrieval task)(neural ranking networks, Used-for, information retrieval task)(cross-lingual document retrieval, Hyponym-Of, information retrieval task)(query-document dataset, Part-of, information retrieval task)(generative semantic hashing, Used-for, information retrieval task)(multi-task learning, Used-for, query translation)(sentiment analysis, Used-for, volatility prediction)(knowledge graphs, Used-for, neural search systems).
(opinion entity extraction, Part-of, opinion mining)(opinion mining, Used-for, aspect-based sentiment analysis)(aspect-based sentiment analysis, Conjunction, aspect term extraction)(aspect-based sentiment analysis, Conjunction, opinion term extraction)(aspect term extraction, Part-of, aspect-based sentiment analysis)(opinion term extraction, Part-of, aspect-based sentiment analysis)(aspect-opinion pairs, Part-of, aspect-based sentiment analysis)(opinion entity extraction, Part-of, opinion summarization)(opinion mining, Is-a-Prerequisite-of, sentiment classification)
(transformer based language model, Hyponym-Of, language model)(u-PMLM, Hyponym-Of, transformer based language model)(MT-DNN, Hyponym-Of, transformer based language model)(SongNet, Hyponym-Of, transformer based language model)(T-TA, Hyponym-Of, transformer based language model)(SPECTER, Hyponym-Of, transformer based language model)(GAN-BERT, Hyponym-Of, transformer based language model)(DIM, Hyponym-Of, transformer based language model)(DIM, Used-for, Joint Learning)(T-TA, Evaluate-for, semantic similarity task)(MT-DNN, Used-for, domain adaptation)(T-TA, Evaluate-for, reranking task)(u-PMLM, Compare, BERT)(MT-DNN, Compare, BERT)(T-TA, Compare, BERT)(DIM, Evaluate-for, semantic parsing)(DIM, Evaluate-for, NL
(dependency parsing performance, Is-a-Prerequisite-of, dependency parsing)(dependency parsing performance, Evaluate-for, dependency parsing)(non-monotonic dynamic oracle, Improves, dependency parsing performance)(word embeddings, Used-for, dependency parsing)(BiLSTM models, Used-for, dependency parsing)(named entity recognition, Compare, dependency parsing)(singlish dependency treebank, Improves, dependency parsing performance)(joint Chinese analysis, Prevents, error propagation problem)(stack-pointer networks, Improves, dependency parsing performance)(parser adaptation, Improves, dependency parsing performance)(parser normalization, Part-of, parser adaptation)(maximum subgraph algorithms, Used-for, semantic dependency parsing)(pagenumber-2, Evaluate-for, dependency graph)(universal dependencies parsing, Part-of, multitask learning)(cross-lingual parsing, Compare, monolingual dependency parsing)(event coreference resolution, Improves, document understanding)(event mention distributional characteristics, Part-of, event core
(modeling morphological, Part-of, natural language processing)(modeling morphological, Used-for, morphological analysis)(morphological typology, Part-of, modeling morphological)(morphological inflection generation, Used-for, morphological analysis)(morphological tagging, Used-for, morphological analysis)(morphological well-formedness, Part-of, modeling morphological)(morphological variation, Part-of, modeling morphological)(morphological inflection generation, Compare, morphological tagging)(subword units, Conjunction, word segments)(subword units, Conjunction, characters)(subword units, Conjunction, character n-grams)(neural networks, Used-for, modeling morphological)(character-level models, Compare, subword units)(morphological typologies, Evaluate-for, character trigram representations)(modeling morphological, Conjunction, semantic role labeling)(machine translation, Compare, neural machine translation)(morphological analysis, Evaluate-for, gender bias)(morphological analysis, Is-a-Prerequisite
(word-embedding models, Used-for, word analogy questions)(word-embedding models, Used-for, caption generation)(word-embedding models, Part-of, Skip-Gram model)(word embeddings, Used-for, dependency parsing)(Skip-Gram model, Part-of, word-embedding models)(Skip-Gram models, Used-for, learning word vectors)(Skip-Gram model, Part-of, Sufficient Dimensionality Reduction (SDR) framework)(Neural word segmentation, Evaluates-for, character embeddings)(Neural word segmentation, Evaluates-for, word embeddings)(pretraining character embeddings, Is-a-Prerequisite-of, neural word segmentation)(Knowledge bases, Used-for, natural language processing tasks)(ITransF, Used-for, knowledge base completion)(sparse attention mechanism, Part-of, ITransF)(bilingual word embeddings, Compare, monolingual word embeddings)(bidirectional language models, Used-for, NLP tasks)(context embeddings, Part-of, bidirectional language models)(G
(multilingual machine translation, Part-of, Neural Machine Translation)(multilingual machine translation, Used-for, Chinese-to-English translation)(multilingual machine translation, Used-for, Japanese-English translation)(Neural Machine Translation, Compare, Statistical Machine Translation)(Neural Machine Translation, Used-for, Grammatical Error Correction)(Neural Machine Translation, Used-for, query-based summarization)(Neural Machine Translation, Used-for, sarcasm interpretation)(query-based summarization, Evaluate-for, ROUGE-L scores)(query-based summarization dataset, Part-of, query-based summarization)(Layer-wise relevance propagation, Used-for, interpreting Neural Machine Translation)(posterior regularization, Used-for, integrating prior knowledge into Neural Machine Translation)(chunk-based decoders, Used-for, Neural Machine Translation)(Multi-modal Neural Machine Translation, Used-for, image description and translation)(bidirectional tree encoder, Part-of, incorporating syntactic information)(sequence-to-sequence models, Used-for, Neural
(discourse segmenters, Part-of, discourse parsers)(discourse segmenters, Used-for, intra-sentential segment boundaries)(discourse segmenters, Used-for, identify elementary discourse units)(discourse segmenters, Evaluated-for, F1 score)(discourse parsers, Is-a-Prerequisite-of, discourse coherence)(discourse coherence, Used-for, many NLP tasks)(NLP tasks, Conjunction, summarization and language assessment)(neural frameworks, Used-for, discourse analysis)(discourse analysis, Part-of, discourse parsing)(discourse parsing, Used-for, tree construction)(neural semantic parser, Used-for, generate meaning representations)(Discourse Representation Theory, Part-of, neural semantic parser)(Discourse Representation Theory, Hyponym-Of, Discourse Representation Structures)(Discourse Representation Structures, Part-of, discourse representation tree structures)(discourse markers, Used-for, logical relationship)(NeuralREG, Compare, traditional Referring Expression Generation models).
(unsupervised morphological, Used-for, morphological analysis)(unsupervised morphological, Used-for, morphological paradigm completion)(unsupervised morphological, Used-for, identifying morphological typology)(unsupervised morphological, Used-for, low-resource languages)(morphological typology, Part-of, morphological analysis)(morphological paradigm completion, Part-of, unsupervised morphological analysis)(morphological paradigm completion, Used-for, generating inflected forms)(low-resource languages, Evaluate-for, unsupervised morphological paradigm completion)(EDITH TREE, Part-of, morphological paradigm completion)(lemma list, Part-of, morphological paradigm completion)(paradigm size discovery, Part-of, morphological paradigm completion)(inflection generation, Part-of, morphological paradigm completion)(inflection generation, Used-for, generating morphological paradigms)(crisis counseling, Evaluate-for, conversation progress)(counselor behaviors, Evaluate-for, conversation flow)(word formation, Used-for, NMT)(word formation, Hyponym-Of
(natural language text, Used-for, learning commonsense knowledge)(natural language text, Used-for, generating answer rationales)(natural language text, Used-for, parsing into source code)(neural semantic parser, Used-for, converting natural language text)(induced predicate-argument structures, Part-of, neural semantic parser)(answer rationales, Part-of, solving algebraic word problems)(natural language text, Part-of, answer rationales)(emotion detection, Used-for, understanding individuals)(text similarity measures, Used-for, plagiarism detection)(reading comprehension, Used-for, evaluating RC datasets)(semantic parser, Is-a-Prerequisite-of, question-answering systems)(encoder-decoder framework, Part-of, COREQA)(affective information, Part-of, conversational text generation)(core programming language, Part-of, naturalized language)(knowledge bases, Used-for, natural language processing tasks)
(neural text matching models, Part-of, multi-turn information-seeking conversation systems)(multi-turn information-seeking conversation systems, Used-for, enhancing chatbot performance)(neural encoder-decoder model, Used-for, single document summarization)(single document summarization, Compare, multi-document summarization)(unsupervised neural text simplification, Used-for, simplifying text)(unlabeled text corpora, Used-for, training unsupervised neural text simplification models)(neural text generation modeling, Used-for, creating intelligent email features)(neural text generation modeling, Evaluate-for, potential for misuse)(continuous-time deconvolutional regressive neural network, Used-for, studying human language processing dynamics)(zero and few-shot text classification, Used-for, building text classifiers with minimal data)(neural theorem-proving, Used-for, proving mathematical theorems).
(language model fine tuning, Used-for, text classification)(language model fine-tuning, Used-for, downstream tasks)(language model, Part-of, text classification)(experimental results, Evaluate-for, sentiment classifiers)(pretraining, Is-a-Prerequisite-of, language model fine tuning)(pretraining, Used-for, language model fine tuning)(supervised learning, Compare, pretraining)(neural language model, Used-for, poetry generation)(Affect-LM, Used-for, conversational text generation)(neural language model, Part-of, conversational text generation)(LSTM language model, Used-for, conversational text generation)(bilingual language model, Compare, monolingual language model)(PMLM, Compare, BERT)
(semantic retrieval, Part-of, information retrieval)(semantic retrieval, Compare, traditional retrieval)(semantic retrieval, Used-for, multi-turn conversation)(semantic retrieval, Part-of, knowledge graph)(semantic retrieval, Part-of, entity-oriented search)(semantic retrieval, Used-for, response selection)(sequential matching network, Used-for, response selection)(response selection, Part-of, multi-turn conversation)(multi-turn conversation, Compare, single-turn conversation)(attention-based recurrent neural network, Used-for, entity mention extraction)(recurrent neural network, Part-of, sequential matching network)(responses, Part-of, multi-turn conversation)(neural encoder-decoder transition-based parser, Used-for, semantic graph parsing)(attention-based LSTM network, Used-for, entity mention extraction)(Auto Content Extraction corpora, Evaluate-for, relation extraction performance)(entity mention extraction, Part-of, joint model)(semantic representation, Part-of, NLP)(semantic representation, Compare, syntactic
(bilingual word embeddings, Part-of, cross-lingual model transfer)(bilingual word embeddings, Evaluate-for, bilingual lexicon induction)(bilingual word embeddings, Used-for, translation model)(bilingual word embeddings, Part-of, bilingual tasks)(cross-lingual model transfer, Used-for, predicting annotations)(bilingual lexicon induction, Part-of, bilingual tasks)(translation model, Used-for, domain adaptation)(cross-lingual word embeddings, Part-of, cross-lingual model transfer)(cross-lingual word embeddings, Used-for, bilingual text embeddings)(cross-lingual word embeddings, Part-of, domain adaptation)(cross-lingual word embeddings, Used-for, joint training)(text embedding, Used-for, cross-lingual word embeddings)(text embedding, Used-for, bilingual text embeddings).
(event extraction, Used-for, knowledge base population)(distant supervision, Used-for, event extraction)(key arguments, Evaluate-for, event extraction)(trigger words, Evaluate-for, event extraction)(neural network, Used-for, event extraction)(supervised learning, Hyponym-Of, event extraction)(argument information, Part-of, event detection)(neural stacking, Used-for, dependency parsing)(event detection, Is-a-Prerequisite-of, event extraction)(syntactic knowledge, Used-for, dependency parsing)(logogram features, Used-for, event detection)(CRF, Used-for, aspect extraction)(Pocket Knowledge Base Population, Used-for, event extraction)(conditional random fields, Hyponym-Of, CRF)(Nugget Proposal Networks, Used-for, event detection)(supervised attention mechanisms, Used-for, event detection)(sharing strategies, Used-for, multi-task architecture)
(entity detection, Part-of, NLP tasks)(entity detection, Evaluate-for, ACE 2005 dataset)(entity detection, Used-for, predicting whether a sentence reports a health condition)(entity detection, Used-for, joint entity relation extraction)(entity detection, Used-for, event coreference)(entity detection, Used-for, relation extraction)(entity detection, Compare, sequence labeling methods)(entity detection, Compare, local detection approach).
(rumor detection, Hyponym-Of, stance classification)(rumor detection, Used-for, identifying different type of rumors)(rumor detection, Used-for, early stage rumor detection)(rumor detection, Used-for, analyzing code-switched text)(rumor detection, Conjunction, stance classification)(multi-task learning, Used-for, rumor detection)(user credibility information, Part-of, rumor detection)(attention mechanism, Part-of, rumor detection)
(harnessing statistical power, Compare, language generation nlg)(harnessing statistical power, Used-for, language understanding)(neural programmer, Part-of, Neural Symbolic Machine)(symbolic computer, Part-of, Neural Symbolic Machine)(sequence-to-sequence model, Used-for, language generation nlg)(sequence-to-sequence model, Part-of, neural programmer)(task reward, Evaluate-for, REINFORCE)(iterative maximum-likelihood training, Evaluate-for, REINFORCE)(similarity measures, Evaluate-for, plagiarism detection)(similarity measures, Evaluate-for, textual entailment)(word-level language detection, Used-for, code-switched text)(multilingual text, Compare, code-switched text)(language generation nlg, Part-of, natural language generation)(reading comprehension datasets, Evaluate-for, natural language understanding)(question answering methods, Compare, knowledge base)(database queries, Used-for, SQL)(SQL, Hyponym-Of, database query languages)(sentence
(natural language understanding, Used-for, extracting knowledge from natural language text)(natural language understanding, Used-for, learning commonsense knowledge)(natural language understanding, Used-for, dialogue state tracking)(natural language understanding, Evaluate-for, reading comprehension datasets)(natural language understanding, Used-for, interpreting multiagent policies)(natural language understanding, Used-for, solving algebraic word problems)(natural language understanding, Used-for, parsing natural language descriptions into source code)(natural language understanding, Used-for, generating answer rationales)(natural language understanding, Used-for, generating answer in natural language sentence)(natural language understanding, Used-for, generating poetic text)(neural symbolic machine, Part-of, natural language understanding)(semantic parsing, Part-of, natural language understanding)(morph-fitting procedure, Used-for, vector space models)(morph-fitting procedure, Improve, representation for low-frequency word forms)(morphologically rich languages, Compare, morphologically poor languages)(distributional vector space models, Part-of, natural language understanding)(
(semantic parsing, Hyponym-Of, lingual semantic parsing)(semantic parsing, Used-for, mapping natural language utterances into executable programs)(semantic parsing, Used-for, question answering)(semantic parsing, Part-of, neural architecture)(semantic parsing, Part-of, multi-task learning problem)(semantic parsing, Part-of, object-oriented neural programming)(semantic parsing, Used-for, confidence modeling)(semantic parsing, Used-for, state-of-the-art QA systems)(semantic parsing, Used-for, sequential question answering task)(semantic parsing, Part-of, reinforcement learning)(semantic parsing, Part-of, maximum marginal likelihood)(confidence modeling, Evaluate-for, neural semantic parsers)(neural architecture, Used-for, capturing target syntax)(multi-task learning problem, Used-for, keyphrase boundary classification)(object-oriented neural programming, Used-for, document parsing in specific domains)(neural architecture, Used-for, generating complex programs)(neural architecture, Part-of, grammar model)(natural language descriptions, Used-for, source
(neural semantic parser, Compare, traditional phrase-based systems)(neural semantic parser, Used-for, converting natural language to intermediate representations)(predicate-argument structures, Part-of, neural semantic parser)(predicate-argument structures, Used-for, representing semantic parsing)(predicate-argument structures, Compare, linguistically motivated structures)(COREQA, Used-for, generating natural answers)(encoder-decoder framework, Part-of, COREQA)(copying and retrieving mechanisms, Part-of, COREQA)(semantic units, Part-of, COREQA)(EviNets, Compare, traditional QA systems)(neural network, Part-of, EviNets)(candidate answer entities, Part-of, EviNets)(BLEU score, Evaluate-for, neural machine translation system)(softmax, Compare, binary codes)(error-correcting codes, Compare, standard encoding methods)(adversarial examples, Used-for, evaluating neural image captioning systems)(Show-and-Fool, Used-for, creating advers
(aspect sentiment classification, Hyponym-Of, sentiment analysis)(aspect sentiment classification, Used-for, classifying sentiment polarities over individual opinion targets)(aspect sentiment classification, Used-for, determining sentiment polarity on a target in the sentence)(aspect sentiment classification, Evaluate-for, memory networks)(aspect sentiment classification, Evaluate-for, attention mechanism)(sentiment analysis, Is-a-Prerequisite-of, aspect sentiment classification)(memory networks, Part-of, aspect sentiment classification)(attention mechanism, Part-of, aspect sentiment classification)(sentiment classification, Hyponym-Of, sentiment analysis)(document-level data, Used-for, improving performance of aspect-level sentiment classification)(negation words, Part-of, sentiment classification)(intensity words, Part-of, sentiment classification)(word embeddings, Used-for, sentiment classification)(word embeddings, Part-of, semantic representations of words)(attention mechanism, Used-for, detecting sentiment context)(target-sensitive memory networks, Evaluate-for, aspect sentiment classification)
(hierarchical attention network, Used-for, reading comprehension)(hierarchical attention network, Part-of, neural Transformer)(hierarchical attention network, Part-of, neural attention mechanisms)(hierarchical attention network, Hyponym-Of, attention mechanisms)(hierarchical attention network, Used-for, claim verification)(hierarchical attention network, Compare, self-attention network)(hierarchical attention network, Part-of, adversarial attention network)(hierarchical attention network, Used-for, Multi-sentiment-resource Enhanced Attention Network)(reading comprehension, Evaluate-for, narrative paragraph)(attention network, Used-for, sentence encoding)(Transformer, Used-for, machine translation)(attention network, Part-of, Transformer)(Transformer, Compare, hierarchical attention network)(attention, Part-of, attention network)(attention mechanisms, Part-of, neural networks).
(spelling error, Used-for, detect native language)(spelling error, Used-for, improve classification accuracy)(spelling error, Part-of, lexical features)(detect native language, Evaluate-for, spelling error)(spelling error, Evaluate-for, TOEFL11 corpus)(spelling error features, Part-of, improve classification accuracy)(spelling error correction, Is-a-Prerequisite-of, language understanding ability)(spelling error, Used-for, detect errors in Chinese characters)(spelling error correction, Match, phonological and visual similarity knowledge)(scene-level task, Compare, token-level models)(scene-level task, Used-for, joint entity mention detection and relation extraction)(spelling errors, Part-of, source of information for native language detection)(neural models, Used-for, common sense inference)(common sense inference, Used-for, detecting gender inequality in movie scripts)(machine translation, Compare, spelling error correction)(machine translation, Compare, spelling error detection)(input modes, Compare, pen,
(adversarial training, Used-for, adversarial example)(adversarial example, Part-of, robustness improvement)(adversarial example, Evaluate-for, neural models)(adversarial example, Evaluate-for, neural classifier)(adversarial example, Evaluate-for, image captioning systems)(adversarial example, Evaluate-for, NLP tasks)(visual question answering model, Evaluate-for, adversarial example)(word replacement, Used-for, adversarial example)(negation, Evaluate-for, adversarial example).
(attention-based explanation, Part-of, attention mechanisms)(attention mechanisms, Used-for, neural networks)(neural networks, Used-for, text classification task)(human attention maps, Used-for, comparative analysis)(comparative analysis, Evaluate-for, machine attention maps)(human attention maps, Part-of, human versus computational attention mechanisms)(machine attention maps, Part-of, human versus computational attention mechanisms)(machine attention maps, Used-for, text classification tasks)(attention-based model, Used-for, AMR parsing)(AMR parsing, Is-a-Prerequisite-of, AMR 2.0 evaluations)(attention-based neural network models, Used-for, aspect-based sentiment analysis)(aspect-based sentiment analysis, Evaluate-for, sentiment polarity)(attention-based neural network models, Conjunction, relational graph attention network (R-GAT))(reinforcement learning algorithm, Used-for, hard-attention mechanism training)(hard-attention mechanism, Used-for, long sequence translation)(h
(morphological analysis, Part-of, morphological inflection generation)(morphological analysis, Used-for, predicting syntactic traits)(morphological analysis, Evaluate-for, language understanding systems)(morphological analysis, Used-for, morphological disambiguation)(morphological inflection generation, Used-for, generating inflectional forms)(morphological disambiguation, Evaluate-for, language-dependent state of the art)(inflectional forms, Hyponym-Of, morphology)(derivational antonyms, Hyponym-Of, morphology)(morph-fitting procedure, Used-for, improving distributional vector spaces)(analogical reasoning, Used-for, capturing linguistic regularities)(Universal Dependencies, Part-of, syntactic tree processing)(syntactic tree processing, Evaluate-for, reducing cross-lingual syntactic variation)(cross-lingual training, Used-for, improving performance in low-resource languages)(factorial conditional random fields, Used-for, cross-lingual morphological tagging)(tokenization
(attention based, Used-for, question-aware passage representation)(attention based, Used-for, self-matching attention)(attention based, Evaluate-for, question answering)(attention based, Used-for, LSTM network)(attention based, Used-for, sequence learning model)(attention based, Used-for, neural machine translation)(attention based, Used-for, morphological inflection generation)(attention based, Used-for, dependency parsing)(attention based, Used-for, dialog systems)(attention based, Used-for, Chinese implicit discourse relations)(attention based, Used-for, multi-turn response selection)(attention based, Used-for, hierarchical attention network)(attention based, Used-for, Transformer)(attention based, Used-for, semantic relatedness tasks)(attention based, Conjunction, LSTM network)(attention based, Conjunction, question-aware passage representation)
(sentence compression model, Used-for, sentence compression)(sentence compression, Hyponym-Of, unsupervised summarization)(long short-term memory neural network model, Used-for, sentence compression model)(explicit syntactic features, Used-for, sentence compression model)(syntactic constraints, Used-for, sentence compression model)(integer linear programming, Used-for, syntactic constraints)(syntactic information, Part-of, sentence compression model)(recurrent neural network, Used-for, sentence compression model)(quantifiers, Evaluate-for, linguistic context)(language model, Used-for, evaluator)(language model, Part-of, syntactic neural language model)(syntactic neural language model, Part-of, evaluator)(reinforcement learning, Used-for, evaluator)(reinforcement learning, Used-for, sentence compression model)(human evaluation, Evaluate-for, grammaticality)(word graph approach, Used-for, multi-sentence compression)(lexical substitution, Used-for, word graph approach)(neural rewriter, Used-for, multi-sentence
(machine translation system, Part-of, neural machine translation)(machine translation system, Used-for, generating translations)(machine translation system, Part-of, neural machine translation systems)(neural machine translation, Used-for, English-Romanian translation)(neural machine translation, Used-for, English-German translation)(neural machine translation, Used-for, English-French translation)(neural machine translation, Used-for, Chinese-English translation)(neural machine translation, Used-for, Japanese-English translation)(neural machine translation, Used-for, grammatical error correction)(neural machine translation, Used-for, integrating prior knowledge)(neural machine translation, Used-for, incorporating syntactic trees)(neural machine translation, Used-for, sequence-to-dependency translation)(neural machine translation, Used-for, chunk-based translation)(neural machine translation, Used-for, multi-modal translation)(neural machine translation, Used-for, zero-resource translation)(machine translation, Hyponym-Of, neural machine translation)(neural machine
None
(neural language model, Hyponym-Of, language model)(phonetic encoding, Used-for, neural language model)(event language model, Hyponym-Of, language model)(neural belief tracking, Part-of, spoken dialogue systems)(Affect-LM, Part-of, LSTM language model)(Affect-LM, Used-for, generation of conversational text)(belief tracker, Part-of, spoken dialogue systems)(LSTM Noisy Channel Model, Part-of, disfluency detection)(unsupervised NMT, Is-a-Prerequisite-of, machine translation)(character representations, Hyponym-Of, subword units)(Text Deconvolution Saliency, Used-for, text classification)(word-level language detection, Used-for, code-switched text analysis)(ULMFiT, Hyponym-Of, transfer learning)(neural language model, Compare, sentence-based model)(MWC, Used-for, open-vocabulary language modeling)(neural language model, Compare
(event argument extraction, Part-of, event argument detection)(event argument detection, Part-of, event extraction)(event extraction, Hyponym-Of, information extraction)(neural architecture, Compare, conventional Open IE systems)(MKPNet, Compare, previous methods)(conventional Open IE systems, Compare, hand-crafted patterns from other NLP tools)(neural Open IE approach, Used-for, event argument extraction)(document-level event extraction, Used-for, event argument extraction)(Heterogeneous Graph-based Interaction Model with a Tracker, Used-for, document-level event extraction)(multi-granularity reader, Used-for, document-level event extraction)(Bi-directional Entity-level Recurrent Decoder, Used-for, event argument extraction)(Frame-aware Event Argument Extraction, Used-for, implicit event argument extraction)(Knowledge projection paradigm, Used-for, event relation extraction)(Potential Relation and Global Correspondence, Used-for, relation prediction)
(unsupervised syntactic parsing, Used-for, unsupervised constituency parsing)(unsupervised syntactic parsing, Used-for, unsupervised discourse dependency parsing)(unsupervised syntactic parsing, Used-for, syntax-aware language model)(unsupervised constituency parsing, Is-a-Prerequisite-of, unsupervised syntactic parsing)(syntax-aware language model, Evaluate-for, core NLP tasks)(Parse Tree, Part-of, unsupervised constituency parsing)(probabilistic context-free grammars (PCFGs), Evaluate-for, syntax-aware language models)(syntax-aware language model, Compare, discriminative model like BERT with a syntax module)(syntax-aware language model, Is-a-Prerequisite-of, unsupervised syntactic parsing)
(Long Short-Term Memory, Hyponym-Of, short term memory)(Hidden Markov Model, Compare, Long Short-Term Memory)(Affect-LM, Used-for, generation of conversational text)(Affect-LM, Used-for, learn affect-discriminative word representations)(attention-based recurrent neural network, Used-for, joint extraction of entity mentions and relations)(bidirectional long short-term memory, Used-for, relation extraction)(disfluency detection, Used-for, spontaneous speech transcripts)(Global Context Layer, Part-of, context-aware neural network)(Tree Long Short-Term Memory Networks, Hyponym-Of, Long Short-Term Memory)(syntax-infused variational autoencoder, Part-of, long short-term memory architectures)(conceptual complexity of texts, Evaluate-for, psycholinguistic theories of reading comprehension)(feature extractor, Used-for, detecting unknown intents)(Transformers, Used-for, modeling language).
(neural network models, Conjunction, multi-task learning)(multi-task learning, Used-for, learning shared layers)(shared layers, Used-for, extract common features)(shared features, Compare, task-specific features)(adversarial multi-task learning, Used-for, alleviate interference between shared and private feature spaces)(neural techniques, Used-for, end-to-end computational argumentation mining)(dependency parsing, Compare, sequence tagging)(sequence tagging, Part-of, argumentation mining)(BiLSTMs, Used-for, classification scenarios)(BiLSTMs, Compare, traditional RNNs)(Neural Symbolic Machine, Conjunction, neural programmer)(neural programmer, Part-of, Neural Symbolic Machine)(symbolic computer, Part-of, Neural Symbolic Machine)(REINFORCE, Used-for, optimize task reward)(stochastic gradient Markov Chain Monte Carlo, Used-for, learn weight uncertainty in RNNs)(Bayesian learning algorithm, Used-for, enhance
(speech translation, Hyponym-Of, speech)(speech, Part-of, speech translation)(s2st, Hyponym-Of, speech translation)(speech-to-speech translation, Hyponym-Of, s2st)(end-to-end models, Used-for, speech translation)(cascaded models, Compare, end-to-end models)(speech encoder, Part-of, SimulSpeech)(speech segmenter, Part-of, SimulSpeech)(text decoder, Part-of, SimulSpeech)(CTC loss, Used-for, speech segmenter)(BLEU, Evaluate-for, translation quality)(auditory speech input, Used-for, speech-to-speech translation)(discrete speech encoder, Part-of, s2st)(sequence-to-sequence speech-to-unit translation, Used-for, s2st).
(neural semantic parser, Used-for, converting natural language utterances)(morph-fitting procedure, Used-for, improving distributional vector spaces)(morph-fitted vectors, Evaluate-for, dialogue state tracking)(kernel methods, Used-for, language learning)(neural machine translation models, Evaluate-for, learning morphology)(layer-wise relevance propagation, Used-for, interpreting neural MT models)(convolutional neural network, Used-for, local coherence model)(word representations, Part-of, morpheme segmentation)(grounded verb semantics, Used-for, human-robot communication)(multimodal word distributions, Evaluate-for, word similarity and entailment)(pretrained context embeddings, Used-for, sequence labeling tasks)(bidirectional language models, Evaluate-for, named entity recognition)(bidirectional tree encoder, Part-of, source-side syntactic trees)(sequential encoder-decoder framework, Compare, bidirectional tree encoder)(word segments, Part-of, word representations)(word representations, Evaluate-for, language modeling
(abstractive summarization model, Used-for, generating a shorter version of the document covering all the salient points)(abstractive summarization model, Compare, extractive summarization)(abstractive summarization model, Hyponym-Of, summarization model)(encode-attend-decode paradigm, Used-for, machine translation)(encode-attend-decode paradigm, Used-for, dialog systems)(encode-attend-decode paradigm, Used-for, extractive summarization)(encode-attend-decode paradigm, Used-for, query-based summarization)(query attention model, Part-of, query-based summarization)(document attention model, Part-of, query-based summarization)(diversity based attention model, Part-of, query-based summarization)(query-based summarization, Used-for, highlighting points relevant to the query)(query-based summarization, Evaluate-for, ROUGE-L scores)(abstractive summarization model, Evaluate-for, ROUGE scores)(pointer-generator network, Part
(semantic decoding, Used-for, generating formal meaning representations)(semantic decoding, Used-for, decoding process into stages)(semantic decoding, Part-of, semantic parsing)(deep highway BiLSTM architecture, Used-for, semantic role labeling)(dynamic neural semantic parsing framework, Hyponym-Of, semantic decoding)(deep neural architecture, Part-of, semantic decoding)(bookembedding framework, Used-for, semantic dependency parsing)(multilingual model, Is-a-Prerequisite-of, semantic parsing in multiple languages)(abstract syntax networks, Used-for, code generation)(constraint satisfaction problem, Used-for, poetry generation)(neural language model, Used-for, learning poetic devices)(Lagrangian Relaxation-based algorithm, Used-for, combining pages into a book)(retrofitting, Used-for, embedding semantic relations)(neural semantic parser, Used-for, generating Discourse Representation Structures)(Variational auto-encoding model, Conjunction, semantic parsing)(sequence-to-tree model, Hypon
(entity representation, Used-for, improving distributional vector spaces)(entity representation, Used-for, question representation)(entity representation, Used-for, feature representations)(entity representation, Used-for, word embeddings)(entity representation, Used-for, answer selection)(distributional vector spaces, Part-of, language understanding systems)(question representation, Part-of, question answering system)(feature representations, Part-of, deep neural networks)(word embeddings, Hyponym-Of, entity representation)(answer selection, Part-of, question answering)
(semantic representation, Compare, syntactic schemes)(phonetic encoding, Part-of, neural language model)(rhyme, Part-of, phonetic encoding)(rhythm, Part-of, phonetic encoding)(alliteration, Part-of, phonetic encoding)(HCN, Used-for, dialog system)(SUMMARIZATION, Used-for, sentence summarization)(grammar, Compare, lexicalized grammars)(probabilistic word embedding, Compare, dictionary-level probabilistic embeddings)(DSMN, Used-for, geometric reasoning)(Low-rank Multimodal Fusion, Used-for, multimodal fusion)(DihEdral, Used-for, link prediction).
(fake news detection, Used-for, deception detection)(fake news detection, Evaluate-for, real-world political impacts)(fake news detection, Used-for, fact-checking research)(fake news detection, Evaluate-for, surface-level linguistic patterns)(fake news detection, Part-of, automatic fake news detection)(fake news detection, Part-of, semi-automatic fake news detection)(fake news detection, Evaluate-for, hyperpartisan news analysis)(fake news detection, Is-a-Prerequisite-of, partisanship detection)(fake news detection, Used-for, stance detection)(fake news detection, Evaluate-for, social media)(fake news detection, Evaluate-for, knowledge element level detection)(fake news detection, Evaluate-for, reasoning over evidence)(fake news detection, Used-for, multi-modal detection)(fake news detection, Evaluate-for, news environment perception)(fake news detection, Part-of, misinformation prevention)(fake news detection, Evaluate-for, detecting biased language)(fake news detection, Evaluate-for, hallucination detection)(LI
(event mention, Part-of, entity mention)(attention, Used-for, joint extraction)(LSTM network, Used-for, joint extraction)(dependency trees, Compare, attention)(attention, Used-for, extraction)(agent-artifact relations, Part-of, semantic relations)(Physical relations, Part-of, semantic relations)(Part-Whole relations, Part-of, semantic relations)(entity typing task, Is-a-Prerequisite-of, entity mention)(given a sentence with an entity mention, Used-for, predict types for target entity)(discontinuous spans, Part-of, entity mention)(nested entity mentions, Part-of, entity mention)(span prediction, Used-for, coreference resolution)(factuality prediction, Is-a-Prerequisite-of, event mention)(multi-task learning, Used-for, dialogue coherence)(discontinuous mentions, Part-of, biomedical NER datasets)(sequence-to-nuggets architecture, Used-for, nested mention detection)(nested entity mention detection, Used-for, fine-grained entity mentions)(LSTM
(document level relation extraction, Evaluate-for, information integration)(document level relation extraction, Used-for, capturing complex interactions)(complex interactions, Part-of, document level relation extraction)(embedding relevant information, Is-a-Prerequisite-of, document level relation extraction)(document-level graph, Used-for, document level relation extraction)(relational reasoning, Used-for, document level relation extraction)(multi-hop reasoning, Part-of, document level relation extraction)(syntactic trees, Compare, co-references)(document, Part-of, document level relation extraction).
(cross lingual summarization, Used-for, summarizing a document in one language into another language)(cross lingual summarization, Conjunction, Chinese-to-English summarization tasks)(cross lingual summarization, Conjunction, English-to-Chinese summarization tasks)(multi-task learning, Used-for, train a Neural Machine Translation model with a Relevance-based Auxiliary Task for search query translation)(Neural Machine Translation, Used-for, translation process for Cross-lingual Information Retrieval)(bilingual lexicon induction, Conjunction, cross-lingual classification)(bilingual tasks, Is-a-Prerequisite-of, overcoming data sparsity in the target language)(domain adaptation, Is-a-Prerequisite-of, bilingual tasks).
(sentence embeddings, Hyponym-Of, word embeddings)(sentence embeddings, Used-for, text classification task)(sentence embeddings, Evaluate-for, sentence classification)(sentence embeddings, Part-of, AMR semantic graphs)(sentence embeddings, Used-for, paraphrastic sentence embeddings)(sentence embeddings, Used-for, multilingual distributed representations of text)(sentence embeddings, Used-for, machine translation)(sentence embeddings, Used-for, sentence similarity evaluation)(sentence embeddings, Used-for, downstream tasks).
(semantic parsing, Is-a-Prerequisite-of, shot semantic parsing)(semantic parsing, Used-for, converting natural language to code)(semantic parsing, Used-for, structured meaning representations)(semantic parsing, Conjunction, error detection)(semantic parsing, Conjunction, named entity recognition)(semantic parsing, Conjunction, chunking)(semantic parsing, Conjunction, POS-tagging)(semantic parsing, Used-for, simplifying text)(semantic parsing, Used-for, generating SQL queries)(semantic parsing, Used-for, answering sequences of questions)(semantic parsing, Used-for, learning classifiers from explanations)(semantic parsing, Hyponym-Of, sequence labeling)(semantic parsing, Used-for, counterfactual learning)(semantic parsing, Hyponym-Of, AMR parsing)(semantic parsing, Used-for, generating semantic graphs)(semantic parsing, Used-for, sentiment classification)(semantic parsing, Used-for, object-oriented neural programming)(semantic parsing, Used-for, active learning)(semantic graph, Part-of, semantic parsing)(
(multi hop reading comprehension, Part-of, machine reading comprehension)(machine reading comprehension, Compare, multi-passage reading comprehension)(multi hop reading comprehension, Used-for, answering questions from multiple documents)(Heterogeneous Document-Entity graph, Used-for, multi hop reading comprehension)(Explore-Propose-Assemble reader, Used-for, multi hop reading comprehension)(cognitive graph, Used-for, multi hop reading comprehension)(BERT and graph neural network, Used-for, multi hop reading comprehension)(multi hop reading comprehension, Evaluate-for, state-of-the-art performance)(multi hop reading comprehension, Evaluate-for, reasoning across documents)
(human annotated explanation, Hyponym-Of, explanation)(explanation, Used-for, generating labels)(explanation, Used-for, providing natural language explanations)(human annotated explanation, Compare, auto-generated explanation)(natural language explanation, Is-a-Prerequisite-of, classification tasks)(human annotated explanation, Used-for, causality understanding)(human annotated explanation, Evaluate-for, causal facts)(causal reasoning, Hyponym-Of, understanding causality)(causal reasoning, Evaluate-for, state-of-the-art models)(human annotated explanation, Used-for, zero-shot classifiers)
(annotated data, Used-for, training)(annotated data, Evaluate-for, performance)(annotated data, Part-of, zero pronoun resolution)(annotated data, Part-of, grammatical error correction)(annotated data, Part-of, implicit discourse relation classification)(annotated data, Part-of, error detection)(annotated data, Part-of, named entity recognition)(annotated data, Part-of, semantic role labeling)(annotated data, Part-of, semantic parsing)(annotated data, Part-of, Japanese predicate-argument structure analysis)(annotated data, Part-of, pun generation)(annotated data, Part-of, natural language inference)(annotated data, Part-of, sentiment analysis)(zero pronoun resolution, Compare, grammatical error correction)(zero pronoun resolution, Compare, implicit discourse relation classification)(zero pronoun resolution, Compare, error detection)(zero pronoun resolution, Compare, named entity recognition
(question answering qa, Part-of, reading comprehension)(question answering qa, Used-for, COREQA)(question answering qa, Used-for, knowledge base question answering KB-QA)(question answering qa, Used-for, generative domain-adaptive nets)(question answering qa, Used-for, triviaQA)(question answering qa, Used-for, semantic parsing)(question answering qa, Used-for, machine reading)(question answering qa, Used-for, EviNets)(question answering qa, Used-for, Open Information Extraction Open IE)(question answering qa, Used-for, universal schema)(question answering qa, Used-for, transfer learning)(reading comprehension, Used-for, SQuAD dataset)(COREQA, Hyponym-Of, question answering qa)(knowledge base question answering KB-QA, Hyponym-Of, question answering qa)(generative domain-adaptive nets, Hyponym-Of, question answering qa)(triviaQA, Hyponym-Of, question answering qa)(semantic parsing
