### Triplets:1. spelling correction, Is-a-Prerequisite-of, Chinese Spelling Correction (CSC)2. spelling correction, Used-for, Autoregressive Decoding3. spelling correction, Is-a-Prerequisite-of, Grammatical Error Correction Systems4. spelling correction, Compare, Language Understanding5. spelling correction, Is-a-Prerequisite-of, GEC System6. spelling correction, Is-a-Prerequisite-of, CSC Methods7. spelling correction, Is-a-Prerequisite-of, Autoregressive Decoding8. spelling correction, Is-a-Prerequisite-of, Language Model
### Triplets:1. (Neural network, Used-for, multi-task learning)2. (Neural network, Compare, recurrent neural networks)3. (Neural network, Compare, symbolic reasoning)4. (kernel methods, Combine, deep neural networks)5. (convolutional layers, Part-of, faster and simpler architecture)6. (RNNs, Is-a-Prerequisite-of, overfitting)7. (DNNs, Is-a-Prerequisite-of, enhanced Neural Machine Translation)8. (Gated self-matching networks, Evaluate-for, reading comprehension style question answering)9. (residual learning, Used-for, hierarchical recurrent neural network)10. (discourse-level diversity, Evaluate-for, diverse responses)
### Triplets:1. probabilistic context free grammar, is-a-Prerequisite-of, grammar induction2. probabilistic context free grammar, is-a-Prerequisite-of, hierarchy of language classes3. probabilistic context free grammar, Compare, stochastic grammar4. probabilistic context free grammar, Evaluate-for, grammar induction5. probabilistic context free grammar, is-a-Prerequisite-of, probabilistic linear context-free rewriting system (LCFRS) formalism6. probabilistic context free grammar, is-a-Prerequisite-of, probabilistic context-free-grammars (PCFGs)7. probabilistic context free grammar, Used-for, parsing decision8. probabilistic context free grammar, Used-for, language model9. probabilistic context free grammar, Compare, compound probabilistic context free grammar10. grammar induction, Used-for, generating sentences11. grammar induction, is-a-Prerequisite-of, probabilistic context free grammar
1. (SCFG, Is-a-Prerequisite-of, natural language processing)2. (SCFG, Hyponym-Of, CFG)3. (STAG, Part-of, SCFG)4. (STAG, Hyponym-Of, CFG)5. (parser input, Evaluate-for, SCFG)6. (parser input, Compare, CFG)7. (parser input, Compare, SCFG)
(<pre-normalization>, Part-of, <processing>).(<pre-normalization>, Is-a-Prerequisite-of, <training>).(<trying encoder-decoder architectures>, Evaluate-for, <training>).(<multi-task learning architecture>, Used-for, <pushing state-of-the-art>).(<ITransF>, Evaluate-for, <knowledge base completion>).(<PositionRank>, Evaluate-for, <keyphrase extraction>).(<Recurrent Neural Networks>, Is-a-Prerequisite-of, <document classification>).(<feature extraction>, Is-a-Prerequisite-of, <statistical NLP>).(<Language identification>, Is-a-Prerequisite-of, <processing multilingual text>).(<TutorialBank>, Used-for, <NLP education and research>).
### Triplets:1. edit distance, Used-for, measuring string similarity2. structural distance, Evaluate-for, graph embeddings3. syntactic distances, Compare, traditional shift-reduce parsing schemes.
```(morphology, Conjunction, lexicon)(Inflection, Is-a-Prerequisite-of, Morphology)(Inflection, Is-a-Prerequisite-of, Lexicon)(morphology, Compare, neural seq2seq models)(Inflection, Is-a-Prerequisite-of, neural seq2seq models)(generealization, Is-a-Prerequisite-of, morphological inflection models)(morphological supervision, Used-for, character language models)(morphological supervision, Compare, inflected words)(morphological supervision, Compare, uninflected words)(Encoder-Decoder architectures, Compare, human speakers)(Encoder-Decoder architectures, Evaluate-for, human-like behavior)(morphological models, Is-a-Prerequisite-of, inflected verbs)(morphological models, Is-a-Prerequisite-of, new words)(morphological models, Is-a-Prerequisite-of, performance)(SIGMORPHON shared-tasks,
(<query concept>, Is-a-Prerequisite-of, counterfactual statements)  (counterfactual statements, Evaluative-for, counterfactual tweet dataset)  (counterfactual tweet dataset, Evaluate-for, detecting counterfactuals)  (Chinese poems, Is-a-Prerequisite-of, poem generation)  (poem generation, Used-for, balance linguistic accordance and aesthetic innovation)  
(`<concept>`, `Used-for`, `deep reinforcement learning`)  (`text-based games`, `Provide`, `interactive way to study natural language processing`)  (`deep reinforcement learning`, `Effective-in`, `developing the game playing agent`)  (`deep reinforcement learning`, `Challenge`, `low sample efficiency`)  (`deep reinforcement learning`, `Challenge`, `large action space`)  (`world-perceiving modules`, `Used-for`, `decomposing tasks and pruning actions`)  (`world-perceiving modules`, `Used-for`, `answering questions about the environment`)  (`two-phase training framework`, `Is-a-Prerequisite-of`, `improving sample efficiency`)  (`experimental results`, `Show`, `proposed method significantly improves performance and sample efficiency`)  (`natural language annotations`, `Used as`, `tool for model interpretability`)  (`domain-specific terms`, `Predict`, `from the intermediate state representations of game-playing agents`)  (`policy networks trained
### Extracted Concepts:1. News recommendation2. Personalized news recommendation3. Neural news recommendation approach4. Idiom recommendation5. Conversational recommendation6. Dialog evaluation7. MIND dataset (for news recommendation)8. Language model pre-training for news recommendation9. Recommendation system### Triplets:1. (News recommendation, Is-a-Prerequisite-of, Personalized news recommendation)2. (Personalized news recommendation, Evaluate-for, User representations)3. (Personalized news recommendation, Evaluate-for, Reading experience improvement)4. (Neural news recommendation approach, Used-for, Learning news representations)5. (Neural news recommendation approach, Is-a-Prerequisite-of, News recommendation performance improvement)6. (Idiom recommendation, Evaluate-for, Elegance of writing enhancement)7. (Conversational recommendation, Is-a-Prerequisite-of, Leading a conversation)8. (Dialog evaluation, Hyponym-
### Triplets:1. computational argumentation mining, Is-a-Prerequisite-of, neural techniques2. computational argumentation mining, Evaluate-for, performance results3. computational argumentation mining, Evaluate-for, robustness4. neural machine translation systems, Used-for, calculating output layer5. neural network model, Is-a-Prerequisite-of, Singlish dependency parsing6. Singlish dependency parsing, Used-for, improving parser accuracy7. neural semantic parser, Used-for, generating formal meaning representations8. restricted recurrent neural tensor networks, Compare, unrestricted recurrent neural tensor networks9. Recurrent Neural Networks, Part-of, Minimalist Grammar parsing10. syntactically supervised Transformer, Evaluate-for, improved BLEU scores
### Concept: text generation1. (text generation, Is-a-Prerequisite-of, natural language generation)2. (text generation, Compare, neural language models)3. (neural language models, Is-a-Prerequisite-of, Affect-LM)4. (Affect-LM, Evaluate-for, emotional content customization)5. (text generation, Used-for, conversational text generation)6. (conversational text generation, Is-a-Prerequisite-of, local coherence model)7. (text generation, Is-a-Prerequisite-of, morphological inflection generation)8. (morphological inflection generation, Compare, hard attention mechanism)9. (text generation, Is-a-Prerequisite-of, deep latent variable models)10. (deep latent variable models, Evaluate-for, response generation)11. (response generation, Compare, template-based summarization)12. (text generation, Is-a-Prerequisite-of, graph-to-sequence
### Triplets:1. character level language model, Used-for, character-level neural machine translation2. character level language model, Compare, character-level and word-level models3. character level language model, Part-of, LSTM language model4. hierarchical LSTM language model, Is-a-Prerequisite-of, character level language model5. self-attention mechanism, Evaluate-for, character level language model6. Large-scale pretrained language models, Compare, character level language model7. syntactic neural language models, Is-a-Prerequisite-of, character level language model
**Extracted Concepts:**1. Neural encoder-decoder models2. Variational autoencoders3. Conditional variational autoencoders4. Latent variables5. Semantic representations6. Generative latent variable model7. Neural parser8. Latent Vector Grammars (LVeGs)9. Gaussian Mixture LVeGs (GM-LVeGs)10. Negation cue11. Generative latent-variable model12. Wake-sleep algorithm13. Latent structural information14. Multilingual word representations15. Distillation of domain signal16. Compound probabilistic context free grammar17. Dynamic programming**Triplets:**- (Generative latent variable model, Is-a-prerequisite-of, Neural encoder-decoder models)- (Generative latent variable model, Evaluate-for, Semantic representations)- (Generative latent variable model, Evaluate-for, Neural parser)- (Generative latent variable model, Is
None.
### Extracted Concepts:1. Graph Convolutional Networks (GCN)2. Neural Network3. Adversarial Model4. Domain Adaptation5. Adversarial Training6. Working Memory Network7. Hierarchical Attention Network8. Attention Mechanisms9. Text Deconvolution Saliency10. Memory Networks (MemNNs)11. Relation Networks (RNs)12. Average Attention Network13. Graph Convolutional Networks14. Authorship Obfuscation15. GraphRel### Triplets:1. (Graph Convolutional Networks, Is-a-Prerequisite-of, Citation Network)2. (Neural Network, Is-a-Prerequisite-of, Graph Convolutional Networks)3. (Adversarial Model, Used-for, Adversarial Training)4. (Domain Adaptation, Evaluate-for, Neural Domain)5. (Adversarial Training, Is-a-Prerequisite-of
### Extracted Concepts:1. Universal Dependencies scheme2. Neural network model3. English syntactic knowledge4. State-of-the-art parser5. Neural stacking6. Chinese implicit discourse relations7. Attention-based Bi-LSTM8. Neural language models (LM)9. LSTM LM10. Prior linguistic context11. Cache-based models12. Self-attentive architecture13. Constituency parser14. Pre-trained word representations15. Treebank conversion16. Bi-tree aligned data17. Event durations18. Temporal relations19. Nested named entity dataset20. PTB (Penn Treebank)### Triplets:1. (Universal Dependencies scheme, Part-of, Dependency parsing of Singlish)2. (Neural network model, Used-for, Training a state-of-the-art parser)3. (Neural stacking, Is-a-Prerequisite-of, Improving cross-lingual dependency
(`<concept>`, `Part-of`, `Biomedical named entities`)  (`<concept>`, `Evaluate-for`, `Biomedical entity normalization`)  
### Extracted Concepts:1. neural semantic parser2. AMR (Abstract Meaning Representation)3. morpheme segmentation4. sequence-to-sequence models5. neural machine translation6. named entity recognition (NER)7. geolocation prediction model8. context embeddings9. discourse structure10. sentence fragment### Triplets:1. (neural semantic parser, Used-for, converting natural language utterances to intermediate representations)2. (neural semantic parser, Compare, syntactic schemes)3. (AMR, Compare, syntactic schemes)4. (AMR, Evaluate-for, parsing and generating text)5. (morpheme segmentation, Part-of, word representations)6. (sequence-to-sequence models, Used-for, parsing and generating text)7. (neural machine translation, Is-a-Prerequisite-of, learning morphology)8. (named entity recognition (NER), Used-for, sequence labeling)9. (
### Triplets:1. lexicalized parsing, part-of, natural language processing2. lexicalized parsing, part-of, neural lexicalized PCFGs3. neural lexicalized PCFGs, Compare, lexicalized parsing4. lexicalized parsing, part-of, Referring Expression Generation (REG) models5. lexicalized parsing, part-of, end-to-end math problem solving system6. lexicalized parsing, part-of, neural machine translation system7. lexicalized parsing, part-of, nested named entity recognition (NER) system
### Triplets:1. named entity recognition, Evaluate-for, handwriting recognition2. natural language processing, Is-a-Prerequisite-of, named entity recognition3. neural network, Used-for, named entity recognition4. named entity recognition, Is-a-Prerequisite-of, sequence labeling5. named entity recognition, Compare, sequence labeling
### Triplets:1. t-SNE (<Is-a-Prerequisite-of, technique, domain-specific neural network-based joint models>)2. t-SNE (<Used-for, visualization, high-dimensional data>)3. domain-specific neural network-based joint models (<Compare, performance, traditional pipeline models>)4. t-SNE (<Used-for, visualization, analyzing data clusters>)5. t-SNE (<Compare, efficiency, PCA>)6. t-SNE (<Used-for, dimensionality reduction, high-dimensional data analysis>)
### Triples:1. (Neural module networks, Achieve, High accuracy)2. (Joint modeling, Make, Joint modeling more difficult)3. (Semantically languages, Have, Multiple interpretations)4. (Neural module networks, Assume, Structure of the network modules)5. (NMNs, Logically lead to, Faithful explanation of model behavior)6. (VALSE, Design, Benchmark for testing general-purpose pretrained vision and language models)
### Triplets:1. feature selection, Used-for, statistical NLP2. feature selection, Evaluate-for, training efficiency3. feature selection, Is-a-Prerequisite-of, model performance
### Triplets:1. Users' preference, modeled as, Matrix Factorization.2. Matrix Factorization, used for, Modeling inter-topic preferences.3. Topics, successfully encoded by, Latent vector representations.4. Modeling inter-topic preferences, represented as, User-topic matrix.5. Text categorization, modeled as, Matrix Factorization.6. Matrix Factorization, utilized in, Knowledge base completion.7. Spatial aggregation, allows, Low-rank approximation.
### Triplets:1. dependency parsers, shift to, first-order models2. neural parsers, benefit from, higher-order features3. first-order methods, use, first-order information4. chain reasoning paradigm, generate, first-order logic rules5. T-norm fuzzy logic, permits, end-to-end learning
### Extracted Concepts:1. Neural semantic parser2. Morph-fitting procedure3. Distributional vector space models4. Morph-fitted vectors5. Vector space representations6. Layer-wise relevance propagation (LRP)7. Word embeddings8. Recurrent network9. Source-side syntactic trees10. Lexical features### Triplets:1. (Neural semantic parser, Is-a-Prerequisite-of, Morph-fitting procedure)2. (Distributional vector space models, Evaluate-for, Language understanding systems)3. (Morph-fitting procedure, Evaluate-for, Distributional vector space models)4. (Word embeddings, Evaluate-for, Word similarity)5. (Source-side syntactic trees, Part-of, Source dependency tree model)6. (Morph-fitted vectors, Compare, Distributional vector space models)
### Extracted Concepts:1. Transition-based AMR parser2. Stack-LSTM3. Policy Learning4. Smatch score5. Attention mechanism### Triplets:1. (Stack-LSTM, Is-a-Prerequisite-of, Transition-based AMR parser)2. (Stack-LSTM, Used-for, Policy Learning)3. (Stack-LSTM, Used-for, Smatch score)4. (Stack-LSTM, Used-for, Attention mechanism)
### Triplets:1. abstractive summarization, Used-for, document summarization research2. document summarization research, Evaluate-for, abstractive summarization3. document summarization research, Worse-than, extractive methods4. impressive progress, Compare, primitive stage5. impressive progress, Is-a-Prerequisite-of, neural abstractive models6. neural abstractive models, Used-for, abstractive sentence summarization7. system, Is-a-Prerequisite-of, RDF triples8. RDF triples, Evaluate-for, natural language form9. RDF triples, Used-for, facts10. RDF triples, Used-for, applications11. RDF triples, Evaluate-for, humans12. GEAR framework, Evaluate-for, fact verification13. GEAR framework, Used-for, evidence aggregation14. IPS algorithm, Is-a-Prerequisite-of, semantic head words15. IPS algorithm, Is-a-
(<phonetics>, Compare, <phonological distinctive features>)  (<phonetics>, Compare, <tongue twisters>)  (<phonetics>, Is-a-Prerequisite-of, <phonotactic patterns>)  
(None)
(<Markov Chain Monte Carlo>, Is-a-Prerequisite-of, <Stochastic Gradient Monte Carlo>)(<Markov Chain Monte Carlo>, Used-for, <Learning Weight Uncertainty in RNNs>)
### Triplets:1. relation extraction, Used-for, finding unknown relational facts2. relation extraction, Is-a-Prerequisite-of, exploiting mono-lingual data3. relation extraction, Compare, distant supervision4. relation extraction, Is-a-Prerequisite-of, joint relation extraction5. relation extraction, Is-a-Prerequisite-of, entity linking6. relation extraction, Is-a-Prerequisite-of, KB relation detection7. relation extraction, Is-a-Prerequisite-of, Attention-based recurrent neural network8. relation extraction, Used-for, improving extraction results9. deep learning, Compare, feature-based joint model10. neural network, Is-a-Prerequisite-of, deep residual bidirectional LSTMs11. relation detection, Compare, state-of-the-art accuracy12. relation detection, Compare, end-to-end tree-based LSTM model13. entity mentioning, Part-of, joint extraction of entities and relations14.
### Triplets:1. speech synthesis, Is-a-Prerequisite-of, language revitalization2. hate speech, Used-for, hate speech detection3. speech translation, Used-for, natural language translation4. speech synthesis, Part-of, multimodal synthesis techniques5. speech synthesis, Part-of, speech synthesis systems6. speech synthesis, Evaluate-for, performance evaluation7. materials science text, Is-a-Prerequisite-of, MatSci-NLP benchmark
1. (clustering, Used-for, spectral clustering)2. (clustering, Used-for, Compositor attribution)3. (clustering, Is-a-Prerequisite-of, unsupervised model)4. (clustering, Is-a-Prerequisite-of, neural sequence-to-sequence model)5. (spectral clustering, Compare, Compositor attribution)6. (clustering, Compare, unsupervised model)7. (clustering, Compare, neural sequence-to-sequence model)
### Triplets:1. dimensionality reduction, Evaluate-for, unsupervised Semantic Textual Similarity (STS)2. dimensionality reduction, Evaluate-for, sentence meta-embeddings3. dimensionality reduction, Evaluate-for, Generalized Canonical Correlation Analysis (CCA)4. dimensionality reduction, Evaluate-for, cross-view auto-encoders5. dimensionality reduction, Is-a-Prerequisite-of, sentence meta-embeddings
### Triplets:1. gated recurrent unit, Used-for, handling long-range dependencies2. gated recurrent unit, Is-a-Prerequisite-of, recurrent neural networks3. LSTM, Compare, gated recurrent unit4. LRN, Compare, gated recurrent unit
### Extracted Concepts:1. SHRG-based parser2. Semantic graphs3. Synchronous production rules4. Syntacto-semantic composition process5. Neural network models6. Common sense natural language inference7. Adversarial filtering8. Unsupervised machine translation9. Task planning10. Lifelong learning11. Language-understanding challenges12. Neural-symbolic approach13. Pre-trained Language Model (PLM)14. DynaInst15. Instruction tuning16. WiC (Word in Context) task17. Lexical Semantic Change Detection model18. XL-LEXEME### Triplets:1. SHRG-based parser, Used-for, Producing semantic graphs2. Synchronous production rules, Is-a-Prerequisite-of, Syntacto-semantic composition process3. Neural network models, Evaluate-for, Learning deep linguistic knowledge4. Neural network models,
### Concept: k-means1. (k-means, Is-a-Prerequisite-of, clustering)2. (k-means, Used-for, grouping data points)3. (clustering, Is-a-Prerequisite-of, unsupervised learning)4. (unsupervised learning, Evaluate-for, identifying patterns in data)
1. (LSTMs, Encode, Source Sentence)2. (Convolutional Layers, Simultaneously Encode, Source Sentence)3. (Deep Neural Networks, Enhance, State-of-the-art Neural Machine Translation)4. (LAUs, Reduce, Gradient Propagation Path)5. (Syntactic Encoders, Improve, Translation Accuracy)6. (Sequence-to-Dependency NMT, Significantly Outperforms, Baselines)7. (Distortion Models, Incorporate, Word Reordering Knowledge)
### Triplets:1. bi-directional LSTMs, Used-for, neural machine translation2. convolutional layers, Is-a-Prerequisite-of, encoding the source sentence3. recurrent networks, Compare, computation constrained by temporal dependencies4. deep Neural Networks (DNNs), Used-for, enhancing the state-of-the-art Neural Machine Translation (NMT)5. linear associative units (LAU), Evaluate-for, reducing the gradient propagation path inside the recurrent unit6. syntactic information, Is-a-Prerequisite-of, incorporating source syntax into NMT effectively7. Sequence-to-Dependency Neural Machine Translation (SD-NMT), Used-for, significantly outperforming state-of-the-art baselines on Chinese-English and Japanese-English translation tasks
(`<head concept>`, `Evaluate-for`, `extractive summarizers`)  (`<head concept>`, `Used-for`, `optimization technique`)  (`<extractive summarizers>`, `Compare`, `optimization technique`)  (`<optimization technique>`, `Is-a-Prerequisite-of`, `extractive summarizers`)  
1. (natural language descriptions, part-of, source code)2. (Python, Is-a-Prerequisite-of, code generation)3. (programming language, Is-a-Prerequisite-of, code generation)4. (NL-to-code generation, Evaluate-for, data augmentation)5. (NL-to-code generation, Evaluate-for, retrieval-based data re-sampling)6. (programming language API documentation, Used-for, NL-to-code generation)
1. (Topical PageRank, uses, Latent Dirichlet Allocation)2. (Latent Dirichlet Allocation, is-a-Prerequisite-of, Topic modeling)3. (Bidirectional Adversarial Topic model, extended-from, BAT)4. (Gaussian-BAT, Is-a-Prerequisite-of, Word relatedness information)5. (Word relatedness information, Conjunction, Semantic patterns)6. (BAT, outperforms, Competitive baselines)
### Triplets:1. word embedding, Used-for, word analogy questions2. word embedding, Used-for, caption generation3. word embedding, Encourages, words in similar contexts to be located close to each other in the embedding space4. word embedding, Encourages, vector calculus in solving word analogies5. word embedding, Encourages, discovering coherent aspects6. word embedding, Encourages, de-emphasizing irrelevant words during training7. word embedding, Exploited-for, neural word segmentation research8. word embedding, Exploited-for, building a modular segmentation model9. word embedding, Exploited-for, joint Chinese word segmentation10. word embedding, Exploited-for, parsing sentences to semantic representations11. word embedding, Utilized-in, financial market volatility prediction12. word embedding, Utilized-in, sentiment analysis in financial markets13. word embedding, Used-for, surpass
### Triplets:1. ensemble learning, Used-for, improving classification accuracy2. ensemble learning, Compare, individual learning models3. individual learning models, Is-a-Prerequisite-of, ensemble learning4. ensemble learning, Is-a-Prerequisite-of, boosting algorithms5. ensemble learning, Part-of, heterogeneous ensemble methods6. ensemble learning, Evaluate-for, performance evaluation metrics
### Triples:1. (gated self-matching networks, Evaluate-for, reading comprehension)2. (self-matching attention mechanism, Used-for, refining representation)3. (pointer networks, Used-for, locating positions of answers)4. (Reading comprehension, Is-a-Prerequisite-of, natural-language understanding systems)5. (comparison paragraph, Part-of, GuessTwo task)6. (sequence learning model, Is-a-Prerequisite-of, question generation)7. (attention mechanism, Is-a-Prerequisite-of, generating questions)8. (TriviaQA dataset, Evaluate-for, reading comprehension models)9. (SQuAD dataset, Hyponym-Of, TriviaQA dataset)10. (hierarchical attention network, Used-for, fine-grained language embeddings)
### Concepts:- Newton method- Optimization- Algorithm- Gradient descent- Convergence- Iterative method- Root-finding algorithm### Triplets:1. (Newton method, Is-a-Prerequisite-of, Optimization)2. (Newton method, Used-for, Root-finding algorithm)3. (Newton method, Evaluate-for, Convergence)4. (Newton method, Used-for, Iterative method)5. (Newton method, Is-a-Prerequisite-of, Gradient descent)
(<log-linear model>, Used-for, integrating prior knowledge into neural machine translation)(<log-linear model>, Is-a-Prerequisite-of, representing prior knowledge sources as features)(<log-linear model>, Used-for, guiding the learning processing of the neural translation model)(<log-linear model>, Is-a-Prerequisite-of, softened constraints at training time)(<log-linear model>, Used-for, providing supervision with structured loss components)
### Triplets:1. deep q-network, Used-for, reinforcement learning2. reinforcement learning, Is-a-Prerequisite-of, deep q-network3. neural networks, Part-of, deep q-network
### Triplets:1. highway network, Used-for, control2. highway network, Used-for, gating3. highway network, Compare, SMN4. highway network, Used-for, text style transfer5. highway network, Used-for, multilingual NMT 6. highway network, Compare, BERTScore7. highway network, Is-a-Prerequisite-of, Generative feature matching network
### Triplets:1. word-embedding models, gained, popularity2. word-embedding models, exhibit, compositionality3. word-vectors, results in, vector4. additive compositionality, holds, assumptions5. Skip-Gram model, connected to, Sufficient Dimension Reduction (SDR) framework6. object naming, sub-task of, referring expression generation7. object names, subject to, communicative preferences8. object names, semantically related to, each other9. referential word meaning models, link, visual to lexical information10. referential word meaning, associated with, distributional word embeddings11. object names, learned by, model12. model, predicts, object names13. object naming task, combination of, lexical and visual information14. lexical and visual information, capture, complementary aspects15. supervised framework, learns, automatic Pyramid scores16. supervised framework, use, automatic
### Triplets:1. heuristic search, Used-for, graph-to-string rules2. heuristic search, Compare, deep learning3. graph-to-string rules, Is-a-Prerequisite-of, AMR-to-text generation4. deep learning, Evaluate-for, content moderation5. deep learning, Is-a-Prerequisite-of, neural architecture search6. neural architecture search, Evaluate-for, architecture learning7. neural architecture search, Is-a-Prerequisite-of, NAS8. NAS, Evaluate-for, search space extension9. NAS, Is-a-Prerequisite-of, new method development10. neural architecture search, Is-a-Prerequisite-of, intra-cell architectures11. NAS, Is-a-Prerequisite-of, inter-cell architectures12. intra-cell architectures, Evaluate-for, search improvement13. inter-cell architectures, Evaluate-for, search enhancement
### Triplets:1. context-sensitive grammar, Used-for, natural language processing2. context-sensitive grammar, Is-a-Prerequisite-of, word embeddings3. context-sensitive grammar, Part-of, rule structure
### Triplets:1. neural machine translation, relies on, bi-directional LSTMs2. neural machine translation, utilizes, deep Neural Networks3. Chinese-English translation, improved by, 11.7 BLEU4. neural machine translation, generates, translations from left to right5. neural machine translation, learns, latent syntactic structures6. neural machine translation, uses, layer-wise relevance propagation7. neural machine translation, made significant progress, recently8. Japanese-English translation, improved by, Sequence-to-Dependency Neural Machine Translation9. technical documentation, translated to, formal representations10. machine translation, interprets, textual descriptions
### Concept: activation function1. (activation function, Part-of, deep learning architecture)2. (activation function, Evaluate-for, performance)3. (activation function, Used-for, neural models)4. (activation function, Compare, compositional functions)
### Triplets:1. vector semantics, Exploit-for, word representations2. vector semantics, Part-of, distributional semantic models3. distributional semantic models, Used-for, sentiment classification4. distributional semantic models, Evaluate-for, sentiment information5. domain-sensitive embeddings, Is-a-Prerequisite-of, sentiment-aware embeddings6. sentiment-aware embeddings, Compare, unsupervised embeddings7. sentiment classification, Used-for, sentiment representations8. sentiment-aware word embeddings, Compare, unsupervised embeddings
### Extracted Concepts:1. Earley Parsing2. Dependency Parsing3. Constituency Parsing4. Sequence Tagging5. BiLSTMs6. Multi-Task Learning7. Neural Techniques8. Transition-Based Parser9. Neural Network-Based Joint Models10. Dependency Treebank11. Universal Dependencies12. Continuous Relaxation13. Voltage-Commanded Control Systems### Triplets:1. (Earley Parsing, Is-a-Prerequisite-of, Dependency Parsing)2. (Earley Parsing, Is-a-Prerequisite-of, Constituency Parsing)3. (Dependency Parsing, Used-for, Constituency Parsing)4. (Earley Parsing, Compare, Sequence Tagging)5. (Dependency Parsing, Compare, Sequence Tagging)6. (BiLSTMs, Is-a-Prerequisite-of, Sequence Tagging)7. (Earley Parsing, Is-a-Prerequisite-of,
### Extracted Concepts:1. Labeled sequence transduction2. Multi-space variational encoder-decoders3. Neural networks4. Semi-supervised learning5. SIGMORPHON morphological inflection benchmark6. Hybrid Code Networks (HCNs)7. Domain-specific knowledge8. End-to-end learning of recurrent neural networks (RNNs)9. Dialog systems10. Unlabeled data11. Generative Domain-Adaptive Nets12. Reinforcement learning13. Arabic word embeddings14. User cognitive structure15. Temporal orientation16. Metaphoric expressions17. Linguistic tasks18. Goal-acts19. Text corpus20. Neural network (NN)21. Regular expressions (RE)22. Natural language processing (NLP)23. Cascading attention24. Neural Machine Translation (NMT)25. Statistical Machine Translation (SMT)26. Language modeling
1. (parsing, Is-a-Prerequisite-of, constituency parsing)2. (dependency parsing, Evaluate-for, parsing)3. (constituency parsing, Is-a-Prerequisite-of, deep learning model)4. (dependency parsing, Evaluate-for, deep learning model)5. (dependency parsing, Is-a-Prerequisite-of, semantic role labeling)6. (dependency parsing, Is-a-Prerequisite-of, natural language descriptions)7. (dependency parsing, Is-a-Prerequisite-of, language generation task)
### Triplets:1. deep pyramid CNN, Is-a-Prerequisite-of, word-level CNN2. word-level CNN, Used-for, text categorization3. word-level CNN, Compare, character-level CNN4. word-level CNN, Is-a-Prerequisite-of, deep pyramid CNN5. convolutional neural network, Evaluate-for, zero pronoun resolution6. convolutional neural network, Is-a-Prerequisite-of, deep pyramid CNN7. convolutional neural network, Evaluate-for, text categorization
### Triplets:1. (segmental neural language model, Compare, character LSTM models)2. (segmental neural language model, Compare, nonparametric Bayesian word segmentation models)3. (word segmentation, Is-a-Prerequisite-of, sentence boundary recognition)4. (visual context, Used-for, conditioning the model)5. (modeling language conditional on visual context, Evaluate-for, improved performance)
### Extracted Concepts:1. Neural language model2. Knowledge bases3. Anchor methods4. Attention mechanism5. Geolocation prediction model6. Implicit discourse relations7. Word embedding models8. Sequence-to-sequence modeling9. Sarcasm detection model10. Temporal relation annotation schemes11. Question answering models12. Sonnet modeling13. Document modeling14. Numeracy modeling### Triplets:1. Neural language model, Used-for, language model perplexity2. Knowledge bases, Used-for, Knowledge Base Completion3. Anchor methods, Compare, single word anchor algorithms4. Attention mechanism, Used-for, selective focus5. Geolocation prediction model, Used-for, geolocation prediction6. Implicit discourse relations, Compare, word order-agnostic approaches7. Word embedding models, Evaluate-for, improved coherence8. Sequence-to-sequence modeling, Part-of, Machine Translation9
(None)
### Triplets:1. (response selection, Used-for, multi-turn conversation)2. (sequential matching network, Is-a-Prerequisite-of, addressing problems)3. (RNN, Is-a-Prerequisite-of, modeling relationships among utterances)4. (PSO, Evaluate-for, task success)5. (RP, Evaluate-for, task success)6. (NLG system, Evaluate-for, recognizing misunderstandings)7. (TextFlow, Is-a-Prerequisite-of, text similarity measure)8. (PositionRank, Is-a-Prerequisite-of, keyphrase extraction from scholarly documents)9. (grid-type recurrent neural networks, Is-a-Prerequisite-of, improving PAS analysis)10. (multimodal word distributions, Is-a-Prerequisite-of, capturing semantic information)11. (sentiment analysis methods, Evaluate-for, volatility prediction)12. (Open IE, Is-a-Prerequisite-of, answering complex questions)13.
### Triplets:1. weakly-supervised learning, used-for, named entity recognition (NER)2. weakly-supervised learning, Evaluate-for, cognitive task analysis (CTA)3. cognitive task analysis (CTA), Is-a-Prerequisite-of, automated CTA transcript parsing4. weakly-supervised learning, Evaluate-for, entity linking5. weakly-supervised learning, part-of, learning discourse structure
### Triplets:1. (Integer Linear Programming, Is-a-Prerequisite-of, Linear Programming)2. (MH₄ algorithm, Used-for, Non-projective Linear Programming)3. (Weak Supervision Methods, Evaluate-for, Classical Supervised Learning)4. (Dynamic Programming, Compare, Generative Step)
### Triplets:1. expert system, Used-for, question answering2. emotional support, Used-for, conversation scenarios3. neural machine translation, Evaluate-for, incorporating bilingual dictionaries4. peer reviews, Evaluate-for, emotional and cognitive empathy5. Visual Question Answering, Evaluate-for, generalization capabilities6. semantic parsing, Evaluate-for, producing structured meaning representations### Relations:- Used-for- Evaluate-for
### Triplets:1. machine learning resource, Used-for, knowledge discovery2. machine learning resource, Evaluate-for, feature selection3. gazetteers, Part-of, machine learning based NER systems4. Named Entity Recognition (NER) systems, Is-a-Prerequisite-of, machine learning models5. feature selection, Evaluate-for, knowledge discovery
(<model architecture>, <used-for>, <stack-based embedding features>)(<unlexicalized predicates>, <part-of>, <model architecture>)(<MRS parser>, <Compare>, <attention-based baselines>)(<MRS parser>, <Is-a-Prerequisite-of>, <high-precision grammar-based parser>)
None.
### Triplets:1. continuous-time deconvolutional regressive neural network, Is-a-Prerequisite-of, logistic regression2. logistic regression, Used-for, probing English, French, and Arabic PTLMs3. logistic regression, Evaluate-for, quantifying potentially harmful content4. logistic regression, Evaluate-for, predict masked tokens5. logistic regression, Is-a-Prerequisite-of, method based on logistic regression classifiers
### Extracted Concepts:1. multilingual Pre-trained Machine Reader (mPMR)2. multilingual machine reading comprehension (MRC)3. pre-training4. natural language understanding (NLU)5. mPLMs (multilingual pre-trained language models)6. cross-lingual generalization7. NLU capability8. downstream tasks9. source language10. target languages11. MRC-style pre-training12. rationales13. sentence-pair classification process### Triplets:1. (mPMR, Evaluate-for, NLU capability)2. (NLU capability, Is-a-Prerequisite-of, sequence classification)3. (sequence classification, Evaluate-for, sentence-pair classification process)4. (mPMR, Is-a-Prerequisite-of, MRC-style pre-training)5. (MRC-style pre-training, Is-a-Prerequisite-of, NLU capability)6. (
(`<concept>`, `Is-a-Prerequisite-of`, `semantic super-sense tagging`)(`<concept>`, `Used-for`, `summarizing scientific articles`)(`<concept>`, `Compare`, `long keyphrases`)(`<concept>`, `Compare`, `manual summaries`)(`<concept>`, `Compare`, `dataset of summaries created manually`)(`<concept>`, `Part-of`, `dataset of paper summaries`)(`<concept>`, `Used-for`, `produce readable content`)
### Triplets:1. deep learning introduction, Used-for, abstractive summarization model2. deep learning introduction, Is-a-Prerequisite-of, training a deep learning model3. deep learning introduction, Is-a-Prerequisite-of, encoder-decoder model4. deep learning introduction, Compare, data augmentation in computer vision tasks5. deep learning introduction, Is-a-Prerequisite-of, neural machine translation6. data augmentation in computer vision tasks, Conjunction, data augmentation in natural language tasks7. deep learning introduction, Compare, adversarial domain adaptation techniques8. adversarial domain adaptation techniques, Evaluate-for, boosting performance in unsupervised adaptation9. adversarial domain adaptation techniques, Evaluate-for, boosting performance in fine-tuning with limited target data10. data augmentation in natural language tasks, Is-a-Prerequisite-of, novel data augmentation method for neural machine translation
### Triplets:1. regular expression, part-of, StructuredRegex2. regular expression, Evaluate-for, metaphoric expression3. StructuredRegex, part-of, dataset4. StructuredRegex, Evaluate-for, prior datasets5. StructuredRegex, Used-for, regex synthesis6. StructuredRegex, Evaluate-for, non-local constraints7. StructuredRegex, Evaluate-for, multi-modal inputs
### Triplets:1. deep recurrent neural networks, Learn-internal-representations, soft hierarchical notions of syntax2. deep recurrent neural networks, Predict, part of speech for each word3. deep recurrent neural networks, Predict, first level constituent labels4. deep recurrent neural networks, Predict, second level constituent labels5. deep recurrent neural networks, Predict, third level constituent labels6. soft syntactic hierarchy, Emerges-from, network depth7. self-attention networks, Have-attracted, a lot of interests8. self-attention networks, Be-weak-at, learning positional information9. self-attention networks, Trained-on, word reordering detection10. word reordering detection task, Quantify, word order information11. machine translation tasks, Strong-performances-on, due to the lack of recurrence structure12. neural network models, Provide, dominant solution to sequence-to-sequence transduction problem13.
1. (Vector space representations of words, Capture, Many aspects of word similarity)2. (Spectral clustering, Is-a-Prerequisite-of, Multi-document summarization)3. (Word embeddings, Similar-to, Sentence embeddings)
### Triplets:1. Recurrent neural networks (RNNs), Used-for, language modeling2. LSTM (Long Short-Term Memory) language model, Evaluate-for, emotional content3. LSTM (Long Short-Term Memory) language model, Compare, Affect-LM
(`<content>`, `mentions`, `sentence representation`)(`<Neural Semantic Parser>`, `is-a-Prerequisite-of`, `Semantic Parsing`)(`<Conjunction>`, `denotes`, `logical or semantic relationship`)(`<Morphologically rich languages>`, `accentuate`, `two properties`)(`<Semantic parsing>`, `Used-for`, `Semantic representation goals`)(`<Recurrent neural networks>`, `Used-for`, `inferring AMR graphs`)(`<Machine Translation models>`, `Used-for`, `maintaining state-of-the-art performance`)(`<Word embeddings>`, `provide`, `point representations of words`)(`<Neural network architectures>`, `rely-on`, `pre-trained word embeddings`)(`<Word Representations>`, `capture`, `semantic information`)(`<Subword units representations>`, `capture`, `morphological regularities of words`)(`<true morphological analyses>`, `improve predictive accuracy of`, `character-level models`)
### Concept: text mining1. (text mining, Used-for, extracting keyphrases)2. (text mining, Part-of, natural language processing)3. (text mining, Used-for, summarizing main points of a document)4. (text mining, Evaluate-for, improving the performance of generative models)5. (text mining, Is-a-Prerequisite-of, generating keyphrases)6. (keyphrases, Part-of, a document)7. (generative models, Compare, RL approach for keyphrase generation)8. (distributional semantic models, Compare, hierarchical information blending)9. (natural language processing, Is-a-Prerequisite-of, text mining)
### Triplets:1. semantic similarity, Evaluate-for, text similarity measures2. semantic similarity, Evaluate-for, document similarity3. document matching, Evaluate-for, semantic similarity4. automatic metrics, Evaluate-for, semantic similarity5. sentence mover’s similarity, Evaluate-for, semantic similarity6. semantic similarity, Part-of, pairwise comparisons7. semantic similarity, Is-a-Prerequisite-of, representation of word meaning in NLP
### Triplets:1. kernel graphical models, Is-a-Prerequisite-of, cluster evolution2. kernel graphical models, Compare, OSDM3. kernel graphical models, Compare, CompareNet
None.
1. (Dirichlet processes, Work-for, Modeling words)2. (Dirichlet processes, Used-for, Learning multiple topic-sensitive representations)3. (Hierarchical Dirichlet Process, Is-a-Prerequisite-of, Topic-sensitive representations)4. (Dirichlet processes, Compare, Existing models)5. (Dirichlet processes, Evaluate-for, Learning multiple topic-sensitive representations)
(<mean field approximation>, Is-a-Prerequisite-of, <second-order semantic dependency parser>)  (<mean field approximation>, Used-for, <approximating second-order parsing>)  
### Extracted Concepts:1. Event Coreference Resolvers2. Event Anaphoricity3. Joint Models4. Error Propagation5. Mention-Ranking Model6. Propagation Trees7. Kernel-Based Method8. Rumor Detection9. State-of-the-Art Rumor Detection Models10. Propagation Tree Kernel11. Neural Machine Translation (NMT)12. Layer-Wise Relevance Propagation (LRP)13. Dependency Parsing14. Transition-Based Dependency Parsers15. Transition System (arc-swift)16. Goal-Acts17. Joint Models for Chinese Word Segmentation, POS Tagging, and Dependency Parsing18. Word Embeddings19. Character Strings Embeddings20. Neural Network-Based Joint Models21. Neural Language Models (LMs)22. Emotion-Cause Pair Extraction23. Transition-Based Model24. Style Transfer25. Sequence-to-Sequence
### Triples:1. distributional vector space models, Compare, state space models2. dialogue state tracking, Evaluate-for, state space models3. Variational auto-encoders (VAEs), Used-for, natural language generation4. reasoning tasks, Used-for, state space models
### Triplets:1. Gaussian graphical model, Used-for, Graphical model.2. Word representation, Evaluate-for, Term ambiguity problem.3. OSDM, Compare, State-of-the-art algorithms.
### Triplets:1. kkt condition, used-for, optimization2. optimization, Is-a-Prerequisite-of, kkt condition
(<Message Passing>, <Is-a-Prerequisite-of>, <Graph Neural Networks>)(<Message Passing>, <Evaluate-for>, <Knowledge Graph Completion>)(<Message Passing>, <Evaluate-for>, <MPNNs>)(<MPNNs>, <Evaluate-for>, <KGC>)(<Single Layer Perceptron Models>, <Compare>, <MPNNs>)(<MPNNs>, <Compare>, <MLP Models>)(<Scoring Function Design>, <Is-a-Prerequisite-of>, <Model Performance>)(<Loss Function Design>, <Is-a-Prerequisite-of>, <Model Performance>)
(<concept>, Is-a-Prerequisite-of, neural sequence labeling)(neural sequence labeling, Used-for, natural language processing)(markov random fields, Is-a-Prerequisite-of, conditional random fields)(markov random fields, Evaluate-for, ner)(markov random fields, Is-a-Prerequisite-of, neurally parameterized conditional random fields)
### Triplets:1. singular value decomposition, Evaluate-for, Contrastive representation learning2. multimodal sentiment analysis, Used-for, Predicting sentiment of video content3. ConFEDE, Is-a-Prerequisite-of, Enhanced representation of multimodal information4. ConFEDE, Compare, Baselines performance on CH-SIMS, MOSI, and MOSEI5. Bias-variance tradeoff, Evaluate-for, Balance model complexity with data size6. neural language models, Used-for, Providing conditional probability distributions over the lexicon7. larger Transformer-based language models, Compare, Predictive ability of early eye-tracking measurements
(`<evaluation of dependency parsing>`, `Evaluate-for`, `<dependency parsing>`)  (`<evaluation of dependency parsing>`, `Compare`, `<other models>`)  (`<dependency parsing>`, `Evaluate-for`, `<accuracy>`)  (`<dependency parsing>`, `Compare`, `<semantic dependency parsing>`)  (`<dependency parsing>`, `Evaluate-for`, `<state of the art>`)  
1. (VAE, used-for, generative modeling)2. (BN-VAE, Is-a-Prerequisite-of, posterior collapse)3. (BN-VAE, Evaluate-for, avoiding posterior collapse)4. (BN-VAE, Is-a-Prerequisite-of, BN-VAE extension to CVAE)5. (CVAE, Evaluate-for, extension)6. (BN-VAE, Evaluate-for, surpassing autoregressive baselines)
### Triplets:1. knowledge base, includes, textual mentions2. DNA sequence alignment algorithms, inspired from, TextFlow3. TextFlow, computed using, actual position of the words4. natural language inference, also known as, Recognizing Textual Entailment5. Recognizing Textual Entailment, requires to infer, logical relationship between two given sentences6. discourse markers, represent, logical relationship between two sentences7. discourse markers, used to, augment the quality of the NLI model
### Triplets:1. generative models, impose restrictions on, scope of features2. generative models, outperformed by, discriminative models3. framework, marries, generative model and discriminative recognition model4. framework, based on, expectation maximization5. framework, enables, parsing and language modeling6. adversarial attacks, reveal, vulnerability of deep neural networks7. adversarial attacks, challenging because, text is discrete8. word-level attacking, regarded as, combinatorial optimization problem9. novel attack model, incorporates, sememe-based word substitution method10. novel attack model, utilizes, particle swarm optimization-based search algorithm11. cross-lingual language models, pretrained with, masked language modeling12. cross-lingual language models, improve, transferability on various datasets13. zero-shot sequence labeling, aims to build, sequence labeler without human-annotated datasets14. zero
### Triplets:1. knowledge representation, Used-for, semantic parsing2. semantic parsing, Evaluate-for, natural language utterances3. semantic parsing, Is-a-Prerequisite-of, mapping to target domains4. neural semantic parser, Compare, sequence-to-sequence models5. semantic parser, Part-of, model6. semantic parser, Used-for, training end-to-end using annotated logical forms7. predicate-argument structures, Is-a-Prerequisite-of, semantic parsing8. sequence-to-sequence models, Compare, semantic parsing9. AMR parsing, Evaluate-for, competitive results of 62.1 SMATCH10. sequence-based AMR models, Is-a-Prerequisite-of, robustness against ordering variations of graph-to-sequence conversions
(None)
### Triplets:1. (Linear Regression, Used-for, Model Update)2. (Linear Regression, Evaluate-for, Model Ensemble)3. (Continuous-Time Deconvolutional Regressive Neural Network, Used-for, Human Language Processing Dynamics)4. (Continuous-Time Deconvolutional Regressive Neural Network, Compare, Continuous-Time Deconvolutional Regression)5. (Regressions, Evaluate-for, Model Updates)6. (Regression Errors, Is-a-Prerequisite-of, Negative Flip Rate)
None.
### Triplets:1. unsupervised learning, Used-for, machine translation2. word embedding, Used-for, unsupervised learning3. end-to-end learning framework, Evaluate-for, dialog system performance4. neural network, Combine-with, regular expressions5. neural network, Used-for, spoken language understanding6. distant supervision, Is-a-Prerequisite-of, relation extraction7. adversarial technique, Used-for, unsupervised cross-lingual word embedding
### Triplets:1. word embeddings, capture, linguistic regularities2. word embeddings, are used for, dependency parsing3. word embeddings, are applied to, Chinese word segmentation4. word embeddings, are enhanced with, multimodal word distributions5. word embeddings, introduce, Probabilistic FastText6. word embeddings, improve, state-of-the-art performance7. word embeddings, are utilized in, neural network architectures
### Triplets:1. expertise style transfer, Used-for, alleviating cognitive biases2. coding ability, Is-a-Prerequisite-of, software development3. Transformer models, Hyponym-Of, Neural Machine Translation4. inductive biases, Is-a-Prerequisite-of, modeling natural languages
### Triplets:1. ResNet is utilized in deep learning.2. ResNet is a neural network architecture.3. ResNet is a type of convolutional neural network.
### Extracted Concepts:1. Abstract Meaning Representation (AMR)2. Neural Machine Translation (NMT)3. Pointer-generator network4. Selective encoding model5. Extractive summarization6. Semantic parsers7. Dynamic memory-based network8. Fluency boost learning9. Tree-based neural machine translation (NMT)10. Exemplar Encoder-Decoder network### Triplets:1. (Sequence-to-sequence, Used-for, Neural Machine Translation)2. (Sequence-to-sequence, Conjunction, Abstract Meaning Representation)3. (Pointer-generator network, Is-a-Prerequisite-of, Selective encoding model)4. (Neural Machine Translation, Compare, Abstract Meaning Representation)5. (Extractive summarization, Is-a-Prerequisite-of, Sequence-to-sequence)6. (Semantic parsers, Is-a-Prerequisite-of, Sequence-to-sequence)7. (Dynamic memory-based network, Used-for, Entity
### Extracted Concepts:1. Caption Generation2. Neural Image Captioning3. Visual Grounding4. Visual Reasoning5. Visual Question Answering (VQA)6. Diverse Aspects of Image Description7. Paragraph-style Image Captions### Triplets:1. (Caption Generation, Used-for, Visual Question Answering)2. (Neural Image Captioning, Is-a-Prerequisite-of, Visual Question Answering)3. (Visual Grounding, Is-a-Prerequisite-of, Visual Question Answering)4. (Visual Reasoning, Is-a-Prerequisite-of, Visual Question Answering)5. (Diverse Aspects of Image Description, Is-a-Prerequisite-of, Visual Question Answering)6. (Paragraph-style Image Captions, Evaluate-for, Visual Question Answering)
### Extracted Concepts:1. Deep Neural Network2. Bilingual Text Embeddings3. Partial Canonical Correlation Analysis (PCCA)4. Deep PCCA (DPCCA)5. Generic Word Embeddings6. Domain Specific (DS) Word Embeddings7. Domain Adapted (DA) Word Embeddings8. Canonical Correlation Analysis (CCA)9. Kernel CCA10. Sentence Meta-Embeddings11. Large Language Models (LLMs)12. Task Definitions13. Automated Coherence Metrics14. Human Judgement### Triplets:1. (canonical correlation analysis, Is-a-Prerequisite-of, PCCA)2. (canonical correlation analysis, Is-a-Prerequisite-of, CCA)3. (PCCA, Evaluate-for, Deep PCCA)4. (Canonical Correlation Analysis, Is-a-Prerequisite-of, Kernel CCA)5. (Domain Adapted Word Embeddings, Compare
(`<Content>`, Evaluate-for, sampling)  (`interacting topic models`, Compare, Anchor methods)  (`Anchor methods`, Compare, Single word anchor algorithms)  (`Interactive topic modeling`, Is-a-Prerequisite-of, Tandem Anchors)  (`Tandem Anchors`, Compare, Single word anchor algorithms)  (`Anchor methods`, Compare, Single word anchor algorithms)  (`combination of words`, Is-a-Prerequisite-of, Tandem Anchors)  (`existing single word anchor algorithms`, Conjunction, Single word anchor algorithms)  (`existing single word anchor algorithms`, Compare, combinations of words)  (`Skip-Gram Negative Sampling (SGNS) word embedding model`, Used-for, stochastic gradient descent)  (`optimization of SGNS objective`, Compare, searching for a good matrix)  (`sampling methods`, Compare, batch gradient learning)  (`AllVec`, Compare, sampling-based SGD methods)  (`knowledge distillation (KD) technique`,
### Triplets:1. (semi-Markov conditional random fields, Compare, conventional conditional random fields)2. (semi-Markov conditional random fields, Is-a-Prerequisite-of, neural sequence labeling)3. (neural sequence labeling, Evaluate-for, assigning labels to segments)4. (semi-Markov conditional random fields, Is-a-Prerequisite-of, hybrid semi-Markov CRF architecture)5. (monolingual word alignment, Evaluate-for, studying fine-grained editing operations)6. (semi-Markov CRF alignment model, Is-a-Prerequisite-of, monolingual word alignment)7. (extractive summarizer, Used-for, summarizing long documents)8. (VisText dataset, Evaluate-for, generating captions for charts)
(None)
### Concept: autoencoders1. (syntax-infused variational autoencoder, Is-a-Prerequisite-of, generative models)2. (autoencoders, Compare, generative models)3. (SIVAE, Part-of, VAEs)4. (SIVAE, Evaluate-for, generating sentences and syntactic trees)5. (multi-level VAE model, Evaluate-for, generating long, and coherent text)6. (autoencoders, Used-for, improving variational autoencoders)7. (Batch Normalized-VAE, Evaluate-for, preventing posterior collapse)8. (Coupled-VAE, Evaluate-for, avoiding trivial local optimum during optimization)9. (Coupled-CVAE, Is-a-Prerequisite-of, improving diversity of dialogue generation)
### Triplets:1. Topical PageRank is a type of method.2. PageRank needs to run multiple times during the ranking procedure.3. Salience Rank is a modification of Topical PageRank.4. PageRank is the first step of Salience Rank.5. Salience Rank extracts keyphrases from benchmark datasets.6. Graph is representing prior information about dialog transitions.7. Dialog corpora are used to construct a conversational graph.8. Graph is essential for policy learning in open-domain dialogue systems.9. Salience Rank is a method used for extracting keyphrases in NLP.10. Automatic readability assessment generally discards linguistic features.
### Extracted Concepts:1. Kernel methods2. Deep neural networks3. Tensor data4. Nystrom low-rank approximation5. Universal Language Model Fine-tuning (ULMFiT)6. Simple Word-Embedding-based Models (SWEMs)7. Order Embeddings (OE)8. Generative Adversarial Networks (GAN)9. Sequence-to-Action10. Variational autoencoders (VAEs)11. Diachronic accuracy12. Language models### Triplets:1. (manifold learning, Used-for, feature representation learning)2. (manifold learning, Evaluate-for, structured information modeling)3. (OE, Is-a-Prerequisite-of, box lattice construction)4. (GAN, Evaluate-for, relation classification)5. (VAEs, Is-a-Prerequisite-of, semantic graph generation)6. (SAARs, Compare, traditional approaches for model evaluation)7
### Extracted Concepts:1. Word-embedding models2. Skip-Gram model3. Probabilistic FastText4. Gaussian Mixture Density5. Mixture Distribution6. Latent Vector Grammars (LVeGs) 7. Gaussian Mixture LVeGs (GM-LVeGs)8. Neural network models9. Recurrent Neural Networks (RNNs)10. LSTM11. Verb-noun combinations (VNCs)### Triplets:1. (Probabilistic FastText, Is-a-Prerequisite-of, Word-embedding models)2. (Gaussian Mixture Density, Compare, Mixture Distribution)3. (Gaussian Mixture Density, Is-a-Prerequisite-of, Probabilistic FastText)4. (Probabilistic FastText, Compare, FastText)5. (Latent Vector Grammars, Is-a-Prerequisite-of, Word-embedding models)6. (GM
### Triplets:1. meta-learning, Compare, multi-task learning2. meta-learning, Evaluate-for, task performance3. task performance, Compare, multi-task learning4. meta-learning, Is-a-Prerequisite-of, reinforcement learning5. reinforcement learning, Is-a-Prerequisite-of, sequence prediction algorithms6. meta-learning, Is-a-Prerequisite-of, neural machine translation7. meta-learning, Is-a-Prerequisite-of, structured projection of intermediate gradients
(None)
(None)
None.
(<support vector machine>, <is superior to>, <other classifiers>)(<complex networks>, <enriched with>, <word embedding>)(<linguistic features>, <used to detect>, <MCI>)(<support vector machine>, <compared with>, <LSTM-based recurrent neural network>)
(`<context>`, `Contains`, `multi-agent system`)  (`<multi-agent system>`, `Used-for`, `teaching agents to communicate with humans in natural language`)  (`<multi-agent system>`, `Part-of`, `end goal of teaching agents to communicate with humans in natural language`)  (`<multi-agent system>`, `Compare`, `traditional data-driven approaches to natural language learning`)  (`<multi-agent system>`, `Conjunction`, `natural language learning`)
1. (Neural encoders, Used-for, dependency parsers)2. (Neural encoders, Compare, non-neural parsers)3. (dependency parsers, Is-a-Prerequisite-of, neural encoders)4. (Neural encoders, Is-a-Prerequisite-of, belief in encoding structural constraints)5. (bert, Is-a-Prerequisite-of, powerful pre-trained encoder)6. (Previous first-order methods, Evaluate-for, compression of PLMs)7. (Pruning, Used-for, compression of PLMs)8. (movement pruning, Is-a-Prerequisite-of, pruning PLMs)9. (Static Model Pruning, Is-a-Prerequisite-of, adapt PLMs to downstream tasks)10. (fine-tuning, Evaluate-for, convergence of PLMs to downstream tasks)11. (first-order pruning, Is-a-Prerequisite-of, Static Model Pruning)12. (Teemporal logiCal
### Extracted Concepts:- Document-level context- Self-supervised learning- Auxiliary pre-training tasks- CNN/DM dataset- Knowledge distillation- Structured prediction problems- Sequence labeling- Dependency parsing models- Knowledge graph embedding- Softmax cross-entropy- Negative sampling loss functions- Token-level adaptive training- Machine translation- Bilingual mutual information (BMI)- Task-oriented dialog system- OOD detection- Autoregressive language models- Maximum likelihood estimation (MLE)- MixCE objective### Triplets:1. Cross-entropy, used-for, training2. Cross-entropy, evaluate-for, models3. Cross-entropy, is-a-prerequisite-of, maximum likelihood estimation (MLE)4. Cross-entropy, evaluate-for, forward cross-entropy5. Cross-entropy, evaluate-for, reverse cross-entropy6. Cross-entropy, evaluate-for, MixCE objective7
None
### Triplets:1. semantic role labeling, Is-a-Prerequisite-of, deep learning model2. deep learning model, Used-for, semantic role labeling3. Chinese SRL, Evaluate-for, Data sparsity4. syntactic backbone, Is-a-Prerequisite-of, neural SRL models5. syntactic information, Evaluate-for, dependency SRL performance6. high semantic similarity, Is-a-Prerequisite-of, source texts and summaries 7. neural SRL systems, Is-a-Prerequisite-of, syntactic backbone8. document-level temporal relation classification, Used-for, syntactic-aware model9. pre-trained language models, Compare, expressive input representations10. semantic dependencies, Part-of, SRL11. syntactic information, Is-a-Prerequisite-of, Semantic Role Labeling12. predicate argument structures, Conjunction, contextualized representation13. structural information, Evaluate-for, claim verification accuracy14.
### Triplets:1. manual extraction of features, Evaluate-for, tackle text subtleties2. classification tasks, Compare, Sentiment Analysis and Sarcasm Detection3. learned text and gaze features, Compare, handcrafted gaze and textual features4. gaze information, Part-of, published sentiment and sarcasm labeled datasets5. framework, Used-for, extract cognitive features from the eye-movement/gaze data6. automatically extracted cognitive features, Is-a-Prerequisite-of, Sentiment polarity and sarcasm detection7. Convolutional Neural Network (CNN), Is-a-Prerequisite-of, classify the input text8. tokenization, Part-of, linguistic resources9. attention-based methods, Is-a-Prerequisite-of, perform alignment between acoustic frames and recognized symbols10. connectionist temporal classification (CTC), Is-a-Prerequisite-of, efficiently solve sequential problems by dynamic programming
### Triplets:1. word sense disambiguation, aims-to-identify, correct meaning 2. WordNet, proved-to-be-helpful-for, word sense disambiguation3. neural networks, rely-on, massive labeled data4. previous neural networks, ignore, labeled data5. proposed GAS model, outperforms, state-of-the-art systems6. neural supervised architecture, taps-into, Lexical Knowledge Bases7. Enhanced WSD Integrating Synset Embeddings and Relations, sets-new-state-of-the-art8. relational information, encoded-in, Lexical Knowledge Bases
(None)
### Triples:1. autonomous car, Used-for, navigation2. autonomous car, Used-for, vision3. navigation, Is-a-Prerequisite-of, autonomous car4. vision, Is-a-Prerequisite-of, autonomous car
### Triplets:1. finite state machine, Is-a-Prerequisite-of, machine translation2. weighted finite state machine, Is-a-Prerequisite-of, NLP systems3. Weighted finite-state machines, Used-for, handle tasks such as part-of-speech tagging4. weighted finite state machine, Evaluate-for, computation of higher-order derivatives5. weighted finite state machine, Evaluate-for, computing second-order expectations
### Triplets:1. neural techniques, Used-for, end-to-end computational argumentation mining2. end-to-end computational argumentation mining, Is-a-Prerequisite-of, AM3. AM, Part-of, variation in models that operate on the argument component level4. models, Compare, framing AM as dependency parsing and as token-based sequence tagging5. local tagging models based on BiLSTMs, Compare, complex models that operate on the argument component level6. joint learning ‘natural’ subtasks, Evaluate-for, performance improvement7. neural machine translation, Used-for, encoding the source sentence8. neural machine translation, Compare, reliance on bi-directional LSTMs for source sentence encoding9. convolutional layers, Used-for, faster and simpler architecture for encoding the source sentence10. recurrent networks, Evaluate-for, constraints by temporal dependencies11. neural language model, Used-for, generating rhythmic poetry12. Mixed R
### Triplets:1. statistical part of speech tagging, is-a-prerequisite-of, part-of-speech tagging2. statistical part of speech tagging, compare, part-of-speech tagging3. statistical part of speech tagging, evaluate-for, part-of-speech tagging
(`<concept>`, `Compare`, `recent work on neural machine translation`)  (`<concept>`, `Compare`, `Transformer architectures`)  (`<concept>`, `Is-a-Prerequisite-of`, `neural machine translation`)  (`<concept>`, `Compare`, `neural chat translation`)  (`<concept>`, `Evaluate-for`, `better translation quality`)
### Triplets:1. dynamic programming, is-a-Prerequisite-of, MH₄ algorithm2. dynamic programming, is-a-Prerequisite-of, Integer Linear Programming (ILP)3. dynamic programming, is-a-Prerequisite-of, global structures4. dynamic programming, is-a-Prerequisite-of, Gumbel perturbations5. dynamic programming, evaluate-for, model score decomposition6. dynamic programming, evaluate-for, dynamic programming algorithm7. dynamic programming, evaluate-for, syntactic constraints8. MH₄ algorithm, is-a-Prerequisite-of, non-projectivity9. MH₄ algorithm, evaluate-for, dynamic programming algorithm10. global structures, used-for, structure induction11. Gumbel perturbations, compare, stochasticity12. dynamic programming algorithm, compare, efficient inference13. syntactic constraints, part-of, model constraints14. Integer Linear Programming (ILP), part
### Concept: text to speech generation1. (text to speech generation, Used-for, fast generation speed)2. (text to speech generation, Evaluate-for, generation speed improvement)3. (text to speech generation, Part-of, NAR-TTS models)4. (text to speech generation, Evaluate-for, voice quality improvement)5. (NAR-TTS models, Is-a-Prerequisite-of, text to speech generation)6. (text to speech generation, Compare, traditional text-to-speech approaches)7. (text to speech generation, Part-of, generative dialogue models)8. (text to speech generation, Is-a-Prerequisite-of, natural language generation)
1. (Neural word segmentation research, Benefited-by, Large-scale raw texts)2. (Statistical segmentation research, Exploited, Richer sources of external information)3. (Neural word segmentation, Investigate-the-effectiveness-of, External training sources)4. (Neural word segmentation, Pretrained-using, Rich external sources)5. (Adversarial multi-criteria learning, Proposed-for, Chinese word segmentation)6. (Neural network-based joint models, Developed-for, Chinese word segmentation)7. (Dependency parsing, Preferred-by, Character trigram representations)
### Triplets:1. query expansion, Used-for, attribute value extraction2. query expansion, Used-for, knowledge-driven query expansion3. query expansion, Evaluate-for, value knowledge4. query expansion, Evaluate-for, AVE5. query expansion, Evaluate-for, rare and ambiguous queries6. query expansion, Evaluate-for, expanding a small seed entity set
1. (LSTM Noisy Channel Model, Used-for, disfluency detection)2. (Noisy Channel Model, Is-a-Prerequisite-of, LSTM Noisy Channel Model)3. (Noisy Channel Model, Used-for, generating n-best candidate disfluency analyses)4. (Noisy Channel Model, Used-for, noisy channel disfluency model)5. (Noisy Channel Model, Used-for, language model prompting)6. (Noisy Channel Model, Evaluate-for, developing channel prompt tuning)7. (Noisy Channel Model, Compare, direct models)
### Triplets:1. Quaternion algebra, Has-a-relationship-with, Natural language processing2. EPT-X model, Is-a-Prerequisite-of, Algebraic word problem3. Plausibility, Used-for, Math word problem solving strategies4. Compressive summarization, Evaluate-for, Information summarization systems5. DUC dataset, Contains, Summarization results
#### Triples:1. natural language processing, Used-for, multi-task learning2. natural language processing, Part-of, Hierarchical Refinement Quantized Variational Autoencoders3. Hierarchical Refinement Quantized Variational Autoencoders, Is-a-Prerequisite-of, syntactic diversit4. neural network architectures, Is-a-Prerequisite-of, natural language understanding5. International Classification of Diseases, Part-of, Hyperbolic and Co-graph Representation method6. Hyperbolic and Co-graph Representation method, Evaluate-for, automatic ICD coding7. Hierarchical Text Classification, Is-a-Prerequisite-of, Hierarchical duality learning for dialogue
### Triplets:1. (named entity recognition, Is-a-Prerequisite-of, entity resolution)2. (named entity recognition, Compare, sequence labeling)3. (named entity recognition, Is-a-Prerequisite-of, natural language processing)4. (named entity recognition, Used-for, information extraction)5. (named entity recognition, Compare, local detection approach)6. (named entity recognition, Is-a-Prerequisite-of, multiple-speaker speech recognition)7. (named entity recognition, Is-a-Prerequisite-of, deep learning)8. (named entity recognition, Evaluate-for, multilingual learning)9. (named entity recognition, Evaluate-for, multi-task learning)10. (named entity recognition, Evaluate-for, language resource creation)
### Triplets:1. dependency syntax, Is-a-Prerequisite-of, transition-based dependency parsers2. dependency syntax, Is-a-Prerequisite-of, sequence tagging problem3. dependency syntax, Used-for, semantic dependency parsing4. transition-based dependency parsers, Used-for, producing certain attachments5. transition-based dependency parsers, Evaluate-for, global information6. transition-based dependency parsers, Used-for, lexical information7. convolutional neural network, Used-for, composing word representations8. BONIE, Evaluate-for, extracting Open IE tuples9. Jensen-Shannon divergence, Compare, JS dependency measure10. Pointer networks, Used-for, dependency parsing11. Stack-pointer networks, Is-a-Prerequisite-of, dependency parsing
(<shift-reduce parsing>, Evaluate-for, <constituency parsing>)(Traditional shift-reduce parsing schemes, Compare, Our approach)(Traditional shift-reduce parsing schemes, Evaluate-for, Potentially disastrous compounding error)(<shift-reduce parsing schemes>, Is-a-Prerequisite-of, Model)(Model, Evaluate-for, Prediction)(Model, Evaluate-for, Determination)(Model, Evaluate-for, State-of-the-art single model F1 score)(Chinese discourse parser, Is-a-Prerequisite-of, Standalone parser)(Chinese discourse parser, Is-a-Prerequisite-of, Parser)(Chinese discourse parser, Evaluate-for, Performance)(Sequence-to-sequence constituent parsing, Part-of, Linearization)(Linearization, Evaluate-for, Best accuracy)(Linearization, Part-of, Top-down tree linearizations)(Linearization, Part-of, In-order linearization)(In-order linearization, Compare, Top-down tree linearizations)(Linearization, Part-of
### Triplets:1. latent semantic indexing, Is-a-Prerequisite-of, semantic textual similarity tasks2. latent semantic indexing, Compare, skip-thought vectors3. latent semantic indexing, Used-for, semantic textual similarity tasks
None.
### Triplets:1. (policy gradient method, Used-for, training neural sequence-to-sequence models)2. (policy gradient method, Evaluate-for, achieving new state-of-the-art performance)3. (neural coreference models, Trained by, policy gradient method)4. (policy gradient training, Leads to, more accurate normalizations)5. (policy gradient training, Help achieve, fast convergence)6. (IRL-based AL policy learning methods, Restricted to learn from, closely related domains)7. (AL policy training strategy, More effective than, existing strong baseline methods)
### Triplets:1. Cross-lingual text classification is a task in natural language processing.2. Cross-lingual text classification is used for classifying documents written in different languages into the same taxonomy of categories.3. CARI is an innovative method for formality style transfer.4. CARI integrates rules for pre-trained language models.5. CARI can be used for pre-trained language models in natural language processing.6. Non-binary gender identities are essential to consider in the context of gender bias in language technologies.
1. (Kernel methods, Enable, direct usage of structured representations)2. (Expressive kernels, Achieve, excellent performance in NLP)3. (Deep neural networks, Have been demonstrated effective, in automatically learning feature representations)4. (Input layer, Can be pre-trained, through the application of the Nystrom low-rank approximation of kernel spaces)5. (Generic word embeddings, Are trained, on large-scale generic corpora)6. (Domain Specific (DS) word embeddings, Are trained, only on data from a domain of interest)7. (Domain Adapted (DA) word embeddings, Are formed, by aligning corresponding word vectors)8. (String kernels, Capture, similarity among strings based on counting common character n-grams)9. (String kernels, Demonstrate, state-of-the-art results in various text classification tasks)10. (String kernels, Are the first to be applied, to automatically score essays)11. (String
### Triplets:1. loss function, **evaluated-for**, output structure2. loss function, **used-for**, finding the max-violating constraint3. loss function, **used-for**, more complex loss functions4. loss function, **used-for**, penalizing inconsistency5. loss function, **used-for**, end-to-end training6. loss function, **compare**, expressivity of effective loss functions.
### Concept: image retrieval1. (image retrieval, Used-for, similarity search)2. (image retrieval, Evaluate-for, relevance ranking)3. (image retrieval, Compare, text retrieval)4. (machine learning, Used-for, feature extraction)5. (lossless representation, Part-of, image retrieval)
### Triplets:1. Seq2Seq models, widely used for response generation, Seq2Seq models2. Seq2Seq models, used for response generation, response generation3. Seq2Seq models, widely used for response generation, response generation4. Seq2Seq models, used for response generation, widely used5. Seq2Seq models, used for response generation, Seq2Seq models6. Seq2Seq models, widely used for response generation, response generation7. Seq2Seq models, widely used for response generation, widely used8. Seq2Seq models, used for response generation, Seq2Seq models9. Seq2Seq models, used for response generation, response generation
### Triplets:1. (neural semantic parser, is interpretable and scalable, model)2. (natural language, to intermediate, natural language representations)3. (natural language, to intermediate, domain-general natural language representations)4. (semantic parser, trained end-to-end, annotated logical forms)5. (semantic parser, trained end-to-end, denotations)6. (state of the art, achieved, on SPADES)7. (state of the art, achieved, on GRAPHQUESTIONS)8. (competitive results, obtained, on GEOQUERY)9. (competitive results, obtained, on WEBQUESTIONS)10. (predicate-argument structures, shed light on, types of representations)11. (predicate-argument structures, induced with, transition system)12. (predicate-argument structures, subsequently mapped to, target domains)13. (learning, commonsense knowledge from natural language text, nontrivial)14. (house, bigger
1. (propositional logic, Is-a-Prerequisite-of, logical form)2. (propositional logic, Used-for, semantic relations)3. (propositional logic, Compare, compositional logical forms)4. (seq2seq models, Evaluate-for, capturing compositional generalizations)5. (semantic parser, Is-a-Prerequisite-of, zero-shot semantic parsing)6. (semantic parser, Used-for, parsing instructions)7. (neural sequence-to-sequence models, Evaluate-for, reducing error rate)8. (attention mechanism, Is-a-Prerequisite-of, NLP models)9. (dense retriever, Compare, BM25)
### Extracted Concepts:1. adversarial training2. neural network3. adversarial examples4. robustness5. Adversarial Attention Network6. why-question answering7. neural machine translation (NMT)8. gradient-based method### Triplets:1. (adversarial search, Used-for, adversarial training)2. (adversarial search, Used-for, neural network)3. (adversarial search, Used-for, adversarial examples)4. (adversarial search, Used-for, robustness)5. (adversarial search, Used-for, Adversarial Attention Network)6. (adversarial search, Compare, why-question answering)7. (adversarial search, Used-for, neural machine translation (NMT))8. (adversarial search, Used-for, gradient-based method)
### Triplets:1. linear discriminant analysis, Is-a-Prerequisite-of, Generative Adversarial Networks (GANs)2. linear discriminant analysis, Used-for, Zero-shot stance detection (ZSSD)
### Triplets:1. (Information Extraction, Used-for, Relation Extraction)2. (Information Extraction, Compare, Relation Extraction)3. (Relation Extraction, Evaluate-for, Distant Supervision)4. (Knowledge Graphs, Used-for, Studying Complex Multi-Relational Data)5. (Relation Extraction, Part-of, Language Processing)
None
(`<query concept>`, `Used-for`, `optimizing task reward`)  (`<neural network>`, `Part-of`, `Neural Symbolic Machine`)  (`<REINFORCE>`, `Used-for`, `optimizing stability`)  (`<iterative maximum-likelihood training process>`, `Is-a-Prerequisite-of`, `training with weak supervision`)  (`<Neural Symbolic Machine>`, `Outperforms`, `state-of-the-art on WebQuestionsSP dataset`)  (`<LSTM>`, `Is-a-Prerequisite-of`, `machine reading`)  
### Triplets:1. speech signal analysis, Used-for, discovering word-like acoustic units2. speech signal analysis, Compare, conventional automatic speech recognition3. speech signal analysis, Compare, text transcriptions4. speech signal analysis, Compare, conventional linguistic annotations5. child-directed speech, Is-a-Prerequisite-of, learning to extract semantic information from speech directly
(None)
### Triplets:1. neural encoder-decoder transition-based parser, **Is-a-Prerequisite-of**, statistical parsing2. neural encoder-decoder transition-based parser, **Used-for**, parsing sentences3. neural encoder-decoder transition-based parser, **Hyponym-Of**, full-coverage semantic graph parser
(<concept>, Used-for, ranking of noun phrases)(pagerank, Compare, Salience Rank)(pagerank, Used-for, extract keyphrases)(pagerank, Part-of, Salience Rank)(pagerank, Evaluate-for, efficiency benefit)(pagerank, Is-a-Prerequisite-of, running PageRank)(pagerank, Compare, hyperdoc2vec)(pagerank, Evaluate-for, superiority of hyperdoc2vec)(pagerank, Compare, hyperdoc2vec)(pagerank, Evaluate-for, superiority of hyperdoc2vec)(pagerank, Compare, EL)(pagerank, Evaluate-for, annotation speed)
### Triplets:1. n-gram model, Used-for, language modeling2. n-gram model, Is-a-Prerequisite-of, language model perplexity3. language model, Is-a-Prerequisite-of, neural language model4. neural language model, Used-for, incorporating document context5. neural language model, Compare, pure sentence-based model6. LSTM, Is-a-Prerequisite-of, Affect-LM7. Affect-LM, Used-for, customization of emotional content8. LSTM, Used-for, generation of conversational text9. language model, Compare, character-level language model10. Weight sharing, Evaluate-for, neural models for classification11. document-level data, Evaluate-for, aspect-level sentiment classification12. LSTM, Used-for, aspect-level sentiment classification
(None)
### Triplets:1. prosody, plays a role in, word segmentation2. prosody, affects, parser3. parser, receives, dialogue turn4. prosody, helps with, parsing disfluent speech5. CUC-VAE, conditions on, cross-utterance information6. CUC-VAE, generates, prosody features7. CUC-VAE, improves, naturalness and prosody diversity8. language model, fine-tuned for, paraphrasing9. prototypes, used for, share graph structures10. prototypical graph, learns, target-based representation
(`<content>`, Used-for, dependency parsing)  (dependency parsing, Part-of, end-to-end computational argumentation mining)  (dependency parsing, Compare, token-based dependency parsing)  (dependency parsing, Compare, token-based sequence tagging)  (dependency parsing, Used-for, semantic dependency parsing)  (dependency parsing, Evaluate-for, performance results)  (dependency parsing, Is-a-Prerequisite-of, neural techniques)  
(None)
### Triplets:1. Rhetorical Structure Theory (RST), Discourse Treebank, availability2. research, RST annotation, parsing 3. RST annotations, Span, Nuclearity4. RST annotations, discourse coherence, scoring rubrics5. coreference resolution, predicate argument structure analysis, structured events6. inter-sentential zero anaphora resolution, predicate argument structure analysis, entity embedding7. Referring Expression Generation (REG) models, salience, grammatical function8. discourse coherence, relations, discourse segments9. long-span dependencies, discourse units, discourse parsing performance10. memory networks, discourse cohesion, discourse parsing11. PhotoBook dataset, visually-grounded dialogues, shared dialogue history12. conversational discourse parsing (CDP), DPR, joint model13. Atomic clauses, complex sentences, discourse parsing, question answering14. graph edit task, neural model, complex sentence decomposition
### Triplets:1. speech processing, Is-a-Prerequisite-of, visually grounded model2. form and meaning-based linguistic knowledge, Is-a-Prerequisite-of, speech processing3. feedforward neural network (FFNN), Used-for, predict entity label4. named entity recognition (NER), Is-a-Prerequisite-of, speech processing5. natural language identification (LID), Is-a-Prerequisite-of, speech processing
### Triplets:1. Chinese word segmentation, Used-for, POS tagging2. Chinese word segmentation, Is-a-Prerequisite-of, Dependency parsing3. Chinese word segmentation, Part-of, Adversarial multi-criteria learning4. Chinese word segmentation, Evaluate-for, Performance improvement5. Chinese word segmentation, Used-for, Neural network-based joint models6. Chinese word segmentation, Used-for, Sentence tokenization7. Chinese word segmentation, Evaluate-for, Text classification8. Chinese word segmentation, Evaluate-for, Entity categorization9. Chinese word segmentation, Evaluate-for, Query understanding10. Character strings embeddings, Used-for, Dependency parsing11. Neural network-based joint models, Is-a-Prerequisite-of, Preventing error propagation in pipeline models12. Attention mechanism, Used-for, Generating Chinese poems13. Neural models, Is-a-Prerequisite-of, Zero pronoun resolution14. Character strings embeddings, Is-a-Pr
### Triplets:1. domain adaptation, used-for, general model2. domain adaptation, used-for, neural network model3. domain adaptation, Evaluate-for, sentiment analysis4. domain adaptation, Compare, machine translation5. domain adaptation, Is-a-Prerequisite-of, transfer learning6. domain adaptation, Evaluate-for, parser adaptation
(`<head concept>`, `Used-for`, `semantic role labeling (SRL)`)  (`<head concept>`, `Evaluate-for`, `state-of-the-art`)  (`<head concept>`, `Part-of`, `deep learning model`)  (`<deep learning model>`, `Is-a-Prerequisite-of`, `semantic role labeling (SRL)`)  
### Extracted Concepts:1. A* CCG parsing model2. deep convolutional neural network (CNN)3. word-level CNN4. neural network model5. Universal Dependencies6. dependency parsing7. holographic embeddings8. Transformer models9. social media language model10. recurrent neural networks (RNNs)11. LSTM12. Minimalist Grammar (MG)13. A* search algorithm14. computational tools15. Twitter-Specific Social Media Language Model### Triplets:1. (A* CCG parsing model, Used-for, modeling sentence structures)2. (deep convolutional neural network (CNN), Used-for, text categorization)3. (Universal Dependencies, Is-a-Prerequisite-of, dependency parsing)4. (dependency parsing, Used-for, constructing a dependency treebank)5. (holographic embeddings, Compare, complex embeddings)6. (Transformer models, Is-a-
### Extracted Concepts:1. Information Retrieval (IR)2. Text Mining3. Natural Language Processing (NLP)4. Sentiment Analysis5. Neural Networks6. Dependency Parsing7. Neural Machine Translation (NMT)8. Natural Language Understanding9. Knowledge Graphs### Triplets:1. (Information Retrieval (IR), Is-a-Prerequisite-of, Sentiment Analysis)2. (Information Retrieval (IR), Evaluate-for, Volatility Prediction)3. (Neural Networks, Used-for, Dependency Parsing)4. (Dependency Parsing, Evaluate-for, Neural Networks)5. (Knowledge Graphs, Evaluate-for, Neural Machine Translation (NMT))6. (Natural Language Processing (NLP), Compare, Text Mining)
### Triplets:1. class logistics, Evaluate-for, model performance2. model performance, affected, model judgements3. class logistics, Evaluate-for, changes in model performance
### Triplets:1. question answering, is-a-Prerequisite-of, reading comprehension2. reading comprehension, Evaluate-for, understanding natural texts3. question answering, Evaluate-for, boosting the performance4. question answering, Used-for, boosting the performance5. question answering, Is-a-Prerequisite-of, knowledge base question answering6. knowledge base question answering, Evaluate-for, accessing substantial knowledge7. knowledge base question answering, Is-a-Prerequisite-of, neural network-based methods8. neural network-based methods, Achieves, impressive results9. neural network-based methods, Evaluate-for, question representation10. neural network-based methods, Is-a-Prerequisite-of, question representation11. neural network-based methods, Used-for, representing questions12. deep residual bidirectional LSTMs, Is-a-Prerequisite-of, detecting KB relations13. KB relations, Evaluate-for, given an input question14. KB relations,
### Triplets:1. entropy, is-a-Prerequisite-of, Information Extraction2. entropy, Compare, diversity3. neural semantic parser, Used-for, encoder4. entropy, Compare, uncertainty5. semantic parser, Evaluate-for, performance6. Probabilistic FastText, is-a-Prerequisite-of, word embeddings
### Triplets:1. characters, is-a-constituent-of, word2. characters, used-for, learning3. characters, evaluate-for, effectiveness4. data-driven sub-word units, Is-a-Prerequisite-of, morphological segmentation5. characters, used-for, word embeddings6. word embeddings, Hyponym-Of, dense representations7. morphological disambiguation, Evaluate-for, effectiveness8. morphological disambiguation, Is-a-Prerequisite-of, morphology-aware alignment model9. morphological disambiguation, Is-a-Prerequisite-of, dense representations
### Triplets:1. classic parsing method < Is-a-Prerequisite-of < supervised semantic parsers2. structured information < Evaluate-for < classic parsing method3. parsing framework < Is-a-Prerequisite-of < classic parsing method
### Extracted Concepts:1. Stochastic Gradient Descent (SGD)2. Negative sampling3. Sampling methods4. Batch gradient learning5. AllVec6. Time complexity7. Adversarial training8. Adversarial samples9. Distribution shift risk minimization (DSRM)10. Gradient Ascent Post-training (GAP)11. Pretrained LMs12. NLP tasks### Triplets:1. (gradient descent, Is-a-Prerequisite-of, Stochastic Gradient Descent (SGD))2. (gradient descent, Compare, Batch gradient learning)3. (gradient descent, Evaluate-for, Time complexity)4. (gradient descent, Evaluate-for, Adversarial training)5. (gradient descent, Is-a-Prerequisite-of, Gradient Ascent Post-training (GAP))6. (Adversarial training, Evaluate-for, Adversarial samples)7. (Adversarial training
### Triplets:1. transliteration, Used-for, translating text between different writing systems2. transliteration, Compare, translation3. transliteration, Used-for, converting text from one script to another without changing its meaning4. transliteration, Is-a-Prerequisite-of, machine translation5. text, Conjunction, transliteration6. transliteration, Used-for, preserving the pronunciation of a word from one language when written in another language7. transliteration, Compare, transcription
1. (Neural network models, Show opportunities for multi-task learning)2. (Structured representations, Used-for Language learning and inference tasks)3. (Multi-modal sentiment analysis, Is-a-Prerequisite-of Sentiment identification in videos)4. (Neural Machine Translation, Used-for Translation tasks)5. (Multi-modal research, Is-a-Prerequisite-of Artificial intelligence studies)6. (Neural Named Entity Recognition, Is-a-Prerequisite-of Multilingual learning)7. (Multilingual learning, Evaluate-for Improving Named Entity Recognition performance)
### Extracted concepts:1. Dynamic oracle2. Hierarchical recurrent neural network3. Knowledge Base Question Answering (KBQA)4. Hierarchical semantic space5. Language model6. Dependency parsing### Triplets:1. Dynamic oracle, Is-a-Prerequisite-of, Knowledge Base Question Answering (KBQA)2. Hierarchical recurrent neural network, Used-for, Knowledge Base Question Answering (KBQA)3. Hierarchical recurrent neural network, Compare, Language model4. Dynamic oracle, Evaluate-for, Non-monotonic transition system based on the non-projective Covington algorithm5. Hierarchical semantic space, Is-a-Prerequisite-of, Integrating text and knowledge into a unified semantic space6. Language model, Used-for, Entity linking7. Dependency parsing, Compare, Language model
### Triplets:1. text similarity measures, used-in, plagiarism detection2. text similarity measures, used-in, information ranking3. text similarity measures, used-in, recognition of paraphrases4. text similarity measures, used-in, textual entailment5. ParaNMT-50M, is-a-Prerequisite-of, paraphrase generation6. LSTM recurrent networks, used-for, transfer learning7. LSTM recurrent networks, used-for, supervised settings8. Gated Recurrent Averaging Network, Is-a-Prerequisite-of, LSTM recurrent networks9. crowdsourcing, Evaluate-for, expanding natural language datasets10. ParaNMT-50M, is-a-Prerequisite-of, SemEval semantic textual similarity competition11. summarization model, Is-a-Prerequisite-of, summarization12. seq2seq models, Is-a-Prerequisite-of, high scores13. Decomposable Neural Paraphrase Generator,
### Triplets:1. neural machine translation, relies on, bi-directional LSTMs2. succession of convolutional layers, allows to encode, source sentence simultaneously3. LSTM setup, is used for, English-French translation4. A* CCG parsing model, decomposed into factors of CCG categories5. null heads, factored out from bottom-up MG parsing6. neural network architectures, guided training and prediction, by introducing declarative knowledge7. data programming, allows a user to label training data using expert-composed heuristics
(`<concept>`, `Compare`, `Lexical relations`)  (`<concept>`, `Part-of`, `Unsupervised PCFG inducers`)  (`<Lexical relations>`, `Used-for`, `Automatic distinction of lexical relations`)  (`<Vision-and-Language Navigation(VLN)>`, `Evaluate-for`, `Language understanding`)  (`<context-free rules>`, `Is-a-Prerequisite-of`, `Compact rules induced by Unsupervised PCFG inducers`)  
### Triplets:1. zero-shot learning, related-to, one-shot learning2. zero-shot learning, involves, pragmatic speaker reasoning3. zero-shot learning, involves, uncertain object categories4. zero-shot learning, involves, neural generator5. zero-shot learning, involves, rational speech acts6. zero-shot learning, involves, automatic listener7. zero-shot learning, involves, novel categories8. zero-shot learning, related-to, Language & Vision9. neural generator, used-for, zero-shot learning10. rational speech acts, used-for, zero-shot learning11. automatic listener, used-for, zero-shot learning12. pragmatic speaker reasoning, evaluates-for, communicative success
(None)
(None)
### Extracted Concepts:1. Kernel methods2. Deep neural networks3. Tree Kernels4. Nystrom low-rank approximation5. Structured prediction6. Model comparison7. Loss functions8. Coreference resolution9. Abstract syntax networks10. Chunk-based decoders11. Transitional dependency parsing12. Cognitive information13. Gaze behavior14. Seq2Seq models15. Table-to-text generation16. Span representation design17. Argumentation structure parsing18. Sequence labeling19. Imitation learning20. Active learning### Triplets:1. (Structured prediction, Compare, Structured sparsity)2. (Span representation design, Is-a-Prerequisite-of, Argumentation structure parsing)3. (Sequence labeling, Compare, Table-to-text generation)4. (Imitation learning, Evaluate-for, Active learning)5. (Transition-based dependency parsing, Evaluate-for
### Triplets:1. information retrieval, Evaluate-for, trustworthiness estimation2. information retrieval, Is-a-Prerequisite-of, information extraction3. entity-oriented search, Compare, neural information retrieval4. EDRM, Used-for, integrating knowledge graph semantics5. contextual word representation models, Compare, deep models6. topic, Is-a-Prerequisite-of, relevant resources7. sentence function, Evaluate-for, conversation models8. Question Answering System, Conjunction, Neural Machine Translation9. Query Translation, Is-a-Prerequisite-of, Cross-Lingual Information Retrieval10. conversational Question Answering, Is-a-Prerequisite-of, DoQA dataset
### Extracted Concepts:1. End-to-end computational argumentation mining2. Dependency parsing3. Sequence tagging problem4. BiLSTMs5. Multi-task learning setup6. Neural Machine Translation (NMT) model7. Sequence-to-Dependency Neural Machine Translation (SD-NMT) method8. Text similarity measures9. Logical forms10. Graph Neural Network (GNN) architecture11. Sequence-to-sequence models12. General purpose sequence-to-sequence models13. Constituency parsing14. Span-based extract-then-classify framework15. Metaphor identification16. Hierarchical Semantic Parsing (HSP) method### Triplets:1. (nn sequence parsing, Is-a-Prerequisite-of, Sequence-to-Dependency Neural Machine Translation (SD-NMT) method)2. (Sequence tagging problem, Part-of, End-to-end computational argumentation mining)3. (BiLST
### Extracted Concepts:1. Domain adaptation method2. CCG parsing3. Dependency trees4. Off-the-shelf CCG parser5. Tree-adjoining grammars (TAG)6. Linear indexed grammars (LIG)7. Head grammars8. Control mechanism9. Controllable pushdown automata (PDAs)10. Labeled distinguished PDAs11. Pushdown Adjoining Automaton (PAA)12. Predicate-argument structure13. Lexical categories14. Supertagging errors15. Decomposed scoring### Triplets:1. (combinatory categorial grammar, Is-a-Prerequisite-of, CCG parsing)2. (combinatory categorial grammar, Used-for, statistical parsing)3. (combinatory categorial grammar, Used-for, evaluating performance)4. (controllable pushdown automata, Is-a-Prerequisite-of, labeled distinguished PDAs
### Triplets:1. structured representations <relation> encoded into a common embedding space2. BiFlaG <relation> novel bipartite flat-graph network for nested named entity recognition3. bidirectional interaction <relation> captured between entities4. ESCOFILT <relation> unifies representation and explanation5. lyric generation framework <relation> disentangles training from inference6. melody-guided text generation <relation> incorporates decoding constraints7. relative overall quality improvement <relation> based on human ratings
### Triplets:1. neural question answering, Is-a-Prerequisite-of, question answering models2. knowledge base, Is-a-Prerequisite-of, neural question answering3. sequence-to-sequence learning, Used-for, neural question answering4. reading comprehension, Compare, neural question answering5. web text, Is-a-Prerequisite-of, neural question answering6. Wikipedia, Used-for, neural question answering
### Triplets:1. facial recognition system, Evaluate-for, emotion recognition systems2. facial recognition system, Compare, named entity recognition (NER) systems
### Triplets:1. (programming language, is-a-Prerequisite-of, software development)2. (programming language, Used-for, code generation)3. (programming language, Evaluate-for, achieving state-of-the-art results)4. (programming language, Part-of, a general-purpose programming language like Python)5. (User, used, programming language)
None.
### Extracted Concepts:1. Continuous bag of words (CBOW) embedding model2. Hierarchical version of BERT3. Bag-of-words method### Triplets:1. (Continuous bag of words (CBOW) embedding model, Is-a-Prerequisite-of, Bag-of-words method)2. (Bag-of-words method, Is-a-Prerequisite-of, Continuous bag of words (CBOW) embedding model)3. (Hierarchical version of BERT, Is-a-Prerequisite-of, Bag-of-words method)4. (Bag-of-words method, Is-a-Prerequisite-of, Hierarchical version of BERT)
### Triplets:1. mathematical model, Used-for, dialogue response selection2. dialogue response selection, Evaluate-for, hierarchical curriculum learning3. hierarchical curriculum learning, Is-a-Prerequisite-of, corpus-level curriculum4. computational psycholinguistics, Evaluate-for, mathematical model5. mathematical model, Evaluate-for, human reading behavior6. human reading behavior, Compare, eye movement
### Triplets:1. model complexity, balances, data size2. width of the network, grows, bias3. network grows, bias, decreases4. width of the network, increases, variance5. width of the network, followed by, decrease6. large pretrained neural models, fine-tuning setting, generalizable enough7. fine-tuning setting, generalizable enough, low bias and variance8. low- and high-resource classes, variance change, ensemble size
### Concepts:1. Event coreference resolvers2. Trigger detection3. Event anaphoricity4. Event extraction5. Named entity recognition (NER)6. Mention detection (MD)7. Multi-document summarization (MDS)8. User feedback9. Event detection10. Sentence-level supporting argument detection11. Neural network models12. Nugget Proposal Networks (NPNs)13. Document date14. Geolocation15. Graph Convolutional Networks (GCN) ### Triplets:- (Event detection, Used-for, Event coreference resolvers)- (Event detection, Used-for, Trigger detection)- (Event detection, Used-for, Event anaphoricity)- (Event extraction, Used-for, Named entity recognition (NER))- (Event extraction, Used-for, Mention detection (MD))- (Multi-document summarization (MDS), Used-for, Content selection)- (Multi
### Triplets:1. neural conversation model, Is-a-Prerequisite-of, dialog system2. reinforcement learning, Evaluate-for, Hybrid Code Networks3. task-oriented spoken dialogue systems, Is-a-Prerequisite-of, novel intelligent assistants4. dialog state tracking, Part-of, task-oriented dialogue systems5. encoder-decoder model, Used-for, response generation6. adverbial presuppositions, Evaluate-for, dialogue systems7. response selection, Is-a-Prerequisite-of, automated dialogue systems
1. (Labeled sequence transduction, Is-a-Prerequisite-of, semi-supervised learning)2. (Generative Domain-Adaptive Nets, Evaluate-for, semi-supervised learning)3. (novel training framework, Evaluate-for, semi-supervised learning)4. (Generative Model, Evaluate-for, semi-supervised learning)5. (Our proposed framework, Used-for, acronym disambiguation for enterprises)6. (deep reinforcement learning strategy, Used-for, semi-supervised learning)
### Triplets:1. language identification, part-of, natural language processing2. language identification, evaluate-for, language diversity3. language identification, used-for, multilingual text processing4. word-level language detection, compare, language classifiers5. language identification, part-of, linguistic diversity6. word-level language detection, evaluate-for, multilingual applications7. language identification, is-a-Prerequisite-of, language processing tasks
### Triplets:1. Supervised Directional Similarity Network, <relation>, neural architecture2. neural architecture, <relation>, learning task-specific transformation functions3. learning task-specific transformation functions, <relation>, general-purpose word embeddings4. general-purpose word embeddings, <relation>, model relation of lexical entailment5. lexical entailment, <relation>, hyperLex dataset6. distributional semantic models, <relation>, decode patterns of brain activity7. negation function, <relation>, specific aspects of semantic composition8. lexical and compositional semantic models, <relation>, decode fMRI patterns9. affirmative sentences, <relation>, contain hand-action verbs10. language models (LMs), <relation>, predict missing spans of text11. language models (LMs), <relation>, off-the-shelf LMs12. text infilling, <relation>, predicting missing spans of text13. text infilling, <relation>,
### Triplets:1. Shallow parsing is used for Online Inference Efficiency.2. Shallow parsing decodes tokens for Instantaneous Grammatical Error Correction.3. Shallow parsing is a part of Shallow Aggressive Decoding.4. Shallow parsing helps to reduce computational cost during inference.
### Triplets:1. paper, introduces, calculus2. paper, experiments on, calculus3. methods, calculate, calculus
### Triplets:1. dependency parsing, Compare, CCG parsing model2. parsing, Evaluate-for, performance results3. neural techniques, Used-for, computational argumentation mining4. dependency parsing, Is-a-Prerequisite-of, semantic dependency parsing5. dependency parsing, Part-of, semantic structure6. neural techniques, Evaluate-for, performance7. dependency parsing, Hyponym-Of, parsing scheme
1. (Neural network models, Used-for, multi-task learning)2. (adversarial multi-task learning framework, Compare, existing approaches)3. (Neural network models, Evaluate-for, text classification tasks)4. (multi-space variational encoder-decoders, Is-a-Prerequisite-of, labeled sequence transduction)5. (Kernel methods, Used-for, structured representations in language learning)6. (expressive kernels, Hyponym-Of, Tree Kernels)7. (Kernel methods, Compare, deep neural networks)8. (State-of-the-art named entity recognition systems, Is-a-Prerequisite-of, supervised machine learning models)
### Triplets:1. Markov decision process, Is-a-Prerequisite-of, reinforcement learning2. MemSum, Used-for, extractive summarization3. MemSum, Conjunction, reinforcement learning4. MemSum, Compare, human summarization5. MemSum, Used-for, reinforcement learning6. MemSum, Is-a-Prerequisite-of, text extraction history7. PET, Is-a-Prerequisite-of, backdoor attack8. Neural metrics, Compare, traditional metrics9. DiffusionNER, Used-for, named entity recognition10. DiffusionNER, Is-a-Prerequisite-of, noisy spans11. DiffusionNER, Evaluate-for, entity boundaries
### Triplets:1. syntax, Is-a-Prerequisite-of, semantic parsing2. syntax, Used-for, natural language descriptions3. neural architecture, Used-for, capture target syntax4. parser, Is-a-Prerequisite-of, syntax5. neural machine translation (NMT), Used-for, incorporate source-side syntactic trees
### Triplets:1. NeuralDater, Used-for, Graph Convolutional Network2. Graph Convolutional Network, Used-for, Relation Extraction3. Aspect-based Sentiment Analysis, Compare, Graph Convolutional Network4. Visual Language Grounding, Is-a-Prerequisite-of, Graph Convolutional Network
### Triplets:1. spectral method, Used-for, learning weighted non-deterministic automata2. learning weighted non-deterministic automata, Part-of, language modeling3. spectral method, Compare, state-of-the-art ngram models
### Concept: pointer network1. (pointer network, Used-for, zero pronoun resolution)2. (pointer network, Used-for, generating large-scale pseudo training data)3. (pointer network, Used-for, transfer cloze-style reading comprehension neural network model)4. (pointer network, Is-a-Prerequisite-of, two-step training mechanism)5. (zero pronoun resolution, Evaluate-for, annotated data)6. (question answering, Is-a-Prerequisite-of, zero pronoun resolution)7. (question answering, Is-a-Prerequisite-of, pointer network)8. (pointer network, Compare, recurrent neural networks)9. (pointer network, Compare, neural machine translation)10. (pointer network, Compare, deep convolutional neural network)11. (neural machine translation, Evaluate-for, pointer network)12. (deep convolutional neural network, Evaluate-for, pointer network)
### Triplets:1. deep neural networks, cannot manage, rich structured information2. structured prediction, modeled as, a search problem3. distillation, improves, single model's performance4. structured information, critical for, semantic parsing tasks5. Graph Neural Network (GNN) architecture, incorporates, information about relevant entities6. shared grammar, enables, sharing of grammar knowledge among different corpora7. structured convolutional decoder, guided by, content structure of target summaries
### Triplets:1. search engine, Used-for, query auto-completion2. search engine, Evaluate-for, MRC on real web data3. multi-passage MRC, Compare, MRC on a single passage4. multi-passage MRC, Is-a-Prerequisite-of, machine reading comprehension5. end-to-end neural model, Used-for, cross-passage answer verification6. recurrent neural network language model, Used-for, generating query completions
(None)
### Concept: A* Search1. (A* Search, Used-for, Projective Arc-Eager Dependency Parser)2. (A* Search, Evaluate-for, Repairing Mistakes)3. (A* Search, Is-a-Prerequisite-of, Non-monotonic Transition System)4. (Dynamic Oracle, Compare, Monotonic Dynamic Oracle)5. (Neural Word Segmentation Research, Evaluate-for, Pretraining Word Embeddings)6. (Statistical Segmentation Research, Evaluate-for, Exploiting Rich Sources of Information)7. (External Training Sources, Part-of, Multimodal Sentiment Analysis)8. (LSTM-based Model, Is-a-Prerequisite-of, Multimodal Sentiment Analysis)9. (Abstractive Summarization, Compare, Extractive Summarization)10. (Neural Models, Is-a-Prerequisite-of, Abstractive Summarization)11. (Graph-based Attention Mechanism
### Triplets:1. automated essay scoring, Used-for, grading essays2. automated essay scoring, Is-a-Prerequisite-of, automated writing evaluation3. automated essay scoring, Evaluate-for, essay ratings4. automated essay scoring, Part-of, neural AES model
(`<concept>`, `Is-a-Prerequisite-of`, `neural networks`)(`neural networks`, `Used-for`, `training`)(`<concept>`, `Used-for`, `multiplying a matrix by a few-hot vector`)
(`<head concept>`, `Compare`, `stochastic optimization`)(`<head concept>`, `Part-of`, `language modeling`)(`<head concept>`, `Evaluate-for`, `model uncertainty`)(`<head concept>`, `Part-of`, `stochastic gradient Markov Chain Monte Carlo`)(`<head concept>`, `Compare`, `true morphological analyses`)
### Triplets:1. collaborative filtering, Used-for, extractive summarization2. collaborative filtering, Is-a-Prerequisite-of, rating prediction3. collaborative filtering, Evaluate-for, user/item explainability4. collaborative filtering, Compare, automatic evaluation5. collaborative filtering, Is-a-Prerequisite-of, MultiScale Collaborative framework
### Triplets:1. semantics, Used-for, lexical semantics2. NLG, Used-for, lexical semantics3. AMR parsing, Evaluate-for, lexical semantics4. Coreference resolvers, Part-of, lexical semantics5. neural search systems, Evaluate-for, lexical semantics6. semantic models, Is-a-Prerequisite-of, lexical semantics
(`<concept>`, `Evaluate-for`, `structured learning`)  (`<structured learning>`, `Part-of`, `generative latent-variable model`)  (`<structured learning>`, `Is-a-Prerequisite-of`, `generating fluent natural language responses`)  (`<machine translation>`, `Used-for`, `generating fluent natural language responses`)  (`<neural networks>`, `Compare`, `expressive kernels`)  (`<neural networks>`, `Used-for`, `detecting events`)  
#### Triplets:1. Universal Language Model Fine-tuning (ULMFiT), transfer learning, NLP2. In-domain tasks, transfer learning, NLP3. Supervised learning, transfer learning, NLP4. Learning embeddings, transfer learning, NLP5. Discourse and state dependencies, transfer learning, SESTRA
### Triplets:1. genetic algorithm, Used-for, automatic Pyramid scores2. genetic algorithm, Used-for, estimation of automatic Pyramid scores3. automatic Pyramid scores, Evaluate-for, extractive multi-document summarization4. genetic algorithm, Is-a-Prerequisite-of, automatic training data generation5. genetic algorithm, Compare, imitation learning6. genetic algorithm, Compare, reinforcement learning7. genetic algorithm, Compare, active learning8. genetic algorithm, Is-a-Prerequisite-of, posterior inference in generative latent-variable model
### Triplets:1. memory network, Used-for, question answering2. memory network, Is-a-Prerequisite-of, reading comprehension3. memory network, Compare, reading comprehension neural network model4. reading comprehension, Evaluate-for, answer production5. question answering, Compare, reading comprehension6. reading comprehension, Is-a-Prerequisite-of, question answering7. reading comprehension, Evaluate-for, state-of-the-art systems8. reading comprehension, Evaluate-for, performance improvement9. reading comprehension, Used-for, pointer networks
### Triplets:1. context sensitive lemmatization, Used-for, identifying edit tree2. context sensitive lemmatization, Evaluate-for, language independent3. lemming, Compare, proposed method4. Lemming, Is-a-Prerequisite-of, proposed method5. context embeddings, Used-for, NLP systems6. neural language models, Evaluate-for, grammar induction7. interactive corpora, Evaluate-for, scaling models8. incremental context-sensitive model, Is-a-Prerequisite-of, TTE prediction9. LCFRS-2, Is-a-Prerequisite-of, probabilistic LCFRS10. parameter learning, Evaluate-for, maximum likelihood
1. (Neural machine translation models, part of, part-of-speech tagging)2. (Part-of-speech tagging, Used-for, lexical features)3. (Natural language processing, Used-for, part-of-speech tagging)4. (Recurrent neural networks, part of, part-of-speech tagging)5. (Sequence generative models, Is-a-Prerequisite-of, text infilling)6. (Transformer architecture, Compare, LSTM)7. (Sequence generative models, Evaluate-for, text infilling)
### Triplets:1. greedy algorithm, Used-for, inference algorithm2. greedy algorithm, Part-of, NMT models3. greedy algorithm, Evaluate-for, text adversarial attack
### Triplets:1. compound splitters, Used-for, extrinsic evaluation2. neural network-based lexicon, Part-of, direct HMM3. linearized, Part-of, syntactic structures4. word coverage, Evaluate-for, tree-coverage model5. syntax-agnostic NMT baseline, Hyponym-Of, sequence-to-sequence model6. syntactic information, Is-a-Prerequisite-of, neural machine translation7. NMT+RNNG, Is-a-Prerequisite-of, attention-based neural machine translation
1. (thesaurus-based similarity, is-a-Prerequisite-of, checking procedure)2. (checking procedure, Used-for, analyzing discrepancies)3. (thesaurus-based similarity, Compare, corpus-based word similarities)
### Triplets:1. neural semantic parser, Used-for, interpretable and scalable2. neural semantic parser, Used-for, converting natural language utterances to intermediate, domain-general natural language representations3. neural semantic parser, Used-for, trained end-to-end using annotated logical forms or their denotations4. semantic parsing, Is-a-Prerequisite-of, natural language understanding5. semantic parsing, Is-a-Prerequisite-of, structured meaning representations6. neural semantic parser, Is-a-Prerequisite-of, state-of-the-art results on SPADES and GRAPHQUESTIONS7. semantic parsing, Compare, statistical parsing focusing almost exclusively on bilexical dependencies or domain-specific logical forms8. semantic parsing, Compare, Abstract meaning representations (AMRs) as broad-coverage sentence-level semantic representations9. semantic parsing, Evaluate-for, annotating NL utterances with their corresponding MRs10. neural semantic parser, Evaluate-for, significantly improved the translation
### Triples extracted from the content:1. (Psycholinguistic and graph theoretic measures, computed from, screenplays)2. (Psycholinguistic metrics, extrapolated to, dialogues in movies)3. (Distantly supervised open-domain question answering (DS-QA), aims to find answers in, collections of unlabeled text)4. (DS-QA models, retrieve, related paragraphs from large-scale corpus)5. (DS-QA models, apply, reading comprehension technique to extract answers from the most relevant paragraph)6. (New DS-QA model, employs, a paragraph selector to filter out noisy paragraphs)7. (New DS-QA model, employs, a paragraph reader to extract the correct answer from denoised paragraphs)8. (Inter-sentence relation extraction, deals with, complex semantic relationships in documents)9. (Inter-sentence relation extraction, requires, local, non-local, syntactic and semantic dependencies)10. (
### Triplets:1. Neural Turing Machine, Inspired-by, Global Context Layer2. Neural Turing Machine, Outperform, LSTM3. Neural Turing Machine, Used-for, Storing processed temporal relations4. Neural Turing Machine, First-to-use, NTM-like architecture in processing information from global context in discourse-scale natural text processing
(<phrase based machine translation>, Is-a-Prerequisite-of, Statistical Machine Translation)(Chunks, Part-of, Phrase Based Machine Translation)(Neural Machine Translation, Is-a-Prerequisite-of, Phrase Based Machine Translation)(Phrase Based Machine Translation, Used-for, Machine Translation)(Evaluate-for, Phrase Based Machine Translation, Translation Performance)
### Triplets:1. uncertainty, Is-a-Prerequisite-of, error propagation2. uncertainty, Evaluate-for, uncovering3. uncertainty, Is-a-Prerequisite-of, performance improvement4. uncertainty, Is-a-Prerequisite-of, efficient task performance5. uncertainty, Evaluate-for, confidence estimation6. uncertainty, Is-a-Prerequisite-of, uncertainty classification
(`<concept>`, Is-a-Prerequisite-of, `Abstractive Summarization`)  (`<concept>`, Is-a-Prerequisite-of, `Document Summarization`)  (`<concept>`, Evaluate-for, `ROUGE Scores`)  (`<concept>`, Used-for, `Text Summarization`)  (`<concept>`, Hyponym-Of, `Abstractive Sentence Summarization`)  
### Extracted Concepts:1. Dialog systems2. Adversarial learning3. Neural network architecture4. Natural language processing5. Classification models6. Neural machine translation7. Adversarial examples### Relationships:1. (Generative adversarial network, Used-for, Generating spurious features)2. (Generative adversarial network, Used-for, Adversarial learning)3. (Adversarial learning, Compare, Supervised learning)4. (Generative adversarial network, Compare, Generative adversarial networks)5. (Neural network architecture, Compare, Generative adversarial network)6. (Adversarial examples, Evaluate-for, Robustness of models)7. (Neural machine translation, Used-for, Adversarial examples)
### Extracted Concepts:1. Non-monotonicity2. Covington algorithm3. Dynamic oracle4. Keyphrase extraction5. Generative model6. Joint extraction of entities and relations7. Neural network8. Dependency parsing9. Dependency treebank10. Neural machine translation11. Source-side syntactic trees12. Coreference resolution13. Transition-based dependency parsers14. LSTM sequence labeling models15. Information extraction16. Spelling errors17. Native language detection18. Word embedding models### Triplets:1. (Dynamic oracle, Evaluate-for, Non-monotonic system)2. (Keyphrase extraction, Used-for, Understanding)3. (Generative model, Used-for, Keyphrase prediction)4. (Joint extraction of entities and relations, Is-a-Prerequisite-of, Tagging problem)5. (Neural network, Used-for, Geolocation prediction)6.
### Triplets:1. attention model, Used-for, Neural Machine Translation2. attention model, Used-for, Neural Multi-source Sequence-to-Sequence Learning3. attention model, Is-a-Prerequisite-of, Hierarchical Encoder4. Neural Machine Translation, Used-for, Attention Mechanism5. Neural Multi-source Sequence-to-Sequence Learning, Used-for, Attention Mechanism
### Triplets:1. probabilistic grammar, implements, StructuredRegex2. probabilistic grammar, assists, regex synthesis3. regex synthesis, involves, probabilistic grammar
### Content:Radial Basis Function Network (RBFN) is a type of artificial neural network that uses radial basis functions as activation functions. It typically has three layers - an input layer, a hidden layer with radial basis functions, and an output layer. RBFNs are often used for function approximation, interpolation, and classification tasks.### Concept: radial basis function network### Triplets:1. radial basis function network, uses, radial basis functions2. radial basis function network, has, three layers3. radial basis function network, used for, function approximation4. radial basis function network, used for, interpolation5. radial basis function network, used for, classification tasks
### Triplets:1. stemming, Used-for, natural language processing2. stemming, Is-a-Prerequisite-of, language processing3. stemming, Used-for, word similarity4. stemming, Is-a-Prerequisite-of, word similarity5. stemming, Is-a-Prerequisite-of, spectral clustering6. stemming, Compare, lemmatization7. stemming, Compare, tokenization
### Triplets:1. computer vision, Used-for, image captioning2. image captioning, Part-of, multimodal problem3. computer vision, Is-a-Prerequisite-of, image captioning4. Transformer architecture, Used-for, machine learning tasks5. multimodal problem, Evaluate-for, image captioning
```(finite state transducer, used-for, part-of-speech tagging)(finite state transducer, used-for, speech recognition)(finite state transducer, Compare, neural semantic parser)(finite state transducer, Compare, morphological complex languages)```
### Concept: dual decomposition1. (dual decomposition, Is-a-Prerequisite-of, compositional question)2. (dual decomposition, Is-a-Prerequisite-of, semantic parsing)3. (dual decomposition, Is-a-Prerequisite-of, multi-hop reading comprehension)4. (embedding models, Used-for, dual decomposition)5. (HSP method, Is-a-Prerequisite-of, dual decomposition)6. (dual decomposition, Used-for, semantic composition)7. (dual decomposition, Is-a-Prerequisite-of, hierarchical semantic parsing)8. (dual decomposition, Is-a-Prerequisite-of, multi-hop reasoning)9. (dual decomposition, Is-a-Prerequisite-of, model training)10. (HSP method, Evaluate-for, multi-hop reasoning)11. (HSP method, Is-a-Prerequisite-of, semantic parsing)12. (embedding methods, Compare, dual decomposition)
### Triplets:1. supertagging, Used-for, constituency parsing algorithm2. supertagging, Evaluate-for, state-of-the-art parsing performance3. supertagging, Is-a-Prerequisite-of, CCG parsing4. supertagging, Conjunction, parsing models5. supertagging, Evaluate-for, high performance6. supertagging, Hyponym-Of, span-based parsing algorithm
(None)
### Extracted Concepts:1. Synchronous context-free grammar (SCFG)2. Synchronous tree-adjoining grammar (STAG)3. Greibach normal form4. Tree-adjoining grammars (TAG)5. Linear indexed grammars (LIG)6. Combinatory categorial grammars7. Head grammars8. Controllable pushdown automata (PDAs)9. Pushdown Adjoining Automaton (PAA)### Triplets:1. (Synchronous context-free grammar, Is-a-Prerequisite-of, Synchronous tree-adjoining grammar)2. (Synchronous context-free grammar, Is-a-Prerequisite-of, Greibach normal form)3. (Synchronous context-free grammar, Is-a-Prerequisite-of, Tree-adjoining grammars)4. (Linear indexed grammars, Is-a-Prerequisite-of, Tree-adjoining grammars)5. (Combinatory categorial grammars, Is
### Triplets:1. recurrent neural networks, Used-for, inferring AMR graphs2. recurrent neural networks, Is-a-Prerequisite-of, establishing hierarchical relationships3. recurrent neural networks, Used-for, detecting KB relations4. recurrent neural networks, Is-a-Prerequisite-of, enhancing the learning of machine reading5. recurrent neural networks, Compare, convolutional neural network6. recurrent neural networks, Is-a-Prerequisite-of, improving part-of-speech tagging accuracy
1. (word embeddings, capture, linguistic regularities)2. (prior knowledge, integrate into, neural machine translation)3. (sense-level information, incorporate into, NLP systems)4. (deep learning methods, address, overfitting issues)5. (neural network training, utilize, regularization techniques)
### Triplets:1. (SGNS, Is-a-Prerequisite-of, Riemannian optimization)2. (GBS, Used-for, extending beam search)3. (GBS, Is-a-Prerequisite-of, including pre-specified lexical constraints)4. (GBS, Hyponym-Of, beam search)5. (neural machine translation models, Used-for, achieving high translation performance with beam search)6. (beam search, Conjunction, maintaining all found hypotheses in a single priority queue)7. (beam search, Compare, prioritizing hypotheses based on a universal score function)
### Triplets:1. wordnet, Is-a-Prerequisite-of, word embedding2. wordnet, Compare, word sense3. wordnet, Evaluate-for, word representation learning4. wordnet, Is-a-Prerequisite-of, word analogy5. language, Conjunction, wordnet6. sense, Is-a-Prerequisite-of, wordnet
### Triplets:1. Hybrid semi-Markov conditional random fields (SCRFs), Used-for, neural sequence labeling2. SCRFs, Is-a-Prerequisite-of, tasks of assigning labels to segments3. SCRFs, Evaluate-for, improved by employing word-level and segment-level information simultaneously4. SCRFs, Part-of, unified neural network5. SCRFs, Used-for, trained jointly with CRF output layer and SCRF output layer6. Conventional conditional random fields (CRFs), Is-a-Prerequisite-of, design of SCRFs7. Existing dialog state tracking (DST) models, Evaluate-for, trained with dialog data in random order8. Schema-aware Curriculum Learning for Dialog State Tracking (SaCLog), Is-a-Prerequisite-of, utilizing curriculum structure and schema structure9. Schema-aware Curriculum Learning for Dialog State Tracking (SaCLog), Used-for, improving DST performance over transformer
### Triplets:1. classification, is-a-Prerequisite-of, document classification2. classification, Used-for, sentiment classification3. classification, Compare, sentiment analysis4. classification, Compare, keyphrase boundary classification5. classification, Conjunction, aspect sentiment classification6. classification, Compare, aspect sentiment classification
### Triplets:1. constraint satisfaction, Is-a-Prerequisite-of, event relation extraction2. constraint satisfaction, Is-a-Prerequisite-of, lexically constrained translation3. constraint satisfaction, Used-for, integration within existing constrained beam-search decoding algorithms
### Triplets:1. syntaxnet, Used-for, code generation2. syntaxnet, Is-a-Prerequisite-of, semantic parsing3. syntaxnet, Part-of, Neural Machine Translation4. unsupervised parsing, Evaluate-for, syntax trees5. ChineseBERT, Is-a-Prerequisite-of, syntax understanding6. neural models, Compare, symbol structures7. syntax-aware language model, Used-for, unsupervised syntactic parsing
### Concept: k-nn1. (kNN-MT, combines, pre-trained NMT model)2. (Adaptive kNN-MT, dynamically determines, number of k for each target token)3. (kNN-MT, retrieves, nearest neighbors for each target token)4. (Meta-k Network, trained with, only a few training samples)
### Triplets:1. (part-of speech tagging, is-a-Prerequisite-of, neural network architecture)2. (part-of speech tagging, Evaluate-for, performance evaluation)3. (part-of speech tagging, Hyponym-Of, CCG parsing)
### Triplets:1. chatbot, Used-for, response selection2. chatbot, Evaluate-for, performance improvement3. chatbot, Compare, Seq2Seq model4. response selection, Is-a-Prerequisite-of, chatbot5. response selection, Compare, SMN6. response selection, Is-a-Prerequisite-of, SMN7. SMN, Is-a-Prerequisite-of, chatbot8. matching model, Evaluate-for, learning improvement9. sequence-to-sequence architecture, Is-a-Prerequisite-of, chatbot
### Triplets:1. ColBERT-PRF approach, Used-for, modifying query representation2. CWPRF, Is-a-Prerequisite-of, selecting useful expansion embeddings3. query representation, Part-of, search engine indexing4. CWPRF, Compare, existing PRF approach for ColBERT
### Triplets:1. effective argument construction, Used-for, human reasoning2. human reasoning, Evaluate-for, decision-making processes3. automatic argument generation model, Used-for, generating arguments4. human, rely-on, multiple sensory modalities5. text reasoning, Is-a-Prerequisite-of, logical reasoning6. cognitive science research, Evaluate-for, connections between human language and analogy-making
(<concept>, Is-a-Prerequisite-of, N3)  (<concept>, Hyponym-Of, computer vision)  
### Triplets:1. recurrent neural networks, suffer-from, gradient diffusion2. neural network-based joint models, prevent, error propagation3. LSTM unit, Used-for, reduce the gradient propagation path4. backpropagation, causes, error propagation
### Concepts:1. Distributional vector space models2. Morph-fitting procedure3. Text similarity measures4. N-grams and skip-grams overlap5. TextFlow6. Vector space representations of words### Triplets:1. (text similarity, Part-of, distributional vector space models)2. (morph-fitting procedure, Used-for, improving distributional vector spaces)3. (text similarity measures, Evaluate-for, plagiarism detection)4. (text similarity, Compare, N-grams and skip-grams overlap)5. (text similarity measures, Compare, TextFlow)6. (TextFlow, Conjunction, sequence matching)7. (vector space representations of words, Compare, synonyms)8. (vector space representations of words, Compare, antonyms)
#### Extracted Concepts:1. Neural machine translation2. Complex networks3. Large-scale assessments4. Multi-document summarization5. Recurrent networks6. Graph Convolutional Networks7. Computational construction of discourse networks8. Few-shot relation classification9. Discriminating antonyms and synonyms#### Triplets:1. (Capsule network, Compare, Recurrent networks)2. (Capsule network, Used-for, Few-shot relation classification)3. (Capsule network, Compare, Neural machine translation)4. (Capsule network, Compare, Multi-document summarization)5. (Capsule network, Compare, Graph Convolutional Networks)
### Triplets:1. Bayesian Theorem, Is-a-Prerequisite-of, Probabilistic Inference2. Bayesian Theorem, Used-for, Making Statistical Inferences3. Bayes Theorem, Compare, Frequentist Statistics
### Extracted Triplets:1. (BONIE, uses, bootstrapping)2. (BONIE, infers implicit relations from, context)3. (bootstrapping, is-a-Prerequisite-of, automatic seed selection for relation extraction)4. (bootstrapping, is-a-Prerequisite-of, noise reduction for relation extraction)5. (LSTM, prone to make mistakes like, neglecting an input slot value)6. (LSTM, prone to make mistakes like, generating a redundant slot value)7. (LSTM, prone to the phenomenon of, hallucination)8. (IRN, uses, bootstrapping algorithm)9. (IRN, incorporates, reinforcement learning)10. (IRN, applies, a bootstrapping algorithm to sample training candidates)11. (framework, utilises, neural network architecture)12. (clarification question framework, based on, self-supervision)13. (
### Concept: text summarization1. (text summarization, Is-a-Prerequisite-of, abstractive summarization)2. (text summarization, Is-a-Prerequisite-of, extractive summarization)3. (text summarization, Compare, abstractive summarization)4. (text summarization, Compare, extractive summarization)5. (text summarization, Used-for, generating concise summaries)6. (text summarization, Part-of, abstractive sentence summarization)7. (text summarization, Part-of, extractive multi-document summarization)
1. (zero pronoun resolution, Used-for, annotated data)2. (zero pronoun resolution, Is-a-Prerequisite-of, pseudo training data)3. (zero pronoun resolution, Compare, cloze-style reading comprehension neural network model)4. (neural machine translation, Is-a-Prerequisite-of, bi-directional LSTMs)5. (neural machine translation, Compare, convolutional layers)6. (neural machine translation, Compare, recurrent networks)7. (question answering, Is-a-Prerequisite-of, knowledge base)8. (question answering, Compare, end-to-end neural network model)9. (labeled sequence transduction, Is-a-Prerequisite-of, multi-space variational encoder-decoders)10. (labeled sequence transduction, Compare, generative model)11. (RNNs, Is-a-Prerequisite-of, Bayesian learning algorithm)12. (RNNs, Compare,
### Concept: normalization1. (normalization, is-a-Prerequisite-of, processing of historical texts)2. (normalization, Used-for, training encoder-decoder architectures)3. (normalization, Evaluate-for, improving semantic role labeling)4. (normalization, Part-of, Automated processing)5. (normalization, Compare, outperforming existing models)6. (normalization, Evaluate-for, parser adaptation)7. (normalization, Evaluate-for, multi-task learning)
(<evaluation of question answering>, Used-for, <assessing the efficiency of question answering systems>)(<natural language processing>, Is-a-Prerequisite-of, <evaluating the performance of question answering models>)(<evaluation of question answering>, Evaluate-for, <assessing the effectiveness of question answering approaches>)
### Extracted Concepts:1. Neural Machine Translation (NMT)2. Knowledge Bases (KBs)3. Cross-Attention Mechanism4. Knowledge Base Question Answering (KB-QA)5. Entity Linking6. Knowledge Base Completion7. Dependency Structure8. Recurrent Neural Networks (RNN)9. Neural Information Retrieval10. Semantic Knowledge Representation11. UCCA Parsing12. Deep Learning for Natural Language Processing13. Natural Language Understanding14. Knowledge Graph Embedding15. Factoid Question Answering16. Aspect Extraction17. Lifelong Learning18. Conditional Random Fields (CRF)19. Supervised Learning20. Multi-turn Dialogue Agents21. Semantic Parsing22. External Knowledge Integration23. Syntactic Knowledge24. Machine Learning Models25. Open-domain Question Answering26. Multi-Layer Recurrent Neural Networks### Triplets:1.
1. (Discourse parsing, part-of, Natural Language Processing)2. (Long-span dependencies, Is-a-Prerequisite-of, Discourse parsing)3. (Rhetorical Structure Theory, Hyponym-Of, Discourse parsing)4. (Pointer Networks, Used-for, Discourse parsing)5. (Natural Language Inference, Evaluate-for, Discourse parsing)6. (Open-domain neural semantic parser, Compare, Discourse parsing)
### Triplets:1. Variational autoencoders, Used-for, text generation 2. Variational autoencoders, Used-for, sentence representation learning 3. Variational autoencoders, Used-for, unsupervised paradoxical generation 4. Variational autoencoders, Is-a-Prerequisite-of, Bayesian Hierarchical Words Representation5. Variational autoencoders, Evaluate-for, generating long, and coherent text 6. Variational autoencoders, Evaluate-for, text classification 7. Variational autoencoders, Is-a-Prerequisite-of, Wasserstein autoencoders 8. Variational autoencoders, Is-a-Prerequisite-of, semi-supervised learning 9. Variational autoencoders, Part-of, Conditional Variational Autoencoders10. Variational autoencoders, Used for, modeling the morphological well-formedness (MWF) of derivatives
(<concept>, Is-a-Prerequisite-of, software development)(<concept>, Used-for, training neural machine translation models)(neural machine translation models, Evaluate-for, translation accuracy)(neural machine translation models, Is-a-Prerequisite-of, dialogue systems)(neural machine translation models, Compare, generative dialogue models)
### Triplets:1. Short texts --> Insufficiency --> Word co-occurrence information2. Generative model --> Aggregates --> Short texts3. Generative model --> Leverages --> Meta information4. Generative model --> Achieves --> Better performance5. Generative model --> Favoured by --> Fully local conjugacy6. Generative model --> Improves --> Document clustering7. Gibbs sampling --> Addresses --> Data non-alignment8. Gibbs sampling --> Addresses --> Long-range dependencies9. Gibbs sampling --> Is --> Effective10. NMT --> Addresses --> Issues11. Word-level training --> Leads to --> Overcorrection12. WMT’14 English->German translation tasks --> Achieve --> Improvements13. Supervised learning --> Is critical to --> Success of deep learning techniques14. Noisy versions --> Generate --> Original review15. Review --> Is --> Pretended as summary
### Triplets:1. random walk, Is-a-Prerequisite-of, reasoning models2. random walk, Evaluate-for, discovering grounded truth graphs3. random walk, Used-for, enhancing the performance of pretrained language models on commonsense tasks
1. (object detection, Used-for, event coreference)2. (object detection, Used-for, personal health mention detection)3. (object detection, Compare, event detection)4. (object detection, Is-a-Prerequisite-of, weakly-supervised spatio-temporally grounding natural sentence)5. (object detection, Is-a-Prerequisite-of, event detection)6. (event detection, Compare, personal health mention detection)
(<Monte Carlo Tree Search>, Evaluate-for, <Dynamic-Tree Driven Theorem Solver>)(<Dynamic-Tree Driven Theorem Solver>, Used-for, <proof state exploration>)(<Dynamic-Tree Driven Theorem Solver>, Evaluate-for, <signficant performance gains>)(<DT-Solver>, Used-for, <proof state exploration>)(<DT-Solver>, Evaluate-for, <performance improvement>)(<DT-Solver>, Used-for, <computing budgets allocation>)(<Dynamic-Tree Driven Theorem Solver>, Evaluate-for, <success rate improvement>)
1. (Variational autoencoder, Is-a-Prerequisite-of, Variational Bayes model)2. (Variational Bayes model, Evaluate-for, Bayesian Hierarchical Words Representation)3. (Variational Bayes model, Conjunction, Bayesian taxonomy)4. (Bayesian Hierarchical Words Representation, Hyponym-Of, Variational Bayes model)5. (Variational Bayes model, Evaluate-for, Posterior collapse)6. (Variational Bayes model, Evaluate-for, Coupled-VAE)
(None)
### Triplets:1. recurrent neural networks, shown promising performance, language modeling2. RNNs, trained using back-propagation through time, suffers from overfitting3. recent advances, Stochastic gradient Markov Chain Monte Carlo, learn weight uncertainty4. end-to-end learning, RNNs, attractive solution, dialog systems5. Hybrid Code Networks (HCNs), combine RNN with domain-specific knowledge6. dependency parsing, word embeddings, key role7. embeddings, of character strings, in addition to words8. external knowledge bases (KBs), used to improve RNNs for machine reading9. neural stacking, improve cross-lingual dependency parsing on low-resource languages10. Gated-Attention (GA) Reader, integrates a multi-hop architecture, novel attention mechanism11. GA Reader, state-of-the-art results on CNN & Daily Mail news stories and Who Did What dataset. ### Additional Triplets:1
### Triplets:1. sentence simplification, **Used-for**, lexical simplification2. sentence simplification, **Is-a-Prerequisite-of**, text simplification3. sequence-to-sequence neural networks, **Used-for**, sentence simplification4. neural text simplification systems, **Compare**, automated text simplification systems5. neural text simplification systems, **Evaluate-for**, grammaticality and meaning preservation6. semantic parsing, **Used-for**, sentence splitting7. Machine Translation, **Hyponym-Of**, neural Machine Translation8. sentence simplification, **Part-of**, simplification operator9. target-sensitive sentiment, **Compare**, context-sensitive sentiment10. sentiment classification, **Is-a-Prerequisite-of**, sentiment linguistic knowledge integration
### Triplets:1. Discourse model, Used-for, identification of narration, exposition, description, argument and emotion expressing sentences2. Discourse model, Used-for, automatic essay scoring improvement3. Discourse model, Evaluate-for, automatic essay scoring4. Discourse model, Is-a-Prerequisite-of, salient discussion point identification in spoken meetings5. Discourse model, Is-a-Prerequisite-of, discourse relation labeling between speaker turns6. Discourse model, Evaluate-for, model evaluation on predicting consistency among team members' understanding7. Discourse model, Compare, automatic discourse relation classification challenges due to lack of connectives8. Discourse model, Compare, effective feature imitation framework for implicit relation classification9. Discourse model, Compare, state-of-the-art performance on the PDTB benchmark for implicit relation classification10. Discourse model, Used-for, state-of-the-art performance achievement through adversarial model for connective discriminability
### Triplets:1. social media - Used-for - estimating a user’s socio-economic profile2. user cognitive structure - Is-a-Prerequisite-of - building a predictive model of income3. tweets distribution - Evaluate-for - building a predictive model of income4. future temporal orientation - Compare - income5. Twitter-specific conventions - Part-of - significant parsing challenge6. social media English - Conjunction - social media African-American English (AAE)7. English dependency parsing - Is-a-Prerequisite-of - handling social media English8. fake news detection - Evaluate-for - reasoning over the relations between sources, articles they publish, and engaging users on social media9. inference operators - Used-for - revealing unobserved interactions between graph elements10. graph framework - Is-a-Prerequisite-of - fake news detection
### Triplets:1. (learning, Used-for, multi-task learning)2. (learning, Evaluate-for, benefits of our approach)3. (learning, Is-a-Prerequisite-of, software development)4. (neural network models, Used-for, multi-task learning)5. (multi-task learning, Compare, adversarial multi-task learning framework)6. (learning, Is-a-Prerequisite-of, knowledge acquisition)7. (learning, Used-for, aspect-based sentiment analysis)8. (agent messages, Conjunction, natural language strings)9. (visual objects, Conjunction, distributional space)10. (approach, Evaluate-for, knowledge extraction)11. (model, Evaluate-for, coherence of aspects)12. (RNNs, Evaluate-for, dialog systems)13. (NMT, Evaluate-for, translation accuracy)
### Triplets:1. reinforcement learning, facilitates, task-oriented dialogue system2. reinforcement learning, is-utilized-in, conversational game3. reinforcement learning, optimizes, coreference evaluation metrics4. reinforcement learning, is-applied-in, automatic taxonomy induction5. reinforcement learning, is-incorporated-into, Deep Dyna-Q framework6. reinforcement learning, improves, sequence prediction algorithms7. reinforcement learning, enables, the training of an end-to-end model8. reinforcement learning, is-proposed-for, unsupervised parsing9. reinforcement learning, enhances, policy gradient fine-tuning10. reinforcement learning, is-integrated-with, planning for dialogue policy learning
### Extracted Concepts:1. Grounded verb semantics2. Human-robot communication and collaboration3. Reinforcement learning4. Interactive learning approach### Triplets:1. Grounded verb semantics, Is-a-Prerequisite-of, Human-robot communication and collaboration2. Grounded verb semantics, Evaluate-for, Robot3. Interactive learning approach, Is-a-Prerequisite-of, Reinforcement learning
(<concept>, Is-a-Prerequisite-of, Affect-LM)  (<concept>, Is-a-Prerequisite-of, Tree Long Short-Term Memory Networks)  (Affect-LM, Evaluate-for, Affective content customization)  (Tree Long Short-Term Memory Networks, Compare, Tree-LSTMs)  (Tree Long Short-Term Memory Networks, Is-a-Prerequisite-of, syntax-infused variational autoencoder)  (Tree Long Short-Term Memory Networks, Is-a-Prerequisite-of, LSTM Noisy Channel Model)   
### Triplets:1. Neural Network, Used-for, Modeling inference2. Event Coreference Resolution, Evaluate-for, Inference3. NLI Model, Evaluate-for, Inference4. Dependency Information, Used-for, Inference5. Neural Network, Evaluate-for, Natural Language Inference6. Semantic Dependency Graph, Used-for, Inference
### Triplets:1. summarization evaluation, Is-a-Prerequisite-of, ROUGE scores2. lexical features, Used-for, representing the context of mentions3. extractive summaries, Compare, abstractive summaries4. abstractive sentence summarization, Is-a-Prerequisite-of, abstractive document summarization5. Document Dating, Is-a-Prerequisite-of, document retrieval6. NeuralDater, Is-a-Prerequisite-of, document dating7. control variates, Evaluate-for, combining automatic metrics with human evaluation
### Extracted Concepts:1. Transition-based dependency parsing2. Dependency parsing3. Parser4. Global information5. Error propagation6. Sentences7. Lexical information8. Arc-swift9. Direct attachments10. Parser performance11. Beam size12. Dependency graphs13. Word representations14. Character composition model15. Constituency parse16. Structured projection17. Neural networks18. Global view on input19. Biaffine model20. Named Entity Recognition (NER)21. Multi-task learning22. Factuality assessment23. Modal dependency parsing24. StructFormer25. Machine translation26. Language understanding27. Question answering28. Text-to-speech synthesis29. Language technologies30. Global utility### Triplets:1. (Transition-based dependency parsing, Is-a-Prerequisite-of, Dependency parsing)2.
1. (multi-task learning, Used-for, text classification tasks)2. (multi-task learning, Evaluate-for, performance improvement)3. (multi-task learning, Is-a-Prerequisite-of, semantic parsing)4. (multi-task learning, Is-a-Prerequisite-of, improving semantic parsing performance)5. (semantic parsing, Part-of, UCCA parsing)6. (sentences, Is-a-Prerequisite-of, semantic dependency graph formalisms)7. (multi-task learning, Is-a-Prerequisite-of, discourse coherence assessment)
(`<concept>`, `Part-of`, `relation extraction`)  (`<concept>`, `Evaluate-for`, `user's occupational class`)  (`<social network extraction>`, `Used-for`, `predictive power`)
### Extracted Concepts:1. Knowledge graph embeddings2. Hierarchical data3. Logical patterns4. Hyperbolic KG embedding models5. Attention-based transformations6. Translational artifacts7. Multilingual model8. NMT model9. Deep transformers10. Text-to-SQL semantic parsing### Triplets:1. Knowledge graph embeddings, <Part-of>, Entities2. Hyperbolic KG embedding models, <Compare>, Euclidean-based efforts3. Hyperbolic KG embedding models, <Is-a-Prerequisite-of>, Complex relational patterns4. Hyperbolic KG embedding models, <Used-for>, Capturing hierarchical and logical patterns5. Attention-based transformations, <Compare>, Geometric transformations6. Attention-based transformations, <Is-a-Prerequisite-of>, Generalizing to multiple relations7. Translational artifacts, <Evaluate-for>, Higher BLEU scores8. Multilingual model, <
### Triplets:1. neural networks, Used-for, handle both discrete and continuous latent variables2. generative model, Evaluate-for, powerful supervised framework3. image-grounded conversation, Is-a-Prerequisite-of, perception of conversational agents
### Triplets:1. generative models, useful for parsing and language modeling2. generative models, married with a discriminative recognition model3. generative models, based on expectation maximization and variational inference4. generative models, provide interpretations for parsing and language modeling5. generative models, utilized in a framework for parsing and language modeling6. generative models, proposed for cross-domain generalization7. generative models, focus on methods for cross-lingual transfer to distant languages8. discriminative models, often outperform generative models9. discriminative models, used in a framework for parsing and language modeling10. discriminative models, transformed from generative framework using heuristics11. discriminative models, employed in data programming paradigm12. discriminative models, improved by skewness loss and distribution distance loss13. discriminative models, surpassed current state-of-the-art on three different datasets14
1. (Tensor data, is-a-Prerequisite-of, deep neural networks)2. (Abstract syntax trees, is-a-Prerequisite-of, abstract syntax networks)3. (Dependency structure, Evaluate-for, Sequence-to-Dependency Neural Machine Translation)4. (Structured representations, Used-for, kernel methods)5. (Tree Kernels, is-a-Prerequisite-of, expressive kernels)6. (Structured representations, Part-of, Kernel methods)
(`<Concept>`, `Part-of`, `natural language processing`)  (`<Concept>`, `Part-of`, `computer science`)  (`<Aspect extraction>`, `Is-a-Prerequisite-of`, `aspect-based sentiment analysis`)  (`<Word embeddings>`, `Used-for`, `semantic information`)  (`<Domain adaptation>`, `Used-for`, `sentiment analysis`)  (`<Document modeling>`, `Conjunction`, `people modeling`)  
1. (neural language modeling, used-for, automatic generation of rhythmic poetry)2. (neural language modeling, Compare, recurrent neural networks)3. (neural language modeling, Evaluate-for, machine-generated poems)4. (neural language modeling, Is-a-Prerequisite-of, language model perplexity)5. (neural language modeling, Part-of, LSTM (Long Short-Term Memory))6. (neural language modeling, Is-a-Prerequisite-of, Affect-LM)7. (neural language modeling, Part-of, sequence labeling)
#### Extracted Concepts:1. end-to-end automatic speech recognition (ASR)2. attention-based methods3. connectionist temporal classification (CTC)4. neural approach5. adversarial model6. sequence-to-sequence framework7. automatic speech recognition (ASR) systems8. weighted finite state transducers (FSTs)9. automatic hate speech detection models#### Triplets:1. (speech recognition, part-of, end-to-end automatic speech recognition (ASR))2. (attention-based methods, Compare, connectionist temporal classification (CTC))3. (neural approach, Evaluate-for, adversarial model)4. (sequence-to-sequence framework, Used-for, automatic speech recognition (ASR) systems)5. (weighted finite state transducers (FSTs), Used-for, language processing tasks)6. (automatic speech recognition (ASR) systems, Evaluate-for, automatic speech recognition (ASR))
``` ("crawling the web", Used-for, "creating the largest publicly available parallel corpora")("creating the largest publicly available parallel corpora", Compare, "benchmark data sets for sentence alignment and sentence pair filtering")("creating the largest publicly available parallel corpora", Evaluate-for, "evaluating the quality and usefulness to create machine translation systems")("creating the largest publicly available parallel corpora", Is-a-Prerequisite-of, "fine-tuning large pre-trained models with task-specific data")("fine-tuning large pre-trained models with task-specific data", Compare, "hidden space being dropped during training")("hidden space being dropped during training", Is-a-Prerequisite-of, "HiddenCut method outperforming other augmentation methods on the GLUE benchmark")("creating the largest publicly available parallel corpora", Compare, "scaling the model size in large pre-trained language models")("creating the largest publicly available parallel corpora", Is-a-Prerequisite-of, "scaling up
### Triplets:1. Tokenization, Used-for, End-to-end automatic speech recognition (ASR)2. Tokenization, Part-of, End-to-end automatic speech recognition (ASR)3. Tokenization, Used-for, Deep convolutional neural network (CNN) architecture4. Tokenization, Compare, Connectionist temporal classification (CTC)5. Tokenization, Compare, Attention-based methods6. Tokenization, Compare, Recurrent neural network (RNN)
#### Triplets:1. Neural machine translation utilizes Deep Neural Networks (<head concept>, Used-for, Neural Machine Translation).2. Linear associative units (LAU) are proposed to reduce gradient diffusion in RNNs (<head concept>, Evaluate-for, Gradient Reduction).3. Deep learning framework optimizes SGNS objective using Riemannian optimization (<head concept>, Used-for, Optimizing SGNS objective).4. Generative models married with discriminative recognition model for parsing and language modeling (<head concept>, Used-for, Parsing and Language Modeling).5. Knowledge base alignment involves latent relation induction (<head concept>, Evaluate-for, Latent Relation Induction).6. Counterfactual learning from human bandit feedback for neural semantic parsing (<head concept>, Evaluate-for, Neural Semantic Parsing).
### Triplets:1. Predicate argument structure analysis, Is-a-Prerequisite-of, Coreference resolution2. RDF triple, Part-of, Knowledge base3. Logical reasoning, Evaluate-for, Language models
#### Extracted Concepts:1. Sequence labels2. Multiple annotators3. Consensus annotations4. Crowd annotations5. Model training6. Long Short Term Memory (LSTM)7. Named-Entity Recognition (NER)8. Information Extraction9. Dialogue Act classification10. Recurrent Neural Network (RNN)11. Attentional technique12. Neural network architecture13. Generative neural network14. Encoder-decoder sequence-to-sequence models15. Attention-based neural machine translation (NMT)16. Source positions17. Sequence learning18. Alignment model19. Lexicon models20. WMT 2017 German↔English and Chinese→English translation tasks21. Transfer learning22. Target domain23. Weak supervision24. Labelling functions25. Entity-level F1 scores26. Sequence labelling model27. Multi-hop reasoning questions28. Multi-hop generation
1. (Tree-LSTMs, Used-for, sentiment analysis)2. (Tree-LSTMs, Compare, traditional tree-LSTMs)3. (tree-LSTMs, Is-a-Prerequisite-of, tree communication model)4. (tree communication model, Compare, bidirectional tree-LSTMs)5. (event extraction, Is-a-Prerequisite-of, document-level event extraction)
(`<concept>`, `<relation>`, `Bayesian Network`)(`<concept>`, `<relation>`, `Entity-Aware Convolutional Neural Networks (CNN)`)(`Generative Adversarial Network (GAN)`, `Used-for`, `MMD`)(`Bayesian Network`, `Used-for`, `interpretable diagnosis system`)(`Bayesian Network`, `Hyponym-Of`, `Adversarial networks for Generating compact-answer Representation`)
### Triplets:1. conditional probability, is computed by, channel models2. channel models, compute, conditional probability3. conditional probability, is estimated using, noisy channel approach4. candidate summaries, assigned probability mass according to their quality, conditional probability5. quality of candidate summaries, correlated with, conditional probability6. conditional probability, enhances, model training7. latent variable, guided with, conditional probability
### Concepts:- Sequence transduction- Labeled sequence transduction- Generative model- Neural networks- Kernel methods- Deep neural networks- Structured representations- Feature representations- End-to-end training- Transfer learning- Language model fine-tuning- Universal Language Model Fine-tuning (ULMFiT)### Triplets:1. (Feature learning, Is-a-Prerequisite-of, End-to-end training)2. (Feature learning, Is-a-Prerequisite-of, Transfer learning)3. (Transfer learning, Is-a-Prerequisite-of, Universal Language Model Fine-tuning (ULMFiT))4. (Universal Language Model Fine-tuning (ULMFiT), Compare, Language model fine-tuning)
1. (linguistics, part-of, natural language processing)2. (linguistics, Conjunction, cultural fit)3. (linguistics, Evaluate-for, stancetaking)
### Triplets:1. (seq2seq, Compare, Pointer Network)2. (seq2seq, Compare, RNN)3. (seq2seq, Compare, Transformer)4. (seq2seq, Evaluate-for, Conversation Scenarios)5. (seq2seq, Hyponym-Of, Neural Generative Models)6. (Neural Nachine Translation, Is-a-Prerequisite-of, Seq2Seq)7. (Seq2Seq, Used-for, Response Generation)
### Triplets:1. contextualization, is-neither-driven-by, polysemy2. contextualization, is-neither-driven-by, pure-context-variation3. transformers, has, advanced-nlp4. hierarchical-matrix, is-similar-to, matrix-structure5. hierarchical-attention, has, linear-run-time6. hierarchical-attention, has, memory-complexity7. DICE, frames, event-extraction8. DICE, introduces, contrastive-learning9. DICE, trains, mention-identification10. DICE, introduces, special-markers11. DICE, composes, MACCROBAT-EE12. adversarial-attack, improves, detecting-inconsistent-nles13. model, alleviates, inconsistencies14. LayerNorm, has, impact-on15. LayerNorm, has, low-generalizability
(`<concept>`, `Part-of`, `math problem solving system`)  (`<concept>`, `Part-of`, `Hybrid approach`)  (`geometry problem solving`, `Part-of`, `Geometry3K`)  (`geometry problem solving`, `Used-for`, `Inter-GPS`)  (`math word problem solving`, `Is-a-Prerequisite-of`, `NumS2T`)  
1. (neural machine translation, Part-of, NMT)2. (neural machine translation, Compare, statistical machine translation)3. (neural machine translation, Evaluate-for, translation performance)4. (neural machine translation, Used-for, encoding source sentence)5. (neural machine translation, Is-a-Prerequisite-of, state-of-the-art performance)6. (NMT, Hyponym-Of, neural machine translation)
### Triplets:1. lexical features, implicitly model, linguistic phenomena2. morphological properties, share, information3. distinctive features, relevant for, phonotactic learning4. phonological distinctive features, important for, segment-level phonotactic acquisition5. Chinese characters, phonological and morphological knowledge, similarities
### Extracted Concepts:1. Pre-trained language model2. Fine-tuning3. Masked language modeling4. Neural transfer method5. Named Entity Recognition (NER)6. Language understanding systems### Triplets:1. (Fine-tuning, Used-for, Pre-trained language model)2. (Fine-tuning, Evaluate-for, Language understanding systems)3. (Fine-tuning, Is-a-Prerequisite-of, Language understanding systems)4. (Masked language modeling, Is-a-Prerequisite-of, Fine-tuning)5. (Neural transfer method, Used-for, Named Entity Recognition)6. (Named Entity Recognition, Is-a-Prerequisite-of, Fine-tuning)
### Extracted Concepts:1. AMR (Abstract Meaning Representation)2. UCCA (Universal Conceptual Cognitive Annotation)3. GMB (Graph-of-meaning Based Graph)4. UDS (Universal Decompositional Semantics)5. Sequence-to-sequence models6. Syntax schemes7. Sequence-based AMR models8. Kernel methods9. Tree Kernels10. Deep neural networks11. Structured information12. Neural language model13. Topic model-like architecture14. Neural machine translation (NMT) models15. Word embeddings16. Vector space representations17. Signed spectral normalized graph cut algorithm18. Natural language interfaces to databases19. Neural sequence models20. SQL21. Reinforcement learning (RL)22. Maximum marginal likelihood (MML)23. Minimal Recursion Semantics (MRS)24. MRS parser25. Geolocation prediction model26. Convolution
### Triplets:1. commonsense question answering, Used-for, question answering2. question answering, Is-a-Prerequisite-of, commonsense question answering3. question answering, Conjunction, natural language processing4. neural network-based methods, Compare, traditional methods
### Triplets:1. Seq2Tree models, Used-for, code generation2. Seq2Tree models, Is-a-Prerequisite-of, semantic parsing3. Branch Selector, Used-for, code generation4. SpanBasedSP, Is-a-Prerequisite-of, code generation5. SMCalFlow, Evaluate-for, conversational semantic parsing6. Knowledge-aware fuzzy semantic parsing framework, Evaluate-for, conversational question answering7. Neural semantic parser, Is-a-Prerequisite-of, semantic parsing
### Extracted concepts:1. event extraction2. supervised learning3. large scale events4. world knowledge5. linguistic knowledge6. form and meaning-based linguistic knowledge7. linguistic-informed derivation8. linguistic knowledge for question-answer pairs9. linguistic knowledge for entity identification10. sentiment linguistic knowledge11. linguistic knowledge for relation extraction12. linguistic knowledge in neural networks13. linguistic knowledge for speech translation14. logical and linguistic knowledge15. psycho-linguistic knowledge16. hierachical heterogeneous graph17. pre-training objectives and linguistic properties### Triplets:1. (event extraction, Used-for, supervised learning)2. (supervised learning, Is-a-Prerequisite-of, event extraction)3. (world knowledge, Evaluate-for, event extraction)4. (linguistic knowledge, Is-a-Prerequisite-of, event extraction)5. (form and meaning-based linguistic knowledge,
### Extracted Concepts:1. Neural generation models2. Discrete control states3. Structured latent-variable approach### Triplets:1. Neural generation models, Used-for, Text generation2. Structured latent-variable approach, Compare, Neural generation models3. Discrete control states, Evaluate-for, Neural generation models
### Extracted Concepts:1. Multi-modal Neural Machine Translation model2. Pre-trained convolutional neural networks3. Image description4. Translation5. Multilingual learning6. Neural Named Entity Recognition (NNER)7. Multilingual coreference model8. Multilingual semantic dependency parsing9. Multilingual word representations10. Multilingual unsupervised NMT### Triplets:1. (Multilingual neural, Is-a-Prerequisite-of, Multi-modal Neural Machine Translation model)2. (Multilingual neural, Compare, Neural Named Entity Recognition (NNER))3. (Multilingual neural, Compare, Multilingual coreference model)4. (Multilingual neural, Compare, Multilingual semantic dependency parsing)5. (Multilingual neural, Compare, Multilingual word representations)6. (Multilingual neural, Compare, Multilingual unsupervised NMT)
### Triplets:1. learn event, Is-a-Prerequisite-of, semantic role labeling2. neural event detection, Compare, Nugget Proposal Networks3. events, Part-of, document topical structures4. learn event, Conjunction, global context layer5. neural event detection, Is-a-Prerequisite-of, fine-grained separation6. learn event, Evaluate-for, extracting events and their arguments
### Triplets:1. visual dialogue, Is-a-Prerequisite-of, dialogue system2. visual dialogue, Evaluate-for, generating correct and desirable responses3. visual dialogue, Evaluate-for, answer a series of questions about an image4. neural image captioning systems, Is-a-Prerequisite-of, visual dialogue5. CNN, Used-for, image feature extraction6. RNN, Used-for, language caption generation
1. (Word Embeddings, Evaluate-for, Semantic similarity)2. (Vector cosine, Used-for, Similarity estimation)3. (Intelligent systems, Require, Common sense)4. (Extraction methods, Is-a-Prerequisite-of, Commonsense knowledge)5. (Neural network models, Have advanced, Coreference resolution task)6. (Current neural coreference models, Trained with, Heuristic loss functions)7. (Higher-order mention ranking approach, Is-a-Prerequisite-of, Reinforced policy gradient model)8. (Coreference resolution model, Uses, Reinforcement learning)9. (Minimum spans, Manually annotated in smaller corpora, Coreference evaluation)10. (Cross-dataset coreference evaluation, Is improved by, Minimum spans)11. (The Winograd Schema Challenge dataset WSC273, Contains, Pronoun disambiguation problem)12. (Language models, Fine-tuned on, Pronoun disamb
#### Triplets:1. sequence labeling model, Used-for, automatic identification2. sequence labeling model, Used-for, Named Entity Recognition3. sequence labeling model, Is-a-Prerequisite-of, Named Entity Recognition4. Named Entity Recognition, Evaluate-for, Automatic Essay Scoring5. sequence labeling model, Compare, Traditional Sequence Labeling Methods6. sequence labeling model, Is-a-Prerequisite-of, Text Simplification7. Named Entity Recognition, Is-a-Prerequisite-of, Text Simplification
### Triplets:1. argument invention, Used-for, automatic argument generation2. automatic argument generation, Evaluate-for, higher BLEU, ROUGE, and METEOR scores3. argument invention, Is-a-Prerequisite-of, effective arguments4. argument invention, Compare, persuasive arguments5. argument invention, Evaluate-for, convincingness6. automatic argument generation, Is-a-Prerequisite-of, counter-argument generation7. argument invention, Used-for, refutation8. counter-argument generation, Part-of, CANDELA9. argument invention, Evaluate-for, persuasive arguments10. event extraction, Compare, event argument extraction
### Triplets:1. Affect-LM, Used-for, generating conversational text conditioned on affect categories2. Affect-LM, Used-for, customizing the degree of emotional content in generated sentences3. babbleLabble, Is-a-Prerequisite-of, training classifiers with natural language explanations4. babbleLabble, Evaluate-for, training classifiers with a semantic parser for noisy labels5. ExpBERT, Compare, matches a BERT baseline with 3–20x less labeled data6. NILE, Evaluate-for, providing testable explanations of natural language inference decisions
```(None)```
#### Triplets:1. LIME, Compare, Input Perturbation2. Input Perturbation, Compare, Attention3. Attention, Evaluate-for, Text classifiers4. Pixie Autoencoder, Uses-for, Functional Distributional Semantics5. Bayesian Network Ensembles, Used-for, Diagnosis system6. SemiORC, Is-a-Prerequisite-of, Operational Risk Classification
### Triplets:1. neural machine translation, used-for, generating translations2. neural machine translation, Is-a-Prerequisite-of, statistical machine translation3. artificial intelligence systems, Used-for, generating natural language explanations4. artificial intelligence systems, Evaluate-for, trust5. artificial intelligence systems, Evaluate-for, predictions
1. (topic model, Used-for, text classification)2. (topic model, Evaluate-for, document context)3. (topic model, Compare, LDA topic model)4. (topic model, Compare, word embedding models)5. (text classification, Is-a-Prerequisite-of, topic model)6. (document context, Evaluate-for, language model)7. (semantic space, Compare, joint semantic space)8. (joint extraction, Evaluate-for, topic model)9. (attention-based recurrent neural network, Evaluate-for, joint extraction)
1. (deep learning model, Used-for, semantic role labeling)2. (deep learning model, Is-a-Prerequisite-of, state-of-the-art improvement)3. (semantic role labeling, Is-a-Prerequisite-of, predicate-argument structure recognition)4. (semantic role labeling, Part-of, natural language processing)5. (natural language processing, Used-for, sentiment classification)6. (deep learning model, Is-a-Prerequisite-of, state-of-the-art improvement)7. (deep learning model, Compare, traditional machine learning models)
### Triplets:1. relation classification, Used-for, sentiment and sarcasm detection2. relation classification, Used-for, containment relation identification3. relation classification, Used-for, temporal relation classification4. relation classification, Compare, Dialogue Act classification5. relation classification, Compare, Discourse relation recognition6. relation classification, Is-a-Prerequisite-of, neural network architecture7. relation classification, Is-a-Prerequisite-of, feature learning
### Extracted Concepts:1. Opinionated Natural Language Generation (ONLG)2. Response selection3. Task-oriented dialogue systems4. Sequence-to-sequence (seq2seq) model5. Opinionated articles6. User simulator7. User experience8. Evaluation9. Neural network-based argument generation model### Triplets:1. Opinionated Natural Language Generation (ONLG), Compare, Response selection2. Response selection, Used-for, Task-oriented dialogue systems3. Task-oriented dialogue systems, Is-a-Prerequisite-of, Sequence-to-sequence (seq2seq) model4. Opinionated articles, Evaluate-for, User experience5. User simulator, Used-for, Task-oriented dialogue systems6. Evaluation, Compare, User experience7. Neural network-based argument generation model, Compare, User experience
### Triplets:1. document, contain, information2. summarization, improve, abstractive summarization3. summarize, entailment, input document4. document, essential for, many tasks5. use, external information for, document modeling6. NeuralDater, outperforms, baseline7. NeuralDater, Graph Convolutional Network, document dating
### Triplets:1. social media, Propagation Tree Kernel, rumor detection2. social media, Automatic political orientation prediction, user groups3. social media, multilingual connotation frames, public sentiments4. social media, user groups, ideology prediction5. social media, social interaction features, deception detection6. social media, antisocial behavior, prediction7. social media, image and text, multimodal communication
1. (distributed word representation, Used-for, NLP tasks)2. (distributed word representation, Benefits, word similarity and word analogy)3. (distributed word representation, Improves, word representation learning)4. (distributed word representation, Used-for, modeling words)
(`<query concept>`, `Part-of`, `Aspect-Based Sentiment Analysis`)(`Cross-domain sentiment analysis`, `Compare`, `Cross-Domain Data Augmentation`)(`Cross-domain sentiment analysis`, `Evaluate-for`, `Cross-Domain Sentiment Classification`)(`Open-domain targeted sentiment analysis`, `Is-a-Prerequisite-of`, `Targeted Aspect-Based Sentiment Analysis`)(`Aspect term extraction and aspect sentiment classification`, `Compare`, `Aspect-Based Sentiment Analysis`)(`Aspect term extraction and aspect sentiment classification`, `Is-a-Prerequisite-of`, `Dual crOss-sharEd RNN framework`)(`Attention-based neural models`, `Compare`, `Dual crOss-sharEd RNN framework`)(`Aspect terms extraction and opinion terms extraction`, `Conjunction`, `Pair-wise Aspect and Opinion Terms Extraction`)(`Aspect terms extraction and opinion terms extraction`, `Is-a-Prerequisite-of`, `Pair-wise Aspect and Opinion Terms Extraction`)(`Aspect terms extraction and opinion terms extraction
(`<concept>`, `Evaluate-for`, `improving semantic parsing performance`)  (`<improving semantic parsing performance>`, `Is-a-Prerequisite-of`, `challenge semantic parsing`)  
1. (representation learning, Used-for, improving distributional vector spaces)2. (representation learning, Compare, distributed word representations)3. (representation learning, Compare, Hierarchical Dirichlet Process)
(<Concept>, Used-for, Pseudofit)(<Concept>, Evaluate-for, semantic similarity)(Pseudofit, Improves, initial embeddings)(external knowledge, Is-a-Prerequisite-of, specializing word embeddings)
### Triples:1. annotated training, is-a-Prerequisite-of, building NLP models2. annotated training, Used-for, estimating the lexical feature weights3. annotated training, is-a-Prerequisite-of, argument components annotation4. annotated training, Evaluate-for, scoring argument persuasiveness5. annotated training, Evaluate-for, training hyperparameters estimation6. annotated training, Evaluate-for, machine comprehension performance improvement7. annotated training, Evaluate-for, natural language inference model improvement8. annotated training, is-a-Prerequisite-of, semantic parsing9. annotated training, Used-for, training neural network architectures10. annotated training, is-a-Prerequisite-of, multi-document summarization model development
### Triplets:1. unsupervised bilingual word embeddings, Are-used-for, cross-lingual model transfer2. unsupervised bilingual word embeddings, Used-for, unsupervised neural machine translation3. unsupervised bilingual word embeddings, Compare, unsupervised mapped BWE4. unsupervised bilingual word embeddings, Evaluate-for, unsupervised machine translation5. unsupervised bilingual word embeddings, Is-a-Prerequisite-of, unsupervised bilingual lexicon induction6. unsupervised bilingual word embeddings, Is-a-Prerequisite-of, bilingual lexicon induction7. unsupervised bilingual word embeddings, Compare, jointly trained BWE
```python(None, None, None)```  
### Triplets:1. dense retrieval, Compare, neural search systems2. neural information retrieval, Used-for, Entity-Duet Neural Ranking Model3. neural search systems, Is-a-Prerequisite-of, knowledge graphs
### Triplets:1. compositional distributional semantics model, Used-for, building an evaluation dataset2. compositional distributional semantics model, Evaluated-for, semantic relatedness and entailment3. semantic relatedness and entailment, Part-of, evaluation dataset4. compositional distributional semantics model, Used-for, prediction of (non)-compositionality5. semantic relatedness, Used-for, prediction of (non)-compositionality6. entailment, Used-for, prediction of (non)-compositionality7. compositional distributional semantics model, Compare, count-based distributional semantic models8. compositional distributional semantics model, Compare, Anchored Packed Trees9. count-based distributional semantic models, Used-for, prediction of (non)-compositionality10. Anchored Packed Trees, Used-for, prediction of (non)-compositionality
### Extracted Concepts:1. Aspect extraction2. Aspect-based sentiment analysis3. Topic models4. Neural word embeddings5. Opinionated Natural Language Generation (ONLG)6. Relational-Realizational grammar7. Word embeddings8. Sentiment analysis9. Bilingual Sentiment Embeddings (BLSE)10. ABSA (Aspect Based Sentiment Analysis)11. Gating mechanisms12. Cold-start problem in sentiment analysis13. Syntax-based neural language model### Triplets:1. (Aspect extraction, Part-of, Aspect-based sentiment analysis)2. (Topic models, Used-for, Aspect extraction)3. (Neural word embeddings, Used-for, Improving coherence)4. (Neural word embeddings, Used-for, Aspect extraction)5. (Opinionated Natural Language Generation (ONLG), Evaluate-for, Automatic response generation)6. (ONLG, Compare, Different grammatical representations)7. (
### Triplets:1. word embeddings, represent, word token2. word embeddings, improved by exploiting, distribution of word co-occurrences3. word embeddings, encourage, words that appear in similar contexts4. word embeddings, used for, spectral clustering5. word embeddings, learned from unlabeled text, standard component of neural network architectures6. word embeddings, trained, on relatively little labeled data7. word embeddings, added to, NLP systems
#### Triplets:1. sarcasm detection, used-for, textual data2. multimodal cues, improve, sarcasm detection3. multimodal messages, include, texts4. sarcasm detection, focus, multi-modal messages5. sarcasm detection, treated, various modalities6. multimodal sarcasm detection, based-on, tweets7. multimodal sarcasm detection, focus, texts and images8. texts, include, multi-modal messages
### Triplets:1. recurrent neural tensor, Compare, recurrent neural networks2. recurrent neural tensor, Is-a-Prerequisite-of, tensor3. recurrent neural networks, Compare, LSTM4. LSTM, Hyponym-Of, recurrent neural networks5. LSTM, Compare, Gated Recurrent Averaging Network
### Extracted Concepts:1. Abstract meaning representations (AMRs)2. Neural parsers3. AMR parsing4. Variational autoencoding framework5. Object-oriented Neural Programming (OONP)6. Neural abstractive summarization### Triplets:1. (neural parser, Is-a-Prerequisite-of, AMR parsing)2. (neural parser, Used-for, parsing documents in specific domains)3. (neural parser, Is-a-Prerequisite-of, Object-oriented Neural Programming (OONP))4. (neural parser, Used-for, semantically parsing documents)5. (neural parser, Evaluate-for, improving parsing results)6. (neural parser, Evaluate-for, generating abstractive summaries)
### Triplets:1. (manual fact-checking, is-a-Prerequisite-of, automated fact-checking)2. (automated fact-checking, Evaluate-for, factual correctness)3. (manually evaluated articles, are-a-Prerequisite-of, automating fact checking)4. (fact checking, is-a-Prerequisite-of, verifying the truthfulness of a claim)5. (claim veracity prediction, is-a-Prerequisite-of, generating justifications for verdicts on claims)6. (automated fact checking, Used-for, mitigating the spread of misinformation and disinformation)
### Triplets:1. generative neural models, Used-for, constituency parsing2. generative neural models, Compare, base parsers3. generative neural models, Compare, conversational systems4. generative neural models, Conjunction, context information5. generative neural models, Compare, various models6. generative neural models, Compare, Dialogue Act classification7. generative neural models, Compare, neural machine translation8. generative neural models, Compare, text-to-SQL systems9. generative neural models, Compare, story generation10. generative neural models, Compare, response generation11. generative neural models, Compare, emotional language generation12. generative neural models, Compare, document modeling13. generative neural models, Evaluate-for, adversarial attacks14. generative neural models, Compare, relation extraction15. generative neural models, Compare, image compression algorithms
### Triplets:1. abstractive summarization task, Used-for, generate a shorter version of the document2. query-based summarization, Evaluate-for, relevance in the context of a given query3. encode-attend-decode paradigm, Used-for, achieve notable success in machine translation4. document summarization, Is-a-Prerequisite-of, abstractive summarization5. neural abstractive models, Is-a-Prerequisite-of, progress in abstractive sentence summarization6. graph-based attention mechanism, Evaluate-for, improve neural abstractive models7. extractive multi-document summarization system, Evaluate-for, produce high-quality summaries8. Integer Linear Programming formulation, Is-a-Prerequisite-of, oracle summary in compressive summarization9. Sentence scoring, Evaluate-for, sentence selection in extractive document summarization systems10. NeuralDater, Is-a-Prerequisite-of, document dating approach11. hierarchical
### Triplets:1. convolutional layers <Used-for> encode the source sentence2. linear associative units (LAU) <Is-a-Prerequisite-of> reduce the gradient propagation path inside the recurrent unit3. bi-directional LSTMs <Compare> convolutional layers4. Neural Machine Translation (NMT) <Part-of> Deep Neural Networks (DNNs)5. sequential encoder-decoder framework <Compare> tree encoder6. Nested attention layers <Used-for> correcting errors in grammatical error correction systems
### Triplets:1. (dialogue state tracking, is-a-Prerequisite-of, DST)2. (Dialogue State Tracking, Evaluate-for, Dialogue systems)3. (Dialogue State Tracking, Part-of, Task-oriented dialogue systems)4. (Dialogue State Tracking, Evaluate-for, User intentions inference)5. (Dialogue State Tracking, Evaluate-for, Dialogues tracking)6. (Dialogue State Tracking, Is-a-Prerequisite-of, Slot correlations modeling)7. (Dialogue State Tracking, Compare, Dialogue State Inference)8. (Dialogue State Tracking, Evaluate-for, User utterance estimation)9. (Semantic frame, Part-of, Structured information)10. (Semantic frame, Evaluate-for, Downstream components)11. (Dialogue State Tracker, Part-of, Task-oriented dialogue system)12. (Dialogue State Tracker, Evaluate-for, User request processing)13. (Task-oriented dialog systems, Conjunction, Dialog State Tracker)14. (Task-oriented dialog
### Triplets:1. global constraints, used-for, cognates detection2. cognitive, hyponym-of, cognates detection3. mention-detection, is-a-prerequisite-of, cognates detection
### Extracted Concepts:1. Selective encoding model2. Abstractive sentence summarization3. Sequence-to-sequence framework4. Recurrent neural networks5. Attention mechanism6. Chinese poem generation7. Memory augmented neural model8. Grid Beam Search (GBS)9. Lexically Constrained Decoding10. Neural Interactive-Predictive Translation11. Domain Adaptation for Neural Machine Translation12. ArgRewrite corpus13. Revision analysis14. MT evaluation metrics15. Parse trees16. Constituency parsing17. Question classification18. Group sparse autoencoders19. Group sparse CNNs20. Neural machine translation decoder21. ROC story cloze task22. Hierarchical recurrent networks23. Aspect sentiment classification24. RNN with attention25. CNN layer26. Aspect sentiment classification with CNN27. Target-oriented sentiment classification28. Target-sensitive
### Triplets:1. visual semantic pretraining, Used-for, improving word embeddings2. visual semantic pretraining, Compare, GPT-2 architecture3. GPT-2 architecture, Is-a-Prerequisite-of, CLIP architecture4. visual semantic pretraining, Compare, GPT-2 word embeddings5. CLIP architecture, Compare, GPT-2 architecture6. visual semantic pretraining, Evaluate-for, enhancing fine-grained semantic representations7. CLIP, Used-for, encoding image captions8. CLIP, Part-of, a zero-shot multimodal image classifier9. CLIP word embeddings, Compare, GPT-2 word embeddings
### Extracted Concepts:- Aspect-based sentiment analysis- Topic models- Neural word embeddings- Attention mechanism- Network embedding (NE)- Context-Aware Network Embedding (CANE)- Cross-lingual word embeddings- Bilingual Sentiment Embeddings (BLSE)- Word embeddings- Reliability-aware name tagging model- Multilingual named entity recognition- FastText- BPEmb- BERT- ELMo- Argument search- Argument clustering- Digital helpdesk service- Multilingual NLP models- Transfer learning- Gender biases in word embeddings- Machine translation evaluation- Iterative Normalization- JW300 parallel corpus- Unsupervised bilingual word embeddings- Neural machine translation (NMT)### Triplets:1. (contextual subword embeddings, Used-for, multilingual named entity recognition)2. (contextual subword embeddings, Compare, FastText)3.
(<lexical expectation>, Is-a-Prerequisite-of, lexical relations)(<lexical expectation>, Evaluate-for, models)(<lexical expectation>, Conjunction, pragmatic phenomena)
(`<concept>`, `Hyponym-Of`, `morphological disambiguation`)  (`<concept>`, `Hyponym-Of`, `morphological inflection`)  (`<concept>`, `Part-of`, `morphological tagging`)  
### Triplets:1. vision language pre-training, Used-for, cross-modal downstream tasks2. weakly supervised vision-and-language pre-training, Compare, Weakly supervised vision-and-language pre-training (WVLP)3. Weakly supervised vision-and-language pre-training (WVLP), Compare, RELIT4. vision language pre-training, Is-a-Prerequisite-of, cross-modal understanding tasks5. Multilingual Vision-Language Pre-training (VLP), Compare, Multilingual Vision-Language Pre-training (VLP)
1. (sparse retrieval, a) Compare, dual-encoder model)2. (sparse retrieval, a) Compare, dense passage retrieval)3. (sparse retrieval, a) Compare, phrase retrieval models)4. (dense passage retrieval, a) Compare, sparse retrieval)5. (phrase retrieval models, a) Compare, sparse retrieval)
### Triplets:1. end-to-end task-oriented dialog systems, Is-a-Prerequisite-of, dialogue state tracking2. end-to-end task-oriented dialog systems, Is-a-Prerequisite-of, response selection3. task-oriented dialogue system, Evaluate-for, collaborative task4. task-oriented dialogue system, Evaluate-for, generating interpretable response5. task-oriented dialogue system, Is-a-Prerequisite-of, global-local self-attentive dialogue state tracker6. task-oriented dialogue system, Is-a-Prerequisite-of, PhotoBook dataset
### Extracted concepts:1. Neural semantic parser2. Natural language utterances3. Semantic parsing4. Transition system5. Domain-general natural language representations6. Predicate-argument structures7. Annotated logical forms8. Reinforcement learning (RL)9. Maximum marginal likelihood (MML)10. State-of-the-art results11. Multilingual context12. Sequence-to-tree model13. Multi-task learning framework14. Universal Dependencies (UD) parsing15. Vector space representations of words16. Embedding learning by concept induction17. Abstract representation18. Counterfactual learning19. Bandit feedback20. Abstract Meaning Representations (AMR)21. BIO-tagging22. End-to-end approach23. PropBank SRL24. Multilingual semantic dependency parsing25. Cross-lingual distributed logical representations26. Abstractive text summarization models27. Sequence
### Triplets:1. dialogue evaluation, is-a-Prerequisite-of, automatic dialogue evaluation2. automatic dialogue evaluation, Used-for, rapid prototyping3. automatic dialogue evaluation, Used-for, testing new models4. automatic dialogue evaluation, Compare, word-overlap metrics5. automatic dialogue evaluation, Compare, BLEU6. automatic dialogue evaluation, Evaluate-for, human judgements7. automatic dialogue evaluation, Is-a-Prerequisite-of, dialogue research8. automatic dialogue evaluation, Is-a-Prerequisite-of, accurate automatic evaluation procedure
#### Extracted Concepts:1. Neural language model2. Poetry generation3. Constraint satisfaction problem4. Generative neural language model5. Finite state machine6. Machine-generated poems7. Natural language descriptions8. Semantic parsing9. Text similarity measures10. TextFlow11. Multi-document summarization12. Abstractive summarization13. Video captioning14. Abstractive document summarization15. Automatic question generation16. AMR-to-text generation17. Sentential paraphrase pairs18. Document summarization19. Multi-task learning20. Dialog models21. Response generation22. Sequence-to-sequence models23. Neural knowledge diffusion24. AMR graph25. LSTM26. Discriminators#### Triplets:1. (Neural language model, Used-for, Poetry generation)2. (Generative neural language model, Is-a-Prerequisite
### Triplets:1. discourse modes, play-an-important-role-in, writing composition2. discourse modes, can-be-identified-automatically-with, average F1-score of 0.73. discourse modes, improve, automatic essay scoring4. discourse modes, describe, neural sequence labeling model5. discourse modes, used-for, features to improve automatic essay scoring6. discourse modes, discussed-in-context-of, impacts for automatic essay scoring7. discourse modes, underlying, a wide range of interactional phenomena
### Triplets:1. models, struggle with, detecting online hate2. state-of-the-art models, evaluated by, measuring performance on held-out test data using metrics such as accuracy and F1 score3. HateCheck, revealing, critical model weaknesses4. getting, affective features from other affective resources, significantly affect performance of hate speech detection5. sentiment knowledge sharing, based on, hate speech detection framework6. sentiment features, fused, from different feature extraction units to detect hate speech
(<Aspect Category Opinion Sentiment>, Is-a-Prerequisite-of, <Aspect-Category-Opinion-Sentiment (ACOS) Quadruple Extraction>)  (<Aspect Category Opinion Sentiment>, Evaluate-for, <Aspect-based Sentiment Analysis>)  (<Aspect Category Opinion Sentiment>, Is-a-Prerequisite-of, <Implicit Aspects>)  (<Aspect Category Opinion Sentiment>, Is-a-Prerequisite-of, <Implicit Opinions>)  
### Extracted Concepts:1. Machine reading comprehension2. Attention-based sequence learning model3. Reading comprehension4. Natural language processing5. Question answering6. Stanford Question Answering Dataset (SQuAD)7. TriviaQA dataset8. Answer candidates9. Document10. Entity attributes### Triplets:1. (Machine reading comprehension, Is-a-Prerequisite-of, Attention-based sequence learning model)2. (Reading comprehension, Is-a-Prerequisite-of, Question answering)3. (Reading comprehension, Is-a-Prerequisite-of, Machine reading comprehension)4. (Reading comprehension, Is-a-Prerequisite-of, Document)5. (Reading comprehension, Is-a-Prerequisite-of, Entity attributes)6. (Question answering, Is-a-Prerequisite-of, Answer candidates)7. (Document, Is-a-Prerequisite-of, Answer candidates)
### Concept: approach topic aware news1. (neural news recommendation approach, Used-for, alleviating information overload)2. (neural news recommendation approach, Used-for, finding interested news)3. (neural news recommendation approach, Evaluate-for, learning accurate news)4. (neural news recommendation approach, Evaluate-for, user representations for news recommendation)5. (neural news recommendation approach, Used-for, topic-aware news representations)6. (approach, Is-a-Prerequisite-of, topic-aware news representations)7. (approach, Is-a-Prerequisite-of, topic-aware news encoder)8. (approach, Is-a-Prerequisite-of, user encoder)9. (news encoder, Is-a-Prerequisite-of, topic-aware news representations)10. (news encoder, Used-for, learning representations of news)11. (news encoder, Used-for, applying attention networks to select important words)12. (user encoder, Used
### Extracted Triplets:1. (Modeling inference, Is-a-Prerequisite-of, Reasoning and inference)2. (Sequential inference models, Is-a-Prerequisite-of, Neural network based inference models)3. (Chain LSTMs, Compare, Previous top models)4. (Recursive architectures, Is-a-Prerequisite-of, Local inference modeling and inference composition)5. (Syntactic parsing information, Used-for, Improving performance)6. (Sequence-to-Action, Is-a-Prerequisite-of, Neural semantic parsing approach)7. (Semantic parsing, Is-a-Prerequisite-of, Sequence-to-Action)8. (Neural network models, Evaluate-for, Prediction ability)9. (Semantic graph, Used-for, Represent the meaning of a sentence)10. (RNN model, Used-for, Mapping sentences to action sequences)11. (End-to-end semantic graph generation process, Compare, Semantic parsing)12. (Prag
### Triplets:1. Argument extraction, Used-for, Event extraction2. Argument extraction, Is-a-Prerequisite-of, Relation extraction3. Argument extraction, Compare, Aspect extraction4. Argument extraction, Evaluate-for, Sentiment Analysis5. Argument extraction, Part-of, Document modeling
### Triplets:1. transition-based parser, Used-for, parsing sentences2. transition-based parser, Compare, state-of-the-art transition-based parser3. transition-based parser, Is-a-Prerequisite-of, generating an RST tree4. transition-based parser, Is-a-Prerequisite-of, parsing rhetorical structures5. transition-based parser, Evaluate-for, dependency accuracy and grammaticality improvements6. transition-based parser, Evaluate-for, dependency parsing task7. transition-based parser, Is-a-Prerequisite-of, semantic graph parser8. transition-based parser, Is-a-Prerequisite-of, data-driven semantic parsing9. transition-based parser, Evaluate-for, semantic graphs10. transition-based parser, Is-a-Prerequisite-of, end-to-end modeling techniques.
(<attention weight>, Is-a-Prerequisite-of, <identifiability>)(<attention weight>, Used-for, <explanation of model's predictions>)(<attention weight>, Evaluate-for, <interpretability of a model>)(<attention weight>, Is-a-Prerequisite-of, <explanation of model's predictions>)
1. (syntactic generalization performance, Is-a-Prerequisite-of, syntactic knowledge)2. (syntactic generalization performance, Compare, perplexity)3. (syntactic generalization performance, Compare, compositional generalization)4. (syntactic generalization performance, Evaluate-for, generalization performance)5. (syntactic generalization performance, Used-for, evaluating neural language models)6. (syntactic knowledge, Is-a-Prerequisite-of, creating and interpreting novel sentences)
### Triplets:1. discourse treebank, Used-for, NLP tasks2. discourse treebank, Annotation-for, scientific articles3. discourse treebank, Evaluation-for, parsing performance4. discourse treebank, Used-for, discourse dependency parsers
#### Triplets:1. level distant relation extraction, Is-a-Prerequisite-of, relation extraction2. distant supervision, Used-for, building training data3. distant supervision, Introduces, noise4. noise, Affects, model performance5. distant supervision, Used-for, labeling data6. labeling data, Evaluate-for, relation extraction7. relation extraction, Has, noise labeling problem8. noise labeling problem, Part-of, labeling data9. adversarial learning framework, Evaluate-for, relation extraction10. HITS algorithm, Evaluate-for, seed selection11. Attention Guided Graph Convolutional Networks, Conjunction, relation extraction12. end-to-end relation extraction model, Is-a-Prerequisite-of, KB enrichment13. entity-relation bipartite graph, Used-for, joint type inference14. DIAG-NRE, Compare, pattern-based labeling methods
1. (single document summarization, Is-a-Prerequisite-of, document modeling)2. (document modeling, Used-for, summarization)3. (single document summarization, Compare, multi-document summarization)4. (single document summarization, Evaluate-for, informativeness)5. (summarization, Is-a-Prerequisite-of, document encoding)6. (single document summarization, Is-a-Prerequisite-of, natural language understanding)
(`<concept>`, <Evaluate-for>, `structured knowledge`)  (`structured knowledge`, <Is-a-Prerequisite-of>, `question answering`)  (`structured knowledge`, <Part-of>, `Neural Symbolic Machine`)  (`structured knowledge`, <Used-for>, `reasoning`)  (`structured knowledge`, <Used-for>, `information extraction`)  
### Triplets:1. Entity Linking, Relies-on, Traditional Entity Linking technologies2. Entity Linking, Essential-for, Aligning textual mentions of named entities to their corresponding entries in a knowledge base3. Entity Linking, Utilizes, Rich hierarchies over types and entities contained in curated ontologies4. Entity Linking, Involves, Disambiguating entity mentions in a text against knowledge bases5. Entity Linking, Requires, Handling noisy texts, low resource settings, and domain-specific knowledge bases6. Entity Linking, Achieves, Best reported scores on the standard benchmark (AIDA-CoNLL)
(`<concept>`, `<relation>`, `<concept>`)(`<concept>`, `Is-a-Prerequisite-of`, `knowledge graph completion`)(`model`, `Used-for`, `knowledge graph embeddings`)(`entity`, `Is-a-Prerequisite-of`, `knowledge graph`)(`message passing`, `Is-a-Prerequisite-of`, `knowledge graph completion`)(`KG completion`, `Used-for`, `entity alignment`)(`relation extraction`, `Evaluate-for`, `knowledge graph completion`)
```None```
### Triplets:1. pre-trained, Used-for, NMT2. multilingual, Is-a-Prerequisite-of, NMT3. pre-trained, Evaluate-for, multilingual word embeddings4. pre-trained, Is-a-Prerequisite-of, multi-task modeling5. multilingual, Is-a-Prerequisite-of, Neural Named Entity Recognition (NNER)6. multilingual, Compare, bilingual7. multilingual, Used-for, translation scenarios
```(Stanford Question Answering Dataset, Is-a-Prerequisite-of, Reading comprehension)(Stanford Question Answering Dataset, Evaluate-for, State-of-the-art performance)(Stanford Question Answering Dataset, Used-for, Training set)(Stanford Question Answering Dataset, Evaluate-for, Ablation study)(Stanford Question Answering Dataset, Evaluate-for, Performance)(Stanford Question Answering Dataset, Evaluate-for, Effectiveness)(Stanford Question Answering Dataset, Evaluate-for, Individual modules)(Stanford Question Answering Dataset, Is-a-Prerequisite-of, SQuAD)(Stanford Question Answering Dataset, Compare, Spades fill-in-the-blank question answering dataset)(Stanford Question Answering Dataset, Is-a-Prerequisite-of, Machine comprehension)(Stanford Question Answering Dataset, Is-a-Prerequisite-of, Factoid question answering)(Stanford Question Answering Dataset, Is-a-Prerequisite-of
(`<head concept>`, Compare, `tail concept`).  (`named entity recognition multiple`, Evaluate-for, `neural network`).  (`multilingual learning for Neural Named Entity Recognition (NNER)`, Improve, `NER performance`).  
### Triplets:1. Neural Symbolic Machine, Contains, Neural Programmer2. Neural Symbolic Machine, Contains, Symbolic Computer3. Neural Symbolic Machine, Apply, REINFORCE4. Neural Symbolic Machine, Outperforms, State-of-the-art5. Neural Semantic Parser, Induced with, Transition System6. Neural Semantic Parser, Trained using, Annotated Logical Forms7. Neural Semantic Parser, Achieves, State of the art8. Neural Language Model, Incorporates, Document Context9. Neural Language Model, Outperforms, LDA Topic Model10. Neural Architecture, Captures, Target Syntax11. LSTM Language Model, Generates, Conversational Text12. LSTM Language Model, Conditioned on, Affect Categories13. LSTM Language Model, Learned, Affect-Discriminative Word Representations
### Extracted Concepts:1. Chinese named entity2. Named entity recognition (NER)3. Gazetteers4. Neural network5. Transformer-based models6. Multilingual learning7. Sequence labeling8. Markable identification9. Hierarchical encoder-decoder10. Neural transfer method11. Conditional Softmax Shared Decoder architecture12. Nested named entity recognition### Triplets:1. Named entity recognition (NER), is-a-prerequisite-of, Chinese named entity2. Gazetteers, used-for, named entity recognition (NER)3. Neural network, used-for, named entity recognition (NER)4. Transformer-based models, used-for, Chinese named entity5. Multilingual learning, is-a-prerequisite-of, named entity recognition (NER)6. Sequence labeling, is-a-prerequisite-of, named entity recognition (NER)7. Markable identification, evaluate-for, neural network8. Hierarchical encoder-decoder,
### Triplets:1. (political debate, involves, actors)2. (claim, made-in, political debate)3. (political debate, forms, structure)4. (political debate, opportunity-for, citizens)5. (political debate, requires, understanding)6. (political debate, contains, controversial topics)7. (actors, make, claims)8. (citizens, compare, candidates)9. (debates, is-application-scenario-for, Argument Mining)10. (research, lacking, empirical investigation)11. (debates, offers, opportunity)12. (data, complex, in Argument Mining)13. (US presidential campaigns, happen, political debates)14. (political debate, involves, fact-checking)15. (political debate, essential-for, democratic decision-making)16. (political debate, is-an-instance-of, discourse networks)17. (new corpus, released, USElecDeb60To16)18
(None)
### Extracted Concepts:1. Media profiling2. Fake news3. Fake content4. Biased content5. Propagandistic content6. News outlets7. Reliability of news sources8. Linguistic aspect of reporting9. Social context of reporting10. Target medium11. Social media audience analysis12. Wikipedia content13. Evaluation results### Triplets:1. (political bias factuality reporting, Part-of, Media profiling)2. (Fake news, Is-a-Prerequisite-of, Media profiling)3. (Biased content, Part-of, Media profiling)4. (Propagandistic content, Part-of, Media profiling)5. (political bias factuality reporting, Used-for, Detecting fake news)6. (Media profiling, Is-a-Prerequisite-of, Detecting fake news)7. (Reliability of news sources, Is-a-Prerequisite-of, Detecting
1. (KB-InfoBot, Used-for, multi-turn dialogue agent)2. (KB-InfoBot, Compare, traditional dialogue systems)3. (Deep Dyna-Q, Evaluate-for, effectiveness)4. (ReCoSa, Used-for, selecting relevant contexts)5. (Multimodal dialogue systems, Part-of, traditional goal-oriented dialogue systems)6. (Multimodal dialogue systems, Is-a-Prerequisite-of, capturing information from both text and image)7. (Multimodal dialogue systems, Conjunction, text and image information)8. (BabelPic, Evaluate-for, non-concrete concepts classification)9. (Scaffolding Structure in Loss (SSiL), Evaluate-for, human evaluation)10. (MERET, Compare, traditional training approaches)
### Triplets:1. factuality prediction, task-of, natural language processing2. factuality prediction, combined-with, syntactic information3. factuality prediction, conducted-by, researchers4. factuality prediction, utilized-for, assessing event occurrence5. factuality prediction, compared-to, named entity recognition6. factuality prediction, technique-for, predicting political bias7. factuality prediction, focuses-on, contextual encoding strategies
### Triplets:1. (KB-QA, Used-for, question answering)2. (NN-based KB-QA, Compare, traditional KB-QA)3. (KB-InfoBot, Is-a-Prerequisite-of, multi-turn dialogue agent)4. (Recurrent neural networks, Evaluate-for, machine reading)5. (KBLSTM, Evaluate-for, recurrent neural networks)6. (Conditional Random Fields, Evaluate-for, aspect extraction)7. (Entity Linking, Used-for, linking mentions)8. (Knowledge Graph, Is-a-Prerequisite-of, KG embedding)9. (KB embeddings, Compare, word embeddings)10. (KB embeddings, Compare, order embeddings)11. (Knowledge graphs, Part-of, large scale knowledge bases)12. (LinkNBed, Is-a-Prerequisite-of, deep relational learning framework)13. (LinkNBed, Evaluate-for, entity linkage)14. (TextWorldsQA, Is
### Triplets:1. bi-directional LSTMs, **Is-a-Prerequisite-of**, neural machine translation2. syntactic information, **Used-for**, improve translation accuracy3. neural machine translation, **Is-a-Prerequisite-of**, state-of-the-art performance4. linear associative units, **Compare**, LSTM unit and GRU5. nested attention layers, **Is-a-Prerequisite-of**, GEC systems6. recurrent neural network grammar, **Evaluate-for**, NMT+RNNG model7. NMT and SMT systems, **Conjunction**, final translation
### Extracted Concepts:1. Parser2. Recurrent Neural Networks3. Grammar-based Parser4. Transition-based Parser5. Dependency Trees### Triplets:1. (Parser, Is-a-Prerequisite-of, Recurrent Neural Networks)2. (Parser, Is-a-Prerequisite-of, Grammar-based Parser)3. (Parser, Is-a-Prerequisite-of, Transition-based Parser)4. (Parser, Is-a-Prerequisite-of, Dependency Trees)5. (Grammar-based Parser, Compare, Transition-based Parser)6. (Recurrent Neural Networks, Compare, Transition-based Parser)
### Triplets:1. argument mining, Used-for, identifying argument components2. argument mining, Is-a-Prerequisite-of, predicting the relationships among argument components3. argument mining, Is-a-Prerequisite-of, argumentative fine-tuning4. argument mining, Part-of, effective argumentation5. argument mining, Used-for, assessing the effect of debiasing on language models6. argument mining, Evaluate-for, removing bias in language models
### Triplets:1. nlg model, Used-for, natural language processing2. nlg model, Compare, neural language model3. neural language model, Used-for, learning poetic devices4. nlg model, Compare, sequence-to-sequence model5. nlg model, Compare, generative model6. nlg model, Compare, conditional variational autoencoders7. nlg model, Compare, sequence-to-sequence neural models
### Triplets:1. task learning, Used-for, multi-task learning2. multi-task learning, Compare, dependency parsing3. multi-task learning, Compare, sequence tagging4. task learning, Evaluate-for, adversarial multi-task learning framework5. multi-task learning, Is-a-Prerequisite-of, learning off-the-shelf knowledge6. task learning, Is-a-Prerequisite-of, computational argumentation mining7. task learning, Is-a-Prerequisite-of, labeled sequence transduction
### Triplets:1. neural semantic parser, Used-for, converting natural language utterances to intermediate representations2. natural language utterances, Is-a-Prerequisite-of, intermediate domain-general natural language representations3. COREQA, Is-a-Prerequisite-of, end-to-end question answering system4. COREQA, Compare, generating natural answers5. natural language interfaces, Used-for, performing complex actions6. COREQA, Is-a-Prerequisite-of, generating coherent and natural answers7. semantic parsing, Is-a-Prerequisite-of, mapping natural language utterances into executable programs8. semantic parsing, Compare, learning decentralized deep multiagent policies9. neural semantic parser, Evaluate-for, inducing predicate-argument structures10. COREQA, Evaluate-for, generating correct answers11. neural semantic parser, Is-a-Prerequisite-of, state-of-the-art on SPADES and GRAPHQUESTIONS12. TAC-KBP2015 and
### Extracted Concepts:1. Response selection for multi-turn conversation2. Sequential matching network (SMN)3. Dialogue perspective on relative information contributions4. Linguistic prior knowledge5. Neural Belief Tracking (NBT) framework6. Generative conversational systems7. Dependency parser using convolutional neural network8. Coherence model for written asynchronous conversations9. Controlled response generation10. Context-aware neural machine translation model11. Exemplar Encoder-Decoder network (EED)12. Sentence function in conversation generation### Triplets:1. (conversation model, Used-for, Response selection for multi-turn conversation)2. (conversation model, Used-for, Sequential matching network (SMN))3. (conversation model, Compare, Dialogue perspective on relative information contributions)4. (conversation model, Used-for, Linguistic prior knowledge)5. (conversation model, Used-for, Neural Belief Tracking (NBT) framework)6
### Triplets:1. NLU task, Used-for, natural language processing2. Neural Machine Translation (NMT), Compare, NLU task3. Sequence-to-Dependency Neural Machine Translation (SD-NMT), Is-a-Prerequisite-of, Neural Machine Translation (NMT)4. Maximum marginal likelihood (MML), Evaluate-for, neural semantic parser
(`<concept>`, `representation learning`, `latent relation learning`)  (`<latent relation learning>`, `part-of`, `knowledge representation learning`)  (`<latent relation learning>`, `Used-for`, `data shift correction`)  (`<latent relation learning>`, `Compare`, `normal word embedding`)  
### Extracted Concepts:1. Neural semantic parser2. Semantic parsing3. LSTM4. Neural machine translation5. Rhythmic poetry generation6. Layer-wise relevance propagation7. Sequence-to-Dependency Neural Machine Translation8. Multi-modal Neural Machine Translation9. Zero-resource NMT10. Source-side syntactic trees incorporation in NMT11. NMT+RNNG12. Direct HMM with neural network-based lexicon and alignment models13. Linearized, lexicalized constituency trees in NMT14. Neural system combination15. Domain adaptation for NMT16. Topical PageRank17. Salience Rank### Triplets:1. (neural ranking, Compare, Layer-wise relevance propagation)2. (Neural machine translation, Evaluate-for, Layer-wise relevance propagation)3. (Sequence-to-Dependency Neural Machine Translation, Is-a-Prerequisite-of, Neural machine translation)4. (
### Triplets:1. neural machine translation model, Is-a-Prerequisite-of, translation2. neural machine translation model, Compare, LSTM setup3. neural machine translation model, Is-a-Prerequisite-of, convolutional layers4. neural machine translation model, Used-for, encoding source sentence5. LSTM, Compare, convolutional layers6. LSTM, Is-a-Prerequisite-of, neural machine translation model7. convolutional layers, Is-a-Prerequisite-of, neural machine translation model
### Extracted Concepts:1. Neural generative model2. Multi-hop attention3. Sequential matching network (SMN)4. Recurrent neural network (RNN)5. Cross-lingual OpenQA6. Entity-Duet Neural Ranking Model (EDRM)### Triplets:1. (Sequential matching network (SMN), Used-for, addressing problems)2. (Sequential matching network (SMN), Used-for, matching a response with each utterance)3. (Sequential matching network (SMN), Is-a-Prerequisite-of, distilling important matching information)4. (Sequential matching network (SMN), Is-a-Prerequisite-of, modeling relationships among utterances)5. (Sequential matching network (SMN), Is-a-Prerequisite-of, calculating final matching score)6. (Recurrent neural network (RNN), Is-a-Prerequisite-of, modeling relationships among utterances)7. (Entity-Duet Neural Ranking Model
(None)
### Concept: summarization model1. (summarization model, used-for, abstractive summarization)2. (summarization model, used-for, extractive summarization)3. (summarization model, Is-a-Prerequisite-of, document summarization research)4. (summarization model, Compare, extractive summarization)5. (summarization model, Compare, abstractive summarization)
(None)
1. (event extraction, Used-for, text classification)2. (event extraction, Part-of, information extraction)3. (event extraction, Evaluate-for, event argument extraction)4. (event extraction, Is-a-Prerequisite-of, trained model tuning)5. (event extraction, Is-a-Prerequisite-of, trigger word identification)6. (event extraction, Compare, generative event extraction)
(None)
### Triplets:1. Word embeddings, Used-for, Cross-lingual word embeddings2. Neural networks, Used-for, Cross-lingual word embeddings3. Word embeddings, Capture, Linguistic regularities4. Word embeddings, Connected-to, Cross-lingual connection5. Adversarial training, Used-for, Cross-lingual word embeddings6. Bilingual lexicon induction, Evaluate-for, Cross-lingual word embeddings7. Word translation, Used-for, Cross-lingual word embeddings
### Triplets:1. semantic parsing datasets, consist-of, Atis dataset2. semantic parsing datasets, consist-of, GeoQuery corpus3. semantic parsing datasets, consist-of, WikiSQL dataset4. Atis dataset, be-part, semantic parsing5. GeoQuery corpus, be-part, semantic parsing6. WikiSQL dataset, be-part, semantic parsing
### Triplets:1. pre trained language model, is-a-prerequisite-of, fine-tuning2. pre trained language model, is-utilized-in, natural language processing3. pre trained language model, achieves, state-of-the-art performance4. pre trained language model, is-trained-on, large-scale corpora5. pre trained language model, enables, language understanding6. pre trained language model, considers, knowledge graphs7. pre trained language model, improves, performance of NLP tasks
### Triplets:1. diverse conversational corpus, consists-of, multi-turn conversations2. diverse conversational corpus, contains, in-depth discussions3. multi-turn conversations, ground-to, knowledge graphs4. diverse conversational corpus, is-annotated-with, knowledge annotations5. dialogue summarization systems, encode, text with semantic features6. DialoGPT, developed-as, unsupervised dialogue annotator7. unsupervised dialogue annotator, labels, features on dialogue summarization datasets8. neural model, used-for, joint dropped pronoun recovery and conversational discourse parsing9. joint model, trained-on, Structure Parsing-enhanced Dropped Pronoun Recovery dataset10. stylistic data, increase, style intensity11. conversational corpora, resource-for, computational linguistics and language technology12. conversation summarization datasets, evaluated-on, widely-used datasets13. summarization models, benchmarked-on, diverse online
(`<concept>`, `Part-of`, `argumentation datasets`)(`<concept>`, `Part-of`, `text summarization datasets`)(`<concept>`, `Part-of`, `DuoRC`)(`<concept>`, `Part-of`, `SQuADRUn`)(`<concept>`, `Part-of`, `BIGPATENT`)(`<concept>`, `Part-of`, `OpenDialKG`)(`<concept>`, `Part-of`, `ATIS dataset`)(`<concept>`, `Part-of`, `OVERNIGHT dataset`)(`<concept>`, `Part-of`, `SQuAD dataset`)
### Concept: event knowledge1. (event knowledge, Is-a-Prerequisite-of, machine reading)2. (event knowledge, Used-for, database verbalisers)3. (event knowledge, Is-a-Prerequisite-of, recurrent neural networks)4. (machine reading, Evaluate-for, KBLSTM)5. (sequence-to-sequence models, Is-a-Prerequisite-of, video captioning)6. (neural machine translation, Evaluate-for, sequence-to-dependency neural machine translation)7. (neural machine translation, Evaluate-for, cross-lingual dependency parsing)
(`<query concept>`, `Compare`, `traditional statistical machine translation`)  (`<source-side syntactic trees`, `Used-for`, `neural machine translation`)  (`<source-side syntax`, `Part-of`, `machine translation`)  (`<bidirectional tree encoder`, `Is-a-Prerequisite-of`, `sequential attentional model`)  
### Extracted Concepts:1. word-embedding models2. word vectors3. Skip-Gram model4. additive compositionality5. vector calculus6. Sufficient Dimensionality Reduction (SDR)7. aspect extraction8. neural word embeddings9. attention mechanism10. bilingual word embeddings11. character and word embeddings12. semantic graph parser13. dependency parsing14. minimal recursion semantics (MRS)15. Natural Language Processing (NLP)16. multimodal word distributions17. volatility prediction18. sentiment analysis19. network embedding (NE)20. bidirectional language models21. sequence labeling tasks### Triplets:1. (Skip-Gram model, Is-a-Prerequisite-of, additive compositionality)2. (vector calculus, Evaluate-for, solving word analogies)3. (bilingual word embeddings, Is-a-Prerequisite-of, large parallel corpora)4.
### Triplets:1. task oriented dialogue system, Used-for, task success prediction models2. task oriented dialogue system, Used-for, chat detection3. task oriented dialogue system, Is-a-Prerequisite-of, end-to-end learning framework4. task oriented dialogue system, Part-of, task success prediction models5. dialogue state tracking, Is-a-Prerequisite-of, task oriented dialogue systems6. end-to-end task-oriented dialog systems, Compare, pipeline-designed methods7. end-to-end task-oriented dialog systems, Is-a-Prerequisite-of, Mem2Seq8. Global-Locally Self-Attentive Dialogue State Tracker (GLAD), Evaluate-for, user goals and requests9. Incremental Dialogue System (IDS), Is-a-Prerequisite-of, unanticipated user needs handling10. Document Grounded Conversations, Evaluate-for, responses generation
### Triplets:1. natural language generation task, Is-a-Prerequisite-of, semantic parsing2. natural language generation task, Evaluate-for, human-like responses3. natural language generation task, Is-a-Prerequisite-of, NLG architecture4. NLG architecture, Evaluate-for, subjective responses5. NLG architecture, Is-a-Prerequisite-of, generative grammars6. generative grammars, Evaluate-for, subjective responses
1. (IR term weighting models, Used-for, volatility prediction)2. (market data resources, Used-for, forecast market risk)3. (NLP education, Is-a-Prerequisite-of, staying up-to-date on the latest research)4. (knowledge graphs, Is-a-Prerequisite-of, neural search systems)5. (NLP datasets, Used-for, facilitating NLP education and research)6. (Inverse Cloze Task, Is-a-Prerequisite-of, retriever and reader joint learning)7. (OpenBookQA, Conjunction, linguistic understanding)8. (Document translation approach, Is-a-Prerequisite-of, Cross-Lingual Information Retrieval)9. (Query translation approach, Is-a-Prerequisite-of, Cross-Lingual Information Retrieval)10. (DoQA dataset, Evaluate-for, building conversational QA interfaces)
(<language model>, Is-a-Prerequisite-of, large language model)(<neural language model>, Compare, large language model)(<word embeddings>, Used-for, large language model)(<neural machine translation models>, Compare, large language model)
### Triplets:1. sentiment classifier, Used-for, sentiment analysis2. sentiment classifier, Used-for, sentence-level sentiment classification3. sentiment classifier, Evaluate-for, performance improvement4. sentiment classifier, Evaluate-for, state-of-the-art results5. sentiment classifier, Is-a-Prerequisite-of, neural network models6. sentiment classifier, Is-a-Prerequisite-of, machine translation7. sentiment classifier, Is-a-Prerequisite-of, aspect sentiment classification8. sentiment classifier, Is-a-Prerequisite-of, cross-domain sentiment classification9. sentiment classifier, Is-a-Prerequisite-of, target-sensitive sentiment classification10. sentiment classifier, Is-a-Prerequisite-of, hybrid contextualized sentiment classifier
1. (Unsupervised machine translation, Used-for, Cross-lingual word embeddings)2. (Lample et al., 2017, Evaluate-for, Fully unsupervised machine translation model)3. (Bilingual lexicon induction, Is-a-Prerequisite-of, Cross-lingual classification)4. (Bilingual lexicon induction, Is-a-Prerequisite-of, Bilingual lexicon induction with semi-supervision)5. (Bilingual lexicon induction with semi-supervision, Evaluate-for, State of the art results)6. (Bilingual lexicon induction, Compare, Morphology-aware alignment model)7. (Morphology-aware alignment model, Used-for, Bilingual lexicon induction)8. (Recent research line, Used-for, Bilingual lexicon induction)9. (Recent research line, Used-for, Unsupervised machine translation)10. (Cross-lingual word embeddings, Used-for, Cross-lingual transfer
### Triplets:1. relation learning, Used-for, relation extraction2. relation learning, Is-a-Prerequisite-of, adversarial multi-task learning framework3. relation learning, Conjunction, multi-task learning4. adversarial multi-task learning framework, Is-a-Prerequisite-of, alleviating the shared and private latent feature spaces5. multi-lingual neural relation extraction framework, Conjunction, multi-lingual attention6. expressive kernels, Compare, deep neural networks7. deep neural networks, Is-a-Prerequisite-of, kernelized neural network8. neural sequence-to-sequence models, Used-for, abstractive text summarization9. adversarial model, Evaluate-for, enabling an adaptive imitation scheme10. extractive multi-document summarization system, Part-of, joint optimization and active learning
### Triplets:1. relation extraction task, Used-for, finding unknown relational facts from plain text2. relation extraction task, Is-a-Prerequisite-of, exploiting mono-lingual data for relation extraction3. relation extraction task, Is-a-Prerequisite-of, introducing a multi-lingual neural relation extraction framework4. relation extraction task, Is-a-Prerequisite-of, employing mono-lingual attention to utilize information within mono-lingual texts5. relation extraction task, Is-a-Prerequisite-of, proposing cross-lingual attention to consider information consistency and complementarity among cross-lingual texts6. relation extraction task, Compare, distant supervision significantly reduces human efforts in building training data for many classification tasks7. relation extraction task, Evaluate-for, assessing the application of distant supervision in relation extraction8. relation extraction task, Evaluate-for, evaluating the dynamic transition matrix for characterizing noise in the training data built by distant supervision9
### Extracted Concepts:- Narrative texts- Temporal event knowledge- Dialogue belief tracking- Dialogue systems- Domain ontology- Dialogue state tracking- Concept normalization- Entity linking- Concept linking- Open-ended questions- Event extraction- Textual Entailment (TE) - Question Answering (QA)- Long-form answers### Triplets:1. (Event ontology, Used-for, Concept normalization)2. (Entity linking, Evaluate-for, Concept normalization)3. (Concept normalization, Is-a-Prerequisite-of, Domain ontology)4. (Entity linking, Compare, Concept linking)5. (Dialogue systems, Is-a-Prerequisite-of, Dialogue state tracking)6. (Dialogue belief tracking, Used-for, Dialogue systems)7. (Event ontology, Compare, Domain ontology)
### Content: Extracting time expressions from free text is a fundamental task for many applications. We analyze the time expressions from four datasets and find that only a small group of words are used to express time information, and the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach, named SynTime, to recognize time expressions. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related regular expressions over tokens. On the types we design general heuristic rules to recognize time expressions. In recognition, SynTime first identifies the time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a light-weight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text of different types and of different domains. Experiment on benchmark datasets and tweets data shows
### Triplets:1. language understanding task, involves, natural language processing2. language understanding task, is part of, natural language processing3. language understanding task, requires, symbolic reasoning4. language understanding task, involves, neural networks5. language understanding task, uses, neural Symbolic Machine6. neural Symbolic Machine, contains, neural "programmer"7. neural Symbolic Machine, contains, symbolic "computer"8. symbolic "computer", is a Lisp interpreter9. language understanding systems, may infer, 'inexpensive' is a rephrasing for 'expensive'10. language understanding systems, may not associate, 'acquire' with 'acquires'11. language understanding systems, are improved by, morph-fitted vectors12. language understanding tasks, require, morphology13. neural Belief Tracking framework, can comprehend, user utterances and dialogue context14. neural Belief Tracking framework, reasons over,
1. (Multilingual neural machine translation, Used-for, machine translation)2. (Cross-lingual transfer learning, Is-a-Prerequisite-of, multilingual translation)3. (Multilingual neural machine translation, Compare, Statistical machine translation)4. (Multilingual neural machine translation, Is-a-Prerequisite-of, Zero-shot translation)
### Triplets:1. bias pretrained language, Used-for, improving language understanding2. BERT, Used-for, enhancing language representation3. transfer learning techniques, Used-for, text classification4. embeddings, Used-for, improving language processing5. language models, Used-for, next-word prediction6. language models, Compare, pre-trained on only 1/30 of the data7. language models, Evaluate-for, performing next-word prediction8. language models, Compare, outperform its equivalent trained on language alone with a 2% decrease in perplexity9. neural language models, Compare, perform better when training with visual context10. neural architecture, Used-for, outperforming the equivalent trained on language alone11. visual context, Compare, robustly improving the performance of language models across different languages and models
### Triplets:1. neural summarization model, Used-for, processing multiple input documents2. neural summarization model, Is-a-Prerequisite-of, hierarchical encoder3. neural summarization model, Compare, Conversing by Reading models4. neural summarization model, Evaluate-for, generating summaries with high semantic relevance5. neural summarization model, Is-a-Prerequisite-of, encoder-decoder model6. encoder-decoder model, Compare, sequence-to-sequence model
### Triplets:1. unsupervised bilingual lexicon induction, Used-for, natural adversarial game2. unsupervised bilingual lexicon induction, Part-of, unsupervised machine translation3. unsupervised bilingual lexicon induction, Is-a-Prerequisite-of, bilingual word embeddings4. unsupervised bilingual lexicon induction, Evaluate-for, morphology-aware alignment model5. morphology-aware alignment model, Used-for, UBLI task6. unsupervised word embeddings, Evaluate-for, bilingual lexicon induction7. unsupervised machine translation, Is-a-Prerequisite-of, unsupervised word embeddings
### Triplets:1. Bilingual Sentiment Embeddings, Used-for, Sentence-level cross-lingual sentiment classification2. Semantic Parsing Dataset, Used-for, Instruction-driven communication3. Semantic Parsing Dataset, Is-a-Prerequisite-of, Parse interpretation4. Syn-QG, Is-a-Prerequisite-of, Cross-lingual summarization5. SParC, Is-a-Prerequisite-of, Text-to-SQL en6. End-to-end speech translation, Is-a-Prerequisite-of, Curriculum pre-training7. Object-oriented Neural Programming, Used-for, Semantically parsing documents
### Triplets:1. semantic parser, used-for, converting natural language utterances to intermediate, domain-general natural language representations2. semantic parser, Evaluate-for, annotated logical forms3. semantic parser, Compare, attention-based baselines on MRS4. semantic parser, Is-a-Prerequisite-of, abstract syntax networks5. semantic parser, Compare, previous full-coverage semantic graph parsers6. semantic parser, Used-for, learning models for grounded verb semantics7. semantic parser, Evaluate-for, learning algorithm performance8. semantic parser, Part-of, new deep learning model for semantic role labeling (SRL)9. semantic parser, Is-a-Prerequisite-of, state-of-the-art transition-based parser
(None)
### Triplets:1. visual features <Used-for> multimodal machine translation2. multimodal data <Evaluate-for> Multi30k dataset3. doubly-attentive decoder <Used-for> multimodal machine translation4. spatial visual features <Is-a-Prerequisite-of> doubly-attentive decoder5. Multi30k dataset <Evaluate-for> multimodal machine translation6. image and source-language words <Conjunction> doubly-attentive decoder
(`<concept>`, `Is-a-Prerequisite-of`, `abstractive summarization`)(`<concept>`, `Part-of`, `abstractive summarization`)(`<concept>`, `Used-for`, `query-based summarization`)(`<concept>`, `Used-for`, `generation of repeated phrases`)
### Triplets:1. dependency parse, Used-for, semantic parsing2. dependency parse, Is-a-Prerequisite-of, natural language processing3. dependency parse, Compare, sequence tagging4. dependency parse, Conjunction, token-based dependency parsing5. sequence tagging, Used-for, token-based sequence tagging6. sequence tagging, Compare, dependency parse7. BiLSTMs, Compare, local tagging models
### Concept: image text retrieval1. (image text retrieval, Used-for, cross-modal information retrieval)2. (image text retrieval, Part-of, VisualSparta)3. (VisualSparta, Used-for, text-to-image retrieval)4. (image text retrieval, Compare, knowledge base question answering (KBQA))5. (image text retrieval, Is-a-Prerequisite-of, dense retrieval)6. (dense retrieval, Compare, fully zero-shot dense retrieval systems)7. (image text retrieval, Evaluate-for, zero-shot learning)8. (zero-shot learning, Evaluate-for, HyDE)9. (HyDE, Used-for, search)10. (search, Compare, dialogue systems) 
(<Natural Questions dataset>, Part-of, <RikiNet>)(<RikiNet>, Used-for, <answer prediction>)(<model>, Evaluate-for, <performance evaluation>)(<QA data>, Is-a-Prerequisite-of, <supervision for other tasks>)(<model-based retrieval>, Conjunction, <text retrieval>)(<TOME>, Used-for, <model-based retrieval approach>)
### Triplets:1. political framing, Compare, political debate2. political framing, Analyze-in, congressional speeches3. collective classification, Used-for, predict frames4. predictive models, Used-for, increase F1 score5. understanding democratic political decision making, Is-a-prerequisite-of, understanding political debates6. discourse networks, Part-of, newspaper reports7. German newspaper reports, Evaluate-for, annotated pilot corpus8. Argument Mining, Evaluate-for, political debates9. empirical investigation, Evaluate-for, argument components10. Neural Network architectures, Used-for, outperform standard baselines
### Triplets:1. attention-over-attention reader, is-a-Prerequisite-of, better solving cloze-style reading comprehension task2. attention-over-attention reader, induces, "attended attention" for final answer predictions3. attention-over-attention reader, is-simpler-than, related works4. attention-over-attention reader, giving, excellent performance5. N-best re-ranking strategy, Used-for, double check the validity of candidates and further improve the performance6. Neural Machine Translation (NMT), is-a-Prerequisite-of, attention-based Neural Machine Translation (NMT)7. Neural Machine Translation (NMT), Used-for, word reordering knowledge into attention-based NMT for improving translation performance
### Triplets:1. sentence representation learning, Used-for, NLP tasks2. sentence representation learning, Evaluate-for, semantic representation3. sentence representation learning, Compare, traditional surface patterns4. traditional surface patterns, Hyponym-Of, sentence representation learning
### Concept: shot relation extraction1. (relation extraction, Used-for, finding unknown relational facts from plain text)2. (relation extraction, Compare, shot relation extraction)3. (relation extraction, Evaluate-for, significant improvements on relation extraction)4. (relation extraction, Is-a-Prerequisite-of, cross-lingual attention)5. (relation extraction, Conjunction, class ties)6. (relation extraction, Used-for, joint relation extraction)7. (relation extraction, Hyponym-Of, multi-lingual neural relation extraction framework)
### Triplets:1. recurrent neural network, utilized in, joint extraction of entity mentions and relations2. LSTM network, combined with, attention3. entity mentions, involved in, relations4. ACE corpora, used for, experiments5. ACE corpora, mentioned in, paper6. Automatic Content Extraction (ACE), used in, experiments7. evaluations, shows, proposed generative model8. lexical feature weights, estimated in, generative model9. generative process, assumed by, lexicon10. SPTree model, compared with, proposed model11. feature-based joint model, significantly outperformed by, proposed model12. model, performs within 1% on entity mentions and 2% on relations, compared with SPTree model13. Lexical resources, used for, part-of-speech induction and named-entity recognition14. lexical resources, treated as observations under, proposed generative model
1. (zero pronoun resolution, used-for, annotated data)2. (zero pronoun resolution, Evaluate-for, automatic pseudo training data generation)3. (reading comprehension mrc model, Compare, recurrent neural networks)4. (reading comprehension mrc model, Evaluate-for, state-of-the-art systems)5. (reading comprehension mrc model, Hyponym-Of, neural reading comprehension model)6. (question answering, Is-a-Prerequisite-of, reading comprehension)7. (reading comprehension mrc model, Part-of, multi-passage MRC)
### Triplets:1. (reading comprehension, is a, challenging task)2. (reading comprehension, aims to understand, natural texts)3. (reading comprehension, can answer, questions)4. (reading comprehension, is a representative, problem)5. (reading comprehension, relies on, recurrent neural networks)6. (reading comprehension, is used for, question answering)7. (reading comprehension, is part of, natural-language understanding systems)8. (reading comprehension, is important for, system development)9. (reading comprehension, can entail, natural-language understanding)10. (RC, significantly outperforms, rule-based systems)11. (RC, integrates, external commonsense knowledge)12. (RC, combines, challenges in language understanding)13. (RC, conducts, attention and fusion)14. (RC, can answer, narrative paragraph questions)15. (RC, has, multi-level soft-alignment)16. (RC, uses,
### Triplets:1. complex named entity, part-of, named entity recognition2. named entity recognition, Used-for, text simplification3. complex named entity, Evaluate-for, improving topic quality
(`<concept>`, Is-a-Prerequisite-of, `recognition relation extraction`)  (`relation extraction`, Is-a-Prerequisite-of, `recognition relation extraction`)  (`recognition relation extraction`, Used-for, `relation extraction`)  (`relation extraction`, Compare, `recognition relation extraction`)  
### Triplets:1. LogicalFactChecker, is-a-Prerequisite-of, fact check2. FEVER, contains, fact check3. LogicalFactChecker, utilizes, semantic reasoning4. LogicalFactChecker, utilizes, symbolic reasoning5. LogicalFactChecker, utilizes, semantics of tables6. LogicalFactChecker, employs, graph module network7. LogicalFactChecker, utilizes, Transformer-based architecture8. LogicalFactChecker, employs, program-driven module network
### Triplets:1. convolutional neural network, Used-for, text categorization2. recurrent neural networks, Used-for, language modeling3. recurrent neural networks, Is-a-Prerequisite-of, dialog systems4. recurrent neural network, Compare, convolutional neural network5. named entity recognition, Used-for, natural language processing
### Triplets:1. context word vector, used-for, capturing semantic regularities2. context word vector, learned from, co-occurrence statistics3. context word vector, evaluated-for, sentiment analysis4. context word vector, part-of, word embedding models5. word embedding models, used-for, learning vector representations6. co-occurrence statistics, used-for, learning vector representations7. context word vector, Evaluate-for, document covariance descriptor
### Triplets:1. word embeddings - encode - meaning2. word embeddings - independent-of - language3. monolingual word embeddings - map-to - shared space4. word similarity - should-be - independent5. word vectors - within - language6. word embeddings - structure - independent7. modularity - measures - strength8. network measurement - modularity - measures9. modularity - correlates-with - downstream tasks10. word vectors - based-on - structure11. modularity - serves-as - validation metric
### Triplets:1. Dialog generation, Used-for, Encoder-decoder dialog model2. Dialog generation, Evaluate-for, Automatic Pyramid scores3. Encoder-decoder dialog model, Is-a-Prerequisite-of, Interpretable response generation 4. Dialog generation, Evaluate-for, Response generation quality5. Neural knowledge diffusion model, Is-a-Prerequisite-of, Knowledge diffusion6. Sentence function, Is-a-Prerequisite-of, Dialog generation
1. (seq2seq models, success in text generation)2. (conditional text generation, Conditional Text Generation task)3. (conditional text generation, NLG)4. (current conditional generation models, cannot handle emerging conditions)5. (PPVAE, framework for conditional text generation)6. (PPVAE, decouples text generation module from condition representation module)
### Triplets:1. unsupervised cross-lingual, Used-for, machine translation2. unsupervised cross-lingual, Is-a-Prerequisite-of, bilingual lexicon induction3. unsupervised cross-lingual, Compare, supervised systems4. unsupervised cross-lingual, Is-a-Prerequisite-of, unsupervised word embeddings5. unsupervised cross-lingual, Evaluate-for, sentiment classification6. bilingual lexicon induction, Is-a-Prerequisite-of, unsupervised cross-lingual7. bilingual lexicon induction, Evaluate-for, morphological paradigms8. bilingual lexicon induction, Compare, unsupervised systems9. machine translation, Is-a-Prerequisite-of, unsupervised bilingual word embeddings10. machine translation, Evaluate-for, unsupervised neural machine translation
### Extracted Concepts:1. race2. natural language processing (NLP)3. social biases4. word embeddings5. downstream NLP tasks6. gender-bias7. sentence-level representations8. BERT9. Transformer-based Domain-aware N-gram Adaptor (T-DNA)### Triplets:1. (race, involves, language)2. (race, may affect, social biases)3. (social biases, impact, NLP)4. (word embeddings, contain, biases)5. (word embeddings, utilized for, downstream NLP tasks)6. (gender-bias, mitigated by, debiasing methods)7. (sentence-level representations, analyzed for, social biases)8. (BERT, improves performance on, downstream NLP tasks)9. (T-DNA, achieves, significant improvements)
### Triplets:1. summarization datasets, Is-a-Prerequisite-of, machine translation2. summarization datasets, Evaluate-for, model evaluation3. summarization datasets, Evaluate-for, performance analysis4. summarization datasets, Is-a-Prerequisite-of, dataset creation5. summarization datasets, Part-of, DUC 20046. summarization datasets, Part-of, English Gigaword7. summarization datasets, Hyponym-Of, MDS dataset
1. (adversarial training, applied to, text classification)2. (adversarial training, used for, neural machine translation models)3. (adversarial training, applied to, self-attentive neural networks)4. (neural image captioning, crafting, adversarial examples)5. (neural image captioning, adversarial perturbations, language grounding)6. (adversarial training, improves, robustness of NMT models)
(`<concept>`, Is-a-Prerequisite-of, `Multimodal Neural Machine Translation model`)  (`<concept>`, Evaluate-for, `Sentiment analysis`)  (`<concept>`, Evaluate-for, `Emotion analysis`)  
### Extracted Concepts:1. Morphologically rich languages2. Distributional vector space models3. Morph-fitting procedure4. Morphological constraints5. Inflectional forms6. Derivational antonyms7. Neural model for morphological inflection8. Morphological regularities of words9. Language modeling task10. Universal Dependencies (UD)11. Syntactic structures12. Cross-lingual transfer13. Statistical morphological inflectors14. Generative latent-variable model for inflection generation15. Morphological analysis16. Morphological tagging17. Analogical reasoning18. Character-level transduction tasks19. Hard attention sequence-to-sequence model20. Morphological supervision21. Text classification22. Gender stereotypes in languages23. Machine translation (MT)24. Multilingual NLP tools25. Inflectional lexica26. Morphological richness and dialect
(`<Concept>`, `Part-of`, `Semantic Parsing`)  (`<Concept>`, `Used-for`, `Answering Questions`)  (`<Semantic Parsing>`, `Is-a-Prerequisite-of`, `Long Form Question Answering`)  
### Triplets:1. automatic question generation, Evaluate-for, sentence generation2. sentence function, Used-for, sentence generation3. type controller, Is-a-Prerequisite-of, sentence generation4. generative neural network model, Compare, sentence generation
### Extracted Concepts:1. Sequential generative model2. Hierarchical recurrent encoder-decoder models3. ReCoSa model4. Open-domain dialogue generation5. Meta-words### Triplets:1. Sequential generative model, Generates, missing symbols2. Hierarchical recurrent encoder-decoder models, Utilizes, all the contexts indiscriminately3. ReCoSa model, Outperforms, baseline models4. Open-domain dialogue generation, Achieved, remarkable progress5. Meta-words, Describes, attributes of a response
### Output:1. (discourse coherence, Is-a-Prerequisite-of, text parsing)2. (discourse coherence, Evaluate-for, discourse segmentation)3. (discourse coherence, Conjunction, discourse relations)4. (discourse coherence, Evaluate-for, discourse coherence assessment)5. (discourse coherence, Compare, dialogue coherence evaluation)6. (discourse coherence, Is-a-Prerequisite-of, dialogue systems)7. (discourse coherence, Is-a-Prerequisite-of, discourse parsers)
### Extracted Concepts:1. NLP research2. Underrepresented languages3. Challenges in Indonesian NLP4. Performance of current NLP systems5. Lack of datasets6. Labelled corpora for African languages7. Sentiment classification8. Cross-domain adaptation9. Machine translation10. Low-resource languages### Triplets:1. (Underrepresented languages, Is-a-Prerequisite-of, Lack of datasets)2. (Underrepresented languages, Is-a-Prerequisite-of, Labelled corpora for African languages)3. (Challenges in Indonesian NLP, Is-a-Prerequisite-of, Performance of current NLP systems)4. (Sentiment classification, Is-a-Prerequisite-of, Cross-domain adaptation)5. (Machine translation, Is-a-Prerequisite-of, Low-resource languages)
### Triplets:1. neural retrieval model, Used-for, generating responses2. neural retrieval model, Is-a-Prerequisite-of, context-dependent semantic parsing3. neural retrieval model, Compare, textual behavioral information4. neural network model, Used-for, generating responses5. neural network model, Evaluate-for, detecting review spam6. neural network model, Evaluate-for, sentence-level sentiment classification7. neural network models, Conjunction, context-aware models
#### Triplets extracted from the context for the concept "zero-shot text classification":1. (Zero-shot text classification, Is-a-Prerequisite-of, Transfer learning methods)2. (Zero-shot text classification, Is-a-Prerequisite-of, Multi-task learning)3. (Zero-shot text classification, Is-a-Prerequisite-of, Domain adaptation techniques)4. (Zero-shot text classification, Evaluate-for, Reducing the need for labeled data)5. (Zero-shot text classification, Evaluate-for, Improving model generalization)6. (Transfer learning methods, Evaluate-for, Improving model adaptation)7. (Multi-task learning, Evaluate-for, Enhancing model capabilities)8. (Domain adaptation techniques, Evaluate-for, Enhancing model adaptability)9. (Zero-shot text classification, Used-for, Automatic text classification)10. (Zero-shot text classification, Compare, Text classification with labeled data)11. (Zero-shot text classification, Compare, Few-shot text classification
### Extracted Concepts:1. Named Entity Recognition (NER)2. Gazetteers3. Neural Named Entity Recognition (NNE)4. Generative models5. Discriminative training6. Multilingual learning7. Named entity dictionaries8. Cross-domain adaptation9. Dual Adversarial Transfer Network (DATNet)10. Nested Named Entity Recognition11. External gazetteers12. Hybrid semi-Markov CRF architecture13. Nested named entity recognition### Triplets:1. (Named Entity Recognition (NER), Used-for, part-of-speech induction)2. (Gazetteers, Used-for, Named Entity Recognition (NER))3. (Generative models, Is-a-Prerequisite-of, Named Entity Recognition (NER))4. (Discriminative training, Compare, Generative models)5. (Multilingual learning, Evaluate-for, Neural Named Entity Recognition (NNE))6. (Named entity
#### Triplets:1. (relation extraction model, Used-for, finding unknown relational facts from plain text)2. (relation extraction model, Evaluate-for, relation extraction)3. (relation extraction model, Compare, existing relation extraction methods)4. (relation extraction model, Is-a-Prerequisite-of, utilizing mono-lingual attention for relation extraction)5. (relation extraction model, Evaluate-for, exploiting massive information from texts in various languages)6. (relation extraction model, Compare, multi-lingual neural relation extraction framework)
### Triplets:1. pre trained model <Is-a-Prerequisite-of> neural network architectures2. pre trained model <Used-for> NLP tasks3. NLP tasks <Used-for> sequence labeling4. neural network architectures <Is-a-Prerequisite-of> context embeddings5. context embeddings <Used-for> NLP systems
### Triplets:1. morphological inflection, Used-for, improving distributional vector spaces.2. improving distributional vector spaces, Evaluate-for, language understanding systems.3. morphological inflection, Is-a-Prerequisite-of, inflectional forms.4. morphological inflection, Is-a-Prerequisite-of, derivational antonyms.5. morphological inflection, Used-for, downstream task of dialogue state tracking.6. morphological inflection, Is-a-Prerequisite-of, semantic quality of the entire word vector collection.7. morphological inflection, Is-a-Prerequisite-of, paradigm completion.8. morphological inflection, Is-a-Prerequisite-of, morphological inflection generation.9. paradigm completion, Is-a-Prerequisite-of, mapping a lemma to its inflected forms.10. morphological inflection generation, Is-a-Prerequisite-of, state-of-the-art results.
### Triplets:1. Social Bias Frames, Is-a-Prerequisite-of, Modeling the pragmatic frames2. Social Bias Frames, Used-for, Pragmatic modeling of social biases3. Social Bias Frames, Evaluate-for, Contextual understanding of social biases
1. (translation model, Is-a-Prerequisite-of, neural machine translation)2. (translation model, Used-for, generating translations)3. (translation model, Compare, NMT+RNNG)4. (translation model, Used-for, improving translation performance)
### Triplets:1. adversarial robustness, is-a-Prerequisite-of, machine translation systems2. adversarial robustness, Evaluate-for, NLP systems3. adversarial robustness, Used-for, defending adversarial attacks4. adversarial robustness, Compare, robust accuracy5. adversarial robustness, Is-a-Prerequisite-of, ATINTER6. adversarial robustness, Evaluate-for, adversarial training7. adversarial robustness, Compare, adversarial samples8. adversarial robustness, Used-for, improving adversarial robustness9. adversarial robustness, Is-a-Prerequisite-of, state-of-the-art methods10. adversarial robustness, Evaluate-for, text classifiers
1. (word embeddings, used-for, word analogy questions)2. (word embeddings, used-for, caption generation)3. (word embeddings, Compare, bag-of-words)4. (word embeddings, Compare, Bag of Words (BoW))5. (word embeddings, Compare, word2vec skip-grams)6. (word embeddings, Compare, Gaussian embeddings)7. (word embeddings, Is-a-Prerequisite-of, word similarity)8. (word embeddings, Is-a-Prerequisite-of, entailment)9. (word embeddings, Conjunction, Gaussian mixtures)10. (word embeddings, Conjunction, word distributions)11. (word embeddings, Conjunction, multiple word meanings)
(`<concept>`, `Used-for`, `question answering`)  (`<concept>`, `Compare`, `Textbook Question Answering`)  (`<concept>`, `Is-a-Prerequisite-of`, `Neural models`)  
### Extracted Concepts:1. Question Answering2. Framework3. Natural language processing4. State-of-the-art models5. Recurrent Neural Networks (RNNs)6. Reinforcement learning7. Supervised learning8. World knowledge9. Linguistic knowledge10. TriviaQA11. Reading comprehension datasets12. Stanford Question Answering Dataset (SQuAD)13. Neural architecture14. Question Answer-Evidence Triples15. Natural-language understanding systems16. Large annotated data17. Open Information Extraction (Open IE)18. Universal schema19. Transfer learning20. Neural paragraph-level question answering models21. Document-level input### Triplets:1. Question Answering Dataset Compare Reading comprehension datasets2. Reading comprehension datasets Is-a-Prerequisite-of Question Answering Dataset3. Stanford Question Answering Dataset (SQuAD) Is-a-Prerequisite-of Question Answering
### Triplets:1. (morphological, Used-for, representation)2. (representation, Is-a-Prerequisite-of, morphological)
#### Extracted Concepts:1. Neural network models2. Multi-task learning3. Shared layers4. Task-specific features5. Adversarial multi-task learning framework6. End-to-end computational argumentation mining7. Multi-task setup8. Dependency parsing9. Sequence tagging10. Semantic dependency parsing11. Multilingual multi-task learning12. Multi-modal multi-task learning architecture13. Abstractive summarization14. Question generation15. Entailment generation16. Named entity recognition (NER)17. Recurrent units with multiple blocks18. Task groups19. Neural network architectures20. Natural language understanding (NLU)21. Rumor detection22. Stance classification23. Multi-hop question answering24. Explainable multi-hop QA25. Semantic Dependency Parsing (SDP)26. Cross-lingual transfer learning (CLTL)27. Implicit discourse relation recognition (ID
### Triplets:1. annotators, insensitivity to differences in dialect, racial bias2. annotators, propagate biases, models3. dialect, race priming, reduce racial bias4. annotators, less likely to label, AAE tweet5. speech directed to children, helps language learners6. models trained on adult-directed speech, generalize better7. code-switching patterns, improve various downstream NLP applications8. switching features, improve humour, sarcasm and hate speech detection tasks9. sentiment analysis, sentiment classification, preprocessing10. preprocessing, important step, natural language processing prediction model11. hate speech detection models, evaluated, held-out test data12. hate speech detection models, struggle with hate speech detection13. automatic detection mechanisms, establish and improve, hate speech14. conventional approaches, generate counterspeech, combating hate speech15. diverse intent-specific counterspeech dataset, 6831 counterspeeches,
### Triplets:1. learning morphological inflection, is-a-prerequisite-of, generative model2. neural model, Used-for, morphological inflection generation3. Latent Meaning Models, Evaluate-for, word similarity datasets4. generative latent-variable model, Evaluate-for, inflection generation5. hard attention sequence-to-sequence model, Compare, non-monotonic models6. morphological tagging, Part-of, morphologically rich languages7. adversarial training, Evaluate-for, state-of-the-art results
### Triplets:1. unsupervised selective rationalization, used-for, producing rationales2. unsupervised selective rationalization, Evaluate-for, predictions3. unsupervised selective rationalization, Compare, unsupervised neural machine translation4. unsupervised selective rationalization, Evaluate-for, explanation_generation5. unsupervised neural machine translation, Part-of, Unsupervised neural machine translation approach
### Triplets:1. multi passage reading comprehension, Used-for, Machine reading comprehension (MRC)2. Machine reading comprehension (MRC), Is-a-Prerequisite-of, multi passage reading comprehension3. multi passage reading comprehension, Is-a-Prerequisite-of, Dynamic Self-attention Network (DynSAN)4. Dynamic Self-attention Network (DynSAN), Compare, PathNet5. PathNet, Is-a-Prerequisite-of, multi passage reading comprehension
### Triplets:1. dialogue agents, access, Knowledge Bases2. neural dialogue agents, addressed by, induced "soft" posterior distribution3. tools, evaluated for, effectiveness4. dialogue agents, interact with, external database5. conversation partners, contribute, information6. agents, achieve, common goal7. dialogue systems, incorporate, knowledge bases8. state tracker, estimates, user goals and requests9. dialogue state tracking, is part of, task-oriented dialogue systems10. dialogue systems, suffer from, challenge of incorporating knowledge bases.
### Extracted Concepts:1. Graph Convolutional Network (SpellGCN)2. BERT (Bidirectional Encoder Representations from Transformers)3. PLOME (Pre-trained masked Language model with Misspelled knowledgE)4. PHMOSpell5. Neural Networks6. Sequence-to-sequence learning7. Adversarial Training8. UMRSpell9. LEMON Benchmark10. Hanyu Pinyin### Triplets:1. (SpellGCN, Used-for, Chinese Spelling Correction)2. (BERT, Evaluate-for, Error Detection)3. (Error Detection, Is-a-Prerequisite-of, Error Correction)4. (PLOME, Evaluate-for, Spelling Error Correction)5. (PHMOSpell, Is-a-Prerequisite-of, Chinese Spelling Correction)6. (Neural Networks, Evaluate-for, Chinese Spelling Correction)7. (Sequence-to-sequence
1. (bi-directional LSTMs, compared-to, recurrent networks)2. (recurrent networks, used-for, document classification)3. (recurrent networks, used-for, machine translation)4. (HNs, combines, RNN)5. (Recurrent Neural Networks, showed, promising performance for language modeling)6. (RNNs, requires, data-intensive and require thousands of dialogs to learn simple behaviors)
### Triplets:1. natural language generation task, Used-for, generating human-like responses2. automatic generation, Is-a-Prerequisite-of, natural language generation task3. generative grammars, Part-of, data-driven architecture4. opinionated articles online, Evaluate-for, natural language generation task
### Triples:1. (Social biases, are encoded in, word embeddings)2. (word embeddings, reflect beliefs about, certain kinds of people)3. (word embeddings, accurately reflect, beliefs about certain kinds of people)4. (word embeddings, mirror survey data, across seventeen dimensions of social meaning)5. (word embeddings, find biases, in various dimensions of meaning)6. (word embeddings, reflect biases, accurately for some dimensions of meaning)7. (word embeddings, reflect biases, much more for gender than for race)8. (word embeddings, mirror survey data, only for the most salient biases)9. (word embeddings, contain, social biases)
(`<concept>`, `Compare`, `encoder pre-training in end-to-end Speech Translation`)  (`<concept>`, `Part-of`, `end-to-end Speech Translation`)  (`<concept>`, `Evaluate-for`, `state-of-the-art BLEU scores`)  (`<concept>`, `Hyponym-Of`, `audio-only models`)  (`<concept>`, `Part-of`, `UnitY architecture`)  (`<concept>`, `Is-a-Prerequisite-of`, `end-to-end paradigm`)  
1. (contextualized embeddings, used-for, semantic representations)2. (contextualized embeddings, Compare, traditional word embeddings)3. (contextualized embeddings, Compare, Sense embeddings)4. (contextualized embeddings, Evaluate-for, Word Sense Disambiguation tasks)5. (contextualized embeddings, Evaluate-for, Named entity recognition)6. (Sense embeddings, Evaluate-for, Deep contextualized embeddings)7. (contextualized embeddings, Is-a-Prerequisite-of, VAMPIRE)8. (contextualized embeddings, Is-a-Prerequisite-of, WSD tasks)9. (contextualized embeddings, Is-a-Prerequisite-of, Named entity recognition)10. (contextualized embeddings, Is-a-Prerequisite-of, Concept-level analyses)
#### Triplets:1. hybrid dialogue systems, Compare, domain-specific task-oriented spoken dialogue systems2. dialogue system, Part-of, domain ontology configurations3. slot filling, Is-a-Prerequisite-of, deep learning-based slot filling models4. slot filling, Evaluate-for, extensive labeled training data5. dialogue systems technology challenge, Evaluate-for, end-to-end multi-domain dialogue system task6. dialogue policy optimization, Is-a-Prerequisite-of, task-oriented dialogue systems7. conversation data, Used-for, non-task oriented dialogue systems
### Triplets:1. emotion cause pair extraction, aims-to, extract all emotion clauses coupled with their cause clauses2. emotion cause pair extraction, involves, identifying the document’s emotion clauses and corresponding cause clauses3. emotion cause pair extraction, has, gained much attention in recent years4. emotion cause pair extraction, aims-to, identify clauses which contain emotion-evoking information for a particular emotion expressed in text5. emotion cause pair extraction, is closely associated with the relationship between sentences6. emotion cause pair extraction, is a task in the area of text emotion analysis, has emerged in recent years7. emotion cause pair extraction, aims-to, extract the potential pairs of emotions and their corresponding causes in a document
### Triplets:1. contextual embeddings, Used-for, document-level information2. contextual embeddings, Part-of, neural approach3. neural approach, Evaluate-for, discovering coherent aspects4. word embeddings, Evaluate-for, improving coherence5. document embedding, Part-of, DEEB-RNN model6. sentence embeddings, Evaluate-for, multilingual skip-gram model7. BERT embeddings, Compare, ELMo embeddings
### Triplets:1. annotated training data, used-for, event extraction2. annotated training data, Evaluate-for, data labeling3. data labeling, Is-a-Prerequisite-of, event extraction4. annotating training data, Compare, automatic labeling of training data5. annotated training data, part-of, high-quality sentence compression models
(Argumentation Mining, Used-for, Neural Techniques)(Neural Techniques, Compare, Multi-task Learning Setup)(Argumentation Mining, Is-a-Prerequisite-of, Relation Identification)(Neural Techniques, Evaluate-for, Performance Results)(EMAR, Is-a-Prerequisite-of, Continual Relation Learning)(Knowledge Base Question Answering Systems, Is-a-Prerequisite-of, Relation Linking)(AMR Semantic Parse, Used-for, Relation Linking)
(`<concept>`, `Part-of`, `reading comprehension datasets`)  (`<concept>`, `Is-a-Prerequisite-of`, `natural-language understanding systems`)  
### Concept: model semantic parsing1. (model semantic parsing, Is-a-Prerequisite-of, natural language descriptions)2. (model semantic parsing, Used-for, generating semantic representations)3. (model semantic parsing, Evaluate-for, automated keyphrase extraction)4. (semantic parsing, Used-for, mapping natural language utterances)5. (model semantic parsing, Used-for, learning a classifier)6. (model semantic parsing, Is-a-Prerequisite-of, improving semantic parsing performance)7. (model semantic parsing, Is-a-Prerequisite-of, neural architecture)8. (semantic parsing, Used-for, mapping natural language sentences)9. (encoder-decoder framework, Used-for, learning an end-to-end model)10. (model semantic parsing, Is-a-Prerequisite-of, neural semantic parsers)
(`<concept>`, Evaluate-for, professional fact checker)  (`<concept>`, Is-a-Prerequisite-of, fact checking model)  (fact checking model, Is-a-Prerequisite-of, state-of-the-art algorithms)  (fact checking model, Used-for, veracity prediction)  
### Triplets:1. abstractive summarization system, Is-a-Prerequisite-of, document summarization research2. abstractive summarization system, Used-for, generating shorter versions of documents3. abstractive summarization system, Compare, with extractive summarization system4. abstractive summarization system, Evaluate-for, generating concise and coherent summaries5. neural abstractive method, Is-a-Prerequisite-of, abstractive summarization system6. extractive summarization system, Is-a-Prerequisite-of, compressive summarization paradigm7. abstractive summarization system, Compare, with document summarization research8. document retrieval, Is-a-Prerequisite-of, abstractive summarization system
(`<concept>`, `Used-for`, `defending against textual adversarial attacks`)  (`<concept>`, `Is-a-Prerequisite-of`, `generative models such as energy-based models and diffusion models`)  
### Extracted Concepts:1. Global-Locally Self-Attentive Dialogue State Tracker (GLAD)2. LSTM encoder3. Self-attentive architecture4. Orthographic similarity5. Self Attentive Revision Encoder (StRE)6. Self-attentive neural syntactic parsers7. ELMo8. BERT9. Lightweight convolutions10. Transformer11. LAIT (Layer-Adjustable Interactions in Transformers)### Triplets:1. (GLAD, Used-for, user utterance and previous system actions)2. (Self-attentive architecture, Compare, LSTM encoder)3. (ELMo or BERT, Hyponym-Of, contextualized word embeddings)4. (Transformer, Used-for, lightweight convolutions)5. (Orthographic similarity, Evaluate-for, predicting the quality of new edits)6. (StRE, Is-a-Prerequisite-of,
(`<concept>`, `Compare`, `arithmetic operations`)  (`<concept>`, `Evaluate-for`, `arithmetic programs`)  (`numerical predictions`, `Conjunction`, `text generation`)  (`recurrent neural networks (RNNs)`, `Used-for`, `language modeling`)  
### Triplets:1. Hate speech detection model, Used-for, identifying rumors2. Hate speech detection model, Compare, state-of-the-art rumor detection models3. Hate speech detection model, Evaluate-for, handling positive sparsity problem4. Hate speech detection model, Is-a-Prerequisite-of, hate detection models' performance5. Hate speech detection model, Compare, standard end-to-end speech recognition models6. Hate speech detection model, Is-a-Prerequisite-of, syntopical reading approach7. Hate speech detection model, Used-for, simultaneously perform translation from streaming source speech to target text
### Triplets:1. argument generation, Is-a-Prerequisite-of, response generation2. ParaNMT-50M, Used-for, argument generation3. discriminative weighted finite state machine, Conjunction, argument generation4. DI-VAE, Compare, argument generation5. medical imaging reports, Evaluate-for, argument generation
### Triplets:1. word embeddings, capture, linguistic regularities2. word embeddings, transfer across languages, linguistic regularities3. word embeddings, overload, linguistic regularities4. word embeddings, extract, Arabic word embeddings5. Arabic word embeddings, evaluate, intrinsic evaluation6. Arabic word embeddings, utilize, benchmark7. holographic embeddings, consider, spectral version8. spectral version, view as, complex embeddings9. complex embeddings, convert to, holographic embeddings
(<concept>, Is-a-Prerequisite-of, semantic parsing)  (coreference resolution, Evaluate-for, neural network models)  (deep learning models, Have, poor performance on commonsense reasoning tasks)
### Triplets:1. vision language, Compare, code generation2. language detection, Evaluate-for, code-switched text3. Neural Belief Tracking, Is-a-Prerequisite-of, Spoken Dialogue Systems4. Universal Language Model Fine-tuning, Is-a-Prerequisite-of, NLP tasks5. Visual Reasoning Language Dataset, Part-of, synthetic images6. connotation frames, Part-of, targeted public sentiments7. unsupervised language detection, Evaluate-for, word-level labeling
### Triplets:1. dependency parser, Used-for, dependency parsing2. dependency parser, part-of, neural techniques3. dependency parser, compare, sequence tagging problem4. dependency parser, Is-a-Prerequisite-of, semantic dependency parsing5. dependency parsing, Evaluate-for, grammaticality improvements6. dependency parser, Compare, Transition-based parser7. dependency parsing, part-of, semantic dependency parsing8. dependency parser, Is-a-Prerequisite-of, neural network model9. dependency parsing, Compare, Maximum Subgraph problem10. dependency parser, Used-for, parsing agglutinative languages11. dependency parser, part-of, character composition model12. dependency parser, Is-a-Prerequisite-of, deep neural architecture
### Triples:1. dialogue agent, learn from conversation, user satisfaction2. dialogue systems, study dialog history, ineffective3. Multimodal EmotionLines Dataset, contains, 13,000 utterances4. E2E NLG Challenge, encourages, development of neural approaches5. OpenDialKG, human-to-human dialogs, manual annotation6. DialogKG Walker model, learns, symbolic transitions7. clinical conversations, annotated by, professional medical scribes
### Triplets:1. NMT models learn about source and target languages during the training process because of continuous representations and non-linearity of neural networks.2. KBLSTM improves the learning of recurrent neural networks for machine reading by leveraging continuous representations of KBs.3. Layer-wise relevance propagation helps to interpret the internal workings of NMT and analyze translation errors.4. Reinforcement learning can train a model to make discrete jumping decisions, enabling faster processing of textual information in tasks like number prediction, sentiment analysis, and news article classification.
### Extracted Concepts:1. Neural Language Model2. LSTM (Long Short-Term Memory)3. Language Modeling4. Affective Information5. Character-Level Language Models6. Recurrent Neural Networks (RNN)7. Treebank8. Universal Language Model Fine-tuning (ULMFiT)9. Named Entities10. Recurrent Neural Network Grammars (RNNG)11. LSTM Noisy Channel Model12. Transformer13. Hybrid Language Modeling14. BERT (Bi-directional Encoder Representation from Transformers)### Triplets:1. Neural Language Model, Used-for, Poetic Device Learning2. LSTM, Evaluate-for, Emotional Content Generation3. Language Modeling, Is-a-Prerequisite-of, Open-Vocabulary Modeling4. Character-Level Language Models, Compare, True Morphological Analyses Access5. RNN, Compare, Syntax Learning6. Treebank, Evaluate-for, Parsing Performance Improvement
### Triplets:1. predict sentiment, Is-a-Prerequisite-of, sentiment classification2. predict sentiment, Evaluate-for, sentiment polarity3. predict sentiment, Used-for, sentiment analysis4. sentiment classification, Used-for, sentiment analysis5. sentiment analysis, Is-a-Prerequisite-of, aspect-based sentiment analysis6. sentiment analysis, Is-a-Prerequisite-of, opinionated natural language generation7. sentiment analysis, Is-a-Prerequisite-of, aspect extraction
### Extracted Concepts:- Attention mechanism- Neural network models- RNN (Recurrent Neural Network)- LSTM (Long Short-Term Memory)- Aspect sentiment classification- Memory networks- Target-sensitive memory networks- Cross-domain sentiment analysis- Sentence embeddings- Probing tasks- Word embeddings- Domain-specific word embeddings- Generic word embeddings- Domain Adapted (DA) word embeddings- Canonical Correlation Analysis (CCA)### Triplets:1. (Attention mechanism, Evaluate-for, Aspect sentiment classification)2. (Neural network models, Used-for, Task sentiment classification)3. (RNN, Evaluate-for, Aspect sentiment classification)4. (LSTM, Evaluate-for, Aspect sentiment classification)5. (Aspect sentiment classification, Is-a-Prerequisite-of, Memory networks)6. (Memory networks, Evaluate-for, Aspect sentiment classification)7. (Memory networks, Compare, Target-sensitive memory networks)8. (Cross
### Triplets:1. Abstractive summarization, Is-a-Prerequisite-of, News summarization2. Query-based summarization, Evaluate-for, News summarization3. Neural sequence-to-sequence models, Used-for, Abstractive text summarization4. Extractive summarization, Compare, Abstractive summarization5. Graph-based attention mechanism, Used-for, Abstractive document summarization6. Multi-document summarization system, Evaluate-for, Content selection7. Hierarchical encoder, Part-of, End-to-end neural network 8. Extractive document summarization, Is-a-Prerequisite-of, Document modeling9. Fine-grained opinion analysis, Is-a-Prerequisite-of, Opinion summarization
```(trained, Is-a-Prerequisite-of, parser)(parser, Used-for, parsing)(parser, Evaluate-for, corpus-level judgments)(parser, Evaluate-for, sentence-level correlation)(parser, Part-of, AMR parsing)```
1. (neural machine translation, Improve, adversarial sample)2. (adversarial sample, crafted by, neural network)3. (self-attentive neural networks, Robust against, adversarial sample)4. (text classification, affected by, adversarial sample)5. (Adversarial Attention Network, Leverage, adversarial sample)6. (gradient-based method, Generate, adversarial sample)
1. (better language, Evaluation, neural semantic parser)2. (better language, Build-for, conveying affective messages)3. (better language, Evaluate-for, perplexity experiments)4. (neural language model, Evaluate-for, language model prediction)5. (machine comprehension task, Evaluate-for, semantic-driven approach)6. (core programming language, Comparison, naturalized language)7. (natural language interfaces, Used-for, analyzing text)8. (natural language interfaces, Used-for, database querying)9. (knowledge bases, Prerequisite, natural language processing tasks)10. (knowledge base completion, Used-for, linguistic typology studies)
### Triplets:1. (neural machine translation, relies-on, bi-directional LSTMs)2. (neural machine translation, relies-on, deep neural networks)3. (neural machine translation, utilizes, convolutional layers)4. (neural machine translation, requires, source sentences)5. (neural machine translation, involves, encoder or decoder RNNs)6. (source syntax, benefits, neural machine translation)7. (neural machine translation, interprets, internal workings)8. (neural machine translation, learns, representations)9. (recurrent neural networks, applied-in, natural language processing)10. (compound splitters, evaluated-in, neural machine translation)11. (neural machine translation, incorporates, linguistic prior)12. (neural machine translation, translates, on its own)
### Triplets:1. universal representor, replaces, encoder and decoder models2. model architecture, includes, cross-lingual sentence similarity model3. pretraining data, used to, generate synthetic data4. Multilingual neural machine translation, outperforms, individual models trained on bilingual5. denoising autoencoding, results in, a universal encoder6. multilingual unsupervised model, performs better than, separately trained bilingual models7. generative latent variable model, fits, multilingual dictionary
### Extracted Concepts:1. Attention mechanism2. Aspect-level sentiment classification3. Sentiment polarity4. Neural models5. Sentence representation6. Neural ASC models### Triplets:1. (Aspect-level sentiment classification, Is-a-Prerequisite-of, Attention mechanism)2. (Aspect-level sentiment classification, Is-a-Prerequisite-of, Sentiment polarity)3. (Aspect-level sentiment classification, Is-a-Prerequisite-of, Neural models)4. (Neural ASC models, Used-for, Aspect-level sentiment classification)5. (Sentence representation, Is-a-Prerequisite-of, Neural ASC models)6. (Neural ASC models, Is-a-Prerequisite-of, Attention mechanism)
### Triplets:1. language understanding systems, Used-for, neural networks2. language understanding systems, Compare, symbolic reasoning3. natural language understanding systems, Compare, language generation systems4. human evaluation, Evaluate-for, automatic metrics5. language understanding systems, Part-of, information ranking6. entailment recognition, Evaluate-for, similarity measures7. neural networks, Is-a-Prerequisite-of, language understanding systems
### Triplets:1. story ending generation, Part-of, natural language generation2. story ending generation, Evaluate-for, coherent and diversified story endings3. story ending generation, Is-a-Prerequisite-of, fine-grained sentiment generation4. natural language generation, Is-a-Prerequisite-of, story ending generation5. sentiment analyzer, Evaluate-for, acquiring sentiment intensities6. sentimental generator, Evaluate-for, controlling sentiment of the output
### Triplets:1. encoder-decoder model, is-used-for, dialog systems2. sequence-to-sequence model, used-for, constituency parsing3. Deep Dyna-Q, used-for, task-completion dialogue policy learning4. neural knowledge diffusion (NKD) model, used-for, dialogue generation5. unsupervised discrete sentence representation learning method, is-a-prerequisite-of, interpretable response generation
### Triplets:1. translation task demonstrate, Include, visual context2. translation task demonstrate, Improve, BLEU score3. translation task demonstrate, Achieve, significant improvements4. translation task demonstrate, Show, better performance5. translation task demonstrate, Involve, reinforcement learning algorithm6. translation task demonstrate, Propose, hard-attention based NMT model7. hard-attention based NMT model, Address, long sequence translation errors
(None)
### Extracted Concepts:1. Automatic Pyramid scores2. Extractive multi-document summarization3. Abstractive summarization4. Neural abstractive models5. Document summarization research6. Abstractive sentence summarization7. Graph-based attention mechanism8. Sequence-to-sequence framework9. Extractive multi-document summarization system10. Joint optimization11. Active learning12. Sentence scoring13. Sentence selection14. Document modeling15. Hierarchical document encoder16. Attention-based extractor17. Natural language understanding tasks18. External information19. Answer selection20. Email subject line generation21. Natural language generation tasks22. Language modeling23. Sentence summarization24. Unsupervised sentence summarization25. Multi-document summarization dataset26. Wikipedia Current Events Portal27. Newsfeeds28. Contextualized embeddings29. Semantic similarity30.
### Extracted Concepts:1. Mention phrase2. Entity mentions3. Coreference4. Neural entity-linking model5. Figure of speech6. Named Entity Recognition (NER)7. Discontinuous Named Entity Recognition (NER)8. Multi-cell compositional LSTM structure### Triplets:1. Mention phrase, Part-of, Bias mention2. Entity mentions, Evaluate-for, Bias mention3. Coreference, Is-a-Prerequisite-of, Bias mention4. Neural entity-linking model, Used-for, Bias mention5. Figure of speech, Is-a-Prerequisite-of, Bias mention6. Named Entity Recognition (NER), Compare, Discontinuous Named Entity Recognition (NER)7. Multi-cell compositional LSTM structure, Is-a-Prerequisite-of, Cross-domain NER
### Triplets:1. multilingual BERT, is-a-Prerequisite-of, zero-shot cross-lingual2. cross-lingual knowledge graph, Used-for, entity embeddings3. deep bilingual query-document representations, Used-for, low-resource cross-lingual document retrieval4. Graph-Attention, Evaluate-for, entity graph matching5. English, Compare, target language6. Devlin et al., Compare, previous state-of-the-art methods
### Triplets:1. multimodal sarcastic tweets, Used-for, multimodal sarcasm detection2. multimodal sarcasm detection, Part-of, Multi-channel Graph Neural Networks3. multimodal sarcasm detection, Compare, Multi-channel Graph Neural Networks4. Multi-channel Graph Neural Networks, Evaluate-for, sentiment detection5. multimodal sarcasm detection, Compare, Multi-channel Graph Neural Networks6. multimodal sarcasm detection, Compare, Dynamic Routing Transformer Network7. multimodal sarcasm detection, Compare, Causal intervention and Counterfactual reasoning based Debiasing framework
(`<concept>`, `Compare`, `aspect-level sentiment classification`)  (`<aspect-level sentiment classification>`, `Evaluate-for`, `attention-based LSTM`)  (`<document level sentiment>`, `Evaluate-for`, `DocRED`)  (`<relation extraction>`, `Hyponym-Of`, `document-level relation extraction`)  
### Extracted Concepts:1. Knowledge Bases (KBs)2. Multi-turn dialogue agent3. Neural dialogue agents4. Reinforcement learner5. End-to-end training6. Task-oriented dialogue agents7. Dialogue responses8. Intelligent assistants9. Symmetric collaborative dialogue setting10. Belief tracker11. Neural Belief Tracking (NBT)12. Dialogue Act classification13. Dependency information14. Transformer model15. Dialogue state tracking (DST)16. Global-Locally Self-Attentive Dialogue State Tracker (GLAD)17. Neural knowledge diffusion (NKD)18. User simulator19. Deep Dyna-Q20. Task-completion dialogue agent21. Statistical update mechanisms### Triplets:1. Knowledge Bases (KBs), Used-for, search by users2. Neural dialogue agents, Evaluate-for, task success rate3. Task-oriented dialogue agents, Con
### Extracted Concepts:1. Event extraction2. Supervised learning3. Data labeling4. World knowledge5. Linguistic knowledge6. Recurrent neural networks (RNNs)7. End-to-end learning8. Hybrid Code Networks (HCNs)9. Dialog systems10. Domain-specific knowledge11. Implicit discourse relation classification12. Semi-supervised question answering13. Generative Domain-Adaptive Nets14. Keyphrase extraction15. Composite deep neural network architecture16. Named entity recognition (NER)17. Supervised lemmatization18. Event detection19. Relation extraction20. Convolutional neural network (CNN)21. Discourse-specific word embeddings22. Arabic word embeddings23. Unsupervised machine translation24. Encoder-decoder dialog model25. Prototypical goal activity### Triplets:1. (supervised learning, Is-a-Prerequisite-of
### Extracted Concepts:1. Evaluation metrics2. Human judgments3. Automatic dialogue evaluation4. Reference-free evaluator5. Semi-supervised training6. Pretrained language models### Triplets:1. (Automatic dialogue evaluation, Compare, Human judgments)2. (Automatic dialogue evaluation, Evaluate-for, Reference-free evaluator)3. (Automatic dialogue evaluation, Used-for, Semi-supervised training)
```(Aspect term extraction, Is-a-Prerequisite-of, Aspect-based sentiment analysis)(Aspect term extraction, Used-for, Supervised learning)(Aspect term extraction, Evaluate-for, Extraction model)(Aspect term extraction, Part-of, Large-scale evaluation)```
### Triplets:1. "manual fact-checking" <Is-a-Prerequisite-of> "saving time on claims that have already been fact-checked"2. "false claims" <Is-a-Prerequisite-of> "detecting previously fact-checked claims"3. "manual fact-checking" <Evaluate-for> "saving time on claims that have already been fact-checked"4. "false claims" <Evaluate-for> "detecting previously fact-checked claims"5. "manual fact-checking" <Evaluate-for> "avoid wasting time on claims that have already been fact-checked"6. "fake news" <Is-a-Prerequisite-of> "manual fact-checking"7. "existing methods" <Used-for> "fact-checking claims"8. "MTM (Memory-enhanced Transformers for Matching)" <Used-for> "ranking FC-articles"9. "MTM (Memory-enhanced Transformers for Matching)" <Used-for> "selecting
### Triplets:1. domain sentiment lexicon, Is-a-Prerequisite-of, sentiment classification2. domain sentiment lexicon, Used-for, sentiment analysis3. domain sentiment lexicon, Evaluate-for, sentiment lexicon induction4. sentiment lexicon, Is-a-Prerequisite-of, Negation words5. sentiment lexicon, Is-a-Prerequisite-of, Intensity words6. sentiment lexicon, Is-a-Prerequisite-of, Sentiment lexicon induction7. sentiment lexicon, Used-for, Multi-sentiment-resource Enhanced Attention Network8. sentiment lexicon, Used-for, building domain-specific sentiment lexicons9. sentiment lexicon, Used-for, Hybrid Contextualized Sentiment Classifier10. Multi-sentiment-resource Enhanced Attention Network, Is-a-Prerequisite-of, sentiment prediction
### Triplets:1. LIME, Is-a-Prerequisite-of, LIMSSE2. attention mechanism, Compare, IP3. DeepLIFT, Compare, IP4. QFE, Is-a-Prerequisite-of, extractive summarization models5. model, Evaluate-for, human annotators6. explanation methods, Evaluate-for, QA models7. DQN, Is-a-Prerequisite-of, retrieval of precise evidences8. post hoc explanation methods, Is-a-Prerequisite-of, IP9. Attribution method, Evaluate-for, model predictions10. RBE, Used-for, textual content moderation
### Triplets:1. question answering, involves, reading comprehension style question answering2. question answering, combines, reading comprehension and neural network-based methods3. reading comprehension, involves, understanding natural texts and answering questions4. reading comprehension, relies on, recurrent neural networks (RNNs)5. reading comprehension, involves, semantic parsing for question answering6. recurrent neural networks, used in, natural language processing7. semantic parsing, focuses on, question answering8. question answering, addresses, factoid questions9. question answering, involves, final answer selection stage10. question answering, benefits from, transfer learning from large QA datasets
### Extracted Triplets:1. (relation extraction, is-a-Prerequisite-of, joint entity relation extraction)2. (relation extraction, Used-for, identification of entity mention spans)3. (joint entity relation extraction, Used-for, identifying relations between pairs of entity mentions)4. (GraphRel, Used-for, joint entity relation extraction)5. (GraphRel, is-a-Prerequisite-of, relation extraction)6. (entity spans, Part-of, joint entity relation extraction)
None.
### Triplets:1. generating natural language, Used-for, question answering systems2. generating natural language, Is-a-Prerequisite-of, semantic parser3. generating natural language, Evaluate-for, coherent natural response4. semantic parser, Is-a-Prerequisite-of, generating natural language5. semantic parser, Used-for, translating natural language messages
(`<content>`, Used-for, question answering system)(`<content>`, Evaluate-for, reading comprehension)(reading comprehension, Evaluate-for, answering questions)(question answering, Is-a-Prerequisite-of, reading comprehension)(question answering, Used-for, answering questions)(question, Is-a-Prerequisite-of, question answering system)(question answering system, Used-for, generating natural answers)
### Extracted Concepts:1. Question answering2. Reading comprehension3. SQuAD dataset4. Recurrent neural networks (RNNs)5. Natural language processing6. Knowledge bases (KBs)7. Semantic parsing8. End-to-end neural network model9. Neural paragraph-level question answering10. Document-level data11. Neural models for question answering### Triplets:1. (Question answering, Is-a-Prerequisite-of, Reading comprehension)2. (Reading comprehension, Is-a-Prerequisite-of, Question answering)3. (Question answering, Used-for, Answering natural questions)4. (Recurrent neural networks (RNNs), Used-for, Question answering)5. (Knowledge bases (KBs), Is-a-Prerequisite-of, Neural network-based KB-QA)6. (Question representation, Is-a-Prerequisite-of, Neural network-based KB-QA)7. (Question answering,
### Concept: image text pair1. (image text pair, Used-for, sentiment detection)2. (image text pair, Is-a-Prerequisite-of, sentiment calibrating)3. (Multi-View Calibration Network, Evaluate-for, sentiment detection)4. (Weakly supervised vision-and-language pre-training, Is-a-Prerequisite-of, relative representation)5. (RELIT, Hyponym-Of, WVLP)6. (mCLIP, Compare, CLIP)7. (mCLIP, Is-a-Prerequisite-of, image-text retrieval)
(None)
### Triplets:1. deep neural network architecture, Used-for, context sensitive lemmatization2. deep neural network architecture, Compare, state-of-the-art lemmatizers3. deep neural network architecture, Compare, hierarchical LSTM language model
### Triples:1. aspect level sentiment, is-a-Prerequisite-of, sentiment analysis2. aspect level sentiment, Used-for, sentiment polarity classification3. aspect level sentiment, Evaluate-for, predicting sentiment polarities4. transfer learning, Used-for, improving aspect level sentiment classification5. attention mechanism, Evaluate-for, aspect-level sentiment classification6. aspect-level sentiment, Part-of, aspect-based sentiment analysis7. targeted sentiment, Compare, sentiment polarity8. sequence prediction, Is-a-Prerequisite-of, aspect level sentiment classification
### Extracted Concepts:1. Conditional Text Generation2. Pre-train and Plug-in Variational Auto-Encoder (PPVAE)3. Emotion-controllable response generation4. Curriculum Dual Learning (CDL)5. Large-scale pre-trained language model (e.g., BERT)### Triplets:- (Conditional Text Generation, Evaluate-for, Pre-train and Plug-in Variational Auto-Encoder (PPVAE))- (Emotion-controllable response generation, Evaluate-for, Curriculum Dual Learning (CDL))- (Large-scale pre-trained language model, Is-a-Prerequisite-of, Conditional Text Generation)
### Triplets:1. Speech text translation, Used-for, trade, law, commerce, politics, literature2. Speech text translation, Used-for, improving translation performance3. Attention mechanism, Used-for, improving translation quality4. Neural machine translation, Used-for, hierarchical translation from chunks to words5. Directed HMM, Used-for, reranking translation results6. Sequence-to-sequence, Part-of, mapping natural language sentences to AMR semantic graphs.
### Triplets:1. language generation, Used-for, natural language understanding2. language generation, Evaluate-for, information ranking3. language generation, Evaluate-for, recognition of paraphrases and textual entailment4. language generation, Evaluate-for, summarization5. language generation, Compare, text similarity measures6. language generation, Compare, opinionated natural language generation7. automatic style transfer, Evaluate-for, language generation8. language understanding, Is-a-Prerequisite-of, language generation9. neural networks, Is-a-Prerequisite-of, language generation10. language generation, Evaluate-for, understanding the knowledge domain
1. (neural machine translation, incorporates, syntactic information)2. (neural machine translation, uses, syntactic trees)3. (neural machine translation, based on, a bidirectional tree encoder)
#### Triplets:1. KB-InfoBot, used-for, search Knowledge Bases2. KB-InfoBot, integrated-with, reinforcement learner3. Neural Belief Tracking (NBT), part-of, Dialogue State Tracking4. Dialogue State Tracking, is-a-prerequisite-of, task-oriented dialogue systems5. Deep Dyna-Q, compare, user simulator6. Deep Dyna-Q, integrated-with, world model7. Neural Belief Tracking (NBT), addressed-by, existing NBT model8. Dialogue State Tracking, achieved-by, Global-Locally Self-Attentive Dialogue State Tracker (GLAD)9. Multi-turn dialogue modelling, is-a-prerequisite-of, response selection10. Spatio-Temporal Matching network (STM), evaluate-for, response selection
### Triplets:1. GloVe, relies-on, co-occurrence statistics2. GloVe, introduce, relation vectors3. models, direct-learn, relation vectors4. GloVe, introduce, variant5. models, learn, semantic information6. morphology-based models, incorporate, morphemes7. models, neglect, latent meanings8. Latent Meaning Models (LMMs), adopt, strategies9. Morphological compositions, incorporate, latent meanings10. Latent Meaning Models (LMMs), employed, train11. Models, validate, feasibility12. Models, outperform, baselines13. DPPs, handle, obstacles14. DPP-based method, strengthen, extractive multi-document summarization15. Method, present, novel similarity measure16. DPP system, performs, competitively17. new dataset, present, literary events18. Literature, poses, complications
### Triplets:1. semantic dependency parsing, Used-for, structured projection of intermediate gradients2. semantic dependency parsing, Is-a-Prerequisite-of, neural techniques for end-to-end computational argumentation mining3. structured projection of intermediate gradients, Evaluate-for, backpropagating through neural networks4. structured projection of intermediate gradients, Compare, structured attention networks5. neural techniques for end-to-end computational argumentation mining, Evaluate-for, deep neural architecture
### Triplets:1. discourse relation, involves, discourse coherence2. explicit signals, indicate, discourse relations3. multiple relations, can be simultaneously operative, two segments 4. discourse segments, connected by, multiple relations5. textual mentions, exploit, relations6. discourse coherence, posits, relations between discourse segments7. discourse segments, endorse, multiple relations 8. temporal relations, dictate, causal relations9. temporal and causal relations, closely related, to each other10. constrained conditional models, enforce, constraints11. joint inference framework, results in, significant improvement
### Triplets:1. knowledge inference, Used-for, reasoning2. reasoning, Compare, inference techniques3. reasoning, Is-a-Prerequisite-of, knowledge inference4. reasoning, Is-a-Prerequisite-of, neural network architectures5. external knowledge, Evaluate-for, neural natural language inference models6. external knowledge, Evaluative-for, neural networks7. neural networks, Part-of, neural reading comprehension model
### Concept: data text generation model1. (data text generation model, Used-for, code generation)2. (data text generation model, Used-for, semantic parsing)3. (data text generation model, Compare, neural network architectures)4. (data text generation model, Compare, representation learning)5. (data text generation model, Evaluate-for, structured outputs)6. (data text generation model, Evaluate-for, semantic relevance)7. (data text generation model, Is-a-Prerequisite-of, end-to-end training)8. (abstract syntax networks, Is-a-Prerequisite-of, data text generation model)9. (entity-centric neural architecture, Compare, data text generation model)
None.
### Triplets:1. morphological paradigm, Part-of, morphological traits2. morphological paradigm, Evaluate-for, morphological analysis3. morphological analysis, Is-a-Prerequisite-of, improved word estimates4. morphological analysis, Compare, low-frequency word estimates5. dialogue state tracking, Used-for, morph-fitted vectors6. word vector collection, Part-of, semantic quality7. language modeling task, Evaluate-for, experiments8. morphological disambiguation, Evaluate-for, dense representations9. language modeling data, Is-a-Prerequisite-of, morphological supervision10. text classification, Evaluate-for, segmented tokens
### Triplets:1. Global-Locally Self-Attentive Dialogue State Tracker, **Is-a-Prerequisite-of**, dialogue state tracking2. Dialogue State Tracker, **Used-for**, inferring user intentions3. Transferable Dialogue State Generator, **Compare**, Dialogue State Tracker4. DSTC2 task, **Evaluate-for**, dialogue state tracking5. Natural Language Understanding component, **Is-a-Prerequisite-of**, dialogue state tracking
(None)
(`<concept>`, `Compare`, `contextual word embeddings`)  (`<concept>`, `Part-of`, `word embeddings`)  (`<concept>`, `Compare`, `BERT`)  (`<contextual word embeddings>`, `Evaluate-for`, `NE models`)  (`<word embeddings>`, `Part-of`, `FastText`)  (`<non contextual subword embeddings>`, `Evaluate-for`, `named entity recognition`)  
### Triplets:1. level sentiment classification, Is-a-Prerequisite-of, Text classification2. level sentiment classification, Evaluate-for, Model training3. level sentiment classification, Evaluate-for, Sentiment expression4. Text classification, Evaluate-for, Level sentiment classification5. Word embeddings, Used-for, Capturing semantic regularities6. Neural network models, Used-for, Level sentiment classification7. Sentiment expression, Evaluate-for, Level sentiment classification
### Triplets:1. semantic parser, Used-for, natural language generation2. semantic parser, Evaluate-for, intermediate representation3. neural language model, Used-for, learning implicit representation4. neural language model, Compare, COREQA5. generation of rhythmic poetry, Used-for, implicit representation6. machine translation, Compare, translation of agents' messages7. neural language model, Evaluate-for, learning poetic devices8. semantic parser, Is-a-Prerequisite-of, question answering system9. COREQA, Evaluate-for, generation of natural answers10. machine-generated poems, Evaluate-for, human-written poems
### Triplets:1. language model, Used-for, natural language processing2. language model, Used-for, downstream task3. downstream task, Part-of, natural language processing4. language model, Evaluate-for, downstream task5. language model, Compare, downstream task6. language model, Is-a-Prerequisite-of, downstream task
(None)
(<neural text generation>, Used-for, <abstractive summarization>)(<neural text generation>, Compare, <sentence summarization using neural models>)(<neural text generation>, Compare, <abstractive document summarization>)(<neural text generation>, Evaluate-for, <abstractive document summarization>)(<neural text generation>, Compare, <natural language generation>)(<neural text generation>, Is-a-Prerequisite-of, <abstractive summarization>)
### Triplets:1. aspect sentiment, is-a-Prerequisite-of, sentiment analysis2. aspect sentiment, Used-for, aspect-based sentiment analysis3. aspect sentiment, Evaluate-for, sentiment classification4. aspect sentiment, Part-of, ABSA (Aspect-based sentiment analysis)5. sentiment analysis, Evaluate-for, aspect sentiment6. sentiment analysis, Used-for, aspect-based sentiment analysis7. sentiment analysis, Part-of, ABSA (Aspect-based sentiment analysis)8. aspect sentiment, g) Hyponym-Of, sentiment polarity9. sentiment polarity, is-a-Prerequisite-of, aspect sentiment
(`<content>`, Induce, speech translation)(speech translation, has-a, end-to-end model)(speech translation, comprises, concatenation-based context-aware ST model)(speech translation, uses, adaptive feature selection)(concatenation-based context-aware ST model, improves, pronoun and homophone translation)(concatenation-based context-aware ST model, reduces, latency)(concatenation-based context-aware ST model, reduces, flicker)(concatenation-based context-aware ST model, delivers, higher quality for simultaneous translation)(Encoder pre-training, is-applied-to, end-to-end Speech Translation (ST))(ST encoder, starts-with, processing the acoustic sequence)(ST encoder, behaves-like, an MT encoder)(ST encoder, incorporates, pre-trained models)(adaptor module, alleviates, representation inconsistency)(multi-teacher knowledge distillation method, preserves, pre-training knowledge)(Our method, achieves, state-of-the-art BLEU scores)(cascaded ST counterpart
### Triplets:1. word-embedding models, gained popularity, remarkable performance2. vectors, exhibit compositionality, small angle away3. word vectors, learned using Skip-Gram model, additive compositionality4. symbol frequencies, added information, parameters of SDR models5. Skip-Gram model, optimal, Globerson and Tishby6. word embeddings, provide point representations, useful semantic information7. neural network architectures, incorporate pretrained context embeddings
### Triplets:1. context sensitive embeddings, Used-for, predicting prepositional phrase (PP) attachments2. context sensitive embeddings, Is-a-Prerequisite-of, learning domain-sensitive and sentiment-aware embeddings3. context sensitive embeddings, Evaluate-for, sentiment classification4. context sensitive embeddings, Evaluate-for, improving the accuracy of the PP attachment model5. context sensitive embeddings, Part-of, word embeddings6. word embeddings, Used-for, semantic representations of words7. word embeddings, Part-of, deep neural network architectures
### Triplets:1. nli model, Used-for, Natural Language Inference2. Natural Language Processing, Is-a-Prerequisite-of, nli model3. nli model, Part-of, Deep Learning Model
### Triplets:1. VAT, improves model robustness2. CRF, boosts accuracy for sequence models3. SeqVAT, improves sequence labeling performance4. Spider-Syn, eliminates explicit correspondence between NL questions and table schemas5. AREC, generates alignment rationale explanations6. LOVE, makes model robust to OOV7. RSMI, improves adversarial robustness
### Triplets:1. domain learning, Used-for, multi-task learning2. domain learning, Evaluate-for, adversarial multi-task learning3. domain learning, Is-a-Prerequisite-of, state-of-the-art accuracy4. domain learning, Used-for, domain-specific knowledge5. domain learning, Is-a-Prerequisite-of, Hierarchical recurrent neural network6. Hierarchical recurrent neural network, Used-for, relation detection7. domain learning, Is-a-Prerequisite-of, Feature-Rich Networks
1. (flat NER, Is-a-Prerequisite-of, nested named entity)2. (machine reading comprehension, Evaluate-for, nested named entity)3. (Bidirectional LSTM, Used-for, nested named entity)4. (graph convolutional network, Used-for, nested named entity)
### Triplets:1. approach, Used-for, visual question answering2. Reinforced Dynamic Reasoning network, Evaluate-for, meaningful questions3. Multi-perspective Convolutional Cube, Is-a-Prerequisite-of, understanding conversation4. Generative Domain-Adaptive Nets, Evaluate-for, performance improvement5. neural paragraph-level question answering models, Is-a-Prerequisite-of, answering multiple documents6. conversational machine reading comprehension, Is-a-Prerequisite-of, considering conversation history7. conversational machine reading comprehension, Compare, multi-turn interactions
1. (Automatic argument generation, Used-for, reasoning)2. (Automatic argument generation, Evaluate-for, effectiveness)3. (Automatic argument generation, Compare, template-based pun generation)4. (Automatic argument generation, Evaluate-for, system arguments)5. (Automated evaluation, Evaluate-for, argument generation)6. (Automatic argument generation, Is-a-Prerequisite-of, refutation)7. (Automatic argument generation, Is-a-Prerequisite-of, refutation procedure)8. (Automatic argument generation, Is-a-Prerequisite-of, content richness)
### Triplets:1. neural machine translation model, used-for, encoding source sentences2. neural machine translation model, Compare, conventional approaches (LSTM unit and GRU)3. neural machine translation model, Evaluate-for, gradient diffusion reduction4. neural machine translation model, Is-a-Prerequisite-of, state-of-the-art Neural Machine Translation (NMT)5. deep architecture, Evaluate-for, gradient diffusion reduction6. deep architecture, Is-a-Prerequisite-of, state-of-the-art Neural Machine Translation (NMT)7. Neural Machine Translation (NMT), Part-of, Sequential LSTM8. Neural Machine Translation (NMT), Compare, Sequence-to-Dependency Neural Machine Translation (SD-NMT)9. neural machine translation model, Is-a-Prerequisite-of, Bandit structured prediction10. neural machine translation model, Is-a-Prerequisite-of, posterior regularization11. neural machine translation model, Is-a-Prerequisite-of, distortion
### Triplets:1. human attention, is-a-Prerequisite-of, keyphrase extraction2. human attention, used-for, integrating into keyphrase extraction models3. human attention, is-a-Prerequisite-of, unsupervised models4. keyphrase extraction, used-for, understanding, organizing and retrieving text content5. keyphrase extraction, is-a-Prerequisite-of, automated keyphrase extraction6. keyphrase extraction, Compare, human attention
### Extracted Concepts:1. Neural machine translation2. End-to-end (E2E) translation3. Machine translation4. Phrase-based translation5. Encoder-decoder architecture6. Transformer translation system7. Soft-attention based NMT8. Self-attention networks (SAN)9. Source sequence10. Target sequence### Triplets:1. (Neural machine translation, Is-a-Prerequisite-of, End-to-end translation)2. (Machine translation, Is-a-Prerequisite-of, Neural machine translation)3. (Phrase-based translation, Is-a-Prerequisite-of, Neural machine translation)4. (End-to-end translation, Evaluate-for, Improved translation quality)5. (Encoder-decoder architecture, Used-for, Machine translation)6. (Transformer translation system, Is-a-Prerequisite-of, Self-attention networks)7. (Soft-attention based NMT, Is-a-Prerequisite-of, Hard
### Triplets:1. Shot learning, Used-for, building dialog systems2. Shot learning, Is-a-Prerequisite-of, end-to-end learning framework
### Triplets:1. aspect-category-opinion-sentiment, Is-a-Prerequisite-of, aspect-based sentiment analysis2. Restaurant-ACOS, Is-a-Prerequisite-of, Aspect-Category-Opinion-Sentiment Quadruple Extraction3. Laptop-ACOS, Is-a-Prerequisite-of, Aspect-Category-Opinion-Sentiment Quadruple Extraction
### Triplets:1. embeddings, capture, linguistic regularities2. embeddings, transfer, across languages3. embeddings, connect, separate monolingual word embeddings4. embeddings, use, for Arabic word embeddings5. embeddings, evaluated, intrinsic evaluation6. embeddings, generate, from text7. embeddings, train, on the sentence pairs8. embeddings, explore, various features9. embeddings, improve, the accuracy of word segmentation10. embeddings, apply, for machine translation and POS tagging11. embeddings, compose, word representations12. embeddings, outperform, previous best greedy parser13. embeddings, model, similarity and dissimilarity of arguments
(None)
### Concepts:1. Text similarity measures2. Sequential models3. Deep learning4. Natural language generation5. DNA sequence alignment algorithms6. TextFlow7. Semantic units of natural language8. Distributional representation of sentences9. Geometry of sentences10. Low-rank subspace11. Sentence representations12. Neural network models13. Semantic textual similarity tasks14. Pretrained language models15. Contrastive learning16. Self-supervised sentence representation transfer17. Reference-free machine translation evaluation18. Sentence meta-embeddings19. Cross-lingual semantic representations20. State-of-the-art methods for STS21. Mutual information22. Dependency parse structures23. Sentence embeddings24. NLI datasets25. Unsupervised dependency graph network26. Dependency relations modeling27. Training objectives28. Sentence similarity29. Interpretation methods30. Updated headline
### Triplets:1. neural machine translation (NMT), Used-for, translation tasks2. neural machine translation (NMT), Is-a-Prerequisite-of, machine translation3. Syntax-aware NMT, Used-for, improving BLEU score4. Seq2Seq model, Part-of, Machine Translation5. Neural Machine Translation, Compare, Statistical Machine Translation6. hybrid architectures, Is-a-Prerequisite-of, further improvements7. Paraphrastic sentence embeddings, Used-for, Semantic textual similarity competition
(`<context>`, Is-a-Prerequisite-of, semantic role labeling (SRL))  (Deep Learning Model, Used-for, semantic role labeling (SRL))  (Constrained Decoding, Used-for, semantic role labeling (SRL))  (Highway BiLSTM architecture, Used-for, semantic role labeling (SRL))  (Progressive Learning Model, Used-for, semantic role labeling (SRL))  (Progressive Neural Network, Part-of, Progressive Learning Model)  (Gated Recurrent Adapters, Part-of, Progressive Learning Model)  (Chinese SemBank Corpus, Part-of, semantic role labeling (SRL))  (Question-Answer driven Semantic Role Labeling (QA-SRL) Annotations, Part-of, QA-SRL Model)  (QA-SRL Bank 2.0, Part-of, QA-SRL Model)  (End-to-End Approach, Part-of, Semantic role labeling (SRL))  (Semantic Proto
### Triplets:1. Chain of thought prompting, Used-for, improve reasoning capabilities2. Chain of thought prompting, Used-for, incorporate external tools3. Chain of thought prompting, Is-a-Prerequisite-of, transferring reasoning capabilities to smaller models
### Triplets:1. unsupervised constituency parsing, Used-for, unsupervised dependency parsers2. unsupervised constituency parsing, Is-a-Prerequisite-of, probabilistic generative models3. unsupervised constituency parsing, Compare, supervised models4. unsupervised constituency parsing, Is-a-Prerequisite-of, sequence labeling5. unsupervised dependency parsers, Compare, generative models6. neural semantic parsers, Evaluate-for, unsupervised constituency parsing7. unsupervised constituency parsing, Is-a-Prerequisite-of, neural approaches to syntax
### Triplets:1. domain question answering <=> is-a-Prerequisite-of <=> end-to-end question answering system called COREQA2. domain question answering <=> part-of <=> Neural Network-based KB Question Answering (KBQA)3. Neural Network-based KB Question Answering (KBQA) <=> evaluate-for <=> end-to-end neural network model for question representation4. hierarchical recurrent neural network <=> hyponym-Of <=> Neural Network-based KB Question Answering (KBQA)5. end-to-end neural network model for question representation <=> used-for <=> representing the questions and their corresponding scores6. domain question answering <=> compare <=> geometric reasoning in visual question answering7. Question Answering model <=> used-for <=> text-based multiple choice question answering
### Triplets:1. cross lingual transfer, is-a-prerequisite-of, parallel corpora2. cross lingual transfer, is-a-prerequisite-of, multilingual sense-annotated resource3. cross lingual transfer, evaluate-for, effectiveness4. cross lingual transfer, used-for, boosting performance5. cross lingual transfer, conjuction, multilingual transfer setting6. cross lingual transfer, compare, monolingual task
(`<concept>`, `Part-of`, `word embeddings`)  (`<concept>`, `Compare`, `contextual embeddings`)  (`<concept>`, `Hyponym-Of`, `dynamic contextualized word embeddings`)  
### Triplets:1. automatic generation, Evaluate-for, summary generation2. sequence-to-sequence model, Is-a-Prerequisite-of, summary generation3. natural language generation, Used-for, summary generation
### Concept: text generation1. (text generation, Is-a-Prerequisite-of, neural language modeling)2. (text generation, Evaluate-for, fluency)3. (text generation, Evaluate-for, semantic coherence)4. (neural language modeling, Evaluate-for, predicting text)5. (neural language modeling, Is-a-Prerequisite-of, text generation)6. (language model, Is-a-Prerequisite-of, text generation)7. (auto-regressive model, Evaluate-for, local fluency)
### Triplets:1. entity recognition, Used-for, relation extraction2. Named entity recognition (NER), Is-a-Prerequisite-of, relation extraction3. gazetteers, Used-for, named entity recognition4. graph neural networks, Used-for, incorporating gazetteers5. Named entity recognition (NER), Is-a-Prerequisite-of, named entity recognition6. Named entity recognition (NER), Is-a-Prerequisite-of, Gazetteers7. gazetteers, Used-for, machine learning8. document-level relation extraction, Is-a-Prerequisite-of, Named entity recognition9. relation extraction, Is-a-Prerequisite-of, entity recognition10. entity mentions, Part-of, entity recognition11. gazetteers, Used-for, resolving ambiguities12. entity mentions, Part-of, relation extraction
### Extracted Concepts:1. Knowledge Bases (KBs)2. Neural dialogue agents3. Reinforcement learner4. End-to-end training5. Morpheme segmentation6. Benchmarks datasets7. Image captioning8. Language understanding9. Reading Comprehension (RC)10. Natural Language Inference (NLI)11. Multi-task learning12. Automatic generation of medical imaging reports13. Section-based thematic similarity14. Online petitions15. Semantic parsing16. Extractive reading comprehension systems17. SQuADRUn dataset18. Premise and Hypothesis in NLI19. Information extraction### Triplets:1. (Knowledge Bases, Used-for, Neural dialogue agents)2. (Neural dialogue agents, Evaluate-for, Reinforcement learner)3. (End-to-end training, Used-for, Morpheme segmentation)4. (Benchmarks datasets, Is-a-Pr
### Triplets:1. visually grounded language <> is-a-prerequisite-of <> natural language understanding2. visually grounded language <> used-for <> learning syntactic representations3. visually grounded language <> used-for <> natural language navigation4. visually grounded language <> part-of <> visually grounded model of speech perception5. visually grounded language <> compare <> multimodal language-vision research6. multimodal language-vision research <> compare <> visually grounded language models
### Extracted Concepts:1. Feature attribution methods2. Model building3. Machine learning practitioners4. Text classifiers5. Unintended bias6. Identity terms7. L2 distance loss8. Classifier performance9. Scarce data setting10. Toxic terms11. Demographic identity-terms12. Selection bias13. Discrimination distribution14. Debiasing training framework15. Document-level label bias16. Keyword bias17. Data-level manipulations18. Model-agnostic debiasing framework19. Counterfactual inference20. Speech-to-speech translation model21. Self-supervised discrete speech encoder22. Sequence-to-sequence speech-to-unit translation model23. Direct S2ST model24. Dual modality output### Triplets:1. (Feature attribution methods, help interpret, predictions of complex models)2. (Feature attribution methods, integrated into,
(`<concept>`, `Used-for`, `relation extraction`)  (`<relation extraction>`, `Evaluate-for`, `model performance`)  (`<relation extraction>`, `Part-of`, `relation extraction framework`)  (`<technique>`, `Compare`, `dynamic transition matrix`)  (`<model>`, `Is-a-Prerequisite-of`, `training data`)  (`<class ties>`, `Hyponym-Of`, `relations in distantly supervised scenario`)  
### Triplets:1. neural semantic parser, Used-for, converting natural language utterances to intermediate representations2. neural semantic parser, Compare, interpretable and scalable3. neural semantic parser, Evaluate-for, annotated logical forms or their denotations4. neural semantic parser, Is-a-Prerequisite-of, state-of-the-art results on SPADES and GRAPHQUESTIONS5. semantic parser, Used-for, inferring AMR graphs6. semantic parser, Used-for, generating corrective referring expressions7. semantic parser, Compare, end-to-end using annotated logical forms or their denotations8. semantic parser, Evaluate-for, generating corrective referring expressions9. semantic parser, Compare, new learning algorithm combining reinforcement learning and maximum marginal likelihood10. word sememes, Evaluate-for, improving word representation learning11. word sememes, Is-a-Prerequisite-of, word representation learning12. word sememes, Used-for, capturing meanings of
1. (query-based summarization, Is-a-Prerequisite-of, encode-attend-decode paradigm)2. (memory augmented neural model, Used-for, Chinese poem generation)3. (neural models, Used-for, generative tasks)4. (neural models, Is-a-Prerequisite-of, abstract rule learning)5. (SWAP-NET, Used-for, extractive summarization)6. (Seq2Seq model, Evaluate-for, semantic representation learning)7. (template Reranking, Evaluate-for, informativeness)8. (base RNN generator, Compare, discriminators)9. (GP-GNNs, Is-a-Prerequisite-of, relational reasoning)10. (commonsense knowledge, Is-a-Prerequisite-of, diverse essay generation)
1. (Document-level event argument extraction, Is-a-Prerequisite-of, end-to-end model).2. (Document-level event argument extraction, Evaluate-for, detecting event arguments beyond sentence boundaries).3. (Event role filler, Part-of, document-level event extraction).4. (Global neural generation-based framework, Used-for, document-level event argument extraction).5. (Heterogeneous Graph-based Interaction Model, Used-for, solving challenges of document-level event extraction).6. (EAE, Compare, structured event generation across multiple sentences).7. (PAIE, Is-a-Prerequisite-of, sentence-level and document-level EAE).8. (Multilingual pre-trained generative language models, Evaluate-for, cross-lingual event argument extraction).9. (FEAE, Is-a-Prerequisite-of, reasoning in event frame-level scope).10. (Code4Struct, Compare, text-to-structure translation capability with EAE).11. (FewDocAE,
### Triplets:1. (Aspect-based sentiment analysis, Part-of, Sentiment analysis)2. (Aspect-based sentiment analysis, Evaluate-for, Sentiment prediction)3. (Aspect-based sentiment analysis, Compare, General sentiment analysis)4. (Aspect-based sentiment analysis, Conjunction, Aspect extraction)5. (Aspect-based sentiment analysis, Is-a-Prerequisite-of, Aspect-term sentiment analysis)6. (Aspect-based sentiment analysis, Evaluate-for, Sentiment polarity prediction)7. (Aspect extraction, Part-of, Aspect-based sentiment analysis)8. (Neural approach, Compare, Topic models)9. (Word embeddings, Conjunction, Distribution of word co-occurrences)10. (Word embedding models, Compare, Topic models)11. (Attention mechanism, Evaluate-for, Improving coherence)12. (Word embeddings, Hyponym-Of, Information Retrieval (IR) term weighting models)13. (Dependency parsing, Used-for, Information extraction)
### Triplets:1. End-to-end neural dialogue generation, Compare, Generating long and informative review text2. End-to-end neural dialogue generation, Compare, End-to-end neural dialogue generation3. A neural knowledge diffusion (NKD) model, Is-a-Prerequisite-of, End-to-end neural dialogue generation4. Dialogue systems, Used-for, Retrieval-Enhanced Adversarial Training (REAT) method5. End-to-end neural dialogue generation, Is-a-Prerequisite-of, Semantically controlled neural response generation6. Structured hierarchical graph, Is-a-Prerequisite-of, Semantically controlled neural response generation
(`<entity relation extraction>`, Is-a-Prerequisite-of, `Distant supervision`)  (`<entity relation extraction>`, Used-for, `Relation extraction`)  (`<entity relation extraction>`, Is-a-Prerequisite-of, `Relation extraction methods`)  (`<Relation extraction>`, Used-for, `Knowledge Base Population`)  (`<Relation extraction>`, Is-a-Prerequisite-of, `Graph neural network`)  (`<Relation extraction>`, Used-for, `Open Information Extraction`)  (`<Relation extraction>`, Is-a-Prerequisite-of, `Machine reading comprehension`)  
(`<concept>`, Is-a-Prerequisite-of, mental health counseling)(`<concept>`, Used-for, computational framework)(`<concept>`, Is-a-Prerequisite-of, paraphrase generation)(`<concept>`, Used-for, downstream tasks)
1. (political debate, Is-a-Prerequisite-of, argument mining)2. (argument mining, Evaluate-for, political debate)3. (political debate, Compare, knowledge base population)4. (newspaper reports, Used-for, constructing discourse networks)5. (political debate, Used-for, understanding democratic political decision making)6. (political debate, Used-for, comparing candidates' positions)7. (political debate, Used-for, citizen engagement)8. (political debate, Is-a-Prerequisite-of, argumentative components identification)9. (political debate, Used-for, typology investigation)10. (political debate, Used-for, empirical task addressing)11. (political debate, Used-for, releasing new corpus)12. (political debate, Used-for, sharing software with research community)13. (political debate, Used-for, promoting scientific advancement)14. (argumentative components identification, Is-a-Prerequisite-of, classifying as premises
### Extracted Concepts:1. Neural semantic parser2. Natural language utterances3. Predicate-argument structures4. Intermediate domain-general natural language representations5. Text similarity measures6. Sequential models7. Natural language generation8. Conditional language models9. Seq2Seq model10. Transfer learning11. Bidirectional LSTM12. Named entity recognition (NER)13. Variational auto-encoders (VAEs)14. Story ending generation15. High entropy generation16. Large-scale pretrained language models17. Question answering (QA)18. Open-domain dialog19. Conversational QA (ConvQA)20. BERT-based classifier21. Online social networks22. User geolocation23. Influence functions24. Content moderation25. Hate speech detection26. Inductive bias27. Relation extraction28. Explanation-guided representations### Triplets:1. (natural language explanation
### Concept: text generation model1. (text generation model, Utilizes, Generative Domain-Adaptive Nets)2. (Generative Domain-Adaptive Nets, Trained using, reinforcement learning)3. (text generation model, Employs, sequence-to-sequence learning)4. (text generation model, Yields, 74.4% execution accuracy)5. (text generation model, Outperforms, rule-based system)6. (text generation model, Achieves, state-of-the-art on all metrics)7. (text generation model, Incorporates, knowledge for dialogue generation)
### Triplets:1. response generation, Used-for, natural language understanding2. response generation, Part-of, dialog systems3. response generation, Is-a-Prerequisite-of, diverse responses4. response generation, Compare, traditional systems5. dialog systems, Used-for, response generation6. Seq2Seq models, Compare, Iterative training process
### Extracted Concepts:1. neural language model2. Long Short-Term Memory (LSTM)3. generative neural language model4. discriminative weighted finite state machine5. LSTM (Long Short-Term Memory) language model6. Universal Language Model Fine-tuning (ULMFiT)7. neural caching model8. continuous probability density function model### Triplets:1. (trained language model, uses, neural language model)2. (trained language model, improved by, LSTM)3. (trained language model, used for, poetry generation)4. (trained language model, fine-tuning method, Universal Language Model Fine-tuning)5. (trained language model, benefited from, neural caching model)6. (trained language model, modeled using, continuous probability density function model)
### Triplets:1. binomial neural topic model, Is-a-Prerequisite-of, neural topic models2. neural topic models, Used-for, automatic topic extraction3. binomial neural topic model, Compare, Bidirectional Adversarial Topic (BAT) model4. binomial neural topic model, Evaluate-for, text clustering5. Bidirectional Adversarial Topic (BAT) model, Is-a-Prerequisite-of, Gaussian-BAT6. Bidirectional Adversarial Topic (BAT) model, Used-for, topic modeling7. Gaussian-BAT, Is-a-Prerequisite-of, text clustering
### Triplets:1. question answering model - Used-for - answering questions2. question answering model - Part-of - end-to-end question answering system3. end-to-end question answering system - Compare - gated self-matching networks4. end-to-end question answering system - Evaluate-for - generating natural answers5. gated self-matching networks - Is-a-Prerequisite-of - pointer networks6. gated self-matching networks - Used-for - reading comprehension style question answering7. reading comprehension style question answering - Evaluate-for - answering questions
(`<concept>`, `Enhances`, `language representation models`)  (`<concept>`, `Utilizes`, `large-scale textual corpora`)  (`<concept>`, `Incorporates`, `knowledge graphs`)
### Triplets:1. translation quality, Depends-on, sizable parallel corpora2. translation quality, Determines, NMT performance3. translation quality, Determines, sentiment preservation performance4. translation quality, Determines, content preservation performance5. translation quality, Determines, BLEU score improvements6. translation quality, Evaluated-by, empirical results7. translation quality, Evaluated-by, human evaluations
1. (relation extraction, Used-for, finding unknown relational facts from plain text)2. (relation prediction, Is-a-Prerequisite-of, relation extraction)3. (relation detection, Is-a-Prerequisite-of, relation prediction)
(<query concept>, Compare, Argument Detection)(event argument extraction eae, Is-a-Prerequisite-of, Frame-aware Event Argument Extraction)(event argument extraction eae, Compare, Joint extraction of entities and relations from unstructured texts)(event argument extraction eae, Compare, Zero-shot cross-lingual event argument extraction)(event argument extraction eae, Compare, Few-shot Document-Level Event Argument Extraction)(event argument extraction eae, Used-for, Document-level event argument extraction)
### Triplets:1. reasoning task, is-a-prerequisite-of, understanding2. reasoning task, involves, inference3. reasoning task, involves, modeling4. reasoning task, has, challenges5. inference, involves, modeling6. modeling, involves, inference
(`<concept>`, `Used-for`, `COREQA`)(`<concept>`, `Compare`, `Text similarity measures`)(`<concept>`, `Is-a-Prerequisite-of`, `Emotion detection from natural language`)(`<concept>`, `Conjunction`, `Semantic parsing`)(`<concept>`, `Hyponym-Of`, `Named entity recognition (NER)`)(`<concept>`, `Part-of`, `Opinionated Natural Language Generation (ONLG)`)(`<concept>`, `Evaluate-for`, `Knowledge base completion`)(`<concept>`, `Used-for`, `Semantic graph parsing`)
### Triplets: 1. recurrent neural tensor network, Compare, recurrent neural networks2. recurrent neural tensor network, Is-a-Prerequisite-of, tensor network3. LSTM, Part-of, recurrent neural tensor network4. Recurrent Neural Networks, Compare, recurrent neural tensor network
1. (neural machine translation, based-on, bi-directional LSTMs)2. (neural machine translation, employs, convolutional layers)3. (neural machine translation, achieves, competitive accuracy)4. (Deep Neural Networks, enhances, Neural Machine Translation)5. (Linear Associative Units (LAU), reduces, gradient propagation path)6. (source syntax, incorporated-into, Neural Machine Translation)7. (Sequence-to-Dependency Neural Machine Translation (SD-NMT), outperforms, baselines)8. (distortion models, incorporate, word reordering knowledge)9. (chunk-based decoders, improve, translation performance)10. (Multi-modal Neural Machine Translation model, bridges, gap between image description and translation)11. (zero-resource NMT, improves, accuracy over baseline)12. (bidirectional tree encoder, outperforms, sequential attentional model)
### Triplets:1. crossmodal attention, Used-for, identify information in models2. crossmodal attention, Evaluate-for, weighting words in an input sequence3. attention layers, Compare, explicitly weight input components' representations4. attention mechanisms, Used-for, boost performance on NLP tasks
1. (CLIP word embeddings, outperform, GPT-2)2. (CLIP, adapt, GPT-2 architecture)3. (CLIP, forms, fine-grained semantic representations of sentences)4. (contrastive visual semantic pretraining, significantly mitigates, anisotropy found in GPT-2)5. (contrastive visual semantic pretraining, evaluate-for, anisotropy mitigation)6. (feedback simulator, generate, high-quality NL feedback)7. (feedback simulator, help achieve, comparable error correction performance)8. (lilGym, based on, 2,661 highly-compositional human-written natural language statements)9. (lilGym, available at, -)
### Concept: text classification task1. (text classification task, Used-for, multi-task learning)2. (text classification task, Is-a-Prerequisite-of, feature extraction)3. (text classification task, Is-a-Prerequisite-of, shared knowledge transfer)4. (text classification task, Evaluate-for, performance improvement)5. (text classification task, Is-a-Prerequisite-of, cognitive NLP systems)6. (feature extraction, Used-for, sentiment analysis)7. (feature extraction, Used-for, sarcasm detection)8. (multi-task learning, Evaluate-for, performance improvement)9. (cognitive NLP systems, Compare, traditional text-based features)10. (CNN, Used-for, sentiment polarity and sarcasm detection)11. (end-to-end ASR, Compare, conventional DNN/HMM systems)12. (end-to-end ASR, Used-for, sequence alignment)13. (attention-based methods, Compare, connection
### Triplets:1. neural language model, Used-for, language modeling2. neural language model, Compare, LSTM (Long Short-Term Memory)3. neural language model, Compare, sentence-based model4. neural language model, Evaluate-for, generation of conversational text5. neural language model, Is-a-Prerequisite-of, Affect-LM6. neural language model, Compare, pure sentence-based model7. neural language model, Used-for, document context incorporation8. neural language model, Used-for, broader document context representation
### Triplets:1. Biomedical entity linking, Used-for, Text alignment2. Entity linking, Is-a-Prerequisite-of, Entity disambiguation3. Entity embeddings, Compare, Word embeddings4. Named entity recognition, Part-of, Named entity disambiguation5. Entity linking, Evaluate-for, Entity embeddings6. Biomedical entity linking, Evaluate-for, Specialized in-domain tasks7. Entity linking, Part-of, Knowledge Base Question Answering8. Entity linking, Evaluate-for, Named entity recognition9. Entity embeddings, Compared, Canonical Wikipedia articles
(`<Concept>`, `Part-of`, `information extraction`)  (`<Concept>`, `Compare`, `sentence-level event extraction`)  (`<Concept>`, `Used-for`, `extracting informative arguments of events from news articles`)  (`sentence-level event extraction`, `Is-a-Prerequisite-of`, `<Concept>`)  
(None)
### Triplets:1. Latent topic, Inducing, Latent vector representations2. Latent topic, Captures, Fine grained differences in topic assignments3. Latent topic, Enhances, Coherence between generated topics4. Latent topic, Used-for, Ranking noun phrases extracted from documents5. Latent topic, Enables, Efficient inference and learning6. Latent topic, Associated with, Continuous vector space7. Latent topic, Associated with, Ancestral and fraternal topic distribution
(`<Content>`, Compare, `aware news representation`)  (`news representation`, Is-a-Prerequisite-of, `aware news representation`)  (`aware news representation`, Evaluate-for, `improving distributional vector spaces`)  
### Triplets:1. COREQA, incorporates, copying and retrieving mechanisms2. COREQA, generates, natural answers3. Abstractive summarization, aims to generate, a shorter version4. ArgRewrite, is, a corpus of between-draft revisions5. Query-based summarization, highlights, relevant points6. Selective encoding model, outperforms, baseline models7. Word representations, are modeled, by Hierarchical Dirichlet Process
### Triplets:1. (compositional distributional semantics, Is-a-Prerequisite-of, semantic relatedness)2. (compositional distributional semantics, Is-a-Prerequisite-of, entailment)3. (compositional distributional semantics, Evaluate-for, models)4. (compositional distributional semantics, Compare, Functional Distributional Semantics)5. (APTs, Compare, Anchored Packed Trees)6. (APTs, Is-a-Prerequisite-of, grammatical type)7. (APT, Used-for, semantic composition)8. (multiword expressions, Evaluate-for, meaning derivation)9. (BERT embeddings, Used-for, semantic composition improvement)10. (Poincaré embeddings, Evaluate-for, hierarchical relationship capturing)11. (distributional model, Evaluate-for, brain activity decoding)12. (ABBA model, Compare, S-STRUCT)13. (grounded visual data, Compare, large
### Triplets:1. (named entity, Is-a-Prerequisite-of, Named Entity Recognition)2. (Named Entity Recognition, Evaluate-for, NLP applications)3. (Named Entity Recognition, Is-a-Prerequisite-of, Named Entity Disambiguation)4. (Metonymy Resolution, Evaluate-for, Named Entity Recognition)5. (Chinese hypernym prediction, Is-a-Prerequisite-of, Entity Linking)6. (Named Entity Disambiguation, Evaluate-for, Named Entity Recognition)7. (Named Entity Recognition, Is-a-Prerequisite-of, Multilingual Neural Named Entity Recognition)
### Triplets:1. task oriented dialogue, Used-for, knowledge retrieval2. task oriented dialogue, Is-a-Prerequisite-of, reinforcement learning3. task oriented dialogue, Part-of, dialogue systems4. task oriented dialogue, Evaluate-for, task success prediction models5. task oriented dialogue, Is-a-Prerequisite-of, dialogue state tracking6. dialogue systems, Part-of, task oriented dialogue7. reinforcement learning, Is-a-Prerequisite-of, task success prediction models8. dialogue state tracking, Is-a-Prerequisite-of, joint goal accuracy9. task-oriented dialog datasets, Compare, natural language inference dataset10. task success, Compare, joint goal accuracy11. information density, Evaluate-for, task success12. task-oriented dialogue systems, Is-a-Prerequisite-of, user demand identification13. working memory model, Is-a-Prerequisite-of, dialog response generation14. incremental learning framework, Evaluate-for
### Triplets:1. spoken dialogue system, Is-a-Prerequisite-of, belief tracker2. spoken dialogue system, Used-for, achieving the goal3. belief tracker, Part-of, spoken dialogue system4. neural model, Evaluate-for, effective at achieving the goal5. neural model, Part-of, spoken dialogue system6. belief tracker, Evaluate-for, estimating the user’s goal7. Neural Belief Tracking (NBT) framework, Is-a-Prerequisite-of, overcoming problems8. belief tracker, Compare, estimate user goals and requests9. belief tracker, Compare, track dialogue believes10. belief tracker, Conjunction, Global-Locally Self-Attentive Dialogue State Tracker (GLAD)
### Triplets:1. document modeling, Used-for, natural language understanding2. document modeling, Used-for, sentence extraction3. document modeling, Is-a-Prerequisite-of, document summarization4. document modeling, Is-a-Prerequisite-of, answer selection5. sentence extraction, Used-for, document summarization6. sentence extraction, Used-for, answer selection
### Extracted Concepts:1. Dialogue state tracker2. Modular dialogue systems3. DST (Dialogue State Tracker)4. Meta-Reinforced Multi-Domain State Generator (MERET)5. Multi-domain dialogue system### Triplets:- (multi-party dialogue, Is-a-Prerequisite-of, dialogue state tracker)- (multi-party dialogue, Is-a-Prerequisite-of, meta-reinforced multi-domain state generator)- (meta-reinforced multi-domain state generator, Hyponym-Of, dialogue state tracker)- (modular dialogue systems, Used-for, meta-reinforced multi-domain state generator)- (modular dialogue systems, Is-a-Prerequisite-of, dialogue state tracker)- (dialogue state tracker, Compare, meta-reinforced multi-domain state generator)
### Answer:1. (Multimodal sentiment analysis, Is-a-Prerequisite-of, LSTM-based model)2. (Opinionated Natural Language Generation (ONLG), Evaluate-for, User agendas)3. (Data-driven architecture, Is-a-Prerequisite-of, Generative grammars)4. (Conditional generation, Compare, Topic models)5. (Volatility prediction, Evaluate-for, Sentiment analysis methods)6. (Subword units, Is-a-Prerequisite-of, Neural machine translation (NMT))7. (Target-oriented sentiment classification, Is-a-Prerequisite-of, RNN with attention)8. (Aspect-based sentiment analysis, Part-of, Sentence)9. (Fine-grained text sentiment transfer (FGST), Evaluate-for, Sentiment intensity)10. (Event extraction, Part-of, Open domain)
### Triplets:1. task-oriented dialogue systems <b>Conjunction</b> summarization2. task-oriented dialogue systems <b>Is-a-Prerequisite-of</b> dialogue policy optimization3. task-oriented dialogue systems <b>Used-for</b> response selection4. Dialogue policy optimization <b>Is-a-Prerequisite-of</b> task-oriented dialogue summarization5. task-oriented dialogue systems <b>Is-a-Prerequisite-of</b> reward learning6. task-oriented dialogue systems <b>Compare</b> automatic generation of summaries from multiple news articles7. task-oriented dialogue systems <b>Is-a-Prerequisite-of</b> dialogue state tracking8. task-oriented dialogue summarization <b>Is-a-Prerequisite-of</b> task-oriented dialogue systems
1. (neural models, shown promise in many sub-areas, of natural language processing)2. (RNNs, used for modeling open-domain conversations)3. (RNNs, employed in the model for joint entity mentions and relations extraction)4. (recurrent neural networks, suffer from overfitting because of traditional training methods)5. (stochastic optimization, does not provide good estimates of model uncertainty)6. (RNNs, utilized in Hybrid Code Networks for dialog systems)7. (LSTM network, used for morphological inflection generation)
(None)
### Triplets:1. Recurrent neural networks (RNNs), Used-for, language modeling2. Stochastic optimization, Used-for, training of RNNs3. Markov Chain Monte Carlo, Is-a-Prerequisite-of, learning weight uncertainty in RNNs4. Stochastic gradient Markov Chain Monte Carlo, Used-for, learning weight uncertainty in RNNs5. Proposed approach, Is-a-Prerequisite-of, stochastic optimization6. Proposed approach, Compare, traditional training of RNNs using back-propagation through time7. Proposed approach, Evaluate-for, stochastic optimization8. Proposed approach, Evaluate-for, traditional training of RNNs using back-propagation through time
(None)
### Concept: textual adversarial1. (textual adversarial, Evaluate-for, adversarial attack)2. (textual adversarial, Used-for, generating adversarial examples)3. (adversarial attack, Is-a-Prerequisite-of, adversarial training)4. (adversarial training, Evaluate-for, robustness improvement)
#### Concept: text classification1. (text classification, Used-for, multi-task learning)2. (text classification, Is-a-Prerequisite-of, sentiment analysis)3. (text classification, Used-for, sarcasm detection)4. (text classification, Is-a-Prerequisite-of, dialogue act classification)5. (text classification, Is-a-Prerequisite-of, Cross-lingual text classification)6. (text classification, Part-of, neural network models)7. (text classification, Evaluate-for, document classification)
1. (Bayesian Network, Is-a-Prerequisite-of, Interpretability)2. (Example-Based GEC, Used-for, Language Learning)3. (Deep Neural Networks, Used-for, Accuracy)4. (CNN, Is-a-Prerequisite-of, Summarization)5. (Bayesian Networks, Conjunction, Deep Neural Networks)
### Concept: End-to-End Relation Extraction1. (Relation extraction, Used-for, finding unknown relational facts from plain text)2. (Relation extraction, Compare, conventional Open Information Extraction (Open IE) systems)3. (Relation extraction, Part-of, Knowledge Base Population methods)4. (Relation extraction, Compare, existing Knowledge Base Population methods)5. (Relation extraction, Compare, multi-lingual attention framework)6. (Relation extraction, Part-of, supervised models)7. (Relation extraction, Compare, generative probabilistic model)8. (Relation extraction, Compare, conventional Open Information Extraction (Open IE) systems)9. (Relation extraction, Compare, neural Open IE approach)10. (Relation extraction, Part-of, graph-based neural network model)11. (Relation extraction, Evaluate-for, state-of-the-art relation extraction technology)12. (Relation extraction, Is-a-Prerequisite-of, transforming the joint extraction task into a tagging problem)
(`<concept>`, Is-a-Prerequisite-of, `Long Short-Term Memory`)  (`Hidden Markov Model`, Used-for, `<concept>`)  (`LSTM`, Used-for, `Text Generation`)  (`Affect-LM`, Compare, `LSTM`)  (`Attention Model`, Used-for, `LSTM Networks`)  (`LSTM Noisy Channel Model`, Evaluate-for, `Disfluency Detection`)  (`Syntax-infused Variational Autoencoder`, Evaluate-for, `Text Generation`)  (`Birectional LSTM`, Is-a-Prerequisite-of, `Unknown Intent Detection`)  
### Concept: event commonsense evaluation1. (ACCENT, evaluates, response)2. (ACCENT, considers, events)3. (Event commonsense evaluation, crucial in, dialogues)4. (ACCENT, empowered by, commonsense knowledge bases)5. (ACCENT, extracts, event-relation tuples)6. (ACCENT, achieves, higher correlations with human judgments)7. (Event commonsense evaluation, focuses on, events)8. (Event commonsense evaluation, crucial in, general commonsense reasoning)
### Concept: text generation task1. (text generation task, based-on, event extraction)2. (text generation task, related-to, supervised learning)3. (text generation task, involves, natural language descriptions)4. (text generation task, requires, data labeling)5. (text generation task, connected-to, neural architecture)6. (text generation task, involves, abstract syntax networks)7. (text generation task, part-of, code generation task)8. (code generation task, used-for, semantic parsing)9. (code generation task, involves, linguistic knowledge)10. (code generation task, is-a-prerequisite-of, software development)11. (natural language descriptions, used-for, code generation task)12. (semantic parsing, used-for, code generation task)13. (code generation task, evaluate-for, logical dynamics learning)14. (code generation task, evaluate-for, temporal dynamics learning)15. (code generation task
### Triplets:1. cross-lingual continual learning, Is-a-Prerequisite-of, cross-lingual retrieval2. cross-lingual continual learning, Evaluate-for, reducing and counter-acting hate speech on social media3. cross-lingual continual learning, Evaluate-for, fine-grained entity typing (FGET)4. cross-lingual continual learning, Is-a-Prerequisite-of, high-quality datasets for dialogue models
(`<Semantic Units>`, `Part-of`, `Concepts in Human Languages`)  (`<Lexical Sememe Prediction Task>`, `Evaluate-for`, `Annotation Efficiency`)  (`<Lexical Sememe Prediction Task>`, `Evaluate-for`, `Consistency`)  (`<Existing Methods>`, `Used-for`, `Representing Meaning`)  (`<Existing Methods>`, `Evaluate-for`, `Dealing with Low-Frequency Words`)  (`<Novel Framework>`, `Used-for`, `Taking Advantage of Internal Character Information`)  (`<Novel Framework>`, `Used-for`, `Taking Advantage of External Context Information`)  (`<Novel Framework>`, `Used-for`, `Improving State-of-the-Art Baselines Performance`)
### Triplets:1. Sentence relation extraction, Used-for, Relation extraction.2. Sentence relation extraction, Is-a-Prerequisite-of, Evaluation.3. Sentence relation extraction, Used-for, Exploiting class ties.4. Sentence relation extraction, Is-a-Prerequisite-of, Relation extraction.5. Sentence relation extraction, Is-a-Prerequisite-of, Document-level relation extraction.6. Sentence relation extraction, Compare, Relation extraction.7. Sentence relation extraction, Compare, DocRED.
### Triplets:1. speech translation model, Used-for, style transfer2. speech translation model, Evaluate-for, adversarial stability training3. speech translation model, Compare, neural machine translation4. speech translation model, Is-a-Prerequisite-of, context-aware neural machine translation model
### Triplets:1. pretrained language, Is-a-Prerequisite-of, improved language understanding2. pretrained language, Used-for, fine-tuning language models3. large-scale textual corpora, Compare, Knowledge Graphs4. pre-trained language models, Evaluate-for, improving performance on various NLP tasks5. BERT, Is-a-Prerequisite-of, better machine reading comprehension6. language representation models, Have, Enhanced Language Representation using External Knowledge7. external knowledge bases, Used-for, improving BERT for Machine Reading Comprehension8. embeddings, Evaluate-for, learning better representations for rare and unseen words
### Extracted Concepts:1. Japanese PAS analysis2. Semi-supervised adversarial training3. Style transfer4. Neural machine translation (NMT)5. Adversarial examples6. Self-attentive neural networks7. Morphological tagging8. Multi-domain learning9. Named Entity Recognition (NER)10. Dialogue systems11. Retrieval-Enhanced Adversarial Training (REAT)12. Why-question answering (why-QA)### Triplets:1. (Japanese PAS analysis, Used-for, zero anaphora resolution)2. (Semi-supervised adversarial training, Used-for, Japanese PAS analysis)3. (Style transfer, Used-for, rephrasing the text)4. (NMT, Used-for, improving translation quality)5. (Adversarial examples, Used-for, impacting neural machine translation models)6. (Self-attentive neural networks, Is-a-Prerequisite-of
### Triplets:1. factoid question answering, Is-a-Prerequisite-of, natural language question answering2. factoid question answering, Compare, complex question answering3. factoid question answering, Evaluate-for, state-of-the-art accuracy4. factoid question answering, Evaluate-for, answer selection5. factoid question answering, Used-for, question answering over knowledge base6. factoid question answering, Used-for, semantic parsing for question answering
### Triplets:1. named entity disambiguation, Is-a-Prerequisite-of, Word Sense Disambiguation2. named entity disambiguation, Evaluate-for, NLP systems3. unsupervised machine translation, Compare, named entity disambiguation4. named entity disambiguation, Used-for, disambiguation of entities5. named entity disambiguation, Is-a-Prerequisite-of, Relation Extraction6. named entity disambiguation, Is-a-Prerequisite-of, Named Entity Recognition7. named entity disambiguation, Used-for, Open-domain NLP tasks
```(topically, related-to, distribution)(distributional word vectors, captures, semantic information)(topic modeling, is a method for, discovering coherent aspects)(topic modeling, utilizes, topic models)(LDA-based models, is utilized in generating, topically coherent segments)```
### Triplets:1. pretraining language, Used-for, word embeddings2. pretraining language, Part-of, machine learning3. pretraining language, Is-a-Prerequisite-of, downstream tasks4. word embeddings, Used-for, improving neural word segmentation5. machine learning, Is-a-Prerequisite-of, downstream tasks
### Triplets:1. entity recognition, Used-for, named-entity recognition2. named-entity recognition, Is-a-Prerequisite-of, NER3. neural network, Used-for, entity recognition4. named-entity recognition, Compare, sequence labeling5. entity recognition, Evaluate-for, emotion recognition6. named-entity recognition, Evaluate-for, NER7. entity recognition, Used-for, language processing
(`<translation task>`, `Used-for`, `Neural machine translation`)  (`<translation task>`, `Evaluate-for`, `BLEU score`)  (`<translation task>`, `Evaluate-for`, `Accuracy`)  (`<translation task>`, `Part-of`, `Natural language processing`)  (`<translation task>`, `Part-of`, `Machine translation`)  
### Extracted Concepts:1. Fake news detection2. Deception detection3. LIAR dataset4. Surface-level linguistic patterns5. Convolutional neural network6. Hate speech detection models7. African American English (AAE)8. Racial bias9. News timeline summarization (TLS)10. Automatic news timeline summarization11. Computational Social Sciences (CSS)12. NLP (Natural Language Processing)13. Fact checking models14. Reasoning in fake news detection15. Evidence in fake news detection16. Fact verification systems### Triplets:1. (Fake news detection, Is-a-Prerequisite-of, Deception detection)2. (LIAR dataset, Part-of, Fake news detection)3. (Convolutional neural network, Used-for, Integrating meta-data with text)4. (Hate speech detection models, Evaluate-for, Racial bias)5. (African American
### Triples:1. Aspect Extraction, is-a-prerequisite-of, Aspect-Based Sentiment Analysis2. Aspect Extraction, is-a-prerequisite-of, Aspect-Based Relation Extraction3. Aspect Extraction, used-for, Sentiment Polarity Detection4. Aspect Extraction, used-for, Sarcasm Detection5. Aspect Extraction, evaluate-for, Sentiment Analysis6. Aspect-Based Sentiment Analysis, part-of, Sentiment Analysis7. Aspect Extraction, part-of, Feature Extraction8. Aspect Extraction, part-of, Relation Extraction9. Aspect Extraction, used-for, Text Summarization10. Aspect Extraction, compare, Topic Modeling
(`<concept>`, `Used-for`, `generating text`)  (`<sequence-to-sequence models`, `Used-for`, `generating text`)  (`<Neural Machine Translation (NMT) model`, `Used-for`, `generating translations`)  (`<sequence-to-sequence neural models`, `Compare`, `pure statistical models`)  
1. (neural language model, is-a-Prerequisite-of, language modeling)2. (neural language model, Used-for, document context)3. (neural language model, Evaluate-for, generation of conversational text)4. (neural language model, Compare, hierarchical LSTM language model)5. (neural language model, Evaluate-for, disfluency detection)6. (neural language model, Evaluate-for, LSTM Noisy Channel Model)
### Triplets:1. word embeddings, are-trained-on, linguistic regularities2. word embeddings, have, cross-lingual connection3. cross-lingual connection, can-be, established without supervision4. word embeddings, capture, linguistic regularities5. word embeddings, require, cross-lingual signals6. cross-lingual connection, be, established without supervision7. embeddings, achieve, high-quality embeddings
### Extracted Concepts:1. Relation extraction2. Multi-lingual neural relation extraction framework3. Neural machine translation (NMT)4. Multi-modal Neural Machine Translation model5. Neural machine translation systems6. Cross-lingual word embeddings (CLWE)7. Multilingual neural machine translation (Multi-NMT)8. Multilingual pre-trained models9. Multilingual Machine Reading Comprehension (MRC)10. Transformer model11. Encoder-Decoder architecture12. Interlingua13. Multi-domain neural machine translation (NMT) models### Triplets:1. (Neural machine translation (NMT), Compare, Multi-lingual neural relation extraction framework)2. (Neural machine translation (NMT), Conjunction, Multi-modal Neural Machine Translation model)3. (Neural machine translation systems, Evaluate-for, Cross-lingual word embeddings (CLWE))4. (Multi-lingual neural relation
### Triplets:1. dialogue summarization, Used-for, query-based summarization2. dialogue summarization, Compare, extractive summarization3. dialogue summarization, Compare, multi-document summarization4. dialogue summarization, Compare, abstractive summarization5. dialogue summarization, Compare, sentence summarization6. multi-document summarization, Compare, dialogue summarization7. abstractive summarization, Compare, dialogue summarization
### Triplets:1. pretrained multilingual model, Used-for, natural language processing2. pretrained multilingual model, Compare, supervised multilingual model3. pretrained multilingual model, Compare, language model pre-training4. supervised multilingual model, Is-a-Prerequisite-of, pretrained multilingual model5. pretrained multilingual model, Evaluate-for, spoken language understanding6. pretrained multilingual model, Evaluate-for, low-resource scenarios
### Triplets:1. entity recognition, Used-for, part-of-speech induction2. entity recognition, Used-for, named-entity recognition3. entity recognition, Is-a-Prerequisite-of, generative process4. lexical resources, Used-for, named-entity recognition5. lexicon, Is-a-Prerequisite-of, generative model6. lexicon, Evaluate-for, generative model7. lexical resources, Evaluate-for, generative model8. lexicon, Conjunction, observations9. lexicon, Is-a-Prerequisite-of, training data10. lexicon, Used-for, generative model11. lexical resources, Evaluate-for, generative model12. lexicon, Evaluate-for, generative model
### Triplets:1. neural text classifier, <Used-for>, text classification2. neural text classifier, <Compare>, traditional text classifiers3. neural text classifier, <Evaluate-for>, accuracy and performance4. text classification, <Is-a-Prerequisite-of>, neural text simplification5. neural text simplification, <Used-for>, simplifying text for better understandability
(<query concept>, Compare, existing datasets)(query concept>, Evaluate-for, human-evaluation)(coherent paragraph summaries, Used-for, human-evaluation)
### Concept: image text1. (text, used-for, images)2. (image, includes, text)3. (recurrent neural network, used-for, text summarization)4. (text simplification, Is-a-Prerequisite-of, neural text simplification)5. (text, Evaluate-for, semantic relevance)6. (social media, Used-for, text summarization)
### Extracted Concepts:1. Sentiment analysis2. Videos3. Utterances4. LSTM-based model5. Mem2Seq6. Visual Attention7. Multimodal Social Media Posts8. Multimodal Named Entity Disambiguation (MNED)9. Multimodal Affective Computing10. CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)11. Multimodal Fusion12. Multimodal Emotion Recognition13. Multimodal Language Processing14. Dynamic Fusion Graph (DFG)15. Low-rank Multimodal Fusion16. Multimodal Time Series Data17. Multilingual Semantic Parsing18. Neural Sequence-to-Sequence Models19. Policy Gradient Training20. Visual Question Answering (VQA)21. Catastrophic Forgetting22. Multi-hop Reading Comprehension (RC)23. Hotpot
### Triplets:1. topic discovery, Is-a-Prerequisite-of, natural language processing2. topic discovery, Used-for, document aggregation3. Spatial aggregation, Is-a-Prerequisite-of, topic discovery4. spatial aggregation, Is-a-Prerequisite-of, spatially distinct topics5. keyphrases, Part-of, high-level topics6. keyphrases, Evaluate-for, natural language processing tasks7. Deep Convolutional Graph Neural Networks, Evaluate-for, natural language premise selection8. Discrete Variational Auto-Encoder with Graph Neural Network, Evaluate-for, dialog structure discovery9. LSTM, Used-for, document encoding10. neural variational inference algorithms, Evaluate-for, model parameters inference
(`<concept>`, `Used-for`, `market comments`)(`<concept>`, `Used-for`, `morphological inflection generation`)(`market comments`, `Generate`, `numerical value`)(`morphological inflection generation`, `Provide`, `state of the art results`)(`morphological inflection generation`, `Produce`, `continuous representations`)(`<concept>`, `Utilize`, `phonetic encoding`)(`<concept>`, `Integrate`, `document context`)(`<concept>`, `Learn`, `word representations`)(`<concept>`, `Predict`, `surrounding words`)(`<concept>`, `Improve`, `accuracy on sequence labeling tasks`)
### Triplets:1. deep syntactic, Used-for, parsing2. parsing, Conjunction, neural networks3. neural networks, Evaluate-for, parsing4. parsing, Hyponym-Of, dependency syntax5. dependency syntax, Evaluate-for, semantic role labeling6. semantic role labeling, Is-a-Prerequisite-of, deep learning-based extended argument labeling7. deep syntactic, Is-a-Prerequisite-of, empty category detection8. empty category detection, Used-for, parsing9. parsing, Compare, treebank conversion10. treebank conversion, Is-a-Prerequisite-of, supervised treebank conversion
### Extracted concepts:1. Word embeddings2. Distributional word embeddings3. Neural word embeddings4. Multimodal word distributions5. Word embedding models6. Character embeddings7. Chinese word embeddings8. Diachronic word embeddings9. Arabic word embeddings10. Dependency parser11. Document analysis12. Bilingual word embeddings13. Knowledge Graph (KG) embeddings### Triplets:1. (Word embeddings, Are-used-in, Distributional word embeddings)2. (Word embeddings, Used-for, Neural word embeddings)3. (Word embeddings, Are-modeled-by, Multimodal word distributions)4. (Word embeddings, Enhances, Document analysis)5. (Word embeddings, Enables, Dependency parser)6. (Word embeddings, Contribute-to, Bilingual word embeddings)7. (Word embeddings, Form-a-relationship-with, Knowledge Graph (KG) embeddings)
1. (implicit aspects, Part-of, implicit opinions)2. (aspect-based sentiment analysis, Evaluate-for, implicit aspects)3. (implicit opinions, Used-for, aspect-based sentiment analysis)
(None)
### Triplets:1. (handling unknown slot values, Is-a-Prerequisite-of, E2E architecture based on a pointer network)2. (Plagiarism detection, Is-a-Prerequisite-of, performance improvement methods)3. (Neural Machine Translation(NMT), Used-for, building syntactic analysis tools in low-resource languages)
(`<concept>`, `Compare`, `neural language model`)  (`<concept>`, `Compare`, `recurrent neural networks (RNNs)`)  (`<neural language model>`, `Evaluate-for`, `language modeling`)  (`<neural language model>`, `Is-a-Prerequisite-of`, `machine learning`)  (`<neural language model>`, `Used-for`, `predicting text`)  (`<recurrent neural networks (RNNs)>`, `Is-a-Prerequisite-of`, `sequence modeling`)  
### Triplets:1. automatic fake news detection, is-a-Prerequisite-of, combating fake news2. automatic fake news detection, Evaluate-for, fake news documents3. automatic fake news detection, Compare, existing methods4. automatic fake news detection, Combating fake news, limited by lack of labeled benchmark datasets5. fake news documents, Evaluate-for, automatic fake news detection6. existing methods, Compare, automatic fake news detection7. combating fake news, is-a-Prerequisite-of, automatic fake news detection
### Triplets:1. named entity recognition, Is-a-Prerequisite-of, neural approach2. named entity recognition, Evaluate-for, performance3. neural approach, Used-for, named entity recognition4. named entity recognition, Part-of, Natural Language Processing5. gazetteers, Used-for, named entity recognition6. named entity recognition, Evaluate-for, data annotation7. MGNER, Is-a-Prerequisite-of, nested/non-overlapping NER tasks
### Triplets:1. parsing idea treebank embedding, Is-a-Prerequisite-of, method for exploiting multiple treebanks2. parsing idea treebank embedding, Is-a-Prerequisite-of, improvement in monolingual dependency parsing3. method for exploiting multiple treebanks, Evaluate-for, improve performance of monolingual dependency parser4. improvement in monolingual dependency parsing, Evaluate-for, enhancement in parsing accuracy5. preprocessing, Part-of, treebank embedding6. dependency parsing, Is-a-Prerequisite-of, monolingual treebank embeddings7. supervised learning, Used-for, training model for treebank embeddings8. concatenating training sets, Used-for, strategies for exploiting multiple treebanks9. treebank embeddings, Is-a-Prerequisite-of, monolingual dependency parsing advancements
1. (Fact checking model, Used-for, claim verification)2. (Fact checking model, Compare, existing matching models)3. (Fact checking model, Evaluate-for, claim veracity)4. (Claim matching, Is-a-Prerequisite-of, fact checking model)5. (Fact checking model, Evaluate-for, claim matching)6. (Fact checking model, Evaluate-for, knowledge graph building)7. (Claim verification, Part-of, fact checking model)
(None)
(`<content>`, `Used-for`, `language technology`)  (`<content>`, `Contains`, `language technology`)  (`<content>`, `Benefit-to`, `tasks`)  (`<content>`, `Applied-to`, `benefit`)  (`<content>`, `Convey`, `benefit`)  (`<content>`, `Concerns`, `representation`)  (`<content>`, `Overlay-on`, `language technologies`)  (`<content>`, `Relates-to`, `linguistic diversity`)  (`<content>`, `Addresses`, `language barriers`)  (`<content>`, `Promotes`, `multilingualism`)
### Triplets:1. seq2seq text generation, Used-for, video captioning2. seq2seq text generation, Used-for, table-to-text generation3. seq2seq text generation, Used-for, Automated pun generation
### Triplets:1. vision language model, is-a-prerequisite-of, FOIL-COCO2. FOIL-COCO, association-with, images3. FOIL-COCO, association-with, captions4. vision language model, used-for, FOIL-COCO5. language model, used-for, generation of conversational text6. vision language model, used-for, disfluency detection7. vision language model, evaluate-for, Affect-LM8. vision language model, compare, LSTM Noisy Channel Model
### Triples:1. (Neural network models, have, promising opportunities for multi-task learning)2. (Neural network models, Used-for, multi-task learning)3. (Neural network models, Used-for, learning shared layers)4. (Neural network models, Used-for, extracting common and task-invariant features)5. (RNNs, have, shown promising performance for language modeling)6. (RNNs, Used-for, language modeling)7. (RNNs, Used-for, back-propagation through time)8. (RNNs, suffer from, overfitting)9. (RNNs, suffer from, severe gradient diffusion)10. (RNNs, Is-a-Prerequisite-of, principled Bayesian learning algorithm)11. (RNNs, Is-a-Prerequisite-of, learning weight uncertainty)12. (RNNs, Evaluate-for, model uncertainty estimation)13. (DNN
### Triplets:1. cross domain sentiment, is-a-prerequisite-of, cross-domain sentiment analysis2. cross domain sentiment, used-for, predicting sentiment polarity3. cross domain sentiment, evaluate-for, cross-domain sentiment classification4. cross domain sentiment analysis, compare, sentiment analysis5. cross domain sentiment analysis, used-for, cross-domain analysis6. cross domain sentiment analysis, evaluate-for, domain adaptation7. cross domain sentiment analysis, is-a-prerequisite-of, pre-training language model BERT8. pre-training language model BERT, compare, Language model9. pre-training language model BERT, part-of, post-training procedure10. domain adaptation, used-for, unsupervised domain adaptation
(`<concept>`, Compare, `attention weights`)(`identifiability`, Is-a-Prerequisite-of, `attention weights`)(`hidden role of key vector`, Is-a-Prerequisite-of, `identifiability`)
### Triplets:1. dialogue model, Is-a-Prerequisite-of, spoken dialogue systems2. spoken dialogue systems, Used-for, task-oriented dialogue3. dialogue model, Is-a-Prerequisite-of, seq2seq model4. spoken dialogue systems, Part-of, joint distributions5. dialogue model, Evaluate-for, response generation6. dialogue model, Is-a-Prerequisite-of, Two Stage CopyNet7. dialogue model, Evaluate-for, response selection8. dialogue model, Evaluate-for, disfluency detection
(`<concept>`, `Part-of`, `language model`)  (`<concept>`, `Is-a-Prerequisite-of`, `transfer learning`)  (`<concept>`, `Used-for`, `sentence compression`)  
### Triplets:1. Skip-Gram model, Used-for, learning word embeddings2. Skip-Gram model, Is-a-Prerequisite-of, vector calculus in solving word analogies3. Word embeddings, Evaluate-for, document analysis4. LSTM recurrent networks, Compare, word averaging in sentence embeddings5. DSWE, Evaluate-for, implicit discourse relation recognition6. Context-sensitive embeddings, Improved-by, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments7. Skip-Gram Negative Sampling (SGNS), Used-for, optimizing word embeddings in "word2vec" software
(`<query concept>`, `Compare`, `symbolic reasoning`)  (`language understanding`, `Part-of`, `Neural Symbolic Machine`)  (`symbolic reasoning`, `Used-for`, `Neural Symbolic Machine`)  (`Structured prediction problem`, `Evaluate-for`, `REINFORCE`)  (`Natural-language understanding systems`, `Is-a-Prerequisite-of`, `Quality of reading comprehension datasets`)  (`Semantic parser`, `Is-a-Prerequisite-of`, `Natural language utterances into executable programs`)  (`Natural language questions`, `Is-a-Prerequisite-of`, `SQL queries`)  (`Spoken dialogue systems`, `Part-of`, `Belief tracker`)  (`Review helpfulness modeling`, `Evaluate-for`, `Mechanisms that affect review helpfulness`)  (`Cross-lingual natural language understanding`, `Is-a-Prerequisite-of`, `Cross-cultural differences and similarities`)  
### Triplets:1. Natural language descriptions, Parsed into source code2. Natural language descriptions, Processed by existing data-driven methods3. Paraphrase generation, Beneficial for downstream natural language understanding tasks4. Paraphrase generation, Utilizes diverse datasets for training5. Paraphrase generation, Enabled by ParaNMT-50M dataset6. ParaNMT-50M dataset, Contains English-English sentential paraphrase pairs7. Sequence-to-Action, Semantic graph generation process8. Sequence-to-Action, Maps sentences to action sequences9. Sequence-to-Action, Achieves state-of-the-art performance10. DI-VAE and DI-VST, Improve VAEs11. DI-VAE and DI-VST, Discover interpretable semantics12. DI-VAE and DI-VST, Validate semantic representations on real-world dialog datasets13. Iterative training process, Improves diversity and
### Triplets:1. (unsupervised machine translation, relies on, adversarial, unsupervised cross-lingual word embedding technique)2. (unsupervised machine translation, examines, limitations of current unsupervised MT)3. (Unsupervised bilingual word embedding, is-a-Prerequisite-of, unsupervised machine translation)4. (unsupervised neural machine translation, achieve, remarkable results)5. (unsupervised bilingual word embeddings, train, mapping function)6. (neural machine translation, central to, word embedding)7. (unsupervised machine translation, set, task)8. (unsupervised machine translation, propose, anchored training)9. (unsupervised Bilingual Lexicon Induction, has attracted, research interest)10. (graph-based paradigm, induces, bilingual lexicons)11. (unsupervised bilingual lexicon induction, based-on, unsupervised cross-lingual word embeddings)
### Triplets:1. aspect extraction, Is-a-Prerequisite-of, aspect-based sentiment analysis2. aspect-based sentiment analysis, Is-a-Prerequisite-of, aspect sentiment classification3. word embedding models, Used-for, improving coherence of aspects4. English syntactic knowledge, Is-a-Prerequisite-of, dependency parsing of Singlish5. attention mechanism, Evaluate-for, detecting sentiment context6. SemAxis, Is-a-Prerequisite-of, characterization of word semantics7. convolutional neural networks, Is-a-Prerequisite-of, efficient sentiment prediction8. aspect routing approach, Is-a-Prerequisite-of, adaptive coupling of semantic capsules9. sentiment predictions, Is-a-Prerequisite-of, mining useful attention supervision10. self-attention mechanism, Is-a-Prerequisite-of, improve aspect extractor accuracy
### Triplets:1. bias nlp, is-a-Prerequisite-of, data augmentation protocol2. bias nlp, evaluate-for, reduction in translation quality3. bias nlp, evaluate-for, reduction in error rate4. bias nlp, Conjunction, factors (outcome disparities, error disparities, label bias, selection bias, model overamplification, semantic bias)5. semantic bias, Hyponym-Of, label bias6. predictibe bias, Hyponym-Of, outcome disparities7. predictibe bias, Hyponym-Of, error disparities
### Triplets:1. classification task, is-a-Prerequisite-of, sentiment analysis2. classification task, Evaluate-for, model performance3. classification task, Evaluate-for, text classification4. classification task, Evaluate-for, feature learning5. target-oriented sentiment classification, Compare, classification task6. Memory networks, Hyponym-Of, target-sensitive memory networks7. classification task, Hyponym-Of, aspect sentiment classification
1. (multilingual model, Used-for, language classification)2. (multilingual model, Compare, named entity recognition)3. (multilingual model, Compare, part-of-speech tagging)4. (Neural Named Entity Recognition, Is-a-Prerequisite-of, multilingual model)5. (multilingual model, Compare, natural language processing)6. (multilingual model, Evaluate-for, low-resource settings)
### Extracted Concepts:1. Natural Language Descriptions2. Target Programming Language Syntax3. Neural Architecture4. State-of-the-art Results5. Semantic Parsing6. Neural Semantic Parser7. Abstract Syntax Networks8. Semantic Graph Generation9. Sequence-to-Action Model10. Confidence Modeling11. Neural Network Models12. StructVAE13. Semi-supervised Semantic Parsing14. Dual-learning Algorithm15. Relation Extraction### Triplets:1. (Semantic Parsing, Is-a-Prerequisite-of, Neural Semantic Parser)2. (Natural Language Descriptions, Evaluate-for, Semantic Parsing)3. (State-of-the-art Results, Compare, Existing Results)4. (Neural Network Models, Used-for, Homographic Pun Generation)5. (Natural Language Descriptions, Evaluate-for, Abstract Syntax Networks)6. (Abstract Syntax Networks, Is-a-Prerequisite-of, Code Generation)7. (Conv
(None)
1. (Neural word segmentation research, Used-for, leveraging raw texts for pretraining character and word embeddings)2. (Statistical segmentation research, Exploited-for, leveraging richer sources of external information)3. (A range of external training sources, Evaluate-for, the effectiveness of neural word segmentation)4. (Chinese word segmentation, Has-a-Prerequisite-of, linguistic perspectives)5. (Existing methods, Focus-on, improving the performance for each single criterion)6. (Adversarial multi-criteria learning for CWS, Used-for, integrating shared knowledge from multiple heterogeneous segmentation criteria)7. (Neural network-based joint models for Chinese word segmentation, Conjunction, POS tagging)8. (Dependency parsing, Is-a-Prerequisite-of, Chinese word segmentation)9. (Word embeddings, Play-a-key-role-in, dependency parsing)10. (Speech register and prosody, Conjunction, word segmentation)11. (Subword units, Used-for
### Extracted Concepts:1. Labeled sequence transduction2. Multi-space variational encoder-decoders3. Generative model4. Neural networks5. Semi-supervised learning6. Unlabeled data7. Review spam detection8. Neural network model9. Detection of review spam10. Textual and behavioral information11. Recurrent neural networks (RNNs)12. Hybrid Code Networks (HCNs)13. Dialog systems14. Supervised learning15. Unsupervised learning16. Event extraction17. World knowledge and linguistic knowledge18. Training data labeling19. Training data20. Large scale data21. Word-level language detection22. Code-switched text23. Arabic word embeddings24. Unsupervised neural machine translation (NMT)25. Shared encoder26. High-level representations27. Generative adversarial networks (GANs)28.
### Triplets:1. linguistic representation, Used-for, semantic parsing2. linguistic representation, Is-a-Prerequisite-of, semantic representation3. linguistic representation, Used-for, natural language tasks
### Triplets:1. Argument extraction, Is-a-Prerequisite-of, Relation extraction2. Argument extraction, Used-for, Sentiment Analysis and Sarcasm Detection3. Argument extraction, Is-a-Prerequisite-of, Aspect extraction4. Argument extraction, Is-a-Prerequisite-of, Relation Schema Induction5. Argument extraction, Used-for, Sentence-level supporting argument detection6. Argument extraction, Is-a-Prerequisite-of, Visual language grounding
### Triplets:1. orthography, Is-a-Prerequisite-of, lexicalized2. morphological feature, Part-of, lexicalized3. morphological feature, Part-of, non-lexicalized4. joint modeling, Used-for, identifying intricate morphological patterns
### Extracted Concepts:1. Semantic parsing2. Unsupervised semantic parsing3. StructVAE4. Dependency triples5. Cross-lingual distributed logical representations6. Discourse Representation Theory (DRT)7. Unsupervised dependency parsing### Triplets:1. (Semantic parsing, Is-a-Prerequisite-of, Unsupervised semantic parsing)2. (StructVAE, Is-a-Prerequisite-of, Unsupervised semantic parsing)3. (Dependency triples, Used-for, Unsupervised dependency parsing)4. (Cross-lingual distributed logical representations, Evaluate-for, Unsupervised semantic parsing)5. (Discourse Representation Theory (DRT), Compare, Unsupervised semantic parsing)
### Extracted Concepts:1. Variational autoencoders (VAEs)2. Unsupervised cross-lingual learning3. Lifelong learning (LL)4. Visual storytelling (VIST)5. Rationale-centric framework6. Rationales7. Human utility8. Synthetic data### Triplets:1. Variational autoencoders (VAEs), Is-a-Prerequisite-of, Free text rationale2. Variational autoencoders (VAEs), Used-for, Text generation3. Unsupervised cross-lingual learning, Compare, Variational autoencoders (VAEs)4. Lifelong learning (LL), Evaluate-for, Catastrophic forgetting issue5. Visual storytelling (VIST), Hyponym-Of, Vision and language task6. Rationale-centric framework, Used-for, Boosting model out-of-distribution performance7. Rationales, Evaluate-for, Free text rationale8
### Content: In this paper, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search.### Concept: video question answeringTriplets:1. video question answering, Used-for, answering sequences of simple but inter-related questions2. video question answering, Part-of, a novel dynamic neural semantic parsing framework3. answering sequences of simple but inter-related questions, Is-a-Prerequisite-of, video question answering
### Extracted Concepts:1. Neural language model2. Rhythmic poetry3. English poetry4. Machine-generated poems5. Recurrent neural networks (RNNs)6. Stochastic optimization7. Gradient noise in RNNs8. LSTM (Long Short-Term Memory)9. Affect-LM (Affect Language Model)10. Evaluation dataset11. Fixed-vocabulary language models12. Character-level language models13. Pre-trained word embeddings14. Sequence labeling tasks15. Word representations16. Morphological representations17. Generative models18. Transfer learning19. LSTMs in disfluency detection20. Cross-lingual word embeddings21. Universal Language Model Fine-tuning (ULMFiT)22. Code-mixed (CM) language modeling23. Reward augmented maximum likelihood24. chattening models25. Named entities### Triplets:1
#### Triplets:1. statistical NLP, is, necessary first step2. algebraic perspective, allows, define message passing algorithm3. feature extraction, can dominate, computation time4. restructuring, does speed up, feature extraction5. Dialog State Tracking (DST), is designed, output unknown values6. E2E architecture, can effectively extract, unknown slot values7. Stochastic Gradient Descent (SGD), suffers, dramatic fluctuation8. AllVec, outperforms, sampling-based SGD methods9. plagiarism detection, is important due to, complex plagiarism10. Corpus normalization, is resilient to, dataset imbalance11. incorporation, can bring, significant improvement12. strategies, are explored for incorporating, target syntax into NMT13. beam search, is formulated using, WFSTs14. new framework, is made publicly available, GitHub repository15. discourse parsing, can benefit from, memory networks
1. (contextualized word embeddings, Used-for, open-domain argument search)2. (contextualized word embeddings, Compared-to, non-contextual subword embeddings)3. (contextualized word embeddings, Compared-to, BERT)4. (contextualized word embeddings, Used-for, argument classification)5. (contextualized word embeddings, Used-for, argument clustering)6. (contextualized word embeddings, Compared-to, ELMo)
(`<head concept>`, `Evalute-for`, `Cross-lingual transfer`)  (`Multi-lingual texts`, `Used-for`, `Relation extraction`)  (`Cross-lingual attention`, `Is-a-Prerequisite-of`, `Information consistency`)  (`Language relatedness`, `Compare`, `Ability to transfer morphological knowledge`)  (`Potential solution to resource scarcity in NLP`, `Compare`, `Transfer or share of knowledge between languages`)  (`Universal Dependencies (UD)`, `Part-of`, `Resource trees`)  (`Cross-lingual syntactic variation`, `Conjunction`, `Anisomorphism`)  (`Syntactic structure compatibility`, `Evaluate-for`, `Boosting cross-lingual transfer in NLP`)  (`Supervised event extraction methods`, `Hyponym-Of`, `Events extraction`)  (`Generic grounding problem`, `Evaluate-for`, `Mapping each event mention to a specific type in a target event ontology`)  
1. (sentiment classification, is-a-Prerequisite-of, domain adaptation)2. (sentiment classification, Compare, text classification)3. (CNN, Used-for, sentiment classification)
### Extracted Concepts:1. Neural networks2. Multi-grained lattice framework3. Chinese Spelling Check (CSC)4. SpellGCN5. BERT6. Spelling error correction7. PLOME8. PHMOSpell9. Adversarial training10. LEMON11. Hanyu Pinyin### Triplets:1. (Chinese Spelling Correction, Used-for, Neural networks)2. (Chinese Spelling Correction, Used-for, Multi-grained lattice framework)3. (Chinese Spelling Correction, Evaluate-for, SpellGCN)4. (SpellGCN, Is-a-Prerequisite-of, Chinese Spelling Correction)5. (Chinese Spelling Correction, Used-for, BERT)6. (Spelling error correction, Is-a-Prerequisite-of, Chinese Spelling Correction)7. (Chinese Spelling Correction, Is-a-Prerequisite-of, PLOME
### Triplets:1. speech register and prosody, are thought to play an important role in early language acquisition2. neural models, have achieved competitive performance against traditional methods for the task of Chinese word segmentation3. neural word segmenter with balanced word and character embedding inputs, is used to alleviate the existing drawbacks4. subword units, are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT)5. subword regularization, trains the model with multiple subword segmentations probabilistically sampled during training6. Chinese short text matching, usually employs word sequences rather than character sequences to get better performance7. unsupervised tokenization, is incorporated into a text classifier to improve performance of text classification8. neural machine translation (NMT), models are typically trained with fixed-size input and output vocabularies9. Chinese word segmentation (CWS), is necessary for deep learning-based Chinese Natural Language Processing10. Bayesian Hierarchical
### Extracted Concepts:1. Language understanding2. Vision3. Natural language processing4. Modeling inference5. Neural machine translation6. Multilingual semantic parsing7. Language identification8. Language-vision integration9. Socially inclusive NLP10. Action recognition### Triplets:1. (Language understanding, Is-a-Prerequisite-of, Vision)2. (Natural language processing, Is-a-Prerequisite-of, Language understanding)3. (Modeling inference, Evaluate-for, Neural machine translation)4. (Neural machine translation, Is-a-Prerequisite-of, Multilingual semantic parsing)5. (Multilingual semantic parsing, Is-a-Prerequisite-of, Neural machine translation)6. (Language identification, Is-a-Prerequisite-of, Multilingual semantic parsing)7. (Language-vision integration, Is-a-Prerequisite-of, Multilingual semantic parsing)8. (Socially inclusive NLP, Used
1. (Neural language model, Is-a-Prerequisite-of, Generative language model)2. (Generative language model, Evaluate-for, Poetry generation)3. (Neural language model, Compare, Recurrent neural networks)4. (Language model, Part-of, Recurrent neural networks)5. (String kernels, Compare, Word embeddings)6. (Generative language model, Used-for, Rescoring candidate outputs)7. (Generative language model, Compare, String kernels)
### Triplets:1. monolingual embeddings, Used-for, word embeddings2. monolingual embeddings, Compare, multilingual model3. monolingual embeddings, Evaluate-for, learning word embeddings4. monolingual embeddings, Compare, bilingual embeddings5. bilingual embeddings, Compare, monolingual embeddings6. monolingual embeddings, Used-for, mapping word embeddings7. monolingual embeddings, Is-a-Prerequisite-of, multilingual model8. multilingual model, Is-a-Prerequisite-of, monolingual embeddings
(`<concept>`, Is-a-Prerequisite-of, embeddings)  (embeddings, Part-of, method)  (method, Evaluate-for, results)  (embeddings, Is-a-Prerequisite-of, word vectors)  (domain specific embeddings, Part-of, Domain Adapted (DA) word embeddings)  (general-purpose embeddings, Part-of, Domain Adapted (DA) word embeddings)  (Domain Adapted (DA) word embeddings, Evaluate-for, sentiment classification tasks)  
### Triplets:1. (Question answering, Is-a-Prerequisite-of, Stanford Question Answering Dataset)2. (Constituent, Part-of, Stanford Question Answering Dataset)3. (Constituent-centric neural architecture, Is-a-Prerequisite-of, Stanford Question Answering Dataset)4. (Constituent-centric neural architecture, Used-for, representing candidate answers and their learning)5. (Principle, Is-a-Prerequisite-of, state of the art performance in question answering)6. (Open-domain question answering, Is-a-Prerequisite-of, using Wikipedia as the knowledge source)7. (Universal schema, Used-for, supporting reasoning on structured and unstructured text)8. (Control variates, Evaluate-for, human evaluation)9. (Convolutional Neural Network model, Used-for, text-based multiple choice question answering)10. (SciDTB, Used-for, evaluating discourse dependency parsers)
### Extracted Concepts:1. COREQA2. Neural network-based KB-QA3. Semantic parsing4. Question generation5. TREC QA6. EviNets7. Open Information Extraction8. Open-domain question answering### Triplets:1. COREQA, Incorporates, Sequence-to-sequence learning2. COREQA, Utilizes, Copying and retrieving mechanisms3. COREQA, Demonstrates, Efficiency4. Neural network-based KB-QA, Achieves, Impressive results5. Neural network-based KB-QA, Focuses on, Question representation6. Semantic parsing, Is used in, Question answering7. Question generation, Uses, Attention-based sequence learning model8. TREC QA, Employs, Support graph optimization framework9. EviNets, Combines, Supporting evidence10. Open Information Extraction, Provides, Semi-structured knowledge11. Open-domain question answering
### Triplets:1. learning morphological, Is-a-Prerequisite-of, morphological analysis2. morphological analysis, Used-for, morphological tagging3. morphological tagging, Evaluate-for, morphological analysis4. morphological tagging, Is-a-Prerequisite-of, neural network potentials5. neural network potentials, Compare, neural networks6. learning morphological, Used-for, NLP tasks7. learning morphological, Is-a-Prerequisite-of, natural language technologies8. morphological inflection generation, Evaluate-for, morphological models9. morphological models, Compare, neural models
```(semantic parsing, Is-a-Prerequisite-of, generalization)(neural parsers, Evaluated-for, generalization)(compositional generalization, Compare, compositional generalization)(neural network models, Evaluated-for, syntactic knowledge)(neural language models, Evaluated-for, syntactic knowledge)(semantic parsing, Is-a-Prerequisite-of, NMT models)(compositional generalization, Is-a-Prerequisite-of, traditional metrics)(neural machine translation models, Used-for, compositional generalization)```
1. (extractive summarization, highlights, key points)2. (summarization research, driven by, empirical approaches)3. (empirical approaches, focus on, standard datasets)4. (establishing theoretical models, advances, understanding of the task)5. (theoretical models, help to further improve, summarization systems)6. (summarization systems, benefit from, different types of model architectures)7. (different types of model architectures, improve, neural extractive summarization systems)8. (empirical approaches, crafting systems, to perform well on standard datasets)9. (modern systems, better than, best systems submitted at the time of shared tasks)10. (evaluation metrics, disagree in higher-scoring range, current systems operate)11. (evaluation metrics, which behave similarly, strongly disagree)12. (collecting human judgments, for high-scoring summaries, resolve the debate over metrics)13. (high-scoring summaries
(None)
### Triplets:1. model, outperform, strong baselines2. innovations, lead to, model3. model, outperforms, strong baselines4. model, improved, systems5. model, remarkably outperforms, strong baselines
(<adversarial multi-task learning framework>, improves, shared and private latent feature spaces)(<adversarial multi-task learning framework>, alleviating, shared and private latent feature spaces)(<unsupervised neural machine translation (NMT) approaches>, improves, robustness of NMT models)(<adversarial stability training method>, improves, robustness of NMT models)(<Neural networks for question answering (QA)>, improves, reductions in training and inference times)(<Adversarial Attention Network approach>, improves, r-values on EMOBANK Reader’s and Writer’s multi-dimensional emotion regression tasks)(<Probability Weighted Word Saliency (PWWS) algorithm>, improves, classification accuracy)(<self-attentive architectures>, improves, robustness against adversarial perturbation)(<Dual Adversarial Transfer Network (DATNet)>, improves, model generalization)
### Concept: textual adversarial attack1. (textual adversarial attack, Used-for, generating adversarial examples)2. (textual adversarial attack, Evaluate-for, impact on deep neural networks)3. (textual adversarial attack, Is-a-Prerequisite-of, generating stable attacks)4. (adversarial examples, Evaluate-for, maintaining lexical correctness)5. (adversarial examples, Evaluate-for, maintaining grammatical correctness)6. (adversarial examples, Evaluate-for, maintaining semantic similarity)
### Extracted Concepts:1. Attention-based recurrent neural network2. LSTM (long short term memory)3. Entity mentions4. Relations5. Dependency trees6. ACE (Automatic Content Extraction) corpora7. Feature-based joint model8. Tree-based LSTM model (SPTree)9. Agent-Artifact relations10. Physical relations11. Part-Whole relations12. Hypernyms13. Chinese hypernym prediction14. Transductive learning approach15. Network embedding (NE)16. Context-Aware Network Embedding (CANE)17. Pre-trained word embeddings18. NLP systems19. Sequence labeling tasks20. Cloze-style questions21. Gated-Attention (GA) Reader22. Character-level models23. Question answering24. EviNets25. Factoid question answering26. Neural machine translation (NMT)27. Domain adaptation
### Triplets:1. dialogue system, Used-for, task-oriented dialog systems2. dialogue system, Is-a-Prerequisite-of, task-oriented dialogue systems3. dialogue system, Is-a-Prerequisite-of, Neural Belief Tracking (NBT)4. task-oriented dialog systems, Is-a-Prerequisite-of, end-to-end task-oriented dialog systems5. task-oriented dialogue systems, Is-a-Prerequisite-of, end-to-end task-oriented dialog systems6. Neural Belief Tracking (NBT), Is-a-Prerequisite-of, Dialogue State Tracking (DST)7. task-oriented dialog systems, Hyponym-Of, Chit-chat models8. task-oriented dialog systems, Part-of, end-to-end task-oriented dialog systems
### Triplets:1. multilingual masked language modeling, is-a-prerequisite-of, zero-shot cross-lingual transfer2. multilingual masked language modeling, is-utilized-in, cross-lingual benchmarks3. pretrained BERT, used-for, multilingual masked language modeling4. multilingual masked language modeling, achieves, +14.6% average accuracy on XNLI5. multilingual masked language modeling, achieves, +13% average F1 score on MLQA6. multilingual masked language modeling, achieves, +2.4% F1 score on NER7. multilingual masked language modeling, is-based-on, multilingual-BERT8. multilingual masked language modeling, is-applied-in, cross-lingual tasks
### Extracted Concepts:1. Neural network models2. Multi-task learning3. Adversarial multi-task learning framework4. Text classification tasks5. Cognitive NLP systems6. Convolutional Neural Network (CNN)7. End-to-end automatic speech recognition (ASR)8. Joint decoding algorithm9. Political framing10. Multimodal sentiment analysis11. LSTM-based model12. Implicit discourse relation classification13. LDA-based model14. Recurrent Neural Networks15. Sequence to sequence neural networks16. Text simplification17. Question classification18. Deep network19. Dialogue Act classification20. SoPa model21. Weighted finite-state automata (WFSAs)22. BiLSTM (RNN)23. Bilingual tasks24. Domain adaptation25. Picturebook embeddings### Triplets:1. (Multi-task learning, Compare, Adversarial multi
### Extracted Concepts:1. Semantic parser2. Natural language utterances3. Program execution4. Abstract syntax networks5. Sequence-to-Action model6. Neural semantic parsing7. Counterfactual learning8. Dual learning algorithm### Triplets:1. (Semantic parser, Maps, Natural language utterances)2. (Semantic parser, Transduces, Natural language utterances)3. (Natural language utterances, Labeled with, Program execution)4. (Program execution, Labeled with, Correct execution result)5. (Abstract syntax networks, Modeling framework for, Problems)6. (Abstract syntax networks, Represent, Outputs as abstract syntax trees)7. (Neural semantic parsing, Model, Semantic parsing as an end-to-end semantic graph generation process)8. (Sequence-to-Action model, Leverages, Neural network models)9. (Counterfactual learning, Applied to, Neural semantic parsing)10. (
### Triplets:1. machine reading comprehension, Used-for, question answering2. machine reading comprehension, Is-a-Prerequisite-of, self-matching attention mechanism3. machine reading comprehension, Compare, reading comprehension4. machine reading comprehension, Is-a-Prerequisite-of, neural network architectures5. neural network architectures, Evaluate-for, guiding training and prediction6. machine reading comprehension, Is-a-Prerequisite-of, multi-passage reading comprehension7. machine reading comprehension, Compare, natural-language understanding systems
```(language model, used-for, downstream tasks)(language model, trained-on, phonetic encoding)(language model, trained-on, Equivalence Constraint Theory)```
### Extracted Concepts:1. Rhetorical Structure Theory2. Discourse parser3. Recursive neural network4. Attention mechanism5. UCCA (Universal Conceptual Cognitive Annotation)6. Bidirectional LSTMs7. Lemma8. Neural variational language model (NVLM)9. Sentiment composition10. Discourse Representation Theory (DRT)11. Discourse Representation Tree Structures12. Structural-aware model13. Graph attention network (GAT)### Triplets:1. (Rhetorical Structure Theory, Is-a-Prerequisite-of, Discourse structure)2. (Discourse parser, Used-for, Text categorization)3. (Attention mechanism, Used-for, Computing representation of the text)4. (UCCA, Is-a-Prerequisite-of, Parser for UCCA)5. (UCCA, Is-a-Prerequisite-of, Semantic representation)6. (Lemma, Is-a-Prerequisite
### Triplets:1. natural language generation, Used-for, subjective responses2. natural language generation, Compare, conversational systems3. natural language generation, Is-a-Prerequisite-of, semantic parsing4. semantic parsing, Evaluate-for, natural language generation5. sequence-to-sequence learning, Is-a-Prerequisite-of, natural language generation6. natural language generation, Evalute-for, readability7. natural language processing, Part-of, natural language generation8. neural semantic parser, Compare, natural language generation
### Triplets:1. conversational machine reading, utilizes, neural models2. neural models, generate, diverse responses3. conversational machine reading, involves, dialog contexts4. dialog contexts, structured traversals, over KG5. conversational machine reading, helps, answer high-level questions6. conversational machine reading, is used for, rumor detection7. conversational machine reading, extends, machine reading comprehension8. machine reading comprehension, involves, multi-turn interactions9. conversational machine reading, entails, joint reasoning and extraction of decision rules10. decision rules, extracted, from the procedural text 11. machine reading systems, involve, strategic disclosures and appeals12. strategic disclosures and appeals, aim to, change people’s opinions and actions
### Triplets:1. semantic parser, Used-for, converting natural language utterances to intermediate, domain-general natural language representations2. semantic parser, Evaluate-for, state-of-the-art on SPADES and GRAPHQUESTIONS3. semantic parser, Evaluate-for, competitive results on GEOQUERY and WEBQUESTIONS4. semantic parser, Hyponym-Of, tool for tackling long-tail phenomena in language understanding tasks5. morph-fitting procedure, Compare, past use of curated semantic lexicons for improving distributional vector spaces6. morph-fitting procedure, Is-a-Prerequisite-of, improving low-frequency word estimates7. morph-fitting procedure, Is-a-Prerequisite-of, boosting the semantic quality of the entire word vector collection8. morph-fitting procedure, Used-for, large gains in the downstream task of dialogue state tracking9. neural language model, Evaluate-for, learning an implicit representation of both the form and content of English poetry10
1. (coherent summary, Compare, factual headlines)2. (coherent summary, Evaluate-for, relevant, fluent headlines)3. (coherent summary, Compare, generated headlines)4. (neural abstractive multi-document summarization (MDS) model, Is-a-Prerequisite-of, coherent and concise summaries)5. (neural abstractive multi-document summarization (MDS) model, Used-for, summarizing long documents)
### Extracted Concepts:1. Neural semantic parser2. End-to-end question answering system (COREQA)3. Knowledge base methods4. Universal schema5. Neural dialogue generation6. BabbleLabble framework7. Multimodal social media posts### Triplets:1. (task natural language generation, Evaluate-for, Neural semantic parser)2. (End-to-end question answering system, Used-for, Generating natural answers)3. (Knowledge base methods, Compare, Web text)4. (Universal schema, Is-a-Prerequisite-of, Natural language question answering)5. (Neural dialogue generation, Compare, End-to-end neural dialogue generation)6. (BabbleLabble framework, Evaluate-for, Training classifiers)7. (Multimodal social media posts, Part-of, Social media sites)
(<concept>, Compare, geometry representation)(<concept>, Compare, semantic properties)(<concept>, Part-of, CLIP)(<concept>, Evaluate-for, mitigates anisotropy)(<concept>, Used-for, encode image captions)(<concept>, Evaluate-for, outperform GPT-2)(<concept>, Evaluate-for, achieve new corpus-based state of the art)(<concept>, Evaluate-for, improve performance)(<concept>, Evaluate-for, improve transfer efficiency)(<concept>, Evaluate-for, Outperforms XLM-R and mT5-Large)(<concept>, Is-a-Prerequisite-of, English performance)(<concept>, Is-a-Prerequisite-of, transfer efficiency)(<concept>, Is-a-Prerequisite-of, multilingual alignment)(<concept>, Is-a-Prerequisite-of, adversarial alignment)(<concept>, Hyponym-Of, Doubly Aligned Multilingual Parser)
(<orthography morphological feature>, Compare, lexicalized forms)  (lexicalized forms, Part-of, morphological features)  (morphological features, Evaluate-for, ambiguous lexical choices)  (orthography, Evaluate-for, dialectal content)  
(<model>, <used-for>, <generate coherent informative comment>)(<model>, <outperforms>, <state-of-the-art baselines>)(<model>, <used-for>, <generating responses>)(informative comment, <is-a-prerequisite-of>, coherent informative comment)(informative comment, <is-a-prerequisite-of>, generating responses)
### Content: Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model Robert Plutick’s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%. ### Concept: hate detection1. (emotion detection, Used-for, hate detection)2. (deep learning models, Used-for, hate detection)
### Triplets:1. word embeddings Conjunction document-aligned corpora2. word embeddings Evaluate-for self-learning approach3. word embeddings Evaluate-for neural network architectures4. word embeddings Is-a-Prerequisite-of capturing semantic regularities5. word embeddings Used-for sentiment analysis6. word embeddings Hyponym-Of Probabilistic FastText7. Probabilistic FastText Hyponym-Of dictionary-level probabilistic embeddings
1. (extractive summarization, Compare, abstractive summarization)2. (extractive summarization, Is-a-Prerequisite-of, sequence-to-sequence models)3. (extractive summarization, Used-for, sentence scoring)4. (sentence, Part-of, document)5. (neural abstractive summarization, Evaluate-for, conventional sequence-to-sequence model)6. (content, Evaluate-for, document summarization)7. (extractive summarization, Is-a-Prerequisite-of, sentence selection)
(`<concept>`, Is-a-Prerequisite-of, "semantic understanding system")  (`<concept>`, Used-for, "improving low-frequency word estimates")  ("semantic understanding system", Evaluate-for, `<concept>`)  (`<concept>`, Part-of, "Functional Distributional Semantics")  (`<concept>`, Part-of, "Pixie Autoencoder")  
1. (satire detection, Part-of, automatic hate speech detection)2. (satire detection, Evaluate-for, sarcasm detection)
### Extracted Concepts:1. Neural Semantic Parser2. End-to-End Training3. Answer Rationales4. Physical Knowledge Inference5. Kernel Methods6. Natural Language Processing7. Neural Machine Translation8. Named Entity Recognition (NER)9. Mention Detection (MD)10. Hypernym Prediction11. Text Segmentation12. Lemmatization### Triplets:1. Neural Semantic Parser, Used-for, Natural Language Processing2. End-to-End Training, Evaluate-for, Answer Rationales3. Answer Rationales, Evaluate-for, Inducing Arithmetic Programs4. Physical Knowledge Inference, Is-a-Prerequisite-of, Joint Inference5. Kernel Methods, Is-a-Prerequisite-of, Expressive Kernels6. Neural Machine Translation, Is-a-Prerequisite-of, End-to-End Architecture7. Named Entity Recognition (NER), Evaluate-for, Named Entity Recognition (
(None)
### Concept: text generation1. (text generation, Used-for, natural language descriptions)2. (text generation, Compare, language generation task)3. (text generation, Used-for, generation of complex programs)4. (text generation, Is-a-Prerequisite-of, state-of-the-art results)5. (text generation, Is-a-Prerequisite-of, abstractive sentence summarization)6. (text generation, Is-a-Prerequisite-of, emotion and sentiment control)
### Triplets:1. (extractive summarization model, Used-for, sentence scoring)2. (extractive summarization model, Used-for, sentence selection)3. (sentence selection, Is-a-Prerequisite-of, output summary)4. (question generation, Is-a-Prerequisite-of, entailment generation)5. (extractive summarization model, Hyponym-Of, SWAP-NET)6. (extractive summarization model, Compare, abstractive summarization model)
### Triplets:1. treebank embeddings, Trained-on, English ClueWeb09 corpus 2. English ClueWeb09 corpus, Linked-to, Freebase3. Freebase, Contains, knowledge base relations4. A neural representation learning model, Utilizes, Hyperspherical Relation Embeddings (SphereRE)  5. Hyperspherical Relation Embeddings (SphereRE), Outperforms, state-of-the-arts
(`<head concept>`, `Used-for`, `improving translation accuracy`)  (`source syntax`, `Part-of`, `multilingual neural machine translation`)  (`Chinese-to-English translation`, `Evaluate-for`, `all the three proposed syntactic encoders`)  (`Neural Machine Translation (NMT) model`, `Is-a-Prerequisite-of`, `multilingual neural machine translation`)
### Triples:1. sememes, are-part-of, linguistic common-sense knowledge bases2. Word sememe information, improves, word representation learning3. Commonsense knowledge base, can help, generate essays4. Language models, used-for, sense-making capability5. External commonsense knowledge, integrated, generator6. Relational knowledge, encode, separate word embedding7. Automatic evaluation metrics, assess, quality of generated essay8. Common sense, introduce, natural language understanding systems9. ConceptNet, enriched by, external commonsense knowledge10. Conversation generation model ConceptFlow, leverages, commonsense knowledge graphs11. Open book facts, required-for, Open book question answering12. Automatic extraction, method used for, needed WSC knowledge13. Sememes, composed of, one or multiple sememes14. WRL, benefits from, sememes15. Word embeddings, encode, attributional knowledge
(`<concept>`, Evaluate-for, Adversarial training)  (`<concept>`, Is-a-Prerequisite-of, Adversarial attack)  (`<concept>`, Compare, Adversarial perturbation)  (`<concept>`, Compare, Adversarial example)  (`<concept>`, Evaluate-for, Text classification)  (`<concept>`, Used-for, Improving the robustness of models)  
(None)
### Triplets:1. word embeddings, represent, semantic information2. word embeddings, are-used-in, document analysis3. semantic information, is-contained-in, word embeddings4. word embeddings, are-trained-on, generic corpora5. machines, learn, word embeddings6. word embeddings, capture, linguistic regularities7. sentiment information, is-incorporated-in, word embeddings
### Triplets:1. (abstractive summarization, Used-for, document summarization)2. (abstractive summarization, Is-a-Prerequisite-of, query-based summarization)3. (abstractive summarization, Compare, extractive summarization)4. (document summarization, Is-a-Prerequisite-of, abstractive summarization)5. (query-based summarization, Is-a-Prerequisite-of, abstractive summarization)6. (neural abstractive summarization, Is-a-Prerequisite-of, abstractive summarization)7. (abstractive summarization, Part-of, machine translation)
### Triplets:1. (Inter-topic preferences, Is-a-Prerequisite-of, Modeling)2. (Matrix factorization, Used-for, Modeling)3. (Linguistic patterns, Used-for, Extracting statements)4. (Automatic identification of good arguments, Evaluate-for, Civics and education)5. (Hierarchical story generation, Is-a-Prerequisite-of, Model fusion)6. (DEISTE, Used-for, Textual entailment)7. (Vision-and-Language Navigation, Is-a-Prerequisite-of, Real-world tasks)8. (Chart-based method, Compare, Previous methods)9. (MetaEvent, Part-of, Few-shot event detection)10. (NatLogAttack, Evaluate-for, Developing attack models)
1. (language model, uses, neural networks)2. (language model, used-for, generating natural language text)3. (language model, part-of, Affect-LM)4. (language model, involves, LSTM)5. (Affect-LM, used-for, generating conversational text)
### Triplets:1. Vision-and-Language Navigation, Is-a-Prerequisite-of, Learning to follow instructions2. Vision-and-Language Navigation, Evaluate-for, Instruction fidelity3. Vision-and-Language Navigation, Part-of, Neural module networks4. Neural module networks, Compare, BabyWalk5. BabyWalk, Part-of, Large-scale referring expression recognition dataset6. Large-scale referring expression recognition dataset, Part-of, Refer360°7. Refer360°, Is-a-Prerequisite-of, Cross-modal language generation tasks8. Cross-modal language generation tasks, Evaluate-for, Biases in language generation9. Biases in language generation, Compare, Substructure distribution projection10. Substructure distribution projection, Evaluate-for, Zero shot cross-lingual dependency parsing
### Triplets:1. fact-checking datasets, part-of, FAVIQ2. FAVIQ, Is-a-Prerequisite-of, fact-checking models3. bgGLUE, part-of, fact-checking datasets4. existing models, Used-for, reasoning5. DialFact, Evaluate-for, fact-checking performance6. datasets, part-of, infoVerse7. FACTIFY-5WQA, Used-for, fact explainability
1. (Grammatical error correction, Is-a-Prerequisite-of, Neural machine translation)2. (Grammatical error correction, Evaluate-for, Correction model performance)3. (Neural machine translation, Compare, New hybrid neural model with nested attention layers for GEC)4. (GEC, Part-of, GEC systems)5. (GEC, Used-for, Correct errors)6. (GEC, Is-a-Prerequisite-of, Correcting global errors in word order and usage)7. (GEC, Evaluate-for, System output evaluation)8. (GEC, Evaluate-for, ERRANT annotation toolkit)9. (GEC, Part-of, Shared task evaluations)10. (Dependency parsing scheme, Is-a-Prerequisite-of, Jointly parses a sentence and repairs grammatical errors)11. (Dependency parsing scheme, Used-for, Dependency accuracy and grammaticality improvements)12. (Bias estimates, Compare, Performance estimates
### Extracted Concepts:1. Aspect extraction2. Aspect-based sentiment analysis3. Sentiment analysis4. Neural networks5. LSTM-based model6. Cross-domain sentiment analysis7. Target-sensitive memory networks8. Cross-lingual sentiment approaches9. Bilingual Sentiment Embeddings (BLSE)10. Memory networks11. Aspect sentiment classification (ASC)12. Memory networks13. Gating mechanisms14. Sentiment polarity15. Attention mechanisms16. Convolutional neural networks (CNN)17. Cold-Start problem18. Fine-grained sentiment analysis19. Supervised aspect extraction20. Interactive multi-task learning network (IMN)### Triplets:1. (Aspect extraction, Part-of, Aspect-based sentiment analysis)2. (Aspect-based sentiment analysis, Is-a-Prerequisite-of, Sentiment analysis)3. (Neural networks, Used-for, Aspect-based sentiment analysis)4
### Triplets:1. Adversarial multi-task learning framework, Used-for, Neural topic modeling2. Sentence-level sentiment classification, Is-a-Prerequisite-of, Neural topic modeling3. Anchor methods, Evaluate-for, Neural topic modeling4. Informed prior-based methods, Compare, Neural topic modeling5. Wasserstein autoencoders (WAE) framework, Used-for, Neural topic modeling
### Triplets:1. Existing methods focus on exploiting mono-lingual data for relation extraction.2. Multi-lingual neural relation extraction framework employs mono-lingual attention.3. Cross-lingual attention considers the information consistency among cross-lingual texts.4. Distant supervision significantly reduces human efforts in building training data for classification tasks.5. Deep look at the application of distant supervision in relation extraction.6. Dynamic transition matrix characterizes the noise in the training data built by distant supervision.7. Neural relation extraction framework consistently achieves significant improvements on relation extraction.8. Proposed class ties model for distantly supervised relation extraction.9. Joint relation extraction with a unified model integrating CNN with a pairwise ranking framework.10. Leverage class ties for relation extraction enhances extraction results.11. Pocket Knowledge Base Population (PKBP) task of dynamically constructing a KB of entities.12. BONIE is the first open numerical relation extractor for extracting Open IE tuples.13.
### Triplets:1. question answering, tasks of, reading comprehension2. reading comprehension, is-a-Prerequisite-of, question answering3. reading comprehension, Used-for, answering questions4. reading comprehension, Compare, machine comprehension5. reading comprehension, Compare, semantic parsing6. reading comprehension, Compare, factoid question answering7. reading comprehension, Compare, question answering on the Stanford Question Answering Dataset (SQuAD)8. task question answering, Is-a-Prerequisite-of, reading comprehension9. task question answering, Evaluate-for, building a hierarchical attention network10. question answering, Evaluate-for, answering questions from a given passage
### Triplets:1. deep learning based Chinese Natural Language Processing, Is-a-Prerequisite-of, Chinese Natural Language Processing2. deep learning based Chinese Natural Language Processing, Compare, word-based models3. deep learning based Chinese Natural Language Processing, Compare, char-based models4. deep learning based Chinese Natural Language Processing, Used-for, enhancing abstractive text summarization5. deep learning based Chinese Natural Language Processing, Evaluate-for, sentiment classification6. deep learning based Chinese Natural Language Processing, Is-a-Prerequisite-of, abstractive text summarization7. Chinese Natural Language Processing, Is-a-Prerequisite-of, Named Entity Recognition8. Named Entity Recognition, Used-for, incorporating gazetteers into machine learning based NER systems9. Named Entity Recognition, Evaluate-for, text classification
### Extracted Concepts:1. Multi-task learning2. Adversarial multi-task learning3. Neural Symbolic Machine4. Neural Machine Translation5. Recursive neural networks6. Deep Convolutional Neural Network (CNN)7. Recurrent Neural Networks (RNNs)8. Constituency Parsing9. Layer-wise Relevance Propagation (LRP)10. Joint Chinese word segmentation, POS tagging, and dependency parsing11. Semantic graph parsing for Minimal Recursion Semantics (MRS)12. Geolocation prediction model13. Local coherence model based on a convolutional neural network### Triplets:1. (neural word, Compare, Multi-task learning)2. (Adversarial multi-task learning, Compare, Multi-task learning)3. (Neural Symbolic Machine, Used-for, handling compositionality)4. (Neural Symbolic Machine, Used-for, program execution)5. (Neural Machine Translation
### Triplets:1. relation extraction, Used-for, finding unknown relational facts2. multi-lingual neural relation extraction framework, Used-for, exploiting mono-lingual data3. multi-lingual neural relation extraction framework, Compare, mono-lingual attention4. multi-lingual neural relation extraction framework, Compare, cross-lingual attention5. abstractive summarization, Is-a-Prerequisite-of, query-based summarization6. encode-attend-decode paradigm, Is-a-Prerequisite-of, query-based summarization7. query-based summarization, Evaluate-for, relevance in the context of a given query8. query-based summarization dataset, Part-of, debatepedia9. neural sequence-to-sequence model, Used-for, extractive summarization10. neural network framework, Evaluate-for, extractive document summarization11. Supervised model, Is-a-Prerequisite-of, sequence labeling12.
(`<concept>`, `Part-of`, `NER system`)  (`<concept>`, `Is-a-Prerequisite-of`, `rule-based detection`)  (`<concept>`, `Evaluate-for`, `effectiveness`)  
### Triplets:1. natural language inference, Used-for, generating human-like responses2. Named Entity Recognition (NER), Used-for, natural language inference3. Neural Semantic Parser, Is-a-Prerequisite-of, natural language inference4. Natural Language Processing (NLP), Is-a-Prerequisite-of, natural language inference5. Neural Semantic Parser, Evaluate-for, creating a natural language interface6. Named Entity Recognition (NER), Evaluate-for, mention detection7. Semantic Parsing, Used-for, natural language inference
### Triplets:1. explanation prediction, Used-for, natural language understanding2. explanation prediction, Is-a-Prerequisite-of, common-sense question answering3. explanation prediction, Evaluate-for, sample-based explanation methods
### Triplets:1. detecting, Used-for, building emotional chatbots2. detecting, Evaluate-for, understanding individuals and their lives3. detecting, Is-a-Prerequisite-of, building emotional chatbots4. detecting, Compare, selecting correct answer sentences5. correcting, Evaluate-for, improving written communication6. correcting, Evaluate-for, enhancing GEC systems7. correcting, Is-a-Prerequisite-of, improving written communication8. correcting, Is-a-Prerequisite-of, enhancing GEC systems
### Triplets:1. named entity recognition, Used-for, information extraction2. named entity recognition, Evaluate-for, performance3. named entity recognition, Part-of, natural language processing4. named entity recognition, Conjunction, mention detection5. named entity recognition, Is-a-Prerequisite-of, domain adaptation
### Triplets:1. dataset bias, Evaluate-for, natural-language understanding systems2. dataset bias, Evaluate-for, RC datasets3. dataset bias, Evaluate-for, Winograd Schema Challenge (WSC) dataset4. dataset bias, Used-for, training5. dataset bias, Used-for, performance improvement6. dataset bias, Is-a-Prerequisite-of, data analysis7. dataset bias, Is-a-Prerequisite-of, system evaluation8. data analysis, Used-for, performance evaluation9. data analysis, Is-a-Prerequisite-of, system evaluation10. data analysis, Is-a-Prerequisite-of, dataset bias
### Triplets:1. syntactic parsing, Is-a-Prerequisite-of, parsing2. syntactic parsing, Is-a-Prerequisite-of, Natural Language Processing3. parsing, Is-a-Prerequisite-of, semantic parsing4. parsing, Is-a-Prerequisite-of, source code parsing5. syntactic parsing, Evaluate-for, a language generation task6. syntactic parsing, Evaluate-for, creating grammar models7. parsing, Evaluate-for, code generation8. syntactic parsing, Used-for, mapping sentences to syntactic structures9. syntactic parsing, Used-for, language modeling10. syntactic parsing, Used-for, learning syntax of the target programming language
### Triplets:1. word embeddings trained, capture, linguistic regularities2. word embeddings trained, improve, document clustering3. word embeddings trained, provide, semantic information4. word embeddings trained, used for, NLP tasks5. word embeddings trained, studied in, document analysis
(`<concept>`, `Used-for`, `multimodal information`)(`<concept>`, `Evaluate-for`, `dialog systems`)(`user sentiment`, `Evaluate-for`, `dialog length`)(`dialog state`, `Part-of`, `Hybrid Code Networks (HCNs)`)(`dialogue partners`, `Compare`, `social power`)(`linguistic alignment`, `Used-for`, `alignment`)(`user sentiment`, `Evaluate-for`, `task success rate`)(`utterance rewriter`, `Is-a-Prerequisite-of`, `multi-turn dialogue modelling`)
(`<entity mentions> and relations`, `Used-for`, `Automatic content extraction (ACE) corpora`)(`<ACE corpora>`, `Evaluate-for`, `Model proposed by Li & Ji (2014)`)(`<ACE corpora>`, `Hyponym-Of`, `Automatic evaluation results`)(`<ACE corpora>`, `Evaluate-for`, `Feature-based joint model by Li and Ji (2014)`)(`<ACE corpora>`, `Part-of`, `Experiments on Automatic Content Extraction (ACE) corpora`)(`<ACE corpora>`, `Used-for`, `Evaluation results`)(`<ACE corpora>`, `Is-a-Prerequisite-of`, `Automatic content extraction`)(`<ACE corpora>`, `Hyponym-Of`, `Li and Ji (2014) model`)(`<Model proposed by Li & Ji (2014)> and <Model proposed by Miwa & Bansal (2016)>`, `Conjunction`, `Model proposed by Li &
### Extracted Concepts:1. Neural language model2. Phonetic encoding3. Rhythmic poetry generation4. Generative neural language model5. Weighted finite state machine6. Generative models7. LSTM LM (Long Short-Term Memory Language Model)8. Taylor's law9. Hierarchical attention network10. Sonnet modelling11. Recurrent Neural Network Language Models12. Numeracy13. Hierarchical models14. Recurrent Neural Network Grammars (RNNGs)### Triplets:1. (Neural language model, Used-for, Rhythmic poetry generation)2. (Neural language model, Compare, Weighted finite state machine)3. (Generative neural language model, Evaluate-for, Poetry generation)4. (Generative neural language model, Is-a-Prerequisite-of, Weighted finite state machine)5. (LSTM LM, Compare, Generative models)6
### Triplets:1. interpretable description, Is-a-Prerequisite-of, understanding process2. interpretable description, Evaluate-for, machine learning models3. interpretable description, Part-of, structured prediction4. interpretable description, Evaluate-for, model predictions5. interpretable description, Part-of, human language processing dynamics6. fine-grained details, Evaluate-for, interpretable description
### Triplets:1. state-of-the-art models, based-on, Transformer-based solutions2. models, struggle-to-address, multi-input tasks3. state-of-the-art models, succeeded-in, single-document NLP tasks4. state-of-the-art models, struggle-to-address, multi-input tasks5. solution, propose, DAMEN6. models, struggle, translating gender morphology7. pre-trained language models, shown, potentials in NLP8. current PLMs, obtained-by, sentence-level pre-training9. PLMs, applied-to, rhetorical structure theory discourse parsing
### Triplets:1. language unseen pretraining, Used-for, sentiment classification2. language unseen pretraining, Compare, language model pretraining3. language unseen pretraining, Is-a-Prerequisite-of, topic-dependent evidence detection4. language unseen pretraining, Used-for, keyphrase prediction5. language unseen pretraining, Is-a-Prerequisite-of, intelligent personal digital assistants6. sentiment classification, Is-a-Prerequisite-of, language unseen pretraining7. topic-dependent evidence detection, Is-a-Prerequisite-of, language unseen pretraining8. keyphrase prediction, Is-a-Prerequisite-of, language unseen pretraining
### Triplets:1. visual question answering, Is-a-Prerequisite-of, question answering2. visual question answering, Is-a-Prerequisite-of, natural language question answering3. visual question answering, Is-a-Prerequisite-of, machine comprehension4. questions, Part-of, visual question answering5. machine comprehension, Is-a-Prerequisite-of, visual question answering6. visual question answering, Compare, text-based question answering7. visual question answering, Compare, geometric reasoning8. visual question answering, Is-a-Prerequisite-of, Convolutional Neural Network (CNN) model
1. (relation extraction, Is-a-Prerequisite-of, multi-lingual data)2. (relation extraction, Used-for, building training data)3. (relation extraction, Is-a-Prerequisite-of, distant supervision)4. (relation extraction, Evaluate-for, noise reduction)5. (relation extraction, Compare, deep look at the application of distant supervision)6. (relation extraction, Compare, dynamic transition matrix effectiveness)7. (relation extraction, Is-a-Prerequisite-of, dynamic transition matrix)8. (relation extraction, Compare, dynamic transition matrix training)9. (relation extraction, Is-a-Prerequisite-of, model evaluation)10. (relation extraction, Is-a-Prerequisite-of, extraction improvements)11. (relation extraction, Compare, state-of-the-art performance)12. (relation extraction, Is-a-Prerequisite-of, extraction results evaluation)13. (relation extraction, Part-of, knowledge base population)14. (relation
1. (Label, part-of, Approach)2. (Approach, compared, HCSC)3. (approach, Evaluate-for, sentiment features)4. (sentiment features, used-for, sentiment classification)5. (HCSC, Evaluate-for, RMSE)6. (HCSC, part-of, Model)7. (Model, Is-a-Prerequisite-of, sentiment classification)
### Extracted Concepts:1. Semantic parsing2. LSTM3. Neural graph-to-sequence model4. Hierarchical attention network5. Dynamic Spatial Memory Network (DSMN)6. Visual reasoning7. Dependency triples### Triplets:1. (Semantic parsing, Evaluate-for, Neural graph-to-sequence model)2. (LSTM, Compare, Neural graph-to-sequence model)3. (Neural graph-to-sequence model, Is-a-Prerequisite-of, Hierarchical attention network)4. (Hierarchical attention network, Part-of, Dynamic Spatial Memory Network (DSMN))5. (Dynamic Spatial Memory Network (DSMN), Is-a-Prerequisite-of, Visual reasoning)6. (Dependency triples, Evaluate-for, Unsupervised semantic frame induction) 
1. (Deep Neural Networks, Used-for, End-to-end automatic speech recognition)2. (Recurrent Neural Networks, Used-for, End-to-end automatic speech recognition)3. (Neural Machine Translation, Compare, End-to-end automatic speech recognition)
1. (neural machine translation, used-for, performance)2. (neural machine translation, Compare, phrase-based machine translation)3. (word embeddings, Evaluate-for, enhancing translation performance)4. (neural machine translation, used-for, language pairs)5. (neural machine translation, Compare, unsupervised NMT)6. (neural machine translation, Evaluate-for, state-of-the-art baselines)7. (sentence splitting, Is-a-Prerequisite-of, further fine-tuned operations)8. (word embeddings, Is-a-Prerequisite-of, neural machine translation)9. (neural machine translation, Compare, machine translation)
(`<content>`, Generate, attention weight)  (`<content>`, Propose, attention weight)  (`<content>`, Employ, attention weight)  (`<content>`, Incorporate, attention weight)
### Triplets:1. Word embeddings, capture, linguistic regularities2. Word embeddings, enable, spectral clustering3. Spectral clustering, uses, word embeddings4. Word embeddings, are used in, FastText model5. FastText model, captures, multiple word senses6. Probabilistic FastText model, outperforms, FastText model7. Probabilistic FastText model, achieves, state-of-art performance8. Probabilistic FastText model, incorporates, subword structures9. Arabic word embeddings, evaluated using, intrinsic evaluation benchmark10. Arabic word embeddings, evaluated using, extrinsic evaluations in NLP tasks
(`<concept>`, `Part-of`, `task-oriented dialogue systems`)(`<concept>`, `Evaluate-for`, `visual dialog`)(`<concept>`, `Used-for`, `answering a series of questions about an image`)(`<concept>`, `Compare`, `task-oriented dialogue systems`)(`<concept>`, `Is-a-Prerequisite-of`, `answering a series of questions about an image`)(`<concept>`, `Is-a-Prerequisite-of`, `task-oriented dialogue systems`)
### Triples:1. (Natural Language Processing, Is-a-Prerequisite-of, Named Entity Recognition)2. (Natural Language Processing, Is-a-Prerequisite-of, Language Modeling)3. (Natural Language Processing, Used-for, Document Classification)4. (Natural Language Processing, Used-for, Machine Translation)5. (Natural Language Processing, Used-for, Automatic Question Answering)6. (Natural Language Processing, Is-a-Prerequisite-of, Syntax Analysis)7. (Natural Language Processing, Is-a-Prerequisite-of, Semantic Analysis)8. (Natural Language Processing, Used-for, Named Entity Recognition)9. (Semantic Parsing, Used-for, Learn Classifier)10. (Semantic Parsing, Used-for, Drive Model Training)11. (Generative Models, Evaluate-for, Parsing)12. (Generative Models, Evaluate-for, Language Modeling)13. (Event Detection, Compare, Event Extraction)14. (Neural Networks, Compare, Recurrent
### Triplets:1. parsing model, achieves, state-of-the-art results2. parsing model, runs, very efficiently3. parsing model, decomposed into, factors of CCG categories4. parsing model, defined on, bi-directional LSTMs5. parsing model, explicitly models, sentence structures via dependencies6. parsing model, used for, English and Japanese CCG parsing7. parsing model, proposes, a new A* CCG parsing model
(`<subject>`, `Is-a-Prerequisite-of`, `form question answering`)(`<body>`, `Is-a-Prerequisite-of`, `form question answering`)(`<subject>`, `Evaluate-for`, `form question answering`)(`<body>`, `Evaluate-for`, `form question answering`)
### Triplets:1. human-annotated data set, Part-of, controllable summarization2. named entities, Is-a-Prerequisite-of, controllable summarization
1. (extracting features, from, statistical NLP)2. (deep learning architecture, learn, relations)3. (graph neural network, used for, relation extraction)4. (multi-task architecture, jointly trains, relation identification)
(`<concept>`, Compare, `low-frequency word forms`)(`<concept>`, Evaluate-for, `language understanding systems`)(`<concept>`, Used-for, `tackling long-tail phenomena in language understanding tasks`)(`<concept>`, Part-of, `dialogue state tracking`)(`<concept>`, Is-a-Prerequisite-of, `state tracking`)(`morphological task`, Compare, `unsupervised bilingual lexicon induction`)(`morphological task`, Compare, `morphological disambiguation`)(`morphological task`, Evaluate-for, `low-resource setting`)
1. (contextualized representation, Used-for, sentiment analysis)2. (contextualized representation, Evaluate-for, identifying author characteristics)3. (contextualized representation, Evaluate-for, lexical relation distinction)4. (contextualized representation, Used-for, multilingual unsupervised NMT)5. (contextualized representation, Is-a-Prerequisite-of, denoising autoencoding)6. (fastText, Is-a-Prerequisite-of, contextualized word embeddings)7. (contextualized representation, Used-for, multilingual training)8. (text-based external word embeddings, Evaluate-for, end-to-end speech recognition)9. (string kernels, Evaluate-for, automatic essay scoring)10. (contextualized word embeddings, Used-for, affect dimension capture)11. (contextualized word embeddings, Evaluate-for, examining gender portrayals)12. (autoencoding, Is-a-Prerequisite-of, inter-lingual representation
### Extracted Concepts and Relationships:1. Sentences and Words from Alternating Pointer Networks: (<extractive summary>, Is-a-Prerequisite-of, SWAP-NET)2. Email subject line generation: (<extractive summary>, Is-a-Prerequisite-of, effective email subject line)3. Deep neural networks: (<extractive summary>, Used-for, text summarization)4. Multi-News dataset: (<extractive summary>, Part-of, MDS news dataset)5. Abstractive summarization: (<extractive summary>, Compare, abstractive summary)6. Sequence-to-sequence network: (<extractive summary>, Used-for, text summarization)7. Hibert model: (<extractive summary>, Is-a-Prerequisite-of, document encoding)8. ACE event extraction: (<extractive summary>, Evaluate-for, event extraction improvement)
(<paper>, Discusses, Topic-Aware News Representation)(<paper>, Proposes, Neural News Recommendation Approach)(Neural News Recommendation Approach, Comprises, News Encoder)(News Encoder, Learns representations of news, CNN networks)(News Encoder, Applies, Attention Networks)(News Encoder, Learns topic-aware news representations, Jointly trains with auxiliary topic classification task)(Neural News Recommendation Approach, Comprises, User Encoder)(User Encoder, Learns representations of users, Browsed news)(User Encoder, Uses, Attention Networks)(Neural News Recommendation Approach, Validates effectiveness on a real-world dataset, Extensive experiments)
### Triplets:1. neural network-based KB-QA, Compare, KB-QA2. NL interfaces, Used-for, databases3. NER, Is-a-Prerequisite-of, MD4. FFNN, Used-for, predicting entity label5. neural language questions, Is-a-Prerequisite-of, SQL queries6. document similarity, Compare, text matching7. reinforcement learning, Evaluate-for, automatic taxonomy induction8. text style transfer, Evaluate-for, translator training9. encoder-decoders, Used-for, non-parallel data training10. cycle consistency loss, Evaluate-for, maintaining style consistency
### Triples:1. entity extraction, Used-for, relation extraction2. word embeddings, Used-for, coherence improvement3. entity extraction, Compare, event extraction4. entity extraction, Is-a-Prerequisite-of, relation extraction5. lexicon, Used-for, part-of-speech induction6. feature extraction, Part-of, statistical NLP7. lexicon, Is-a-Prerequisite-of, lexical feature weights
### Extracted Concepts:1. Neural language models2. Transformer-based models3. LSTM (Long Short-Term Memory)4. Natural Language Processing (NLP)### Triplets:1. (Transformer language model, Used-for, Natural Language Processing)2. (Transformer language model, Compare, LSTM)3. (Transformer language model, Compare, Neural language models)4. (Neural language models, Evaluate-for, Transformer language model)5. (LSTM, Is-a-Prerequisite-of, Transformer language model)
### Concept: error correction1. (error correction, Used-for, neural machine translation)2. (error correction, Part-of, GEC systems)3. (error correction, Evaluate-for, global errors)4. (error correction, Evaluate-for, local errors)5. (error correction, Compare, seq2seq models)6. (error correction, Is-a-Prerequisite-of, fluency boost learning)7. (error correction, Is-a-Prerequisite-of, fluency boosting inference)8. (error correction, Used-for, metric validation)9. (error correction, Is-a-Prerequisite-of, semantic parse correction)10. (error correction, Is-a-Prerequisite-of, masked language model incorporation)11. (error correction, Compare, Shallow Aggressive Decoding)12. (error correction, Used-for, multilingual GEC models)
### Triplets:1. sentiment analysis, Is-a-Prerequisite-of, aspect-based sentiment analysis 2. aspect-based sentiment analysis, Used-for, aspect extraction 3. aspect extraction, Is-a-Prerequisite-of, aspect-based sentiment analysis
### Triplets:1. sentiment polarity, Used-for, sentiment classification2. sentiment lexicons, Part-of, sentiment polarity3. negation words, Part-of, sentiment polarity4. intensity words, Part-of, sentiment polarity5. sentiment polarity, Is-a-Prerequisite-of, sentiment expression6. sentiment classification, Is-a-Prerequisite-of, sentence-level sentiment classification7. sentiment polarity, Compare, sentiment context8. sentiment words, Part-of, sentiment polarity9. negation words, Part-of, sentiment expression10. intensity words, Part-of, sentiment expression
### Extracted Concepts:1. Multimodal dialogue systems2. Goal-oriented conversations3. Artificial agents4. End-to-end goal-oriented visual dialogue system5. Reinforcement learning6. Rational Speech Act framework7. Dialogue flow8. Human evaluation9. Multilingual task-oriented dialogue (ToD) systems10. GlobalWoZ dataset### Triplets:1. (goal-oriented visual dialogue, Is-a-Prerequisite-of, Multimodal dialogue systems)2. (goal-oriented visual dialogue, Evaluate-for, Human evaluation)3. (goal-oriented visual dialogue, Used-for, Rational Speech Act framework)4. (Multimodal dialogue systems, Is-a-Prerequisite-of, Goal-oriented conversations)5. (Artificial agents, Is-a-Prerequisite-of, Goal-oriented conversations)6. (End-to-end goal-oriented visual dialogue system, Evaluate-for, Human evaluation)7. (End-to-end goal-oriented visual dialogue system
(`<concept>`, `Part-of`, `structured sentiment`)  (`<concept>`, `Part-of`, `structured sentiment`)  (`<concept>`, `Compare`, `semantic parsing`)  (`<concept>`, `Part-of`, `natural language processing`)  (`<concept>`, `Evaluate-for`, `natural language processing`)  (`<concept>`, `Hyponym-Of`, `sentiment analysis`)  (`<concept>`, `Used-for`, `affect prediction`)  
#### Triplets:1. (RNN, Compare, CNN)2. (RNN, Compare, DRNN)3. (CNN, Conjunction, Text categorization)4. (Model, Is-a-Prerequisite-of, Text categorization)5. (Model, Is-a-Prerequisite-of, Question answering)6. (NNs, Evaluate-for, Question similarity)
### Triplets:1. Multi modal dialogue, Enhances, Engagement2. Multi modal dialogue, Integrates-with, Text3. Multi modal dialogue, Incorporates, Images4. Multi modal dialogue, Utilizes, Audio5. Multi modal dialogue, Resolves, Ambiguity6. Multi modal dialogue, Combines, Modalities7. Multi modal dialogue, Requires, Contextual Understanding
### Triplets:1. neural text generation, Used-for, information-seeking conversation systems2. neural text generation, Compare, structured latent-variable approach3. neural text generation, Compare, Transformers4. neural text generation, Compare, text modeling approaches5. structured latent-variable approach, Evaluate-for, fine-grained control6. Transformers, Compare, state-of-the-art in several tasks
### Triplets:1. fact checking, Used-for, verifying the truthfulness of a claim2. fact checking, Evaluate-for, automating the veracity prediction of claims based on metadata and evidence3. fact checking, Evaluate-for, improving the performance of fact-checking systems4. fact checking, Compare, automated scientific fact checking5. premise articles, Part-of, fact checking6. scientific claim generation, Used-for, generating verifiable claims for fact checking7. scientific claim generation, Compare, zero-shot fact checking
### Triplets:1. recurrent neural network language models, Compare, reward augmented maximum likelihood approach2. maximum likelihood estimation, Evaluate-for, reward augmented maximum likelihood approach3. token-level loss smoothing, Compare, sequence-level smoothing approach4. dialog response generation, Is-a-Prerequisite-of, neural models
(None)
### Concepts:- Knowledge base (KB)- Question answering over knowledge base (KB-QA)- Neural network-based methods (NN-based)- End-to-end neural network model- Reading comprehension (RC)- Semi-supervised question answering- Generative Domain-Adaptive Nets- TriviaQA- Universal schema- Memory networks- Distantly supervised open-domain question answering (DS-QA)- Hierarchical attention network- Convolutional Neural Network (CNN) model- Text-based multiple choice question answering- SciTB- SciTail- DEISTE- HEAD-QA- Capsule networks### Triplets:- (end-to-end neural network model, Evaluate-for, question representation)- (Generative Domain-Adaptive Nets, Is-a-Prerequisite-of, semi-supervised question answering)- (Hierarchical attention network, Used-for, reading comprehension)- (Convolutional Neural Network (CNN) model
(`<document>`, Evaluate-for, question answering)(`<COREQA>`, Used-for, generating natural answers)(`<reading comprehension>`, Is-a-Prerequisite-of, question answering)(`<natural language processing>`, Is-a-Prerequisite-of, question answering)(`<document classification>`, Is-a-Prerequisite-of, question answering)(`<automatic question answering>`, Is-a-Prerequisite-of, question answering)
### Triplets:1. machine reading comprehension, is-a-Prerequisite-of, reading comprehension2. machine reading comprehension, used-for, question answering3. machine reading comprehension, Evaluate-for, answering generic questions4. machine reading comprehension, Compare, human reading comprehension5. machine reading comprehension, used-for, multi-passage reading comprehension6. reading comprehension, Evaluate-for, question answering7. reading comprehension, is-a-Prerequisite-of, answering generic questions
### Triplets:1. word semantics, captured using SemAxis, beyond sentiment2. SemAxis, outperforming state-of-the-art approaches in building domain-specific sentiment lexicons3. sentiment axis, outperforming the state-of-the-art approaches in building domain-specific sentiment lexicons
### Triplets:1. Category opinion sentiment quadruple, Is-a-Prerequisite-of, Aspect-Category-Opinion-Sentiment Quadruple2. Aspect-Category-Opinion-Sentiment Quadruple, Part-of, Implicit Aspects3. Aspect-Category-Opinion-Sentiment Quadruple, Part-of, Implicit Opinions
(`<concept>`, `Compare`, `computational linguistics`)  (`<concept>`, `Conjunction`, `Neural language models`)  (`<concept>`, `Compare`, `linguistic anomalies`)  (`<concept>`, `Is-a-Prerequisite-of`, `psycholinguistic knowledge`)  
### Triplets:1. Cross-View Language Modeling, Explain, Unsupervised machine translation2. Cross-View Language Modeling, Integrate, Cross-lingual pre-training3. Cross-View Language Modeling, Incorporate, Cross-modal pre-training4. Cross-View Language Modeling, Align, Multi-lingual data5. Cross-View Language Modeling, Align, Multi-modal data6. Cross-View Language Modeling, Utilize, Conditional masked language modeling7. Cross-View Language Modeling, Maximize, Mutual information
### Extracted Concepts:1. LSTM model2. Chinese NER3. Semantic parsing4. Seq2Seq models5. Neural dependency parser### Triplets:1. LSTM model, Compare, Seq2Seq models2. Semantic parsing, Evaluate-for, Seq2Seq models3. LSTM model, Used-for, Chinese NER4. Neural dependency parser, Is-a-Prerequisite-of, Structured topic model5. Semantic parsing, Is-a-Prerequisite-of, Neural dependency parser
1. ("character level language modeling", "Is-a-Prerequisite-of", "word-based models")2. ("character level language modeling", "Used-for", "processing Chinese text")3. ("character level language modeling", "Evaluate-for", "performance improvements")4. ("language models", "Compare", "character-level language models")5. ("language models", "Compare", "word-based models")6. ("Chinese word segmentation", "Is-a-Prerequisite-of", "character level language modeling")7. ("structured models", "Compare", "sequential recurrent neural networks")8. ("character level language modeling", "Used-for", "text infilling")
### Triplets:1. multilingual pre training, Used-for, improving performance on low resource languages2. training data, Is-a-Prerequisite-of, multilingual pre training3. multilingual pre training, Compare, sentence classification tasks4. multilingual pre training, Is-a-Prerequisite-of, unsupervised multilingual models5. multilingual pre training, Used-for, cross-lingual transfer
### Triplets:1. Word embeddings, capture, linguistic regularities2. Word embeddings, used-for, document analysis3. Word embeddings, have, enriched semantics4. Word embeddings, enable, effective model of word analogies5. Word embeddings, have, internal structures6. Word embeddings, learn, semantic information7. Word embeddings, neglect, latent meanings of morphemes
(`<concept>`, Compare, abstractive summarization)  (neural abstractive summarization, Is-a-Prerequisite-of, abstractive summarization)  (encode-attend-decode paradigm, Used-for, machine translation)  (neural sequence-to-sequence models, Used-for, abstractive text summarization)  (neural abstractive summarization, Evaluate-for, reduction of repetition)  (neural abstractive summarization, Evaluate-for, improvement of factual accuracy)  
### Triplets:1. gender bias, found in, machine translation2. gender bias, amplified by, NLP systems3. gender bias, reduced by, Semantic Autoencoder4. gender bias, mitigation technique is, Hard Debias algorithm5. gender bias, traced in, Knowledge base (KB) embeddings6. gender bias, problematic for, neural machine translation (NMT) models7. machine translation, impacted by, gender bias8. gender bias, addressed through, transfer learning9. gender bias, measured using, WinoMT challenge set10. gender bias, studied in, speech translation11. gender bias, identified in, semantic dependency parsing 12. gender bias, integrated into, post-processing procedures
### Triplets:1. unsupervised semantic parsing, is-a-prerequisite-of, StructVAE2. unsupervised machine translation, is-a-prerequisite-of, Lample et al. (2017)3. unsupervised machine translation, is-a-prerequisite-of, adversarial, unsupervised cross-lingual word embedding technique4. unsupervised machine translation, is-a-prerequisite-of, simple trick5. unsupervised machine translation, is-a-prerequisite-of, weak supervision signal6. unsupervised machine translation, is-a-prerequisite-of, identical words7. unsupervised machine translation, evaluate-for, robust induction8. unsupervised machine translation, evaluate-for, graph similarity metric9. unsupervised machine translation, evaluate-for, near-perfect correlation
### Triplets:1. RNNs, Used-for, language modeling2. RNNs, Compare, recurrent models3. RNNs, Evaluate-for, overfitting4. traditional training, Is-a-Prerequisite-of, overfitting5. RNNs, Used-for, back-propagation through time6. RNNs, Evaluate-for, model uncertainty7. RNNs, Is-a-Prerequisite-of, dialog systems8. RNNs, Used-for, machine reading
### Triplets:1. topic aware news, Is-a-Prerequisite-of, pre-trained language models2. topic aware news, Is-a-Prerequisite-of, masked language models
### Triplets:1. entity mention relation, Used-for, joint extraction of entity mentions and relations2. joint extraction of entity mentions and relations, Compare, end-to-end tree-based LSTM model3. tagging based methods, Evaluate-for, existing pipelined and joint learning methods4. entity typing task, Is-a-Prerequisite-of, predicting a set of free-form phrases for the target entity5. predicting entity types, Is-a-Prerequisite-of, head words that indicate the type of noun phrases6. predicting entity types, Evaluate-for, entity linking7. KG embeddings, Used-for, link prediction8. relation composition, Is-a-Prerequisite-of, effective reasoning on KG9. DihEdral model, Compare, deep learning models such as ConvE10. relation extraction, Is-a-Prerequisite-of, multiple entities in a document11. joint entity relation extraction, Evaluate-for, graph convolutional network12.
### Triplets:1. cross lingual embeddings, Used-for, bilingual lexicon induction2. cross lingual embeddings, Compare, bilingual word embeddings3. bilingual word embeddings, Compare, monolingual embeddings4. cross lingual embeddings, Compare, bilingual Sentiment Embeddings5. cross lingual embeddings, Is-a-Prerequisite-of, cross-lingual transfer6. bilingual word embeddings, Conjunction, cross-lingual classification7. bilingual lexicon induction, Evaluate-for, cross-lingual sentiment classification8. bilingual word embeddings, Is-a-Prerequisite-of, domain adaptation9. cross lingual embeddings, Compare, multilingual distributed representations10. cross lingual embeddings, Used-for, cross-lingual NLP
### Triplets:1. graph neural, Used-for, structured information2. graph neural, Is-a-Prerequisite-of, neural semantic parsers3. structured information, Part-of, entities4. neural semantic parsers, Evaluate-for, semantic parsing tasks5. neural semantic parsers, Is-a-Prerequisite-of, natural language processing6. semantic parsing tasks, Compare, syntactic-then-semantic dependency parsing7. neural networks, Used-for, powerful representation learning8. neural networks, Compare, neural conversational models
### Extracted Concepts:1. Natural Language Processing2. Relation Extraction3. Neural Language Model4. LSTM (Long Short-Term Memory)5. Neural Encoder-Decoder Transition-Based Parser6. Parse Syntax7. Sentence-Level Language Model### Triplets:1. Neural Language Model, Is-a-Prerequisite-of, LSTM2. Relation Extraction, Used-for, Natural Language Processing3. Neural Encoder-Decoder Transition-Based Parser, Is-a-Prerequisite-of, Parse Syntax4. LSTM, Evaluate-for, Neural Language Model5. Sentence-Level Language Model, Is-a-Prerequisite-of, Neural Language Model
1. (neural language model, Used-for, rhythmic poetry generation)2. (neural language model, Compare, document context)3. (neural language model, Evaluate-for, representing the broader document context)4. (neural dialogue, Evaluate-for, human-like dialogue responses)5. (neural dialogue, Is-a-Prerequisite-of, successful task-oriented dialogue systems)6. (neural dialogue, Used-for, guided generation)7. (neural dialogue, Used-for, incorporating knowledge into dialogue generation)8. (neural dialogue, Part-of, end-to-end neural dialogue generation)9. (neural dialogue, Is-a-Prerequisite-of, successful multi-turn dialogue modeling)10. (neural dialogue, Is-a-Prerequisite-of, automatic diagnosis dialogue systems)
### Triplets:1. prototype mention embeddings, Is-a-Prerequisite-of, Multi-Prototype Mention Embedding model2. prototype mention embeddings, Evaluate-for, disambiguation method3. Multi-Prototype Mention Embedding model, Used-for, disambiguation method4. prototype mention embeddings, Part-of, multi-prototype mention embeddings
### Triplets:1. annotated semantic relatedness, Compare, Agent-Artifact relations2. annotated semantic relatedness, Compare, Physical and Part-Whole relations3. annotated semantic relatedness, Evaluate-for, transformer-based language models4. transformer-based language models, Is-a-Prerequisite-of, ability to recognize analogies5. transformer-based language models, Is-a-Prerequisite-of, transformer-based language models capture knowledge6. transformer-based language models, Evaluate-for, identifying analogies7. transformer-based language models, Compare, GPT-2 and RoBERTa8. GPT-2 and RoBERTa, Compare, BERT9. transformer-based language models, Is-a-Prerequisite-of, pre-trained language models capture knowledge10. tree structures, Used-for, boosting performance in AS211. tree structures, Is-a-Prerequisite-of, ability of tree-structured models to absorb syntactic information12.
### Triplets:1. phonological typology, Compare, morphological typology2. morphological typology, Is-a-Prerequisite-of, phonological typology3. morphological typology, Evaluate-for, probabilistic treatment4. morphological typology, Evaluate-for, computational approaches5. morphological typology, Evaluate-for, simulation-based approaches6. morphological typology, Is-a-Prerequisite-of, morphological disambiguation7. morphological disambiguation, Evaluate-for, morphological analysis8. morphological typology, Is-a-Prerequisite-of, morphological tagging9. morphological tagging, Evaluate-for, performance10. morphological tagging, Is-a-Prerequisite-of, morphological analysis11. morphological analysis, Evaluate-for, syntactic traits12. morphological richness, Evaluate-for, dialectal variations13. morphology-aware alignment model, Evaluate-for, unsupervised
(`<context>`, Conjunction, `distributional vector space models`)  (`distributional vector space models`, Evaluate-for, `language understanding systems`)  (`morphologically rich`, Is-a-Prerequisite-of, `accurate representations for low-frequency word forms`)  (`morphologically rich`, Is-a-Prerequisite-of, `sensitivity to distinct lexical relations`)  
(<methods proposed recently for specializing word embeddings>, <rely-on>, <external knowledge>)  (<Pseudofit>, <specializing word embeddings according to semantic similarity>, <external knowledge>)  (<Pseudofit>, <exploits>, <notion of pseudo-sense>)  (<Pseudofit>, <building>, <several representations for each word>)  (<Pseudofit>, <using>, <representations for making initial embeddings more generic>)  
### Triplets:1. (word embedding model, gained popularity on, several tasks)2. (word embedding model, exhibits, compositionality)3. (vector, adds, small angle)4. (word vectors, represent, semantic composite)5. (Skip-Gram model, provides, theoretical justification)6. (Skip-Gram model, shows, additive compositionality)7. (Assumptions, hold, strictly sense)8. (distance, explains, success)9. (Skip-Gram model, related to, Sufficient Dimensionality Reduction framework)10. (Skip-Gram embeddings, optimal in, sense of Globerson and Tishby)
### Triplets:1. contextual entity, Used-for, text similarity measures2. contextual entity, Is-a-Prerequisite-of, sentiment analysis3. LSTM-based model, Conjunction, textual entailment recognition4. bidirectional gated recurrent structures, Part-of, composite deep neural network architecture5. character level dependencies, Part-of, bidirectional gated recurrent structures6. text representations, Evaluate-for, document matching approach7. Document matching approach, Compare, document similarity measures
#### Triplets:1. knowledge graph completion, Used-for, knowledge base population2. knowledge graph completion, Is-a-Prerequisite-of, natural language processing3. common goal, Conjunction, strategic communication4. Pocket Knowledge Base Population, Used-for, constructing a knowledge base5. PathNet, Evaluate-for, identifying implicit relations from text6. relational triple extraction, Evaluate-for, large-scale knowledge graph construction7. COMmonsEnse Transformers, Evaluate-for, generating diverse commonsense descriptions8. EWISE, Evaluate-for, fine-tuning sense embeddings9. dialogue systems, Compare, knowledge graphs10. Fact induction, Evaluate-for, link prediction model11. Open Information Extraction methods, Evaluate-for, finding trigger words12. Entity-Duet Neural Ranking Model, Used-for, integrating knowledge graphs in search systems13. neural model, Is-a-Prerequisite-of, dynamic knowledge graph embeddings14. deep learning architecture, Is-a
#### Triplets:1. hybrid neural model, outperforms, previous neural models2. ERRANT, facilitates, error type evaluation3. dependency parsing scheme, repairs, grammatical errors4. models, benefit, additional contextual information5. adversarial attacks, maintain, lexical correctness
### Triplets:1. offensive language classifier, Used-for, combat hateful, racist, and other forms of offensive speech2. offensive language classifier, Evaluate-for, robustness3. offensive language classifier, Is-a-Prerequisite-of, machine learning models4. offensive language classifier, Evaluate-for, against adversarial attacks5. offensive language classifier, Evaluate-for, detection of offensive language6. offensive language classifier, Used-for, maintain civility in online discussions
### Extracted Concepts:1. collaborative dialogue setting2. dialogue systems3. neural model4. structured knowledge5. unstructured language6. knowledge graph embeddings7. link prediction8. KG triples9. continuous vector spaces10. complex embeddings11. spectral version12. Fourier transform13. non-negativity constraints14. entailment constraints15. entity representations16. relation representations17. geometric understanding18. KG entities19. high-dimensional space20. KG embeddings methods21. entity alignment22. Multi-channel Graph Neural Network model23. graph embeddings24. vector representations25. knowledge graph completion26. query-dependent representation27. entities neighborhood attention28. automatic knowledge base construction29. generative models30. commonsense knowledge graphs31. COMmonsEnse Transformers (COMET)32. Word Sense Disambiguation (WSD)
(`<explanation attention>`, `Compare`, `Attention Mechanisms in NLP`)  (`<Explanation attention>`, `Evaluate-for`, `Model Predictions`)  (`<Attention Mechanisms in NLP>`, `Evaluate-for`, `Model Training`)  (`<Explanation attention>`, `Compare`, `Explanation Methods`)  (`<Explanation attention>`, `Evaluate-for`, `Model Interpretability`)  
### Triplets:1. recurrent neural networks, Have-Shown-Promising-Performance, language modeling2. LSTM (Long Short-Term Memory), Used-for, generation of conversational text3. model averaging, Used-for, learning weight uncertainty in RNNs4. Affect-LM, Is-an-extension-of, LSTM language model5. Universal Language Model Fine-tuning (ULMFiT), Used-for, transfer learning method6. Universal Language Model Fine-tuning (ULMFiT), Outperforms, the state-of-the-art7. LSTM Noisy Channel Model, Used-for, disfluency detection8. LSTM Noisy Channel Model, Applied-to, spontaneous speech transcripts9. Taylor’s law, Applies-to, natural language10. hierarchical multi-scale language model, Implements, hierarchical representation of a sequence11. hierarchical multi-scale language model, Involves, lower-level and higher-level networks12. Recurrent
### Triplets:1. neural model, Used-for, constituency parsing2. constituency parsing, Part-of, syntactic structures3. constituency parsing, Used-for, parsing4. constituency parsing, Is-a-Prerequisite-of, state-of-the-art results5. direct search, Compare, generative models6. constituency parsing, Evaluate-for, semantic dependency parsing7. constituency parsing, Is-a-Prerequisite-of, second-order semantic dependency parser
### Concept: event causality identification1. (Event Causality Identification, Is-a-Prerequisite-of, Neural Approach)2. (Event Causality Identification, Evaluate-for, Identifying Span and Nuclearity)3. (Event Causality Identification, Compare, Supervised Learning)4. (Event Causality Identification, Evaluate-for, Event Trigger Identification)5. (Supervised Learning, Is-a-Prerequisite-of, Event Causality Identification)6. (Supervised Learning, Is-a-Prerequisite-of, Event Trigger Identification)7. (Document-level Event Causality Identification, Is-a-Prerequisite-of, DECI Model)8. (DECI Model, Compare, Event Causality Identification)9. (DECI Model, Evaluate-for, Document-level Reasoning)10. (DECI Model, Part-of, Document-level Reasoning)
### Triplets:1. morphological compositionality, Used-for, word embeddings2. word embeddings, Used-for, train and enhance3. word embeddings, Hyponym-Of, embeddings4. word embeddings, Hyponym-Of, character language models5. character language models, Evaluate-for, inflected words6. character language models, Used-for, morphology data7. inflected words, Is-a-Prerequisite-of, explicitly modeling morphology8. inflected words, Is-a-Prerequisite-of, inflected words benefit9. character language models, Used-for, modeling morphology10. morphological supervision, Used-for, improve performance11. morphological supervision, Is-a-Prerequisite-of, improve bits-per-character performance12. morphological supervision, Is-a-Prerequisite-of, models outperform the baselines13. morphological supervision, Is-a-Prerequisite-of, enhance the diversity of the generated key
### Triplets:1. (pre-trained word embeddings, used-for, neural network architectures)2. (word embeddings, evaluate-for, linguistic regularities)3. (word embeddings, part-of, embeddings semantic concepts)4. (word embeddings, part-of, context-sensitive embeddings)5. (distributional vector spaces, specialize, the vectors of words)6. (distributional vector spaces, part-of, linguistic constraints)7. (sentence embeddings, compare, word vectors)8. (sentence embeddings, extracted-from, document-level information)
### Triplets:1. vision language, Used-for, neural image captioning systems2. neural image captioning systems, Used-for, visual language grounding3. neural image captioning systems, Compare, Multilingual BERT4. Multilingual BERT, Used-for, zero-shot cross-lingual model transfer5. Multilingual BERT, Compare, M-BERT6. Multilingual BERT, Compare, Knowledge graph language model7. Knowledge graph language model, Part-of, neural language model
(`<concept>`, `Compare`, `relation extraction`)  (`<concept>`, `Is-a-Prerequisite-of`, `multi-lingual neural relation extraction framework`)  (`<concept>`, `Is-a-Prerequisite-of`, `cross-lingual attention`)  (`<concept>`, `Evaluate-for`, `relation extraction`)  
1. (natural language processing, Used-for, pretrained language model)2. (state-of-the-art, Evaluate-for, pretrained language model)3. (LSTM, Is-a-Prerequisite-of, pretrained language model)
(`<compositional generalization semantic parsing>`, `Is-a-Prerequisite-of`, `<seq-to-seq models>`)(`<compositional generalization semantic parsing>`, `Evaluate-for`, `<performance improvement>`)(`<seq-to-seq models>`, `Used-for`, `<semantic parsing>`)(`<seq-to-seq models>`, `Evaluate-for`, `<compositional generalization>`)(`<compositional generalization semantic parsing>`, `Is-a-Prerequisite-of`, `<measurement of compositional generalization ability>`)(`<compositional generalization semantic parsing>`, `Is-a-Prerequisite-of`, `<handling natural language variation>`)(`<compositional generalization semantic parsing>`, `Is-a-Prerequisite-of`, `<structural hints for a Transformer model>`)(`<seq-to-seq model>`, `Evaluate-for`, `<modification of training distribution>`)(`<seq-to-seq models>`, `Used-for`, `<generalization performance improvement>`)
#### Triples:1. (explainable nlp, Used-for, explainable systems)2. (explainable nlp, Compare, traditional nlp models)3. (rules-based models, Compare, neural models)4. (explanations, Part-of, generative explanation framework)5. (explanations, Evaluate-for, model performance)6. (Transformer, Evaluate-for, personalized Transformer)7. (explainable recommendation models, Evaluate-for, personalized explanations)
### Triplets:1. (speech text translation, Is-a-Prerequisite-of, speech translation)2. (speech text translation, Part-of, end-to-end model)3. (speech text translation, Evaluate-for, translation quality improvement)4. (end-to-end model, Compare, cascaded model)5. (translation quality improvement, Evaluate-for, proposed approach)6. (end-to-end model, Is-a-Prerequisite-of, encoder pre-training)7. (end-to-end model, Compare, traditional cascade solutions)
### Triplets:1. multimodal embeddings, Used-for, multimodal sentiment analysis2. multimodal embeddings, Compare, word embeddings3. multimodal embeddings, Is-a-Prerequisite-of, LSTM-based model4. multimodal embeddings, Evaluate-for, Sentiment analysis5. multimodal embeddings, Part-of, Multimodal Named Entity Disambiguation concept6. multimodal embeddings, Evaluate-for, Multimodal Named Entity Disambiguation task7. word embeddings, Is-a-Prerequisite-of, multimodal word distributions8. LSTM-based model, Is-a-Prerequisite-of, multimodal sentiment analysis9. Sentiment analysis, Evaluate-for, Sentiment analysis in low-resource languages
### Triplets:1. document summarization, is-a-Prerequisite-of, abstractive summarization2. document summarization, Part-of, query-based summarization3. NeuralDater, Is-a-Prerequisite-of, document dating4. NeuralDater, Is-a-Prerequisite-of, document modeling5. extractive document summarization, Compare, abstractive document summarization6. extractive document summarization, Part-of, models7. extractive document summarization, Is-a-Prerequisite-of, SWAP-NET8. extractive document summarization, Part-of, sentence scoring9. extractive document summarization, Part-of, sentence selection
(`<concept>`, `Is-a-Prerequisite-of`, `data`)  (`bias mitigation`, `Is-a-Prerequisite-of`, `tolerance training`)  (`bias mitigation`, `Is-a-Prerequisite-of`, `label debiasing`)  (`bias mitigation`, `Is-a-Prerequisite-of`, `knowledge graph construction`)  (`bias mitigation`, `Used-for`, `pretrained language models`)  (`bias mitigation`, `Is-a-Prerequisite-of`, `evaluative model performance`)  
1. (language target language, Is-a-Prerequisite-of, NMT models)2. (language target language, Evaluate-for, translation)3. (language target language, Used-for, document retrieval)4. (translation, Evaluate-for, word generation)5. (image description, Conjunction, target-language decoder)6. (textual features, Compare, visual features)
1. (FAQ sit, Is-a-Prerequisite-of, conversational Question Answering)2. (Natural Questions dataset, Evaluate-for, Multi-grained machine reading comprehension framework)3. (Assessment, Evaluate-for, ASSET)4. (User traits, Is-a-Prerequisite-of, debate outcome)5. (MRC datasets, Part-of, Reading long documents)6. (Natural Questions dataset, Used-for, Answering open-domain questions)
### Triplets:1. Critical Role Dungeons and Dragons Dataset, contains, transcribed speech2. Critical Role Dungeons and Dragons Dataset, collected from, 159 Critical Role episodes3. Critical Role Dungeons and Dragons Dataset, contains, abstractive summaries4. Critical Role Dungeons and Dragons Dataset, contains, turns5. Critical Role Dungeons and Dragons Dataset, consists of, 398,682 turns6. VoxPopuli, provides, 400K hours of unlabeled speech data in 23 languages7. VoxPopuli, contains, 1.8K hours of transcribed speeches in 15 languages8. VoxPopuli, aligned, oral interpretations into 15 target languages9. QASR, collected from, broadcast domain10. QASR, contains, 2,000 hours of speech sampled at 16kHz11. QASR, suitable for, training and evaluating speech recognition systems12. N-best ASR
```(reading comprehension datasets, Is-a-Prerequisite-of, reading comprehension system)(reading comprehension datasets, Evaluate-for, system evaluation)(reading comprehension datasets, Compare, natural-language understanding datasets)(reading comprehension datasets, Is-a-Prerequisite-of, dataset analysis)(reading comprehension datasets, Evaluate-for, machine learning model)(reading comprehension datasets, Is-a-Prerequisite-of, neural reading comprehension model)(reading comprehension datasets, Is-a-Prerequisite-of, reading comprehension performance)(reading comprehension datasets, Is-a-Prerequisite-of, question answering system)(reading comprehension datasets, Is-a-Prerequisite-of, machine comprehension system)(reading comprehension datasets, Evaluate-for, model performance)(reading comprehension datasets, Is-a-Prerequisite-of, TriviaQA dataset)(reading comprehension datasets, Is-a-Prerequisite-of, squad dataset)(reading comprehension datasets, Is-a-Prerequisite-of, WikiReading dataset)(reading comprehension datasets, Is-a-
### Triplets:1. (machine translation, relies on, bi-directional LSTMs)2. (machine translation, enhanced by, Deep Neural Networks (DNNs))3. (machine translation, propose, linear associative units (LAU))4. (machine translation, improves by, incorporating source syntax)5. (machine translation, enhances, statistical machine translation)6. (machine translation, interprets, agent messages)7. (neural networks, contribute to, neural machine translation)
### Triplets:1. dialogue modeling, Used-for, building dialog systems2. dialogue modeling, Conjunction, natural language understanding3. dialogue modeling, Evaluate-for, state-of-the-art models4. dialogue modeling, Is-a-Prerequisite-of, interpretable response generation5. dialogue modeling, Compare, current models in single-turn dialogue modeling6. interpretable response generation, Is-a-Prerequisite-of, human understanding7. interpretable response generation, Evaluate-for, interpretable actions8. interpretable response generation, Used-for, integrating with encoder-decoder dialog models
1. (cross-lingual, is-a-Prerequisite-of, cross-modal)2. (cross-lingual, part-of, SAN)3. (cross-modal, part-of, SAN)4. (cross-lingual, Evaluate-for, modeling positional relationships)5. (cross-modal, Evaluate-for, modeling positional relationships)6. (cross-lingual, Used-for, tackle word order divergences)
1. (Neural network models, Have shown opportunities for, Multi-task learning)2. (Neural network models, Extract common and task-invariant features, Shared layers)3. (Neural network models, Adversarial multi-task learning framework, Alleviating shared and private latent feature spaces)4. (Neural network models, Learned shared knowledge, Off-the-shelf knowledge)
(`<concept>`, Compare, `distributional vector space models`)  (`word vector`, Is-a-Prerequisite-of, `distributional vector space models`)  (`word vector`, Used-for, `word similarity`)  
### Concept: textual feature1. (textual feature, used-for, part-of-speech induction)2. (textual feature, used-for, named-entity recognition)3. (part-of-speech induction, Is-a-Prerequisite-of, coreference resolution)4. (named-entity recognition, Is-a-Prerequisite-of, coreference resolution)5. (textual feature, used-for, relation classification)6. (relation classification, Is-a-Prerequisite-of, relation extraction)7. (relation classification, used-for, explainable multi-hop QA)
(`<language model like>`, `Is-a-Prerequisite-of`, `LSTM (Long Short-Term Memory) model`)  (`<language model like>`, `Used-for`, `generation of conversational text`)  (`<LSTM (Long Short-Term Memory) model>`, `Used-for`, `language modeling`)  (`<LSTM (Long Short-Term Memory) model>`, `Is-a-Prerequisite-of`, `Affect-LM model`)  
### Triplets:1. attention weight, is-utilized-in, model predictions2. model predictions, are-based-on, attention weights3. attention weight, is-used-for, explaining model predictions4. attention weight, erroneously-represents, importance of input tokens
### Triplets:1. neural word segmentation, Used-for, aspect-based sentiment analysis2. neural word segmentation, Is-a-Prerequisite-of, Chinese word segmentation3. neural word segmentation, Evaluate-for, coherence of aspects4. neural word segmentation, Used-for, Chinese relation extraction5. neural word segmentation, Compare, lattice LSTM6. neural word segmentation, Used-for, Chinese NER7. neural word segmentation, Is-a-Prerequisite-of, word embeddings8. neural word segmentation, Evaluate-for, coherence of aspects9. LSTM, Is-a-Prerequisite-of, lattice LSTM10. neural networks, Part-of, lattice-based encoders
#### Triplets:1. gazetteers, Used-for, part-of-speech induction2. LSTM-CRF architecture, Is-a-Prerequisite-of, nested named entity recognition3. LSTM-CRF architecture, Is-a-Prerequisite-of, neural network architectures4. neural network architectures, Is-a-Prerequisite-of, nested named entity recognition5. nested named entity recognition, Is-a-Prerequisite-of, LSTM-CRF architecture6. nested named entity recognition, Is-a-Prerequisite-of, neural network architectures7. ELMo, Is-a-Prerequisite-of, nested named entity recognition8. BERT, Is-a-Prerequisite-of, nested named entity recognition9. Flair, Is-a-Prerequisite-of, nested named entity recognition
(None)
(None)
### Triplets:1. entity recognition ner, used-for, named entity recognition2. entity recognition ner, Evaluate-for, entity mentions3. gazetteers, used-for, named entity recognition4. named entity recognition, Is-a-Prerequisite-of, named entity recognition5. entity recognition ner, Compare, named entity recognition6. entity recognition ner, Compare, sequence labeling7. entity recognition ner, Evaluate-for, NER data8. entity recognition ner, Evaluate-for, performance9. named entity recognition, Hyponym-Of, named entity recognition10. gazetteers, Is-a-Prerequisite-of, part-of-speech induction
(`<query concept>`, `Used-for`, `improving language model performance`)  (`<perplexity`, `Evaluate-for`, `language model prediction`)  (`<language model`, `Evaluate-for`, `training`)  
1. (link prediction, Used-for, knowledge graph completion)2. (link prediction, Is-a-Prerequisite-of, automatic knowledge graph completion)3. (holographic embeddings, Equivalent-to, complex embeddings)4. (OE, Used-for, transitive relational data modeling)5. (factuality prediction, Evaluate-for, factuality corpus)
1. (multilingual language models, Enhance performance for typologically diverse languages, pretrained monolingual models)2. (monolingual representation, Train on same data, designated monolingual tokenizer)3. (multilingual model, Exhibit performance decrease over corresponding monolingual representations, specialized monolingual tokenizer)4. (language representation models, focus on specific levels of linguistic units)5. (language representation models, Usually focus on specific levels of linguistic units)6. (universal language representation learning, Embeddings of different levels of linguistic units in uniform vector space)
### Concept: model generated summary1. (model, Used-for, generating summaries)2. (summarization model, Is-a-Prerequisite-of, model generated summary)3. (source text, Evaluate-for, summary generation)4. (model, Compare, existing models)5. (abstractive summarization, Is-a-Prerequisite-of, model generated summary) 
### Triplets:1. information retrieval task, is-a-prerequisite-of, complex reasoning2. information retrieval task, is-a-prerequisite-of, detecting concealed information3. detecting concealed information, is-a-prerequisite-of, deception detection4. information retrieval task, used-for, boosting Neural Machine Translation performance5. information retrieval task, evaluate-for, trustworthiness estimation of information sources6. boosting Neural Machine Translation performance, evaluate-for, NMT performance7. boosting Neural Machine Translation performance, compare, baseline systems
### Triplets:1. opinion entity extraction, Is-a-Prerequisite-of, aspect term extraction2. opinion entity extraction, Evaluate-for, sentiment classification3. aspect term extraction, Part-of, opinion entity extraction4. opinion entity extraction, Evaluate-for, sentiment classification5. opinion entity extraction, Used-for, opinion mining systems
### Triplets:1. transformer based language, used-for, generating dialogue responses2. transformer based language, Compare, BERT3. transformer based language, Compare, XLNet4. transformer based language, is-a-Prerequisite-of, fine-tuning models5. BERT, Compare, XLNet6. transformer based language, Part-of, SPECTER7. transformer based language, Part-of, T-TA
(None)
#### Concept: modeling morphological1. (modeling morphological, Used-for, capturing linguistic regularities)2. (modeling morphological, Involves, predicting the syntactic traits of a word)
### Triplets:1. word embedding, gained popularity on tasks, word analogy questions and caption generation2. vector, adding two word vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words3. embedding models, used for knowledge base completion4. embeddings, capturing linguistic regularities of the language5. embeddings, transferable across languages6. embeddings, learning general-purpose, paraphrastic sentence embeddings7. embeddings, improving the accuracy of prepositional phrase (PP) attachment model8. embeddings, used for predicting prepositional phrase (PP) attachments9. embeddings, learn discourse-specific word embeddings10. embeddings, extracting pseudo-parallel sentences from monolingual corpora
### Triplets:1. multilingual machine translation, Used-for, improving translation accuracy2. multilingual machine translation, Compare, separating the processes of representations and word generation3. multilingual machine translation, Compare, outperforming traditional phrase-based systems4. multilingual machine translation, Compare, leveraging multi-source NMT and SMT systems5. chunk-based decoders, Part-of, multilingual machine translation6. source-side syntactic trees, Is-a-Prerequisite-of, multilingual machine translation
(`<concept>`, Is-a-Prerequisite-of, `discourse parsers`)  (`discourse segmenters`, Used-for, `end-to-end discourse parsing`)  (`discourse segmenters`, Evaluate-for, `natural language inference`)  (`discourse segmenters`, Compare, `neural semantic parsers`)  
### Triplets:1. unsupervised morphological analysis, Is-a-Prerequisite-of, neural model for morphological inflection generation2. unsupervised morphological analysis, Is-a-Prerequisite-of, unsupervised morphological paradigm completion3. unsupervised morphological analysis, Evaluate-for, language-independent model4. unsupervised morphological analysis, Evaluate-for, AlephBERT5. unsupervised morphological paradigm completion, Is-a-Prerequisite-of, neural model for morphological inflection generation6. neural model for morphological inflection generation, Evaluate-for, unsupervised morphological analysis7. AlephBERT, Is-a-Prerequisite-of, unsupervised morphological analysis.
### Extracted Concepts:1. Neural semantic parser2. Question answering system3. Rhythmic poetry generation4. Parsing natural language descriptions5. Text similarity measures6. Reading comprehension datasets7. GuessTwo task8. Natural language interfaces to databases### Triplets:1. (natural language text, Used-for, neural semantic parser)2. (natural language text, Used-for, question answering system)3. (natural language text, Used-for, rhythmic poetry generation)4. (natural language text, Used-for, parsing natural language descriptions)5. (natural language text, Used-for, reading comprehension datasets)6. (natural language text, Used-for, inference in GuessTwo task)7. (natural language text, Used-for, natural language interfaces to databases)
(`<head concept>`, `Compare`, `efficient neural text matching models`)(`<head concept>`, `Used-for`, `transfer learning for multi-turn information seeking conversations`)(`<advance neural text matching models>`, `Evaluate-for`, `industrial applications`)(`<neural encoder-decoder model>`, `Part-of`, `Single document summarization systems`)(`<neural encoder-decoder model>`, `Evaluate-for`, `large datasets`)(`<Multi-News>`, `Is-a-Prerequisite-of`, `Multi-document summarization of news articles`)(`<Multi-News>`, `Used-for`, `end-to-end model for multi-document summarization`)(`<unlabeled text corpora>`, `Part-of`, `unsupervised neural text simplification framework`)(`<unlabeled text corpora>`, `Used-for`, `training the text simplification model`)(`<neural text generation modeling>`, `Evaluate-for`, `societal concerns`)(`<neural text generation modeling
(`<language model fine tuning>`, `Is-a-Prerequisite-of`, `Transfer Learning`)(`<language model fine tuning>`, `Used-for`, `NLP tasks`)(`<language model fine tuning>`, `Compare`, `Training from scratch`)(`<Transfer Learning>`, `Is-a-Prerequisite-of`, `Language model fine tuning`)(`<NLP tasks>`, `Require`, `<language model fine tuning>`)
1. (semantic representation, Compare, syntactic schemes)2. (response selection, Is-a-Prerequisite-of, multi-turn conversation)3. (sequential matching network, Used-for, address problems)4. (sequential matching network, Evaluate-for, response selection)5. (response selection, Compare, state-of-the-art methods)6. (spoken language acquisition, Part-of, spoken instances)7. (transition-based parser, Is-a-Prerequisite-of, handle graph structures)8. (transition-based parser, Evaluate-for, semantic parsing)9. (parser, Compare, end-to-end tree-based LSTM model)10. (parser, Compare, traditional parsing techniques)11. (semantic parsers, Compare, bilexical dependencies)12. (parser, Used-for, encoding domain)13. (semantic relations, Compare, prepositions and possessives)14. (prepositions and possessives, Part-of, marker's role in context)15. (AMR
### Triplets:1. word embeddings, capture, linguistic regularities2. bilingual word embeddings, Used-for, bilingual tasks3. bilingual tasks, Include, bilingual lexicon induction4. bilingual word embeddings, Used-for, cross-lingual classification5. bilingual word embeddings, considered-for, domain adaptation6. cross-lingual twitter sentiment classification, Improve, by using bilingual embeddings7. bilingual word embeddings, Improve, medical bilingual lexicon induction8. bilingual word embeddings, Leverage, images
### Concept: event extraction1. (event extraction, Is-a-Prerequisite-of, aspect-based sentiment analysis)2. (aspect-based sentiment analysis, Used-for, existing works)3. (event extraction, Compare, supervised event extraction)4. (event extraction, Is-a-Prerequisite-of, key arguments detection)5. (event extraction, Is-a-Prerequisite-of, trigger words identification)6. (event extraction, Is-a-Prerequisite-of, arguments exploitation in event detection)7. (supervised learning, Is-a-Prerequisite-of, events extraction)8. (supervised learning, Evaluate-for, efficient model performance)9. (event extraction, Part-of, joint extraction task)10. (event extraction, Compare, feature extraction)11. (event extraction, Is-a-Prerequisite-of, relation extraction)12. (relation extraction, Used-for, class ties leveraging)13. (relation extraction, Compare, supervised relation extraction)14
### Triplets:1. Document Dating, Is-a-Prerequisite-of, Document Embedding Enhanced Bi-RNN2. Named Entity Recognition (NER), Is-a-Prerequisite-of, Sequence Labeling3. Named Entity Recognition (NER), Is-a-Prerequisite-of, Entity Linking4. Named Entity Recognition (NER), Is-a-Prerequisite-of, Mention Detection5. Named Entity Recognition (NER), Is-a-Prerequisite-of, Ordinally Forgetting Encoding (FOFE)6. Named Entity Recognition (NER), Is-a-Prerequisite-of, Fixed-Size Representation7. Named Entity Recognition (NER), Is-a-Prerequisite-of, Feedforward Neural Network (FFNN)8. Named Entity Recognition (NER), Is-a-Prerequisite-of, Label Prediction9. Named Entity Recognition (NER), Is-a-Prerequisite-of, Local Detection Approach10. Named Entity Recognition (NER), Is-a-Prerequisite-of, Entity
### Triplets:1. neural network model, outperforms, state-of-the-art rumor detection approaches2. BiLSTM, capture, contextual information3. recursive neural models, achieve, better performance4. automatic detection, improve, parsing quality5. neural ECD models, outperform, state-of-the-art6. recursive neural models, achieve, much better performance7. self-labeled data, analyze, hashtag usages
1. (Natural Symbolic Machine, Used-for, Neural programming)2. (Natural Symbolic Machine, Used-for, Symbolic computer)3. (Natural Symbolic Machine, Evaluate-for, Task reward)4. (Neural Symbolic Machine, Is-a-Prerequisite-of, Structured prediction problem)5. (Natural Language Generation system, Evaluate-for, Corrective referring expressions)6. (Text similarity measures, Evaluate-for, Plagiarism detection)7. (Text similarity measures, Evaluate-for, Paraphrase recognition)8. (Text similarity measures, Evaluate-for, Textual entailment recognition)9. (Universal schema, Used-for, Natural language question answering)10. (Universal schema, Used-for, Memory networks)11. (Universal schema, Is-a-Prerequisite-of, End-to-end training)12. (ParaNMT-50M dataset, Used-for, Paraphrase generation)13. (ParaNMT-50M
1. (Neural Symbolic Machine, Used-for, Language understanding)2. (Neural Symbolic Machine, Used-for, Symbolic reasoning)3. (Semantic parser, Is-a-Prerequisite-of, Natural language understanding)4. (COREQA, Evaluate-for, Natural language understanding)5. (TEXTFLOW, Compare, COREQA)6. (Neural language model, Part-of, Natural language understanding)
### Triples:1. lingual semantic parsing, Used-for, transducing natural language utterances into formal meaning representations2. neural semantic parsing, Is-a-Prerequisite-of, lingual semantic parsing3. semantic parsing, Is-a-Prerequisite-of, lingual semantic parsing4. StructVAE, Used-for, semi-supervised semantic parsing5. Semantic parsing, Is-a-Prerequisite-of, StructVAE6. Semantic parsing, Used-for, transducing natural language utterances into formal meaning representations
### Triplets:1. COREQA, Used-for, natural answers2. answering system, Is-a-Prerequisite-of, factoid question answering3. factoid question answering, Is-a-Prerequisite-of, final answer selection stage4. EviNets, Evaluate-for, factoid question answering5. HEAD-QA, Evaluate-for, answering system
1. (Memory networks, Used-for, aspect sentiment classification)2. (Attention mechanism, Used-for, aspect sentiment classification)3. (Target-sensitive memory networks, Compare, Memory networks)4. (Target-sensitive memory networks, Is-a-Prerequisite-of, aspect sentiment classification)5. (Neural models, Used-for, aspect sentiment classification)
### Triplets:1. hierarchical attention network, Used-for, encoding sentences in a story2. hierarchical attention network, Used-for, score candidate endings3. hierarchical attention network, Compare, human accuracy estimation4. hierarchical attention network, Is-a-Prerequisite-of, novel hierarchical attention network5. hierarchical attention network, Used-for, reading comprehension style question answering6. hierarchical attention network, Used-for, attention and fusion across layers7. hierarchical attention network, Is-a-Prerequisite-of, multi-granularity fusion approach8. hierarchical attention network, Used-for, hierarchical attention on answer span9. hierarchical attention network, Used-for, structured projection of intermediate gradients10. hierarchical attention network, Used-for, Adversarial Attention Network for multi-dimensional emotion regression11. hierarchical attention network, Used-for, assessing discourse coherence12. hierarchical attention network, Used-for, learning accurate user representations for news recommendation13. hierarchical attention network, Used-for
### Triplets:1. spelling error, is-a-Prerequisite-of, detecting the native language2. spelling error, used-for, identifying relations between pairs of entity mentions3. spelling error, compare, grammatical error4. spelling error, part-of, natural language understanding5. spelling error, evaluate-for, Chinese Spelling Check task 6. spelling error, hyponym-of, grammatical error correction7. spelling error, conjunction, language understanding8. spelling error correction, used-for, improving written communication9. spelling error correction, evaluate-for, language learners10. spelling error correction, is-a-Prerequisite-of, error type identification
(None)
### Triplets:1. attention based explanation, Used-for, neural networks2. neural networks, Used-for, adjusting focus3. neural networks, Compare, human attention4. human attention, Compare, machine attention5. human attention, Evaluate-for, quantitative assessment6. machine attention, Evaluate-for, quantitative assessment7. machine attention, Conjunction, deep learning models
(None)
### Triplets:1. attention based, Used-for, interpretation2. attention based, Evaluate-for, performance3. attention based, Compare, performance4. attention based, Is-a-Prerequisite-of, understanding5. attention based, Part-of, neural model6. LSTM network, Used-for, attention based7. attention based, Used-for, translation8. Bi-LSTM, Used-for, attention based9. self-attention, Evaluate-for, performance10. Tree-LSTM Networks, Compare, LSTM Networks11. Multi-Source Sequence-to-Sequence Learning, Part-of, attention based
1. (sentence compression model, uses, Long Short-Term Memory (LSTM))2. (sentence compression model, based on, recurrent neural network)3. (sentence compression model, used-for, deleting words)4. (sentence compression model, has, syntactic features)5. (sentence compression model, includes, syntactic constraints)6. (sentence compression model, applies, Integer Linear Programming (ILP))7. (sentence compression model, evaluated in, cross-domain setting)
1. (<Source sentence>, Encode-by, bi-directional LSTMs)2. (<Machine translation system>, Achieve-accuracy-on, WMT'16 English-Romanian translation)3. (<Architectural model>, Based-on, succession of convolutional layers)4. (<Neural Machine Translation>, Enhanced-by, Deep Neural Networks)5. (NMT, Suffers-from, severe gradient diffusion)6. (<Proposed method>, Reduces-gradient-propagation-path-inside, recurrent unit)7. (model, Achieve-comparable-results-with, state-of-the-art)
### Triplets:1. Vector cosine, Used-for, Semantic similarity2. Word Embeddings, Is-a-Prerequisite-of, Semantic similarity3. Multilingual contextual embedding models, Compare, Monolingual contextual embedding models4. MLQA, Evaluate-for, Multilingual evaluation benchmarks5. MRS, Evaluate-for, Multilingual reply suggestion dataset
### Concept: event language model1. (event language model, Used-for, generation of rhythmic poetry)2. (event language model, Compare, neural language model)3. (neural language model, Evaluate-for, learn common poetic devices)
### Concept: event argument extraction1. (Document-level event extraction, Is-a-Prerequisite-of, Event argument extraction)2. (Implicit Event Argument Extraction, Is-a-Prerequisite-of, Event argument extraction)3. (Frame-aware Event Argument Extraction, Is-a-Prerequisite-of, Event argument extraction)4. (Joint extraction of entities and relations, Evaluate-for, Event argument extraction)5. (Sentence-level extractors, Compare, DEE task)6. (End-to-end model, Used-for, Document-level event extraction)7. (Heterogeneous Graph-based Interaction Model with a Tracker, Used-for, Document-level event extraction)
### Triplets:1. unsupervised syntactic parsing, is-a-Prerequisite-of, dependency triples2. unsupervised syntactic parsing, Evaluate-for, Neural Networks (NNs)3. unsupervised syntactic parsing, is-a-Prerequisite-of, syntactic parsing systems4. unsupervised syntactic parsing, Evaluate-for, syntactic dependency parsing5. unsupervised syntactic parsing, is-a-Prerequisite-of, syntax-aware language model6. unsupervised syntactic parsing, Evaluate-for, Parsing-Reading-Predict architecture7. unsupervised syntactic parsing, is-a-Prerequisite-of, semantic parsing8. unsupervised syntactic parsing, is-a-Prerequisite-of, syntactic dependency parsing9. unsupervised syntactic parsing, is-a-Prerequisite-of, Synchronous Semantic Decoding (SSD)10. dependency triples, is-a-Prerequisite-of, unsupervised semantic
### Triplets:1. (LSTM, Used-for, short term memory)2. (Long Short Term Memory, Used-for, sequence prediction)3. (Affect-LM, Is-a-Prerequisite-of, Long Short-Term Memory)4. (LSTM, part-of, neural network model)5. (Noisy Channel Model, Evaluate-for, LSTM)6. (Affect-LM, Evaluate-for, emotional content)7. (context-aware neural network model, Is-a-Prerequisite-of, LSTM)
```('neural network models', 'Used-for', 'multi-task learning')('multi-task learning', 'Is-a-Prerequisite-of', 'learning the shared layers')('shared layers', 'Part-of', 'extract the common and task-invariant features')('neural techniques', 'Evaluate-for', 'end-to-end computational argumentation mining')('argument component level', 'Compare', 'dependency parsing')('dependency parsing', 'Is-a-Prerequisite-of', 'perform robustly across classification scenarios')('deep neural networks', 'Evaluate-for', 'enhanced the state-of-the-art Neural Machine Translation')('Neural Machine Translation', 'Compare', 'modeling complex functions and capturing complex linguistic structures')('RNNs', 'Evaluate-for', 'language modeling')('LSTM unit and GRU', 'Compare', 'LAUs')('geolocation prediction model', 'Compare', 'ensemble approaches')('ensemble approaches', 'Is-a-Prerequisite-of', 'accuracy
### Triplets:1. (End-to-end models for speech translation, Compare, Traditional cascade of separate ASR and MT models)2. (Encoder pre-training, Is-a-Prerequisite-of, End-to-end Speech Translation)3. (Encoder pre-training, Is-a-Prerequisite-of, SATE method)4. (Multi-teacher knowledge distillation, Evaluate-for, Pre-training models)5. (Direct speech-to-speech translation model, Is-a-Prerequisite-of, Sequence-to-sequence speech-to-unit translation model)
### Extracted Concepts:1. Natural language processing2. Neural semantic parser3. Sequence-to-sequence models4. Expressive kernels5. Deep neural networks6. Word embeddings7. Bidirectional language models8. Subword units9. Character representations### Triplets:1. Natural language processing, Used-for, Neural semantic parser2. Bidirectional language models, Evaluate-for, Pre-trained word embeddings3. Word embeddings, Compare, Gaussian embeddings4. Word embeddings, Evaluate-for, Sequence labeling tasks5. Word embeddings, Part-of, NLP systems6. Subword units, Is-a-Prerequisite-of, Word representations
### Triplets:1. abstractive summarization model, Used-for, generate a shorter version of the document2. abstractive summarization model, Compare, query-based summarization3. abstractive summarization model, Is-a-Prerequisite-of, document summarization research4. abstractive summarization model, Compare, extractive summarization models5. document, Is-a-Prerequisite-of, abstractive summarization6. abstractive summarization model, Is-a-Prerequisite-of, summarize of higher quality and reducing repetition7. abstractive summarization model, Is-a-Prerequisite-of, neural text summarization models
(`<concept>`, `Generates`, `formal meaning representations`)  (`<formal meaning representations>`, `Is-a-Prerequisite-of`, `semantic decoding`)  (`<ParaNMT-50M>`, `Contains`, `sentential paraphrase pairs`)  (`<annotating NL utterances>`, `Is-a-Prerequisite-of`, `structured meaning representations`)  
### Triplets:1. distributional vector space models, used-for, language understanding systems2. expressive kernels, achieve, excellent performance3. neural network-based methods, used-for, KB-QA4. neural machine translation, improve, performance5. layer-wise relevance propagation, used-for, interpret internal workings6. neural encoder-decoder transition-based parser, used-for, semantic representation7. multimodal word distributions, used-for, capture uniquely expressive semantic information8. Gated-Attention Reader, obtain, state-of-the-art results
1. (semantic schemes, compare, phonological representation)2. (neural language model, used-for, phonological representation)3. (selective encoding model, evaluate-for, phonological representation)
### Triples:1. (fake news detection, is-a-Prerequisite-of, fact-checking research)2. (fake news detection, Used-for, combating fake news)3. (fake news detection, compares, news content)4. (fake news detection, Used-for, verify whether a news document is trusted or fake)5. (fake news detection, Used-for, differentiate fake news from real ones)6. (fake news detection, Used-for, prevent dissemination of misinformation on social media)7. (fake news detection, Used-for, fake news detection at the knowledge element level)8. (fake news detection, Used-for, detecting biased language)9. (fake news detection, evaluated-for, accuracy gain)10. (fake news detection, evaluated-for, improving performance of basic fake news detectors)
### Triplets:1. (joint extraction of entity mentions and relations, utilizes, attention-based recurrent neural network)2. (joint extraction of entity mentions and relations, utilizes, long short term memory (LSTM) network)3. (joint extraction of entity mentions and relations, Extracts, semantic relations)4. (joint extraction of entity mentions and relations, does not have access to, dependency trees)5. (semantic relations, include, Agent-Artifact relations)6. (semantic relations, include, Physical and Part-Whole relations)
```(document level relation extraction, part-of, relation extraction)(document level relation extraction, Used-for, integrating information within and across multiple sentences)(document level relation extraction, Compare, span-level relation extraction)(document level relation extraction, Is-a-Prerequisite-of, inter-sentence relation extraction)(document level relation extraction, Is-a-Prerequisite-of, aggregating relevant information in the document)(document level relation extraction, Is-a-Prerequisite-of, multi-hop reasoning)(document level relation extraction, Is-a-Prerequisite-of, relational reasoning across sentences)(document level relation extraction, part-of, Neural Relation Extraction)(document level relation extraction, part-of, inter-sentence relations)(document level relation extraction, part-of, Document-level Relation Extraction)(document level relation extraction, part-of, NRE)(document level relation extraction, part-of, Automated Knowledge Base Construction)(document level relation extraction, Compare, span-level task)(document level relation extraction, Compare, token-level models)(document level relation extraction
1. (cross-lingual summarization, Uses, unsupervised machine translation)2. (cross-lingual summarization, Uses, bilingual tasks)3. (cross-lingual summarization, Uses, CL-HYPERLEX)4. (cross-lingual summarization, Used-for, translation process)5. (CL-HYPERLEX, Part-of, MONOLINGUAL GR-LE datasets)6. (translation process, Evaluate-for, final summary)
### Triplets:1. sentence embeddings, Used-for, generic sentence embeddings2. sentence embeddings, Evaluate-for, common sense knowledge extraction3. sentence embeddings, Is-a-Prerequisite-of, sentence similarity prediction4. sentence embeddings, Compare, word vectors5. sentence embeddings, Is-a-Prerequisite-of, sentence classification6. sentence embeddings, Evaluate-for, semantic similarity model7. sentence embeddings, Evaluate-for, cross-lingual sentence similarity model8. sentence embeddings, Compare, generic word embeddings 
### Triplets:1. semantic parsing, is-a-prerequisite-of, Shot semantic parsing2. model, is-used-for, semantic parsing3. Document, part-of, Object-oriented Neural Programming4. Dual-purpose, Evaluate-for, concept5. natural language descriptions, part-of, Shot semantic parsing
### Triplets:1. EPAr, Explores, Relevant documents2. EPAr, Proposes, Answer3. EPAr, Assembles, Key sentence4. HDE graph, Accomulates, Evidences5. HDE graph, Delivers, Competitive result6. BERT, Handles, Millions of documents7. BERT, Efficiently, Builds cognitive graph8. DynSAN, Achieves, New state-of-the-art performance
### Triplets:1. human annotated explanation, Used-for, generating QA pairs2. human annotated explanation, Evaluate-for, training classifiers3. human annotated explanation, Evaluate-for, generating explanations4. human annotated explanation, Part-of, CLUES dataset5. generating explanations, Is-a-Prerequisite-of, providing testable explanations6. explanation-guided representations, Used-for, interpret explanations7. text–image matching discrimination, Is-a-Prerequisite-of, METER approach
### Extracted Concepts and Relationships:1. annotated data <relation> heavily relying2. annotated data <relation> major obstacle3. annotated data <relation> needed for training4. annotated data <relation> lack of5. annotated data <relation> facilitates error type evaluation6. annotated data <relation> required for state-of-the-art performance7. annotated data <relation> expensive to obtain8. annotated data <relation> automates the error detection process9. annotated data <relation> essential for various NLP tasks
### Triplets:1. question answering, Used-for, reading comprehension2. question answering, Evaluate-for, models3. question answering, Is-a-Prerequisite-of, reading comprehension4. reading comprehension, Is-a-Prerequisite-of, question answering5. question answering, Compare, knowledge base question answering 6. knowledge base question answering, Evaluate-for, neural network-based methods7. neural network-based methods, Is-a-Prerequisite-of, knowledge base question answering8. question answering, Is-a-Prerequisite-of, knowledge base question answering9. knowledge base question answering, Is-a-Prerequisite-of, question answering10. question answering, Used-for, answer generation for questions11. knowledge base question answering, Is-a-Prerequisite-of, understanding natural texts and answering questions
