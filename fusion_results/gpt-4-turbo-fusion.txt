(spelling correction, Used-for, reducing character errors)  (spelling correction, Used-for, reducing word errors)  (spelling correction, Used-for, enhancing readability)  (spelling correction, Hyponym-Of, text correction)  (spelling correction, Part-of, OCR post-correction)
(shared layers, Part-of, neural network)(neural network, Evaluate-for, text classification tasks)(neural network, Used-for, transfer learning)(Neural Symbolic Machine, Part-of, neural network)(neural network, Used-for, Neural Machine Translation (NMT))(neural network, Evaluate-for, gradient diffusion problem)(neural network, Used-for, reading comprehension)(neural network, Used-for, answering cloze-style questions)(neural network, Used-for, sequence-to-sequence learning problems)(neural network, Used-for, predicting predicate argument structure)(recurrent neural network, Part-of, neural network)(gated self-matching networks, Part-of, neural network)(recurrent neural network, Hyponym-Of, neural network)(neural network, Used-for, transition based dependency parsing)(neural network, Used-for, relation classification)(neural network, Used-for, building pre trained language models)(neural network, Used-for, text generation model)(neural network, Used-for, learning from data)(neural network, Used-for, dependency parsing)
(probabilistic context free grammar, Used-for, modelling sentences)(probabilistic context free grammar, Compare, mildly context-sensitive grammars)(probabilistic context free grammar, Evaluate-for, reducing computational complexity in parsing)(probabilistic context free grammar, Used-for, probabilistic linear context-free rewriting system formalism)(probabilistic context free grammar, Part-of, natural language processing)(probabilistic context free grammar, Compare, context sensitive grammar)
(context free grammar, Is-a-Prerequisite-of, weakly equivalent synchronous tree-adjoining grammar)(context free grammar, Is-a-Prerequisite-of, compound probabilistic context free grammar)(context free grammar, Is-a-Prerequisite-of, controllable context free grammar)(context free grammar, Is-a-Prerequisite-of, cky parsing)(context free grammar, Is-a-Prerequisite-of, context sensitive grammar)
(preprocessing, Part-of, feature extraction)(preprocessing, Used-for, NLP tasks)(preprocessing, Is-a-Prerequisite-of, feature extraction)(preprocessing, Used-for, normalization of historical texts)
(edit distance, Used-for, bilingual lexicon induction)
EMPTY G1.morphology and lexicon
EMPTY G1.dual problem
EMPTY G1.game playing in ai
EMPTY G1.recommendation system
EMPTY G1.computation theory
(text generation, Hyponym-Of, language generation task)(text generation, Used-for, video captioning)(text generation, Used-for, conversational text generation)(text generation, Used-for, AMR-to-text generation)(text generation, Used-for, language understanding tasks)(text generation, Compare, abstractive summarization)(abstractive summarization, Used-for, text generation)(text generation, Used-for, automatic pun generation)(text generation, Used-for, data-to-text generation)(text generation, Used-for, table-to-text generation)(text generation, Part-of, natural language processing)(text generation, Used-for, knowledge acquisition)(text generation, Used-for, poetry generation)(poetry generation, Is-a-Prerequisite-of, text generation)(seq2seq text generation, Hyponym-Of, text generation)(text generation, Used-for, AMR-to-text generation)(text generation, Part-of, NLP)(language model, Used-for, text generation)(text generation, Part-of, downstream tasks)(neural architecture, Used-for, text Tool)(neural language models, Hyponym-Of, text generation)
(character level language model, Evaluate-for, improving language model prediction)(character level language model, Is-a-Prerequisite-of, handling unsegmented languages)(character level language model, Part-of, hybrid language modeling)(character level language model, Evaluate-for, creating new word types not attested in training corpus)(character level language model, Compare, fixed-vocabulary language models)(character level and language model, Compare, lexicalized grammars)
(latent variable model, Used-for, capturing discourse-level diversity in encoding conversational intents)(latent variable model, Used-for, controlled response generation in dialog systems)(latent variable model, Used-for, prediction of team members consistency understanding in group decisions)(latent variable model, Used-for, learning multilingual word representations)(latent variable model, Is-a-Prerequisite-of, generative modeling of text)(latent variable model, Hyponym-Of, stochastic models)(conditional variational autoencoders, Part-of, latent variable model)(latent variable model, Used-for, semi-supervised learning of morphological inflectors)
EMPTY G1.lexicography
EMPTY G1.citation network
EMPTY G1.penn treebank
(biomedical named entities, Part-of, bio text mining)(bio text text mining, Used-for, learning representations of biomedical entities)(biomedical text mining tools, Part-of, bio text mining)(bio text mining, Is-a-Prerequisite-of, nlp for biology)
EMPTY G1.document representation
(NeuralREG, Used-for, lexicalized parsing)(lexicalized parsing, Hyponym-Of, parsing)(encoder-decoder parser, Used-for, lexicalized parsing)(L-PCFGs, Used-for, lexicalized parsing)
EMPTY G1.handwriting recognition
EMPTY G1.tsne
(nlp and vision, Used-for, describing images with text)(nlp and vision, Used-for, dual supervised learning)
(feature extraction, Is-a-Prerequisite-of, feature selection)(message passing algorithm, Used-for, feature selection)(domain adaptation, Used-for, feature selection)(algebraic perspective, Used-for, feature selection)(linear transformation, Used-for, feature selection)(text chunking, Evaluate-for, feature selection)
(matrix factorization, Used-for, representing usersâ€™ preference as a user-topic matrix)(matrix factorization, Used-for, mapping users and topics onto a latent feature space)(matrix factorization, Evaluate-for, modeling inter-topic preferences)(matrix factorization, Used-for, low rank approximation of the original document-word matrix)(matrix factorization, Used-for, topic discovery through spatial aggregation)(latent topic, Is-a-Prerequisite-of, matrix factorization)(matrix factorization, Used-for, topic discovery)
(None, Is-a-Prerequisite-of, first order logic)(first order logic, Used-for, creating compositional reasoning rules)(first order logic, Is-a-Prerequisite-of, predicate logic)
(vector representation, Used-for, capturing semantic information)(vector representation, Compare, point representations)(vector representation, Used-for, spectral clustering)(vector representation, Used-for, multimodal word distributions)(vector representation, Used-for, entailment tasks)(vector representation, Part-of, natural language processing models)(vector representation, Used-for, morpheme segmentation)(vector representation, Evaluate-for, semantic quality of entire word vector collection)
EMPTY G1.stack lstm
(graph-based nlp, Used-for, abstrative summarization)(graph-based nlp, Used-for, synset induction)(graph-based nlp, Used-for, meeting speech summarization)(graph-based nlp, Used-for, semantic frame induction)(graph-based nlp, Used-for, relation extraction)(graph-based nlp, Used-for, fact verification)(graph-based attention mechanism, Part-of, graph-based nlp)(semantic dependency parsing, Used-for, graph-based nlp)(dependency parsing, Used-for, graph-based nlp)(event factuality prediction, Used-for, graph-based nlp)(neural semantic parsing, Used-for, graph-based nlp)(text classification, Used-for, graph-based nlp)(AMR-to-text generation, Used-for, graph-based nlp)(graph-based nlp, Is-a-Prerequisite-of, citation networks)(graph-based nlp, Is-a-Prerequisite-of, pagerank)
EMPTY G1.phonetics
(syntax based machine translation, Is-a-Prerequisite-of, sequence-to-sequence models)(syntax based machine translation, Used-for, enhancing structural accuracy in translated text)(Syntax Trees, Part-of, syntax based machine translation)(syntax based machine translation, Evaluate-for, translation quality improvement)(Bi-directional Tree Encoder, Used-for, syntax based machine translation)(SD-NMT, Hyponym-Of, syntax based machine translation)(syntax based machine translation, Used-for, improving statistical machine translation)
(markov chain monte carlo, Used-for, learning weight uncertainty in RNNs)
(graph-based nlp, Used-for, relation extraction)(transformer-based architecture, Used-for, relation extraction)(relation extraction, Evaluate-for, generalization in NLP)(knowledge base enrichment, Used-for, relation extraction)(relation extraction, Used-for, extracting entities and relationships)(relation extraction, Part-of, relation learning)(recognition relation extraction, Hyponym-Of, relation extraction)(relation extraction, Evaluate-for, multi-lingual information utilization)(temporal relation classification, Is-a-Prerequisite-of, relation extraction)(relation extraction, Hyponym-Of, information extraction)(model performance, Used-for, relation extraction)(relation extraction, Used-for, knowledge base population)(class ties, Used-for, relation extraction)(CNN, Used-for, relation extraction)(relation extraction, Hyponym-Of, event relation)(relation extraction, Used-for, feature extraction efficiency)(knowledge base population, Part-of, relation extraction)(relation extraction, Used-for, information extraction)(relation extraction, Compare, temporal relation classification)(relation extraction, Hyponym-Of, natural language processing)(relation extraction, Used-for, finding unknown relational facts)(relation extraction, Evaluate-for, model performance)(neural relation extraction framework, Used-for, relation extraction)(Graph Neural Network, Used-for, relation extraction)(information extraction, Used-for, relation extraction)(relation extraction, Part-of, information extraction)(textual feature, Used
(speech synthesis, Used-for, language revitalization)(speech synthesis, Part-of, multimodal synthesis techniques)(speech of source language, Compare, text-to-speech systems)(speech synthesis, Is-a-Prerequisite-of, spoken audio captions)
EMPTY G1.clustering
(dimensionality reduction, Part-of, meta-embedding methods)(dimensionality reduction, Used-for, generalized Canonical Correlation Analysis)(dimensionality reduction, Used-for, cross-view auto-encoders)(Sufficient Dimensionality Reduction, Hyponym-Of, dimensionality reduction)(dimensionality reduction, Is-a-Prerequisite-of, Manifold Learning)(dimensionality reduction, Is-a-Prerequisite-of, Principal Component Analysis)(dimensionality reduction, Is-a-Prerequisite-of, tsne)(dimensionality reduction, Is-a-Prerequisite-of, latent semantic indexing)(dimensionality reduction, Is-a-Prerequisite-of, singular value decomposition)
(gated recurrent unit, Used-for, extracting character level dependencies)(gated recurrent unit, Used-for, capturing contextual information)
EMPTY G1.course introduction
EMPTY G1.k mean
(neural machine translation, Used-for, translating languages)(neural machine experience, Part-of, artificial intelligence)(neural machine translation, Compare, statistical machine translation)(bi-directional LSTMs, Used-for, neural machine translation)(convolutional layers, Used-for, neural machine translation)(LAUs, Used-for, neural machine translation)(deep multiagent policies, Evaluate-for, neural machine translation)(posterior regularization, Used-for, neural machine translation)(neural machine translation, Hyponym-Of, machine translation)(output layer optimization, Evaluate-for, neural machine translation)(sequence-to-sequence models, Used-for, neural machine translation)(cross-lingual word embedding, Used-for, neural machine translation)(subword units, Used-for, neural machine translation)(training neural network, Used-for, neural machine translation)(neural machine translation, Evaluate-for, domain adaptation method success)(neural machine translation, Used-for, machine translation)(neural machine translation, Used-for, source and target language understanding)(neural machine translation, Used-for, generating fluent translation results)(neural machine translation, Used-for, Chinese-to-English translation)(neural machine translation, Used-for, Japanese-English translation)(neural machine translation, Evaluate-for, sentence splitting)(neural machine translation, Used-for, paraphrase generation)(regularization, Used-for, neural machine translation)(neural machine translation, Used-for, translating languages using neural networks)(seq2seq
EMPTY G1.machine translation technique
EMPTY G1.kernel function
(python, Is-a-Prerequisite-of, code generation)(python, Is-a-Prerequisite-of, NL-to-code generation)(python, Is-a-Prerequisite-of, modular code generation)(python, Is-a-Prerequisite-of, tools for dl)(python, Is-a-Prerequisite-of, preprocessing)
EMPTY G1.latent dirichlet allocation
(word embedding, Used-for, improving multitask ST model)(word embedding, Used-for, solving word analogies)(word embedding, Used-for, caption generation)(word embedding, Evaluate-for, compositional capability)(word embedding, Is-a-Prerequisite-of, vector semantics)(word embedding, Used-for, capturing linguistic regularities)(word embedding, Used-for, vector space representation of words)(word embedding, Is-a-Prerequisite-of, aspect extraction in sentiment analysis)(word embedding, Is-a-Prerequisite-of, semantic analysis)(word embedding, Used-for, modeling word analogies)(word embedding, Used-for, bilingual word embeddings learning)(word embedding, Used-for, improving performance in NLP tasks)(word embedding, Hyponym-Of, word representation)(word embedding, Is-a-Prerequisite-of, syntaxnet)(word embedding, Is-a-Prerequisite-of, word embedding variations)
EMPTY G1.ensemble learning
(neural network, Used-for, reading comprehension)(cloze-style reading comprehension, Hyponym-Of, reading comprehension)(training neural network, Used-for, reading comprehension)(neural question answering, Used-for, reading comprehension)(sentence representation learning, Evaluate-for, reading comprehension)(recurrent network, Used-for, reading comprehension)(comprehension datasets, Part-of, reading comprehension)(reading comprehension, Used-for, question answering)(hierarchical attention network, Used-for, reading comprehension)
EMPTY G1.newton method
EMPTY G1.log-linear model
EMPTY G1.deep q-network
(highway network, Used-for, controlling useful neighbourhood expansion in GCin)  (highway network, Part-of, multilayer recurrent highway network)
(word embedding, Used-for, caption generation)(word-embedding models, Used-for, caption generation)(word embeddings, Used-for, caption generation)(caption generation, Used-for, evaluating word embeddings)(question answer, Used-for, caption generation)(word embedding model, Used-for, caption generation)
(heuristic search, Used-for, learning graph-to-string rules)(heuristic search, Used-for, bootstrap a neural transducer)(heuristic search, Evaluate-for, AMR-to-text generation)(heuristic search, Part-of, paradigm discovery problem)(heuristic search, Evaluate-for, lexical clustering in paradigm discovery)(heuristic search, Is-a-Prerequisite-of, a* search)
(context-sensitive grammar, Is-a-Prerequisite-of, SCFG)(context-sensitive grammar, Compare, CFG)(context-sensitive grammar, Evaluate-for, natural language processing applications)
(bi-directional LSTMs, Used-for, machine translation)(neural machine translation, Hyponym-Of, machine translation)(Grammatical error correction, Part-of, machine translation)(word embeddings, Used-for, machine translation)(neural machine translation, Used-for, machine translation)(encode-attend-decode paradigm, Used-for, machine translation)(seq2node, Used-for, machine translation)(bilingual word embeddings, Part-of, machine translation)(machine translation, Evaluate-for, bilingual lexicon induction)(recurrent network, Evaluate-for, machine translation)(machine translation, Used-for, producing translations)(language technology, Used-for, machine translation)(machine translation, Evaluate-for, overcoming language barriers)(machine translation, Used-for, seq2seq text generation)(sequence tasks, Part-of, machine translation)(Natural Language Processing, Used-for, machine translation)(language modeling, Is-a-Prerequisite-of, machine translation)(unsupervised machine translation, Hyponym-Of, machine translation)(machine translation, Part-of, natural language processing)(machine translation, Is-a-Prerequisite-of, morphology and semantics in machine translation)(machine translation, Is-a-Prerequisite-of, machine translation techniques)(machine translation, Is-a-Prerequisite-of, syntax based machine translation)(machine translation, Is-a-Prerequisite-of, statistical machine translation)(machine translation, Is-a-Prerequisite-of, noisy channel model)(machine translation, Is-a-Prerequisite-of,
EMPTY G1.activation function
(vector semantics, Used-for, representing words in NLP tasks)(word embedding, Is-a-Prerequisite-of, vector semantics)(semantic relevance, Evaluate-for, vector semantics)(vector semantics, Compare, distributional semantic models)(semantic models, Hyponym-Of, vector semantics)(vector semantics, Used-for, semantic similarity evaluation)(sentence embeddings, Part-of, vector semantics)(vector semantics, Used-for, decode fMRI patterns)(unsupervised embeddings, Part-of, vector semantics)(sentiment-aware embeddings, Part-of, vector semantics)(domain-specific embeddings, Part-of, vector semantics)(sentence content task, Evaluate-for, vector semantics)(paragraph embeddings, Part-of, vector semantics)(vector semantics, Is-a-Prerequisite-of, semantic similarity)(vector semantics, Is-a-Prerequisite-of, automated essay scoring)(vector semantics, Is-a-Prerequisite-of, document representation)(vector semantics, Is-a-Prerequisite-of, bio text mining)(vector semantics, Is-a-Prerequisite-of, recommendation system)(vector semantics, Is-a-Prerequisite-of, context free grammars)(vector semantics, Is-a-Prerequisite-of, tsne)(vector semantics, Is-a-Prerequisite-of, text mining)(vector semantics, Is-a-Prerequisite-of, word sense disambiguation)(vector semantics, Is-a-Prerequisite-of, kernels)
EMPTY G1.earley parsing
EMPTY G1.semi supervised learning
(lexicalized parsing, Hyponym-Of, parsing)(parsing, Used-for, semantic role labeling)(natural language descriptions, Is-a-Prerequisite-of, parsing)(parsing, Evaluate-for, generation of complex programs)(tree structures, Used-for, parsing)(parsing, Used-for, dependency graph generation)(syntax, Used-for, parsing)(parsing, Used-for, sequence tagging)(token-based dependency parsing, Hyponym-Of, parsing)(sequence-based parsing models, Hyponym-Of, parsing)(CCG parsing, Hyponym-Of, parsing)(parsing, Used-for, translation)(parsing, Is-a-Prerequisite-of, syntaxnet)(parsing, Is-a-Prerequisite-of, statistical parsing)(parsing, Is-a-Prerequisite-of, semi supervised learning)(parsing, Is-a-Prerequisite-of, neural parsing)(parsing, Is-a-Prerequisite-of, parsing evaluation)(parsing, Is-a-Principle-of, shallow parsing)(parsing, Is-a-Prerequisite-of, event detection)(parsing, Is-a-Prerequisite-of, semantic parsing)(parsing, Is-a-Prerequisite-of, dependency parsing)
(convolutional neural network, Used-for, text categorization)(convolutional neural network, Used-for, local coherence model)(convolutional neural network, Used-for, multi-modal neural machine translation)(convolutional neural network, Used-for, character embeddings generation)(convolutional neural network, Used-for, entity grid representation in text)(convolutional neural network, Used-for, domain adaptability in sentence compression)(convolutional neural network, Compare, recurrent neural network)(convolutional neural network, Is-a-Prerequisite-of, deep pyramid CNN)(convolutional neural network, Hyponym-Of, deep pyramid CNN)(convulsion, Used-for, text classification)(convulsion, Used-for, image feature extraction)(convolutional neural network, Used-for, semantic feature extraction from visual data)(convolutional neural network, Used-for, composing word representations in transition based dependency parsing)(convolutional neural network, Used-for, feature extraction)(convolutional neural network, Used-for, domain question answering qa)(convolutional neural network, Used-for, integrating metadata with text)(convolutional neural network, Used-for, producing visual character embeddings)(convolutional neural network, Used-for, entity representation in coherence model)(convolutional neural network, Is-a-Prerequisite-of, neural question answering)(convolutional neural network, Is-a-Prerequisite-of, ResNet)
EMPTY G1.sentence boundary recognition
(latent Dirichlet Allocation, Used-for, topic modeling)(topic modeling, Used-for, document context modeling)(topic modeling, Used-for, understanding large text collections)(topic modeling, Used-for, generating related sentences for a topic)(topic modeling, Compare, LDA topic model)
EMPTY G1.monte carlo method
(information retrieval, Used-for, augmenting Neural Machine Translation training data)(information retrieval, Used-for, support in conversational QA systems)(information retrieval, Is-a-Prerequisite-of, information retrieval task)(document dating, Evaluate-for, information retrieval)(information retrieval, Is-a-Prerequisite-of, toolkits for information retrieval)(information retrieval, Is-a-Prerequisite-of, document ranking)(information retrieval, Is-a-Prerequisite-of, search engines)(information retrieval, Is-a-Prerequisite-of, text mining)(information retrieval, Is-a-Prerequisite-of, image retrieval)
EMPTY G1.weakly-supervised learning
EMPTY G1.linear programming
EMPTY G1.expert system
(supervised machine learning models, Part-of, machine learning resource)(unsupervised machine learning, Part-of, machine learning resource)(autoML, Part-of, machine learning resource)(feature attribution methods, Used-for, machine learning resource)
(unlexicalized parsing, Used-for, predicting graphs jointly with token alignments)(neural encoder-decoder transition-based parser, Is-a-Prerequisite-of, unlexicalized parsing)
EMPTY G1.data structure and algorithm
(logistic regression, Used-for, probing PTLMs)(logistic regression, Used-for, quantifying harmful content)
EMPTY G1.sequence classification and conditional random field
(Keyphrase boundary classification, Part-of, scientific article summarization)(Semantic super-sense tagging, Used-for, scientific article summarization)(Multi-word expressions identification, Used-for, scientific article summarization)(Deep recurrent neural networks, Used-for, scientific article summarization)(Video of talks, Used-for, scientific article summarization)(Paper summaries dataset, Hyponym-Of, scientific article summarization)(Transformer-based model, Used-for, scientific article summarization)
EMPTY G1.deep learning introduction
(regular expression, Used-for, pattern matching)(regular expression, Compare, natural language)(regular expression, Used-for, search and replace operations)
EMPTY G1.morphology and semantics in machine translation
(vector representation, Used-for, spectral clustering)(spectral clustering, Used-for, linking synonyms and antonyms)(spectral clustering, Used-for, word embeddings)(Word embeddings, Used-for, spectral clustering)(spectral clustering, Evaluate-for, analyzing synonyms and antonyms in word embeddings)(vector space representations, Used-for, spectral clustering)
(character language models, Used-for, language modeling)(RNNs, Part-of, language modeling)(language modeling, Used-for, generation of conversational text)(language modeling, Used-for, generation of rhythmic poetry)(language modeling, Used-for, parsing sentences to semantic representations)(language modeling, Used-for, sentence prediction)(language modeling, Part-of, artificial intelligence)(language modeling, Evaluate-for, language model perplexity)(language modeling, Hyponym-Of, natural language processing)(rhythmic poetry, Used-for, language modeling)(conversational text, Used-for, language translation)(semantic representations, Used-for, language modeling)(sentence prediction, Used-for, language modeling)(language model perplexity, Evaluate-for, language modeling)(neural networks, Used-for, language modeling)(recurrent neural networks, Used-for, language modeling)(language modeling, Hyponym-Of, machine learning)(kernelized neural network, Part-of, language modeling)(machine learning, Hyponym-Of, artificial intelligence)(language modeling, Part-of, knowledge graph building)(training neural network, Used-for, language modeling)(language modeling, Used-for, testing morphological typologies)(sequential recurrent neural networks, Used-for, language modeling)(Stochastic optimization, Evaluate-for, language modeling)(Generative models, Used-for, language modeling)(Taylorâ€™s law, Evaluate-for, language modeling)(language modeling, Used-for, understanding linguistic structures)(unsupervised parsers, Evaluate
EMPTY G1.sentence representation
(text mining, Used-for, extracting knowledge of and objects from language)
(semantic similarity, Used-for, plagiarism detection)(semantic similarity, Used-for, information ranking)(semantic similarity, Used-for, recognition of paraphrases)(semantic similarity, Used-for, textual entailment recognition)(TextFlow, Evaluate-for, semantic similarity)(semantic similarity, Hyponym-Of, text similarity measures)(semantic similarity, Part-of, NLP tasks)(semantic similarity, Compare, lexical overlap)(semantic similarity, Evaluate-for, image description generation)(semantic similarity, Used-for, sentence semantic similarity prediction)(semantic similarity, Evaluate-for, specializing word embeddings)(semantic similarity, Used-for, dialogue state tracking (DST))(semantic similarity, Evaluate-for, cross lingual word embeddings)(semantic similarity, Used-for, information sharing across dialogue domains)(semantic relevance, Hyponym-Of, semantic similarity)(semantic similarity, Evaluate-for, source texts and summaries comparison)(semantic similarity, Is-a-Prerequisite-of, specializing word embeddings)(external knowledge, Compare, semantic similarity)(semantic similarity, Is-a-Prerequisite-of, thesaurus-based similarity)(semantic similarity, Is-a-Prerequisite-of, automated essay scoring)(semantic similarity, Is-a-Prerequisite-of, sentence simplification)(semantic similarity, Is-a-Prerequisite-of, relation extraction)(semantic similarity, Is-a-Prerequisite-of, text mining)(semantic similarity, Is-a-Prerequisite-of, word sense dis
EMPTY G1.kernel graphical models
EMPTY G1.hilbert space
EMPTY G1.dirichlet processes
(mean field approximation, Used-for, second-order semantic dependency parsing)(mean field approximation, Hyponym-Of, variational inference)
(belief propagation,Used-for,evaluating the similarities between propagation tree structures in rumor detection)(belief propagation,Used-for,computing the contribution of contextual words in neural machine translation)(belief propagation,Used-for,analyzing token contributions in neural machine translation)(belief propagation,Used-for,capturing robust structural features in rumor detection)(belief propagation,Used-for,approximating second-order semantic dependency parsing)
EMPTY G1.state space models
EMPTY G1.gaussian graphical model
EMPTY G1.kkt condition
EMPTY G1.message passing
EMPTY G1.markov random fields
EMPTY G1.singular value decomposition
EMPTY G1.evaluation of dependency parsing
EMPTY G1.kullback leibler divergence
(multimodal word embeddings, Used-for, entailment)(entailment, Used-for, Natural Language Inference)(entailment, Used-for, Recognizing Textual Entailment)
(expectation maximization algorithm, Used-for, parsing and language modeling)(expectation maximization algorithm, Evaluate-for, denoising word alignment)(expectation maximization algorithm, Is-a-Prerequisite-of, understanding neural decoder optimization)(expectation maximization algorithm, Used-for, sequence labeling)(expectation maximization algorithm, Used-for, learning latent variable models)(expectation maximization algorithm, Used-for, unsupervised reconstruction of ancient word forms)(expectation maximization algorithm, Used-for, modeling annotator group bias)(expectation maximization algorithm, Used-for, multi-party dialogue response generation)(expectation maximization algorithm, Used-for, discovering new intents in dialogue systems)
(semantic parser, Used-for, knowledge representation)(intermediate representations, Part-of, knowledge representation)(AMR parsing, Used-for, knowledge admiral)(AMR generation, Used-for, knowledge representation)(knowledge representation, Is-a-Prerequisite-of, predicate logic)
EMPTY G1.variable elimination
EMPTY G1.linear regression
EMPTY G1.lagrange duality
(unsupervised learning, Used-for, generating Arabic word embeddings)(unsupervised learning, Used-for, word-level language detection)(unsupervised learning, Used-for, Neural Machine Translation)(unsupervised learning, Evaluate-for, metaphor identification task)(unsupervised learning, Is-a-Prerequisite-of, creating language resources without labeled data)(unsupervised learning, Used-for, analyzing code-switched text)(unsupervised Neural Machine Translation, Hyponym-Of, unsupervised learning)(unsupervised learning, Used-for, training models without labeled data)(unsupervised learning, Used-for, inducing bilingual dictionaries)(unsupervised learning, Compare, supervised learning)
(word embedding variation, Evaluate-for, effectiveness in unsourced bilingual dictionary induction)(word embedding variation, Used-for, capturing uniquely expressive semantic information)
EMPTY G1.naive bayes
EMPTY G1.resnet
EMPTY G1.sequence to sequence
EMPTY G1.visual qa
(canonical correlation analysis, Compare, Kernel CCA)
(sampling, Used-for, generating word representations)(sampling, Used-for, learning model parameters)(sampling, Evaluate-for, efficiency in machine learning tasks)
EMPTY G1.markov chain
EMPTY G1.principal component analysis
(autoencoders, Used-for, natural language generation)(autoencoders, Used-for, latent variable approximation)(autoencoders, Part-of, variational autoencoders)(autoencoders, Used-for, unsupervised paraphrasing)(autoencoders, Used-for, syntax transfer generation)(autoencoders, Used-for, generating syntactically controlled sentences)(autoencoders, Part-of, syntax-infused variational autoencoder)(autoencoders, Used-for, preventing posterior collapse)(autoencoders, Used-for, generative modeling)(autoencoders, Used-for, machine translation evaluation)
EMPTY G1.alphago
EMPTY G1.manifold learning
(mixture models, Used-for, formulating weights in Gaussian Mixture LVeGs)(mixture models, Used-for, representing word senses in Probabilistic FastText)
(meta-learning, Used-for, domain adaptive dialog generation)
EMPTY G1.variations of gans
EMPTY G1.tool for dl
EMPTY G1.restricted boltzmann machine, deep belief network
EMPTY G1.support vector machine
EMPTY G1.multi-agent system
(first-order logic, Used-for, reasoning in TECHS)(first-order, Used-for, pruning in SMP)(first-order logic, Used-for, document-level event argument extraction)
(cross entropy, Used-for, training extractive summarization models)(cross entropy, Used-for, knowledge distillation)(cross entropy, Evaluate-for, perfecting token-level adaptive training in machine translation)(cross entropy, Used-for, training autoregressive language models)(cross entropy, Is-a-Prerequisite-of, structured prediction in knowledge distillation)(cross entropy, Compare, negative sampling loss functions)(cross entropy, Part-of, MixCE objective)
EMPTY G1.random walk and harmonic function
(parsing, Used-for, semantic role labeling)(semantic role labeling, Used-for, predicate-argument structure recognition)(semantic role labeling, Used-for, dependency SRL)(semantic role labeling, Evaluate-for, improving Chinese SRL)(semantic role labeling, Used-for, argument span detection)(semantic role labeling, Used-for, question generation)(semantic role labeling, Used-for, syntactic information utilization)(semantic role labeling, Used-for, modeling the syntax-semantic dependency correlation)(semantic role labeling, Evaluate-for, span-level accuracy)(deep highway BiLSTM architecture, Part-of, semantic role labeling)(semantic role labeling, Used-for, high semantic similarity promotion)(deep learning tool, Used-for, semantic role labeling)(multi-task learning, Used-for, semantic role labeling)(semantic role labeling, Evaluate-for, performance improvement)(recursive neural network, Is-a-Prerequisite-of, semantic role labeling)(semantic role labeling, Used-for, understanding sentence structure)(deep learning model, Used-for, semantic role labeling)(semantic role labeling, Used-for, improving semantic parsing accuracy)(semantic role labeling, Used-for, state representation in neural networks)(deep learning, Used-for, semantic role labeling)(semantic role labeling, Used-for, high-level semantic analysis)(semantic role labeling, Is-a-Prerequisite-of, character-level semantic analysis)(character-level models, Evaluate-for, semantic role labeling)(morphological task,
(evaluation of text classification, Used-for, analyzing model performance)(evaluation of texticia classification, Evaluate-for, perplexity)(evaluation of text classification, Evaluate-for, Normalized Pointwise Mutual Information)(evaluation of text classification, Evaluate-for, sentence compositionality)(evaluation of text classification, Evaluate-for, handling rare words through character embeddings)(evaluation of text classification, Evaluate-for, semantic coherence)(evaluation of text classification, Evaluate-for, adversarial robustness)(evaluation of text classification, Used-for, discourse structure analysis)(evaluation of text classification, Is-a-Prerequisite-of, sentiment analysis)(evaluation of text classification, Is-a-Prerequisite-of, language identification)(evaluation of text classification, Is-a-Prerequisite-of, relation extraction)
(word sense disambiguation, Used-for, identifying the correct meaning of polysemous words)(WordNet, Used-for, word sense disambiguation)(lexical ambiguity, Is-a-Prerequisite-of, word sense disambiguation)(disambiguation algorithm, Used-for, word sense disambiguation)(lexical resources, Used-for, word sense disambiguation)(contextual embeddings, Used-for, word sense disambiguation)(Neural Language Modelling, Used-for, word sense disambiguation)(neural architectures, Used-for, word sense disambiguation)(knowledge bases, Used-for, word sense disambiguation)(lexical knowledge, Used-for, word sense disambiguation)
(semantic parsing, Is-a-Prerequisite-of, nlp for database)(nlp for database, Used-for, supporting database queries)(nlp for database, Evaluate-for, reasoning over sets of relevant facts)
EMPTY G1.autonomous car
(finite state machine, Used-for, part-of-speech tagging)(finite state machine, Used-for, speech recognition)(finite state machine, Used-for, higher-order derivatives computation)(finite state machine, Used-for, morph-based auto-completion)(FST composition operation, Part-of, finite state machine)
(BERT, Used-for, neural parsing)(neural parsing, Used-for, linguistically-expressive semantic representations)(neural parsing, Is-a-Prerequisite-of, semantic dependency graph formalisms)(neural parsing, Evaluate-for, Minimal Recursion Semantics (MRS))(neural parsing, Evaluate-for, Abstract Meaning Representation (AMR))(NMT+RN&G, Evaluate-for, neural parsing)(neural parsing, Part-of, Natural Language Processing)(neural encoder-decoder transition-based parser, Used-for, neural parsing)(neural parsing, Used-for, building semantic graphs)(neural parsing, Used-for, sentence parsing)(stack-based embedding features, Used-for, neural parsing)(neural parsing, Used-for, predicting graphs jointly)
EMPTY G1.statistical part of speech tagging
EMPTY G1.the ibm model
(dynamic programming, Used-for, constituency parsing)(dynamic programming, Used-for, greedy top-down inference algorithm)(dynamic programming, Used-for, integer linear programming formulation)(dynamic piano, Used-for, non-projectivity in dependency parsing)(dynamic programming, Used-for, latent tree learning)(dynamic programming, Used-for, temporal and causal relations extraction)(dynamic programming, Used-for, sentence segmentation)(dynamic programming, Part-of, subword segmentation algorithm)(dynamic programming, Used-for, unsupervised summarization)(dynamic programming, Used-for, length-control decoding)(dynamic programming, Is-a-Prerequisite-of, cky parsing)(dynamic programming, Is-a-Prerequisite-of, course introduction)
(Non-autoregressive text to speech models, Part-of, text to speech generation)(text to speech generation, Compare, Non-autoregressive text to speech models)(response generation, Is-a-Prerequisite-of, text to speech generation)(text to speech generation, Evaluate-for, overcoming low coverage bias)(text to speech generation, Evaluate-for, voice quality improvement)
(word segmentation, Used-for, neural word segmentation)(word segmentation, Used-for, statistical segmentation)(word segmentation, Is-a-Prerequisite-of, Chinese word segmentation)(character embeddings, Part-of, word segmentation)(prosody, Used-for, word segmentation)(word segmentation, Used-for, Arash NLP application)
EMPTY G1.query expansion
EMPTY G1.noisy channel model
(linear algebra, Used-for, solving algebraic word problems)  (linear general algebra, Is-a-Prerequisite-of, Integer Linear Programming (ILP))  (linear algebra, Is-a-Prerequisite-of, spectral methods)  (linear algebra, Is-a-Prerequisite-of, structured learning)  (linear algebra, Is-a-Prerequisite-of, mathematical models)  (linear algebra, Is-a-Prerequisite-of, structured sparsity)  (linear algebra, Is-a-Prerequisite-of, machine learning resources)  (linear algebra, Is-a-Prerequisite-of, latent variable models)  (linear algebra, Is-a-Prerequisite-of, dimensionality reduction)  (linear algebra, Is-a-Prerequisite-of, Message Passing)  (linear algebra, Is-a-Prerequisite-of, State Space Models)  (linear algebra, Is-a-Prerequisite-of, Variations of GANs)  (linear algebra, Is-a-Prerequisite-of, Mixture Models)  (linear algebra, Is-a-Prerequisite-of, Manifold Learning)  (linear algebra, Is-a-Prerequisite-of, Principal Component Analysis)  
EMPTY G1.chomsky hierarchy
(context sensitive representations, Evaluate-for, named entity recognition)(fixed-size ordinally forgetting encoding, Used-for, named entity recognition)(seq2seq, Used-for, named entity recognition)(sequence labeling model, Used-for, named entity recognition)(named entity recognition, Compare, mention detection)(named entity relation, Part-of, named entity recognition)(named entity recognition, Hyponym-Of, natural language processing)(named entity recognition, Is-a-Prerequisite-of, mention detection)(mention detection, Used-for, named entity recognition)(FOFE method, Used-for, named entity recognition)(multimodal named entity disambiguation, Hyponym-Of, named entity recognition)(cross-domain knowledge transfer, Used-for, named entity recognition)(language model downstream task, Used-for, named entity recognition)(language model downstream task, Evaluate-for, named entity recognition)(named entity recognition, Used-for, named entity)(metonymy resolution, Evaluate-for, named entity recognition)(named entity recognition, Is-a-Prerequisite-of, entity linking)(named entity recognition, Used-for, taxonomic classification)(named entity recognition, Evaluate-for, multilingual learning)(named entity recognition, Used-for, domain adaptation)(entity recognition, Is-a-Prerequisite-of, named entity recognition)(named entity recognition, Compare, joint model by Li and Ji)(named entity recognition, Compare, tree-based LSTM model by Miwa and Bansal)(named entity recognition, Is-a-Prerequisite-of
(dependency syntax, Is-a-Prerequisite-of, transition-based parsing)(dependency syntax, Is-a-Prerequisite-of, statistical machine translation)(dependency syntax, Part-of, natural language processing)(dependency syntax, Evaluate-for, error reduction in NMT models)(dependency syntax, Used-for, sentence semantic analysis)(dependency syntax, Evaluate-for, increasing parsing accuracy)
(shift-reduce parsing, Evaluate-for, Chinese discourse parsing)(shift-reduce parsing, Used-for, rhetorical relation recognition)(shift-reduce parsing, Is-a-Prerequisite-of, syntax analysis)(shift-reduce parsing, Is-a-Prerequisite-of, transition based dependency parsing)(shift-reduce parsing, Part-of, constituency parsing)(shift-reduce parsing, Compare, sequence-to-sequence constituent parsing)
EMPTY G1.latent semantic indexing
EMPTY G1.agent-based view of ai
(policy gradient method, Used-for, directly optimizing for exact matches)(policy gradient method, Used-for, reducing exposure bias during training)(policy gradient method, Used-for, optimizing coreference evaluation metrics)(policy gradient method, Used-for, training semantic dependency parsing models)(policy gradient method, Used-for, fine-tuning language models)(policy gradient method, Part-of, reinforcement learning)(constituency parsers, Is-a-Prerequisite-of, policy gradient method)(coreference resolution, Is-a-Prerequisite-of, policy gradient method)(semantic dependency parsing, Is-a-Prerequisite-of, policy gradient method)(language model training, Is-a-Prerequisite-of, policy gradient method)
(Cross-lingual text classification, Part-of, nlp for the humanity)(Model distillation, Part-of, nlp for the humanity)
EMPTY G1.kernel
EMPTY G1.loss function
(image retrieval, Compare, image annotation)  (image retrieval, Compare, scene understanding)  (image_damage retrieval, Used-for, image editing)  (image retrieval, Used-for, action recognition)  (action recognition, Part-of, image retrieval)  (image retrieval, Compare, image compression)  (image retrieval, Compare, natural language compression)  
(parsing evaluation, Used-for, semantic representations)(parsing evaluation, Used-for, multilingual model training)(multilingual model, Is-a-Prerequisite-of, parsing evaluation)(parsing evaluation, Evaluate-for, multilingual GeoQuery corpus)(parsing evaluation, Evaluate-for, multilingual ATIS corpus)(statistical significance testing, Evaluate-for, parsing evaluation)(parsing evaluation, Is-a-Prerequisite-of, semantic parsing)
EMPTY G1.natural language processing intro
EMPTY G1.propositional logic
(adversarial search, Used-for, creating robust self-learning algorithms)(adversarial search, Used-for, improving model generalization through adversarial training)(adversarial search, Used-for, evaluating cross-lingual word embeddings)(adversarial search, Used-for, generating white-box adversarial examples)(adversarial training, Part-of, adversarial search)
EMPTY G1.linear discriminant analysis
(Distant supervision, Used-for, information extraction)(Keyphrase prediction, Used-for, information of extraction)(Text similarity measures, Used-for, information extraction)(Knowledge graphs, Used-for, information extraction)(Joint extraction of entities and relations, Used-for, information extraction)(Sequence labeling model, Used-for, information extraction)(Named entity recognition multiple, Compare, information extraction)(Relation extraction, Hyponym-Of, information extraction)(Entity extraction, Part-of, information extraction)(Relation extraction, Used-for, information extraction)(Named entity recognition, Is-a-Prerequisite-of, information extraction)(Joint extraction of entities and relations, Is-a-Prerequisite-of, information extraction)(Information extraction, Used-for, relation extraction)(Relation extraction, Part-of, information extraction)(Event extraction, Hyponym-Of, information extraction)(Information extraction, Is-a-Prerequisite-of, social network extraction)(Information extraction, Is-a-Prerequisite-of, social media analysis)
EMPTY G1.grammar checker
(None, Used-for, training neural network)(None, Is-a-Prerequisite-of, training neural network)(None, Evaluate-for, training neural network)(training neural network, Used-for, language modeling)(training neural network, Used-for, zero pronoun resolution)(training neural network, Used-for, question answering)(training neural network, Used-for, sentence compression)(training neural network, Used-for, relation detection)(training neural sounditional file, Used-for, reading comprehension)(training neural network, Used-for, neural machine translation)(training neural network, Used-for, neural symbolic machine)(Neural Symbolic Machine, Part-of, training neural network)(iterative maximum-likelihood training, Hyponym-Of, training neural network)(cloze-style reading comprehension neural network model, Part-of, training neural network)
(spoken language acquisition, Is-a-Prerequisite-of, speech signal analysis)(speech signal analysis, Used-for, detecting word-like acoustic units)(speech signal analysis, Used-for, associating words with image regions)(acoustic units, Part-of, speech signal analysis)(speech signal analysis, Is-a-Prerequisite-of, speech processing)
(factor graph model,Is-a-Prerequisite-of,graphical model)(graphical model,Used-for,argument mining)(graphical model,Used-for,argumentative relation prediction)(graphical model,Used-for,network analysis)(graphical model,Used-for,learning context-aware network embeddings)(graphical model,Used-for,enhancing abstractive text summarization)(graphical model,Used-for,learning representation of political actors)(graphical model,Used-for,enhanced performance of neural models)(graphical model,Evaluate-for,structure constraints enforcement)(graphical model,Evaluate-for,expressing dependencies between relations)
(statistical parsing, Is-a-Prerequisite-of, semantic representations)(statistical parsing, Used-for, Minimal Recursion Semantics (MRS))(statistical parsing, Evaluate-for, knowledge base completion)(statistical parsing, Evaluate-for, semantic graph construction)(combinatory categorial grammar, Used-for, statistical parsing)
EMPTY G1.pagerank
EMPTY G1.n-gram model
EMPTY G1.bagging
(prosody, Used-for, word segmentation)(speech register, Conjunction, prosody)(prosody, Used-for, segmenting speech units)(prosody, Compare, pitch and intensity features)(prosody variation, Hyponym-Of, prosody)(acoustic features, Conjunction, prosody)
(dependency parsing, Used-for, graph-based nlp)(neural encoders, Used-for, dependency parsing)(word embeddings, Used-for, dependency parsing)(None, Is-a-Prerequisite-of, dependency parsing)(dependency parsing, Compare, sequence tagging)(non-monotonic system, Used-for, dependency parsing)(Universal Dependencies, Used-for, dependency parsing)(Chinese NLP, Used-for, dependency parsing)(Chinese word segmentation, Is-a-Prerequisite-of, dependency parsing)(dependency parsing, Part-of, Chinese NLP)(dependency parsing, Used-for, handling Twitter-specific conventions)(Universal Dependencies 2.0, Part-of, dependency parsing)(dependency parsing, Is-a-Prerequisite-of, error propagation)(dependency tree, Part-of, dependency parsing)(semantic dependency parsing, Hyponym-Of, dependency parsing)(dependency parsing, Used-for, building dependency trees)(dependency parsing, Is-a-Prerequisite-of, semantic dependency parsing)(neural network, Used-for, dependency parsing)(dependency parsing, Evaluate-for, event extraction)(dependency parsing, Is-a-Prerequisite-of, transition based dependency parsing)(dependency parsing, Is-a-Prerequisite-of, evaluation of dependency parsing)(dependency parsing, Part-of, semantic dependency parsing)(dependency parsing, Used-for, repairing grammatical errors)(cross-lingual dependency parsing, Evaluate-for, dependency parsing)
EMPTY G1.q learning
(discourse analysis, Used-for, evaluation of spoken language coherence)(Rhetorical Structure Theory, Used-for, discourse analysis)(discourse analysis, Part-of, natural language processing)(EDU, Used-for, discourse analysis)(discourse segmenter, Used-for, discourse analysis)(discourse parser, Used-for, discourse analysis)(discourse analysis, Compare, predicate argument structure analysis)(discourse analysis, Used-for, understanding human communication)(variable-in-situ logico-semantic graphs, Used-for, discourse analysis)(discourse treebank, Part-of, discourse analysis)(Rhetorical Structure Theory, Part-of, discourse analysis)(DSWE, Used-for, discourse analysis)(segmentation, Used-for, discourse analysis)(discorce analysis, Is-a-Prerequisite-of, discourse parsing)
EMPTY G1.speech processing
EMPTY G1.chinese nlp
(domain adaptation,Used-for,feature selection)(domain adaptation,Used- for,generalizing to new domain)(None,Is-a-Prerequisite-of,domain adaptation)(cross-lingual classification,Evaluate-for,domain adaptation)(domain adaptation,Used-for,enhancing performance in low-resource language scenarios)(domain adaptation,Used-for,sentiment analysis)(cross-domain sentiment analysis,Is-a-Prerequisite-of,domain adaptation)(phrase-based machine translation,Used-for,domain adaptation)(adversarial training,Used-for,domain adaptation)(domain adaptation,Compare,method adaptation)(domain adaptation,Used-for,reducing distribution mismatch)(domain adaptation,Used-for,feature learning)(cross lingual word embeddings,Part-of,domain adaptation)(domain adaptation,Used-for,task oriented dialogue system)(bilingual lexicon induction,Used-for,domain adaptation)(domain adaptation,Used-for,entity  recognition ner aim)(domain adaptation,Used-for,improving machine translation with contextual embeddings)(domain adaptation,Used-for,parser trained)(domain adaptation,Used-for,improve NMT performance)(domain adaptation,Evaluate-for,distributional semantics)(named entity recognition,Used-for,domain adaptation)(domain adaptation,Evaluate-for,lingual embeddings)(sentiment classification,Used-for,domain adaptation)(domain adaptation,Used-for,enhancing performance of bilingual word embeddings)(domain sentiment classification,Is-a-Prerequisite-of,domain adaptation)(domain adaptation,Used-for,handling domain dependence
(deep learning tool,Used-for,text similarity measurement)(deep learning tool,Used-for,automatic question generation)(deep learning tool,Used-for,natural language generation)(deep learning tool,Used-for,fake news detection)(deep learning tool,Used-for,morphological disambiguation)(deep learning tool,Used-for,extractive document summarization)(deep learning tool,Used-for,structured query generation)(deep learning tool,Evaluate-for,performance in different NLP tasks)(deep highway BiLSTM architecture,Part-of,deep learning tool)(constrained decoding,Part-of,deep learning tool)(TextFlow,Is-a-Prerequisite-of,deep learning tool)
EMPTY G1.computational phonology
EMPTY G1.toolkits for information retrieval
EMPTY G1.class logistics
(Open-domain TableQA models, Used-for, question answering)(training neural network, Used-for, question  answering)(gated self-matching networks, Used-for, question answering)(semi-supervised question answering, Part-of, question answering)(question answering, Evaluate-for, capsule network)(recurrent neural network, Evaluate-for, question answering)(sentence representation learning, Evaluate-for, question answering)(reading comprehension, Used-for, question answering)(question answering, Part-of, open domain question answering)(open domain question question answering, Hyponym-Of, question answering)(support graph optimization, Used-for, question answering)(semantic parsing, Is-a-Prerequisite-of, question answering)(question answering, Used-for, assessing machine reading comprehension)(question answering, Used-for, accessing knowledge from structured and unstructured sources)
(entropy, Is-a-Prerequisite-of, information theory)
(deep learning tool, Used-for, morphological disambiguation)(morphological disambiguation, Used-for, refine machine translation)(morphological disambiguation, Used-for, optimize POS tagging)(morphological disambiguation, Is-a-Prerequisite-of, context-sensitive representation learning)(morphological disambiguation, Evaluate-for, agglutinative languages)(morphological disambiguation, Used-for, improving language understanding)(morphological task, Used-for, morphological disambiguation)(morphological typology, Used-for, morphological disambiguation)(cross-lingual transfer tasks, Evaluate-for, morphological disambiguation)(morphological disambiguation, Used-for, handling long-tail phenomena)
EMPTY G1.classic parsing method
EMPTY G1.gradient descent
EMPTY G1.transliteration
(multi-modal learning, Used-for, sentiment analysis)(multi-modal learning, Used-for, emotion recognition)(multi-modal learning, Used-for, speaker trait analysis)(Deep Fusion Graph, Used-for, multi-modal learning)(Multi-task learning, Compare, multi-modal learning)(Low-rank Multimodal Fusion, Used-for, multi-modal learning)(multi-modal learning, Used-for, integrating multiple unimodal representations)(multi-modal data, Part-of, multi-modal learning)(structured information, Compare, multi-modal learning)(multi-modal learning, Used-for, multilingual learning)
EMPTY G1.search
(paraphrasing, Used-for, plagiarism detection)(paraphrasing, Used-for, information ranking)(paraphrasing, Used-for, textual entailment recognition)(paraphrasing, Used-for, paraphrase detection)(paraphrasing, Used-for, semantic content abstraction)(paraphrasing, Conjunction, sentence splitting)(paraphrasing, Conjunction, lexical substitution)(paraphrasing, Conjunction, content restructuring)(paraphrasing, Conjunction, text simplification)(paraphrasing, Hyponym-Of, text transformation)(paraphrasing, Is-a-Prerequisite-of, natural language understanding)(monolingual paraphrasing, Part-of, paraphrasing)
EMPTY G1.theory of computation
EMPTY G1.others
EMPTY G1.one-shot learning
(multilingual word embedding, Used-for, cross-landingual classification)  (multilingual word embedding, Used-for, bilingual tasks)
EMPTY G1.document ranking
EMPTY G1.structured sparsity
(evaluation of information retrieval, Used-for, predicting volatility from sentiment analysis in financial markets)(evaluation of information retrieval, Used-for, characterizing financial sector reports)(evaluation of information retrieval, Hyponym-Of, Natural Language Processing)(evaluation of information retrieval, Used-for, estimating trustworthiness of information sources)(evaluation of information retrieval, Is-a-Prerequisite-of, query expansion)
EMPTY G1.nn sequence parsing
(combinatory categorial grammar, Used-for, statistical parsing)(combinatory categorial grammar, Used-for, generating language classes)(decomposed scoring, Is-a-Prerequisite-of, combinatory categorial grammar)
EMPTY G1.particle filter
(neural question answering, Used-for, reading comprehension)(neural question answering, Evaluate-for, SQuAD dataset)(gated self-matching networks, Part-of, neural question answering)(COREQA, Part-of, neural question answering)(EviNets, Part-of, neural question answering)(recurrent neural networks, Used-for, neural question answering)
EMPTY G1.facial recognition system
(Python, Is-a-Prerequisite-of, programming language)(programming language, Used-for, code generation)(programming language, Hyponym-Of, general-purpose programming language)(programming language, Used-for, naturalize)(declarative programming language, Part-of, programming language)(programming language, Used-for, querying databases)(programming language, Used-for, manipulating text)(programm `language, Used-for, analyzing data)
EMPTY G1.nlp for biology
(bag of word model, Compare, continuous bag of words (CBOW))(bag of word model, Used-for, legal judgment prediction)(paragraph embedding, Compare, bag of word model)(bag of word model, Used-for, feature extraction in text classification)
(mathematical model, Used-for, semantic-based text generalization)(mathematical model, Is-a-Prerequisite-of, semantic-based text generalization)(mathematical model, Used-for, news content structure analysis)(mathematical model, Used-for, arithmetic reasoning in AI systems)(mathematical model, Used-for, network analysis)
(bias-variance, Compare, variance due to optimization)
(event detection,Used-for,event type classification)(event detection,Used-for,argument identification)(trigger detection,Is-a-Prerequisite-of,event detection)(event coreference,Is-a-Prerequisite-of,event detection)(supervised learning,Used-for,event detection)(Nugget Proposal Networks,Used-for,event detection)(event detection,Evaluate-for,emotion detection from text)(event detection,Evaluate-for,knowledge base population)(argument information,Used-for,event detection)(supervised attention mechanisms,Used-for,event detection)(event relation,Used-for,event detection)(Attention mechanisms,Used-for,event detection)(event detection,Used-for,ACE 2005 dataset)(document modeling,Used-for,event detection)(document date,Used-for,event detection)(document embedding,Used-for,event detection)(event detection,Part-of,event extraction)(event detection,Evaluate-for,event analysis)
(dialog system, Evaluate-for, domain adaptation method success)(Neural Belief Tracking, Part-of, dialog system)(memory-to-sequence, Part-of, dialog system)(encoder-decoder dialog model, Used-for, dialog system)(dialog system, Used-for, automatic diagnosis)(dialog belief tracking, Part-of, dialog system)
(semi-supervised learning, Used-for, exploiting unlabeled data in tasks)(semi-supervised learning, Is-a-Prerequisite-of, multi-space variational encoder-decoders)(semi-supervised learning, Is-a-Prerequisite-of, Generative Domain-Adaptive Nets)(semi-supervised learning, Used-for, boosting performance of question answering models)(semi-supervised learning, Used-for, inflection generation)(structured learning, Hyponym-Of, semi-supervised learning)(semi-supervised learning, Used-for, enhancing semantic parsing)(semi-supervised learning, Used-for, exploiting labeled and unlabeled data)(semi-supervised learning, Used-for, improving constituency parsing)(semi-supervised learning, Is-a-Prerequisite-of, graph convolutional networks)(semi-supervised learning, Is-a-Prerequisite-of, neural networks)
(language identification, Used-for, processing multingual text)(language identification, Used-for, analyzing code-switched text)(feature hashing, Used-for, language identification)(character-based sequence-to-sequence model, Used-for, language identification)(socially inclusive, Evaluate-for, language identification)(language identification, Is-a-Prerequisite-of, developing NLP tools)(pretrained multilingual, Used-for, language identification)(language identification, Evaluate-for, multilingual model)
EMPTY G1.harmonic function
(shallow parsing, Used-for, identifying phrase structures in sentences)(shallow parsing, Part-of, natural language processing)(shallow parsing, Compare, deep parsing)(shallow parsing, Evaluate-for, speed in processing text)(shallow parsing, Is-a-Prerequisite-of, cky parsing)
EMPTY G1.calculus
EMPTY G1.cky parsing
EMPTY G1.planning
EMPTY G1.markov decision process
(syntax, Used-for, parsing)(syntax, Used-for, improving number agreement in model architecture)(synthax, Hyponym-Of, language structures)(syntax, Used-for, enhancing language models)(syntax, Is-a-Prerequisite-of, cky parsing)(syntax, Is-a-Prerequisite-of, syntaxnet)(syntax, Is-a-Prerequisite-of, syntax based machine translation)(syntax, Is-a-Prerequisite-of, dependency syntax)(syntax, Is-a-Prerequisite-of, penn treebank)(syntax, Is-a-Prerequisite-of, shift-reduce parsing)
(graph convolutional network, Used-for, extracting drug-drug interactions from texts)(graph convolutional network, Used-for, model training in NLP)(graph convolutional network, Used-for, multimodal data fusion)(graph convolutional network, Used-for, predicting drug-drug interactions from molecular structures)(graph convolutional network, Used-for, extracting relations in text)(graph convolutional network, Used-for, capturing structural information in natural language)(graph convolutional aetwork, Part-of, NeuralDater)(graph convolutional network, Part-of, multimodal geolocation model)(graph convolutional network, Part-of, drug-drug interaction extraction system)
(spectral method, Part-of, language modeling tasks)(language modeling tasks, Evaluate-for, spectral method)(spectral models, Part-of, spectral method)(statistics from long substrages, Used-for, spectral method)(large matrices, Used-for, spectral method)(moment matching, Used-for, spectral method)(ngram models, Compare, spectral method)
(pointer network, Used-for, extracting unknown slot values)
(structured prediction, Part-of, knowledge distillation)  (structured prediction, Used-for, mapping unstructured inputs to structured outputs)  (structured prediction, Used-for, search problem solving)  (structured prediction, Evaluate-for, output structure evaluation)  
(search engine, Used-for, multi-passage machine reading comprehension)(entity linking, Evaluate-for, search engine)(search engine, Used-for, personalized query completions)
(sentiment analysis, Used-for, volatility prediction)(sentiment analysis, Used-for, volatility prediction in financial markets)(sentiment analysis, Is-a-Prerequisite-of, text classification)(sentiment analysis, Part-of, natural language processing)(sentiment lexicons, Part-of, sentiment analysis)(domain sentiment classification, Part-of, sentiment analysis)(sentiment polarity, Part-of, sentiment analysis)(sentiment analysis, Compare, aspect-based sentiment analysis)(hate speech detection, Compare, sentiment analysis)(sentiment analysis, Conjunction, emotion classification)(Information Retrieval, Used-for, sentiment analysis)(multi-modal learning, Used-for, sentiment analysis)(domain adaptation, Used-for, sentiment analysis)(Cold-Start Aware Attention, Used-for, sentiment analysis)(multimodal language, Used-for, sentiment analysis)(recurrent network, Used-for, sentiment analysis)(domain sentiment lexicon, Used-for, sentiment analysis)(distributional semantics, Is-a-Prerequisite-of, sentiment analysis)(text classification, Used-for, sentiment analysis)(word embeddings, Used-for, sentiment analysis)(sentiment lexicon, Used-for, sentiment analysis)(domain-specific sentiment lexicons, Used-for, sentiment analysis)(multimodal embeddings, Used-for, sentiment because)(textual feature, Used-for, sentiment analysis)(Aspect-Opion Pair Extraction, Used-for, sentiment analysis)(annotated data, Used-for, sentiment analysis)(sentiment analysis, Is-a-Prerequisite-of
EMPTY G1.a* search
(GCN-based coherence model, Used-for, automated essay scoring)(automated essay scoring, Is-a-Prerequisite-of, argument persuasiveness scoring)(automated essay scoring, Compare, neural AES)(automated essay scoring, Used-for, grading essays)(automated essay scoring, Evaluate-for, essay quality)(essay quality, Part-of, automated essay scoring)(automated essay scoring, Hyponym-Of, automated writing evaluation)(string kernels, Used-for, automated essay scoring)(automated essay scoring, Used-for, measuring thesis strength)
(matrix multiplication, Used-for, multiplying a matrix by a few-hot vector)(multiplying a matrix by a few-hot vector, Hyponym-Of, matrix multiplication)(neural networks, Used-for, matrix multiplication)(matrix multiplication, Evaluate-for, GPU efficiency)(matrix multiplication, Is-a-Prerequisite-of, Message Passing)
EMPTY G1.evaluation of language modeling
(collaborative filtering, Used-for, personalization of content)(collaborative filtering, Part-of, machine learning techniques)(collaborative filtering, Used-for, rating prediction)(ESCOFILT, Used-for, collaborative filtering)(rating prediction, Evaluate-for, collaborative filtering)(user-item interactions, Part-of, collaborative filtering)
(lexical semantics, Part-of, natural language processing)(distributional vector space models, Evaluate-for, lexical semantics)(semantic quality, Hyponym-Of, lexical semantics)(lexical entropy series, Hyponym-Of, lexical semantics)(lexical semantics, Is-a-Prerequisite-of, thesaurus-based similarity)(lexical semantics, Is-a-Prerequisite-of, event detection)
(structured learning, Used-for, taxonomy learning)(structured learning, Used-for, hypernym prediction)(structured learning, Hyponym-Of, semi-supervised learning)(structured learning, Compare, supervised learning)(structured learning, Part-of, transductive learning approach)(structured learning, Used-for, query understanding)(structured learning, Used-for, fine-grained entity categorization)(structured learning, Is-a-Prerequisite-of, word distributions)(structured learning, Is-a-Prerequisite-of, sentence representations)(structured learning, Is-a-Prerequisite-of, vector semantics)(structured learning, Is-a-Prerequisite-of, vector representations)(structured learning, Is-a-Prerequisite-of, semantic similarity)(structured learning, Is-a-Prerequisite-of, automated essay scoring)(structured learning, Is-a-Prerequisite-of, word embedding)(structured learning, Is-a-Prerequisite-of, document representation)(structured learning, Is-a-Prerequisite-of, bio text mining)(structured learning, Is-a-Prerequisite-of, recommendation system)(structured learning, Is-a-Prerequisite-of, context free grammars)(structured learning, Is-a-Prerequisite-of, tsne)(structured learning, Is-a-Prerequisite-of, text mining)(structured learning, Is-a-Prerequisite-of, linear discriminant analysis)(structured learning, Is-a-Prerequisite-of, kernels)
(neural network, Used-for, transfer learning)(transfer learning, Used-for, improving question answering model performance)(feature learning, Used-et, transfer learning)(transfer learning, Used-for, Universal Language Model Fine-tuning)(transfer learning, Used-for, sentiment classification)(multi-task learning, Used-for, transfer learning)(transfer learning, Used-for, domain adaptation in NLP)(transfer learning, Hyponym-Of, language model fine tuning)
(genetic algorithm, Used-for, automatic training data generation)(genetic algorithm, Evaluate-for, optimization-based extractive multi-document summarization)(genetic algorithm, Used-for, automatic Pyramid as the fitness function)(genetic algorithm, Used-for, training data generation for automatic Pyramid scores estimation)
(memory network, Used-for, modeling inference in human language)(sequential inference models based on chain LSTMs, Part-of, memory network)
(context sensitive grammar, Part-of, natural language processing)(probabilistic context free grammar, Compare, context sensitive grammar)
(part of speech, Is-a-Prerequisite-of, morphological tagging)(part of speech, Used-for, syntax analysis)(part of speech, Used-for, language modeling)
(greedy algorithm,Used-for,recursive partitioning)(greedy algorithm,Evaluate-for,prediction schemes)(greedy algorithm,Conjunction,Lagrangian Relaxation-based algorithm)
(neural machine translation, Compare, statistical machine depends on)(dependency syntax, Is-a-Prerequisite-of, statistical machine translation)(statistical machine translation, Compare, neural machine translation)(statistical machine translation, Used-for, better translation adequacy than NMT)(statistical machine translation, Used-for, Chinese-to-English translation)(statistical machine translation, Evaluate-for, compound splitting)(statistical machine translation, Evaluate-for, recognizing textual entailment)(statistical machine translation, Used-for, model generated summary)(statistical machine translation, Is-a-Prerequisite-of, multilingual machine translation)
(corpus-based similarity, Compare, thesaurus-based similarity)(thesaurus-based similarity, Evaluate-for, word sense description)
(synchronous context-free grammar, Used-for, semantic parsing)(compound probabilistic context free grammar, Used-for, semantic parsing)(natural language processing, Is-a-Prerequisite-of, semantic parsing)(semantic parsing, Part-of, natural language processing)(predicate-argument structures, Used-for, semantic parsing)(abstract syntax networks, Used-for, semantic parsing)(semantic parsing, Used-for, mapping natural language utterances to formal meaning representations)(semantic parsing, Used-for, executing natural language commands in a machine-interpretable format)(semantic representation, Used-for, semantic parsing)(logical forms, Is-a-Prerequisite-of, semantic parsing)(executable programs, Is-a-Prerequisite-of, semantic parsing)(encoder-decoder model, Used-for, semantic parsing)(dual decomposition, Used-for, semantic parsing)(AMR graph, Part-of, semantic parsing)(semantic parsing, Used-for, mapping unstructured inputs to executable outputs)(code generation, Evaluate-for, semantic parsing)(semantic parsing, Used-for, sentence splitting)(code generation, Is-a-Prerequisite-of, semantic parsing)(semantic parsing, Used-for, mapping natural language to executable programs)(abstract syntax trees, Part-of, semantic parsing)(knowledge base, Used-for, semantic parsing)(semantic parsing, Used-for, transducing natural language utterances into formal meaning representations)(semantic parsing, Conjunction, reinforcement learning and maximum marginal likelihood)(semantic parsing, Evaluate
(graph theory, Used-for, constructing document-level graphs)(graph theory, Used-for, building labelled edge graph convolutional neural network model)(graph theory, Evaluate-for, inter-sentence relation extraction)(graph theory, Is-a-Prerequisite-of, Message Passing)
EMPTY G1.neural turing machine
EMPTY G1.phrase based machine translation
(uncertainty, Used-for, confidence modeling)(uncertainty, Used-for, error propagation analysis)(uncertainty, Used-for, rumour verification)(uncertainty, Used-for, uncertainty estimation)(uncertainty, Is-a-Prerequisite-of, robotics)
(neural summarization, Used-for, document summarization)(neural summarization, Used-for, meeting speech summarization)(query-based summarization, Part-of, neural summarization)(encode-attend-decode paradigm, Used-for, neural summarization)(query attention model, Part-of, neural summarization)(diversity based attention model, Part-of, neural summarization)(pointer-generator network, Part-of, neural summarization)(coverage mechanism, Part-of, neural summarization)(selective encoding model, Part-of, neural summarization)(SWAP-NET, Hyponym-Of, neural summarization)(template-based summarization, Compare, neural summarization)(neural summarization, Is-a-Prerequisite-of, scientific article summarization)
(generative adversarial network, Used-for, enhancing cross-language translation)(generative adversarial network, Used-for, generating spurious features in adversarial attacks)(generative adversarial network, Used-for, eliminating fake features in adversarial training)(generative adversarial network, Compare, Maximum Mean Discrepancy)
(information theory,Used-for,quantifying information)(entropy,Is-a-Prerequisite-of,information theory)(Shannon,Hyponym-Of,information theory)(mutual information,Part-of,information theory)(algorithm efficiency,Evaluate-for,information theory)
(attention model, Used-for, focusing on salient content in text)(attention model, Hyponym-Of, machine learning models)(attention model, Used-for, word reordering in neural machine translation)(attention model, Used-for, sentence scoring in document summarization)(soft attention mechanism, Part-of, attention model)(attention model, Used-for, enhance Tree-LSTMs)
(probabilistic grammar, Is-a-Prerequisite-of, regex synthesis)(probabilistic grammar, Used-for, generating complex regexes from StackOverflow posts)
EMPTY G1.radial basis function network
(stemming, Used-for, reducing words to their base form)
(Neural Machine Translation, Used-for, computer vision)(Transkimmer architecture, Used-for, computer vision)(UniCoRN, Used-for, computer vision)(Adversarial attacks, Evaluate-for, computer vision)(Deep neural networks, Is-a-Prerequisite-of, computer vision)(Image captioning, Used-for, computer vision)(Transfer learning, Used-for, computer vision)(Parameter adaptation, Evaluate-for, computer idh)(fMRI2text, Used-for, computer vision)(computer vision, Is-a-Prerequisite-of, nlp and vision)
EMPTY G1.finite state transducer
(dual decomposition, Used-for, semantic parsing)(Complex question semantic parsing, Is-a-Prerequisite-of, dual decomposition)(Hierarchical Semantic Parsing, Is-a-Prerequisite-of, dual decomposition)(ComplexWEBQUESTIONS dataset, Evaluate-for, dual decomposition)(sub-question generation, Is-a-Prerequisite-of, dual decomposition)(span prediction, Is-a-Prerequisite-of, dual decomposition)
(supertagging, Used-for, dependency tree parsing)(holographic embeddings, Used-for, supertagging)
EMPTY G1.robotic locomotion
(synchronous context-free grammar, Is-a-Prerequisite-of, tree adjoining grammar)  (tree adjoining grammar, Compare, synchronous tree-adjoining grammar)  (tree adjoining grammar, Part-of, language classes hierarchy)  (context-free grammar, Is-a-Prerequisite-of, tree adjoining grammar)  (Pushdown Adjoining Automaton, Compare, tree adjoining grammar)  (configurable pushdown automata, Is-a-Prerequisite-of, tree adjoining grammar)  (tree adjoining grammar, Is-a-Prunequisite-of, context sensitive grammar)
EMPTY G1.bidirectional recurrent neural network
(regularization, Used-for, neural machine translation)(Monte-Carlo Dropout, Hyponym-Of, regularization)(regularization, Used-for, optimizing model parameters)
(beam search, Used-for, decoding sentences in neural machine trading)(beam search, Used-for, finding good candidate translations)(beam search, Hyponym-Of, search algorithms)(beam search, Is-a-Prerequisite-of, neural summarization)(Grid Beam Search, Part-of, beam search)
EMPTY G1.wordnet
EMPTY G1.random forest
(classification, Is-a-Prerequisite-of, text classification)(classification, Used-for, evaluating model performance)(classification, Hyponym-Of, machine learning tasks)(classification, Is-a-Prerequisite-of, sentiment analysis)(classification, Is-a-Prerequisite-of, sequence classification and conditional random fields)(classification, Is-a-Prerequisite-of, sentence boundary recognition)
(constraint satisfaction, Used-for, poetry generation)
(syntaxnet, Is-a-Prerequisite-of, abstract syntax networks)(syntaxnet, Used-for, parsing natural language into dependency trees)
(k-nn, Used-for, Noise reduction in prediction)(k-nn, Hyponym-Of, Machine Learning Algorithms)
(part of speech tagging, Used-for, linguistic pattern analysis)(part of speech tagging, Used-for, syntactic task performance improvement)(part of speech tagging, Part-of, natural language processing)(neural networks, Used-for, part of speech tagging)(recurrent neural networks, Used-for, part of speech tagging)(Stanford Parser, Evaluate-for, part of speech tagging)(bidirectional networks, Evaluate-for, part of speech tagging)(Weighted finite state transducers, Used-for, part of speech tagging)(part of speech tagging, Is-a-Prerequisite-of, syntaxnet)(part of speech tagging, Is-a-Prerequisite-of, course introduction)(part of posch tagging, Is-a-Prerequisite-of, statistical part of speech tagging)
(AliMe Chat, Is-a-Prerequisite-of, chat bot)(chat bot, Used-for, real-world industrial application)
EMPTY G1.search engine indexing
(logic and reasoning, Used-for, human reasoning)(logic and reasoning, Used-for, decision-making processes)(logic and reasoning, Is-a-Prerequisite-of, predicate logic)(logic and reasoning, Is-a-Prerequisite-of, propositional logic)(logic and reasoning, Is-a-Prerequisite-of, knowledge representation)
EMPTY G1.imagenet
(backpropagation, Used-for, training neural networks)(backpropagation, Evaluate-for, optimization of deep learning models)(backpropagation, Part-of, gradient descent optimization)(backpropagation, Used-for, error correction in learning process)(backpropagation, Is-a-Prerequisite-of, Variations of GANs)(backpropagation, Is-a-Prerequisite-of, Autoencoders)
(text similarity,Part-of,natural language processing)(TextFlow,Used-for,text similarity)(document matching,Used-for,text similarity)(thematic similarity,Part-of,text similarity)(skip-grams,Used-for,text similarity)(vector space models,Used-for,text similarity)(n-grams,Used-for,text similarity)(text similarity,Is-a-Private of,event detection)(text similarity,Is-a-Prerequisite-of,document representation)(text similarity,Is-a-Prerequisite-of,bio text mining)(text similarity,Is-a-Prerequisite-of,recommendation system)(text similarity,Is-a-Prerequisite-of,text mining)(text similarity,Is-a-Prerequisite-of,word sense disambiguation)(text similarity,Is-a-Prerequisite-of,information extraction)
(capsule network, Used-for, Paraphrastic sentence embeddings)(paraphrastic sentence embeddings, Evaluate-for, capsule network)(capsule network, Compare, Neural machine translation)(capsule network, Used-for, Question Answering)(question answering, Evaluate-for, capsule network)(capsule network, Used-for, Text Classification)(text classification, Evaluate-for, capsule network)
EMPTY G1.bayes theorem
(bootstrapping,Is-a-Prerequisite-of,BONIE)  (bootstrapping,Used-for,learning specific dependency patterns)  (bootstrapping,Evaluate-for,seed selection)  (bootstrapping,Evaluate-for,noise reduction)  (bootstrapping,Used-for,relation extraction methods)  (bootstrapping,Used-for,sampling training candidates)  (bootstrapping,Used-for,self-training neural relation classifiers)  (self-training,Hyponym-Of,bootstrapping)  (bootstrapping,Used-for,refining labels of passages)  (bootstrapping,Used-for,creation of ClarQ dataset)  
(Seq2seq network, Used-for, text summarization)(sequence-to-sequence models, Used-for, text summarization)(text summarization, Is-a-Prerequisite-of, abstractive summarization)(text summarization, Is-a-Prerequisite-of, query-based summarization)(text summarization, Is-a-Prerequisite-of, extractive summarization)(text summarization, Is-a-Prerequisite-of, multi-document summarization)(neural abstractive summarization, Hyponym-Of, text summarization)(text summarization, Is-a-Prerequisite-of, neural summarization)(text summarization, Is-a-Prerequisite-of, summarization evaluation)
(convolutional neural network, Compare, recurrent neural network)(recurrent neural deep learning architectures)(recurrent neural network, Part-of, neural network)(recurrent neural network, Hyponym-Of, neural network)(recurrent neural network, Is-a-Prerequisite-of, deep learning architectures)(LSTM, Hyponym-Of, recurrent neural network)(recurrent neural network, Used-for, language modeling)(recurrent neural network, Evaluate-for, question answering)
(normalization, Evaluate-for, incompleteness of provided synonyms)(normalization, Part-of, pre-processing step)(normalization, Used-for, improving parsing performance)(normalization, Used-for, retaining content in sentiment-to-sentiment translation)
(evaluation of question answering, Used-for, assessing QA models effectiveness)
(knowledge graph, Part-of, Knowledge Bases)(knowledge graph, Used-for, integrating prior knowledge into algorithms)(semantic representation, Is-a-Prerequisite-of, knowledge graph)(knowledge graph, Part-of, dialogue systems)
(discourse parsing, Used-for, building discourse structures)(discourse segmentation, Is-a-Prerequisite-of, discourse parsing)(statistical discourse segmenters, Used-for, discourse parsing)(memory networks, Used-for, discourse parsing)(Rhetorical Structure Theory, Used-for, discourse parsing)(RST discourse treebank, Used-for, discourse parsing)(discourse relation identification, Part-of, discourse parsing)(discourse coherence, Evaluate-for, discourse parsing)(transition-based discourse parser, Used-for, discourse parsing)(neural framework for sentence-level discourse analysis, Used-for, discourse parsing)(Pointer Networks, Used-for, discourse parsing)
(variational autoencoders, Used-for, end-to-end text generation framework)(autoencoders, Part-of, variational autoencoders)
EMPTY G1.maximum likelihood estimation
EMPTY G1.gibbs sampling
(random walk, Used-for, reasoning models)(random walk, Part-of, biased random walkers)
(Zero-shot learning, Used-for, object detection)(object detection, Part-of, Language & Vision tasks)(VID-sentence dataset, Used-for, object detection)(spontaneous speech transcripts, Is-a-Prerequisite-of, object detection)(pragmatic speaker, Evaluate-for, object detection)(Adaptive scaling, Used-for, object detection)(detection tasks, Hyponym-Of, object detection)
(monte carlo tree search, Used-for, guiding the search procedure)(monte carlo tree search, Used-for, dynamically allocating computing budgets)(monte carlo tree search, Is-a-Prerequisite-of, AlphaGo)
(Variational Autoencoder, Is-a-Prerequisite-of, variational bayes model)(StructVAE, Hyponym-Of, variational bayes model)(Bayesian Hierarchical Words Representation, Evaluate-for, variational bayes model)
EMPTY G1.probability
(recursive neural network, Used-for, computing text representation with attention model)(recursive neural search model,Used-for,explicitly modeling introvable things)(recursive neural tree search,Is-a-Prerequisite-of,semantic role maintenance)(recursive task model,Is-a-Prerequisite-of,inferencing questions in action point)(recursive memory mode,Compare,chain LSTMs)(recursive recur model,Used-for,computing avail representation)
(sentence simplification, Used-for, seq2seq text generation)
EMPTY G1.discourse model
(social medium analysis, Used-for, estimating socio-economic profiles)(social medium analysis, Used-for, predictive modeling of income)(social medium analysis, Used-for, fake news detection)
EMPTY G1.learning
(imitation learning, Compare, reinforcement learning)(policy gradient method, Part-of, reinforcement learning)(reinforcement learning, Hyponym-Of, machine learning)(reinforcement learning, Used-for, improving semantic parsing accuracy)(reinforcement learning, Used-for, acquiring optimal policy for question-asking behaviors)(reinforcement learning, Used-for, exploration in learning algorithms)(reinforcement learning, Conjunction, maximum marginal likelihood)(reinforcement learning, Used-for, train sentence selection as latent variable)(reinforcement learning, Used-for, training form question answering models on semi-supervised data)(reinforcement learning, Used-for, training the latent variable in sentence selection for MRC)(reinforcement learning, Used-for, optimizing sentence compression operations)(reinforcement learning, Is-a-Prerequisite-of, robotics)(reinforcement learning, Is-a-Prerequisite-of, agent-based view of ai)
(robotics, Used-for, human-robot communication and collaboration)(robotics, Is-a-Prerequisite-of, robotic locomotion)
EMPTY G1.long short term memory network
EMPTY G1.inference
(summarization evaluation, Evaluate-for, discourse modes)(Compressive summarization, Evaluate-for, summarization evaluation)(extractive summarization, Hyponym-Of, summarization evaluation)(abstractive summarization, Hyponym-Of, summarization evaluation)(human evaluation, Is-a-Prerequisite-of, summarization evaluation)(automatic metrics, Used-for, summarization evaluation)(BLEU, Used-for, summarization evaluation)(summarization evaluation, Evaluate-for, extractive and abstrace methods)(DUC dataset, Used-for, summarization evaluation)(DUC-2002, Used-for, summarization evaluation)(summarization evaluation, Is-a-Prerequisite-of, scientific article summarization)
(transition based dependency parsing, Used-for, producing certain attachments between tokens)(arc-swift, Is-a-Prerequisite-of, transition based dependency parsing)(shift and reduce operations, Used-for, transition based dependency parsing)(lexical information, Evaluate-for, transition based MVP)(neural network, Used-for, transition based dependency parsing)
(encoder-decoder architectures, Is-a-Prerequisite-of, multi-task learning)(multi-task learning, Used-for, semantic role labeling)(grapheme-to-phoneme dictionary, Used-for, multi-task learning)(multi-task learning, Is-a-Prerequisite-of, neural network models)(shared layers, Part-of, multi-task learning)(task-specific features, Part-of, multi-task bags)(adversarial multi-task learning framework, Used-for, multi-task learning)(end-to-end computational argumentation mining, Used-for, multi-task learning)(multi-task learning, Part-of, relation learning)(multi-task learning, Used-for, extracting common features from different tasks)(multi-task learning, Is-a-Prerequisite-of, adversarial training in multi-task frameworks)(multi-task learning, Used-for, improving performance on multiple tasks using shared knowledge)(shared knowledge, Part-of, multi-task learning)(multi-task learning, Used-for, transfer learning)(multi-task learning, Used-for, semantic parsing in multilingual contexts)(multi-task learning, Hyponym-Of, machine learning)(multi-task learning, Used-for, knowledge transfer in neural network models)(multi-task learning, Used-for, neural machine translation)(multi-task learning, Used-for, enhancing QA model performance)(text classification operations, Part-of, multi-task learning)(multi-task learning, Used-for, text classification)(Neural network rnns, Used-for, multi-task learning)(m
EMPTY G1.social network extraction
EMPTY G1.logic and logical agent
EMPTY G1.perceptron
(generative and discriminative model, Compare, language modeling performance)(generative and discriminative model, Part-of, encoder-decoder framework)(generative and discriminative model, Used-for, unsupervised relation extraction)
EMPTY G1.data structure
(word distribution, Compare, semantic representation)(word distribution, Compare, semantic model)(word distribution, Hyponym-Of, statistical distribution)(word distribution, Is-a-Prerequisite-of, language modeling)(word distribution, Part-of, morphological constraints)(word distribution, Part-of, NLP applications)(word distribution, Used-for, learning word representations)(word distribution, Used-for, inducing accurate representations)
(RNNs,Is-a-Prerequisite-of,neural language modeling)
(finite state machine, Used-for, speech recognition)(weighted finite state transducers, Used-for, speech recognition)(speech recognition, Hyponym-Of, natural language processing)(speech recognition, Part-of, multimodal language processing)(hybrid CTC/attention architecture, Used-for, speech recognition)
(crawling the web, Used-for, building publicly available parallel corpora)
(morphological analyzer, Used-for, tokenization)(tokenization, Part-of, text classification process)(tokenization, Used-for, neural text classifier)
(stochastic gradient descent, Used-for, optimization)(optimization, Evaluate-for, improving BLEU score in neural machine translation)(optimization, Evaluate-for, training deep neural networks)(optimization, Evaluate-for, search problem solving in structured prediction)(optimization, Is-a-Prerequisite-of, memory networks)(optimization, Is-a-Prerequisite-of, dual decomposition)(optimization, Is-a-Prerequisite-of, newton method)(optimization, Is-a-Prerequisite-of, machine learning resources)(optimization, Is-a-Prerequisite-of, KKT conditions)(optimization, Is-a-Prerequisite-of, Lagrange duality)(optimization, Is-a-Prerequisite-of, Meta-Learning)
(predicate logic, Is-a-Prerequisite-of, predicate argument structure analysis)(predicate logic, Used-for, assessing commitment towards predicates)(predicate logic, Hyponym-Of, formal ontology)(predicate logic, Used-for, semantic disambiguation)(predicate logic, Hyponym-Of, typed entailment graphs)(predicate logic, Evaluate-for, Logical forms production)(predicate logic, Used-for, logical reasoning over text)
EMPTY G1.hidden markov model
EMPTY G1.decision tree
EMPTY G1.bayesian network
EMPTY G1.conditional probability
(feature learning, Used-for, inducing embeddings for rare or unseen words)(feature learning, Part-of, multi-space variational encoder-decoders)(feature hashing, Is-a-Prerequisite-of, feature learning)(neural networks, Used-for, feature learning)(feature learning, Used-for, transfer learning)(domain adaptation, Used-for, feature learning)(feature learning, Used-for, enhancing performance in NLP tasks)(kernel methods, Hyponym-Of, feature learning)(Convolutional Neural Network, Used-for, feature learning)(feature learning, Part-of, Convolutional Neural Network)(feature learning, Is-a-Prerequisite-of, Manifold Learning)
EMPTY G1.linguistics basic
(seq2seq, Used-for, named entity recognition)(seq2seq, Compare, RNN-based approaches)(seq2seq, Compare, convolutional seq2seq model)(seq2seq, Part-of, neural machine translation)(seq2seq, Used-for, response generation in conversation)(seq2seq, Part-of, Multi-Task Learning)(seq2seq, Used-for, grammatical error correction)(seq2seq, Used-for, task-oriented dialog systems)(seq2seq, Used-for, response selection in retrieval-based chatbots)(seq2seq, Used-for, slot filling in spoken language understanding systems)(seq2seq, Is-a-Prerequisite-of, neural summarization)
EMPTY G1.informed search
(problem solving and search, Is-a-Prerequisite-of, NumS2T)(problem solving and search, Is-a-Prerequisite-of, game playing in ai)
EMPTY G1.neural machine translation nmt
(phonological feature, Evaluate-for, learning phonotactic patterns)(distinctive feature, Compare, phonological feature)
EMPTY G1.tuning pre trained language
(semantic representation, Used-for, semantic parsing)(semantic representation, Is-a-Prerequisite-of, knowledge graph)(word distribution, Compare, semantic representation)(semantic representation, Part-of, NLP systems)(semantic representation, Compare, syntactic representation)(AMR, Part-of, semantic representation)(UCCA, Part-of, semantic representation)(GMB, Part-of, semantic and representation)(UDS, Part-of, semantic representation)(semantic representation, Part-of, semantic parser)(semantic representation, Is-a-Prartment-of, linguistic representation)(semantic representation, Evaluate-for, achieving general goals of research on semantic schemes)(semantic representation, Compare, syntactic schemes)
EMPTY G1.commonsense question answering
EMPTY G1.code generation semantic parsing
EMPTY G1.linguistic knowledge identification
(controlled text generation, Is-a-Prerequisite-of, fine-grained control in text output)(controlled text generation, Used-for, conditional text generation)(controlled text generation, Evaluate-for, compliance with specific rules and conditions)
EMPTY G1.multilingual neural
EMPTY G1.learn event
(visual dialogue, Used-for, question-answering about images)(visual dialogue, Hyponym-Of, multimodal dialogue systems)(visual dialogue, Compare, traditional text-based dialogue systems)
(commonsense evaluation, Used-for, assessing AI models on commonsense reasoning tasks)  (commonsense evaluation, Used-for, improving performance of machine learning models)  (commonsense evaluation, Compare, traditional cognitive assessment)  (commonsense reasoning, Is-a-Prerequisite-of, commonsense evaluation)  (domain-specific knowledge, Is-a-Prerequisite-of, commonsense evaluation)  (cross-lingual commonsense reasoning, Evaluate-for, commonsense evaluation)
(automatic identification, Used-for, sequence labeling model)(sequence labeling model, Used-for, named entity recognition)(sequence labeling model, Is-a-Prerequisite-of, training data)(neural sequence labeling model, Hyponym-Of, sequence labeling model)(F1-score, Evaluate-for, sequence labeling model)(named entity recognition, Used-for, sequence labeling model)(fofe method, Used-for, sequence labeling model)(feedforward neural network, Used-for, sequence labeling model)(sequence modeling, Hyponymous-Of, sequence labeling model)(sequence labeling model, Used-for, information extraction)
EMPTY G1.argument invention
EMPTY G1.language explanation prediction
EMPTY G1.ground truth parse tree
EMPTY G1.interpretability method
EMPTY G1.generated explanation
(topic model, Compare, neural language model)(topic model, Used-for, generation of related sentences)(topic model, Hyponym-Of, latent Dirichlet allocation (LDA))(neural topic model, Hyponym-Of, topic model)(topic model, Part-of, neural language model)
EMPTY G1.deep learning model nlp
(relation classification, Used-for, implicit discourse recognition)(relation classification, Hyponym-Of, NLP systems)(implicit relation network, Used-for, relation classification)(feature imitation framework, Used-for, relation classification)(neural network, Used-for, relation classification)(re a relation classification, Used-for, assigning correct relation class to entity pairs)
(automatic dialogue, Used-for, collecting symptoms)(automatic dialogue, Used-for, response selection)(encoder-decoder, Part-of, automatic dialogue)(belief spans, Part-of, automatic time dialogue)(world model, Part-of, automatic dialogue)(SQuADRUn, Evaluate-for, automatic dialogue)(Two Stage CopyNet, Used-for, automatic dialogue)(EuroSense, Evaluate-for, automatic dialogue)
EMPTY G1.faceted summarization
EMPTY G1.social bias
(distributed word representation, Used-for, modeling words in NLP tasks)(distributed word representation, Hyponym-Of, word vectors)(Neural Belief Tracking, Evaluate-for, distributed word representation)(Semantic similarity, Evaluate-for, distributed word representation)
EMPTY G1.domain aspect based sentiment
EMPTY G1.challenge semantic parsing
(representation learning, Used-for, biomedical concepts)(multimodal word distributions, Part-of, representation learning)(vector space representations, Part-of, representation learning)(word embeddings, Part-of, representation learning)(distributed word representations, Part-of, representation learning)(deep neural links, Is-a-Prerequisite-of, representation learning)(representation learning, Part-of, natural language understanding)
(specializing word embeddings, Used-for, semantic similarity tasks)(Pseudofit, Is-a-Prerequisite-of, specializing word embeddings)(external knowledge, Used-for, specializing word embeddings)(semantic similarity, Evaluate-for, specializing word embeddings)(representations, Part-of, specializing word embeddings)(Pseudofit, Used-for, specializing word embeddings)(semantic similarity, Is-a-Prerequisite-of, specializing word embeddings)
(low-resource languages, Used-for, annotated training)
(unsupervised bilingual word embeddings, Used-for, bilingual dictionary induction)(bilingual dictionary induction, Evaluate-for, unsupervised bilingual word embeddings)(unsupervised bilingual word embeddings, Is-a-Prerequisite-of, unsupervised neural machine translation)(unsupervised bilingual word embeddings, Part-of, cross-lingual NLP tasks)(unsupervised machine translation, Used-for, unsupervised bilingual word embeddings)(unsupervised bilingual word embeddings, Used-for, mining parallel sentences)(unsupervised bilingual word embeddings, Compare, bilingual word embeddings)(unsupervised neural machine translation, Evaluate-for, unsupervised bilingual word embeddings)(unsupervised bilingual lexicon induction, Evaluate-for, unsupervised bilingual word embeddings)
EMPTY G1.natural language explanation nles
EMPTY G1.dense retrieval
(compositional distributional semantics model, Used-for, evaluation of Polish language semantics)(compositional distributional semantics model, Evaluate-for, semantic relatedness and entailment)(PoincarÃ© embeddings, Part-of, compositional distributional semantics model)(SentiBERT, Is-a-Prerequisite-of, compositional distributional semantics model)(Functional Distributional Semantics, Hyponym-Of, compositional distributional semantics model)(Anchored Packed Trees, Part-of, compositional distributional semantics model)
EMPTY G1.based sentiment
(existing word embedding, Compare, newly proposed word embedding methods)(existing word casual analysis, Used for,  analysis)(existing word lexical analysis, Is-a-Prerequisite-of, robust sentiment analysis methods)(existing word pre-trained context embeddings for NLP tasks, Used-for, NLP tasks)
(Sentiment Analysis, Is-a-Prerequisite-of, sarcasm detection)(cognitive features, Used-for, sarcasm detection)(multi-modal sarcasm detection, Part-of, sarcasm detection)(iSarcasm dataset, Used-for, sarcasm detection)(sarcasm detection, Evaluate-for, sarcasm annotation)(sarcasm detection dataset, Hyponym-Of, sarcasm detection)(sarcasm detection, Hyponym-Of, multi modal sarcasm detection)(sarcasm detection, Part-of, multi modal sarcasm detection)(text classification, Used-for, sarcasm detection)(sarcasm detection, Is-a-Prerequisite-of, text classification)(sentiment classification, Is-a-Prerequisite-of, sarcasm detection)(sentiment classification, Compare, sarcasm detection)(sarcasm detection, Is-a-Prerequisite-of, satire detection)(textual data analysis, Used-for, sarcasm detection)(multimodal cues, Used-for, sarcasm detection)(textual feature, Used-for, sarcasm detection)(sarcasm detection, Compare, textual sarcasm detection)(sarcasm detection, Compare, multi-modal sarcasm detection)
EMPTY G1.recurrent neural tensor
(neural parser, Used-for, AMR parsing)(neural parser, Used-for, parsing benchmark treebanks)(neural parser, Evaluate-for, accuracy improvement)(neural parser, Hyponym-Of, natural language processing tool)(neural parser, Used-for, aligning nodes in semantic graphs)(neural parser, Used-for, interpreting neural summarization decisions)(neural parser, Is-a-Prerequisite-of, achieving state-of-the-art results on LDC2016E25)(neural parser, Is-a-Prerequisite-of, parser trained)
EMPTY G1.fact checking article
EMPTY G1.generative retrieval
(query-based summarization, Is-a-Prerequisite-of, summarization task)(abstractive summarization, Is-a-Prerequisite-of, summarization task)(extractive summarization, Is-a-Prerequisite-of, summarization task)
(machine translation mt, Compare, bi-directional LSTMs)  (machine translation mt, Compare, convolutional layers)  (machine translation mt, Compare, deep neural networks)
EMPTY G1.dialogue state tracking dst
EMPTY G1.cognate detection
(generated sentence, Is-a-Prerequisite-of, neural machine translation)(decoder, Used-for, generated sentence)(sentence encoder, Used-for, generated sentence)(target words, Part-of, generated sentence)(neural model, Used-for, generated sentence)
(visual semantic pretraining, Used-for, mitigating anisotropy in word embeddings)(visual semantic pretraining, Compare, traditional semantic pretraining)(CLIP, Hyponym-Of, visual semantic pretraining)(Socratic pretraining, Compare, visual semantic pretraining)(multimodal neural architecture, Part-of, visual semantic pretraining)
EMPTY G1.contextual subword embeddings
EMPTY G1.lexical expectation
(morphological family, Hyponym-Of, linguistic classification)
(vision language pre training,Is-a-Prerequisite-of,cross-modal downstream tasks)(vision language pre training,Used-for,multilingual multimodal representation learning)(weakly supervised vision-and-language pre-training,Hyponym-Of,vision language pre training)(E2E-VLP,Part-of,vision language pre training)
(sparse retrieval,Used-for,open-domain question answering)(phrase retrieval problem,Hyponym-Of,sparse retrieval)(sparse retrieval,Evaluate-for,scalability and speed benefit)(sparse retrieval,Evaluate-for,lowering accuracy)(contextualized sparse representation,Part-of,sparse retrieval)(DensePhrases,Compare,sparse retrieval)
(KB-InfoBot, Is-a-Prerequisite-of, oriented dialogue system)
(neural semantic parser,Used-for,converting natural language to domain-specific languages)(neural semantic parse, Used-for,semantic parsing)(neural semantic parser,Used-for,creating intermediate domain-general natural language representations)(neural semantic parser,Is-a-Prerequisite-of,achieving state-of-the-art results in semantic parsing tasks)(neural semantic parser,Used-for,mapping natural language to executable programs)(predicate-argument structures,Part-of,neural semantic parser)(neural encoder-decoder transition-based parser,Hyponym-Of,neural semantic parser)(natural language representations,Part-of,neural semantic parser)(neural semantic parser,Used-for,natural language question)(neural semantic parser,Hyponym-Of,semantic parser)(neural semantic parser,Used-for,summarization system)(neural semantic parser,Part-of,interpretable and scalable models)(neural semantic parser,Evaluate-for,state-of-the-art performance on SPADES)(neural semantic parser,Part-of,natural language generation)(neural semantic parser,Used-for,converting natural language utterances into intermediate representations)(neural semantic parser,Is-a-Prerequisite-of,building semantic content representations)(neural semantic parser,Used-for,converting natural language utterances to predicate-argument structures)(neural semantic parser,Used-for,mapping natural language utterances to domain-specific logic)(neural semantic parser,Used-for,generating formal meaning representations in Discourse Representation Theory
(ADEM, Used-for, dialogue evaluation)(human response scores, Used-for, dialogue evaluation)(dialogue models, Evaluate-for, dialogue evaluation)(dialogue evaluation, Used-for, model testing)
(data text generation, Used-for, parsing natural language descriptions into source code)
(discourse mode, Evaluate-for, writing composition evaluation)(discourse mode, Part-of, narrative essays)(discourse mode, Evaluate-for, discourse structure analysis)
(hate speech detection,Is-a-Prerequisite-of,racial bias mitigation)(hate speech detection,Evaluate-for,accuracy)(hate speech detection,Evaluate-for,F1 score)(hate speech detection,Used-for,toxicity assessment)(hate speech detection,Used-for,social media monitoring)(hate speech detection,Compare,sentiment analysis)(dialect priming,Used-for,hate speech detection)(race priming,Used-for,hate speech detection)(hate speech detection,Used-for,determining offensive content)(annotations,Used-for,hate speech detection)
EMPTY G1.aspect category opinion sentiment
(reading comprehension task, Used-for, understanding natural texts)(reading comprehension task, Used-for, answering questions)(TriviaQA dataset, Used-for, reading comprehension task)(cloze-style reading comprehension, Hyponym-Of, reading comprehension task)(open-domain question answering, Used-for, reading comprehension task)(natural-language understanding systems, Evaluate-for, reading comprehension task)
(news recommendation, Part-of, approach topic aware news)(topic-aware news encoder, Part-of, approach topic aware news)(user encoder, Part-of, approach topic aware news)
(neural network models, Used-for, reasoning ability)  (reasoning ability, Evaluate-for, Stanford Natural Fishing Language Inference Dataset)(reasoning ability, Evaluate-for, syntactic parsing)(recursive architectures, Part-of, reasoning ability)(syntactic parsing, Is-a-Prerequisite-of, reasoning ability)(reasoning about the unspoken implications, Part-of, reasoning ability)(naive psychology, Evaluate-for, reasoning ability)(reasoning ability, Is-a-Prerequisite-of, engaging dialogue)(reasonable ability, is -a-Prerequisite-of, QA systems)(reasoning ability, Evaluate-for, conversational intelligence)(reasoning ability,Evaluate-for, pragmatic reasoning training)
(feature extraction, Part-of, argument extraction)
(transition based parser, Used-for, Parsing sentences)(transition based parser, Compare, state-of-the-art transition-based parser)(transition based parser, Used-for, semantic representations)(transition based parser, Used-for, arc-swift)(transition based parser, Used-for, two-stage parsing method)(transition based parser, Used-for, RST tree generation)(transition based parser, Used-for, error repair in sentences)(transition based parser, Used-for, non-projectivity support)
(identifiability attention weight, Used-for, decoding process in ReCoSa)(attention weights, Hyponym-Of, identifiability attention weight)(key vector, Used-for, identifiability attention weight)
(Natural Language Sentence Matching, Used-for, syntactic generalization performance)(syntactic generalization performance, Compare, perplexity scores)(syntactic generalization performance, Evaluate-for, real-world adoptions)(QuoraQP, Evaluate-for, syntactic generalization performance)(syntactic generalization performance, Part-of, QuoraQP dataset)(syntactic generalization performance, Evaluate-for, state-of-the-art neural network models)(syntactic generalization performance, Evaluate-for, syntactic knowledge)(syntactic generalization performance, Hyponym-Of, generalization performance)
(discourse treebank, Is-a-Prerequisite-of, discourse dependency parsers)(discourse treebank, Used-for, evaluating discourse dependency parsers)(discourse treebank, Part-of, discourse analysis)(annotation corpus, Part-of, discourse treebank)(discourse treebank, Used-for, discourse relation identification)(discourse treebank, Compare, RST-DT)(discourse treebank, Compare, PDTB)(ScIDTB, Hyponym-Of, discourse treebank)(discourse treebank, Used-for, NLP tasks)
EMPTY G1.level distant relation extraction
(single document summarization, Is-a-Prerequisite-of, email subject line generation)(single document summarization, Compare, multi-document summarization)(single document summarization, Used-for, neural encoder-decoder models)(unsupervised approach, Used-for, single document summarization)(neural network models, Evaluate-for, single  document summarization)(abstractive summarization, Part-of, single document summarization)(extractive summarization, Part-of, single document summarization)(neural abstractive models, Hyponym-Of, single document summarization)(end-to-end model, Used-for, single document summarization)(generating summary, Used-for, single document summarization)
EMPTY G1.structured knowledge
EMPTY G1.entity linkage
(knowledge graph completion kgc, Is-a-Prerequisite-of, relation prediction)(knowledge graph completion kgc, Used-for, infer unseen entity relationships)(link prediction, Conjunction, knowledge graph completion kgc)(knowledge graph completion kgc, Evaluate-for, plausibility of unobserved facts)(neural model with dynamic knowledge graph embeddings, Used-for, knowledge graph completion kgc)(knowledge bases, Used-for, knowledge graph completion kgc)(Pocket Knowledge Base Population, Used-for, knowledge graph completion kgc)(relational triple extraction, Used-for, knowledge graph completion kgc)(MuGNN, Used-for, knowledge graph completion kgc)(attention-based entity representation, Used-for, knowledge graph completion kgc)(relation clusters, Part-of, knowledge graph completion kgc)
(fine grained entity typing, Used-for, classifying named entity mentions)(ultra-fine entity typing, Compare, fine grained entity typing)
(multilingual pre trained, Used-for, neural machine translation)  (multilingual pre trained, Is-a-Prerequisite-of, low-resource neural machine translation)  (multilignual pre trained, Is-a-Prerequisite-of, zero-shot translation)  
EMPTY G1.stanford question answering dataset
(None,Used-for,named entity recognition multiple)(named entity recognition multiple,Is-a-Prerequisite-of,markable identification)(named entity recognition multiple,Used-for,text segmentation)(named entity recognition multiple,Compare,information extraction)(None,Part-of,named entity recognition multiple)(None,Hyponym-Of,named entity recognition multiple)(None,Evaluate-for,named entity recognition multiple)(markable identification,Evaluate-for,named entity recognition multiple)(sequence labeling,Conjunction,named entity recognition multiple)
(neural language, Compare, structured representations)(neural language, Used-for, generation of conversational text conditioned on affect categories)
EMPTY G1.chinese named entity
(political debate, Part-of, democratic political decision making)(political debate, Used-for, comparing candidate positions)(Argument Mining, Used-for, political debate)(political debate, Used-for, understanding democratic political decision making)(political debate, Evaluate-for, Argument Mining)
EMPTY G1.sentence image
EMPTY G1.political bias factuality reporting
(multimodal dialogue, Used-for, understanding user intention in diverse modalities)(multimodal dialogue, Hyponym-Of, dialogue systems)(multimodal dialogue, Used-for, capturing non-verbal cues like visual expressions and acoustic signals)(multimodal dialogue, Part-of, multimodal language processing)(multimodal dialogue, Compare, unimodal dialogue systems)(multimodal dialogue, Used-for, improving interaction in goal-oriented systems)(multimodal dialogue, Evaluate-for, task completion in complex scenarios)
(factuality prediction, Evaluate-for, commitment towards a predicate)(factuality prediction, Used-for, mapping annotated corpora onto a single factuality scale)(factuality prediction, Used-for, testing models across different datasets)(rule-based factuality prediction system, Part-of, factuality prediction)(dependency trees, Used-for, factuality prediction)(supervised classifier, Used-for, factuality model)(model performance, Evaluate-for, factuality prediction)(unified factuality corpus, Used-for, factuality prediction)(event factuality prediction, Part-of, factuality prediction)
(knowledge graph kg, Is-a-Prerequisite-of, knowledge base (KB))(entities and relationships, Part-of, knowledge graph kg)(knowledge graph kg, Used-for, link prediction)(knowledge graph kg, Used-for, embedding methods)(knowledge graph kg, Used-for, neural network-based KB-QA)
EMPTY G1.human translator
EMPTY G1.based parser
(graphical model, Used-for, argument mining)(argument mining, Used-for, identifying argument components)(argument mining, Used-for, predicting argumentative relations)(factor graph model, Is-a-Prerequisite-of, argument mining)(argument mining, Part-of, artificial intelligence)(SVM, Used-for, argument mining)(RNN, Used-for, argument mining)(stance polarity and intensity prediction, Evaluate-for, argument mining)(model variations, Used-for, argument mining)(argument components, Hyponym-Of, argument mining)(argumentative relation prediction, Evaluate-for, argument mining)(non-tree argument mining, Is-a-Prerequisite-of, argument mining)(argument mining, Hyponym-Of, natural language processing)(debating system, Used-for, argument mining)(argument quality, Evaluate-for, argument mining)(text classification task, Used-for, argument mining)
(nlg model, Used-for, automatic generation of rhythmic poetry)(neural language model, Part-of, nlg model)(neural model, Hyponym-Of, nlg model)(generative neural language model, Part-of, nlg model)(conditional variational autoencoders, Part-of, nlg model)
EMPTY G1.task learning
(neural semantic parser, Used-for, natural language question)  (natural language question, Evaluate-for, semantic parsing effectiveness)  (natural language question, Is-a-Prerequisite-of, answer rationales generation)  (natural language question, Evaluate-for, COREQA efficiency)  (natural language question, Used-for, semantic parser training)  
(response generation, Part-of, conversation &emsp;model)
EMPTY G1.nlu task
(document clustering, Evaluate-for, latent relation learning)(graph autoencoder, Used-for, latent relation learning)(local and global features, Used-for, latent relation literal learning)(latent relation learning, Part-of, knowledge graph representation techniques)
(neural ranking, Used-for, prediction of binary code for each word)(neural ranking, Used-for, improving decoding speed on CPUs)(neural ranking, Hyponym-Of, neural machine translation)(Salience Rank, Hyponym-Of, neural ranking)(neural ranking, Used-for, keyphrase extraction)(neural ranking, Evaluate-for, ranking efficiency)(neural ranking, Evaluate-for, translation performance improvements)
(machine translation model, Used-for, English-Romanian translation)(machine translation model, Used-for, English-German translation)(machine translation model, Used-for, English-French translation)(machine translation model, Evaluate-for, translation accuracy)(machine translation model, Part-of, Deep Neural Networks)(machine translation model, Part-of, Neural Machine Translation)(Sequence-to-Dependency Neural Machine Translation, Hyponym-Of, machine translation model)
(retrieval model, Used-for, document retrieval)(retrieval model, Used-for, finding similar datapoints)(retrieval model, Used-for, context-dependent semantic parsing)(retrieval model, Part-of, memory-to-sequence model)(retrieval model, Part-of, Entity-Duet Neural Ranking Model)(retrieval model, Evaluate-for, task-oriented dialog systems)(retrieval model, Is-a-Prerequisite-of, multi-layer recurrent neural network for detecting answers)
(nlp community, Used-for, modeling documents and words)(nlp community, Used-for, studying people behind the language)(nlp community, Used-for, developing technologies to detect online abusive behavior)(nlp community, Part-of, natural language processing)(nlp community, Evaluate-for, mitigating online abusive behavior)(nlp community, Evaluate-for, improvements in community question answering)(nlp community, Evaluate-for, improvements in social scientific prediction tasks)
EMPTY G1.summarization model
EMPTY G1.bias pretrained language model
(clinical event extraction, Is-a-Prerequisite-of, contrastive learning objective)(clinical event extraction, Is-a-Prerequisite-of, mention identification task)(clinical event extraction, Used-for, identifying domain-specific terminologies)(clinical event extraction, Used-for, deciding biomedical mention boundaries)(clinical event extraction, Part-of, universal information extraction)(contrastive learning objective, Used-for, clinical event extraction)(DICE, Used-for, clinical event extraction)
EMPTY G1.sentiment element
(cross lingual word embeddings, Used-for, bilingual lexicon induction)(cross lingual word embeddings, Used-for, cross-lingual classification)(cross lingual word embeddings, Compare, monolingual word embeddings)(unsupervised bilingual dictionary induction, Evaluate-for, cross lingual word embeddings)(adversarial training, Used-for, cross lingual word embeddings)(cross lingual word embeddings, Part-of, domain adaptation)(cross lingual word embeddings, Is-a-Prerequisite-of, low-resource language sentiment analysis)(lexical information, Evaluate-for, cross lingual word embeddings)(cross lingual word embeddings, Used-for, cross-lingual sentiment classification)(word embeddings, Hyponym-Of, cross lingual word embeddings)(cross lingual word embeddings, Used-for, unsupervised machine translation)(semantic similarity, Evaluate-for, cross lingual word embeddings)
(semantic parsing datasets, Used-for, training semantic parsing models)(semantic parsing datasets, Part-of, multilingual GeoQuery corpus)(semantic parsing datasets, Part-of, ATIS corpus)(semantic parsing datasets, Part-of, OVERNIGHT dataset)(semantic parsing datasets, Evaluate-for, semantic parsing performance)(semantic parsing datasets, Used-for, evaluating code generation models)(semantic parsing datasets, Used-for, multilingual model training)(semantic parsing datasets, Used-for, dual learning algorithms)(semantic parsing, Is-a-Prerequisite-of, semantic parsing datasets)(code generation, Is-a-Prerequisite-of, semantic parsing datasets)
(pre trained language model, Evaluate-for, semantic pattern capture from text)(pre trained language model, Evaluate-for, language understanding)(pre trained language model, Used-for, query auto-completion improvement)(pre trained language model, Used-for, sentiment classification enhancement)(pre trained language model, Used-for, language fluency in style transfer tasks)(pre trained language model, Used-for, natural language understanding)(pre trained language model, Used-for, semantic parsing)(pre trained language model, Part-of, NLP system)(pre trained language model, Evaluate-for, effectiveness in procedural task completion)(pre trained language model, Evaluate-for, effectiveness in language generation tasks)(pre trained language model, Hyponym-Of, machine learning models)
(diverse conversational corpus, Part-of, linguistically diverse conversational corpora)(diverse conversational corpus, Used-for, computational linguistics)
(reasoning datasets, Used-for, language understanding evaluation)(reasoning datasets, Used-for, multi-hop QA)(multi-hop QA, Is-a-Prerequisite-of, reasoning datasets)(DuoRC, Part-of, reasoning datasets)(SQuADRUn, Part-of, reasoning datasets)(BIGPATENT, Part-of, reasoning datasets)
(event knowledge, Is-a-Prerequisite-of, machine reading)(event knowledge, Used-for, temporal relation classification)
EMPTY G1.machine translation nmt
(spectral clustering, Used-for, word embeddings)(word embeddings, Compare, sentence embeddings)(word embeddings, Used-for, dependency parsing)(word embeddings, Hyponym-Of, distributed word representations)(character strings, Used-for, word embeddings)(word embeddings, Used-for, machine translation)(probabilistic FastText, Hyponym-Of, word embeddings)(multimodal word distributions, Hyponym-Of, word embeddings)(semantic model, Conjunction, word embeddings)(neural word embeddings, Hyponym-Of, word embeddings)(word embeddings, Part-of, representation learning)(word embeddings, Compare, multimodal word distributions)(word embeddings, Used-for, semantic textual similarity tasks)(word embeddings, Used-for, lexical substitution task)(word embeddings, Used-for, modeling semantics in sentences)(word embeddings, Used-for, coherent aspect discovery)(word embeddings, Hyponym-Of, vector space representations)(word embeddings, Compare, character-level embeddings)(word embeddings, Hyponym-Of, cross lingual word embeddings)(word embeddings, Used-for, caption generation)(Additive compositionality, Part-of, word embeddings)(word embeddings, Compare, bag-of-words)(word embeddings, Used-for, query understanding)(Multimodal word distributions, Hyponym-Of, word embeddings)(word embeddings, Is-a-Prerequisite-of, Skip-Gram model)(Skip-Gram model, Part-of, word embeddings)(caption generation, Used-for
(dialogue system, Is-a-Prerequisite-of, task oriented dialogue system)(task oriented dialogue system, Used-for, tracking user goals and requests)(belief spans, Part-of, task oriented dialogue system)(dialogue state tracking, Used-for, task oriented dialogue system)(neural model, Used-for, task oriented dialogue system)(end-to-end learning framework, Used-for, task oriented dialogue system)(domain adaptation, Used-for, task oriented dialogue system)(DAML, Used-for, task oriented dialogue Wardens)(WMM2Seq, Used-for, task oriented dialogue system)
(natural language generation task, Used-for, generating corrective referring expressions)(natural with generating corrective referring expressionsnal language generation task, Used-for, semantic parsing for source code generation)(natural language utterances, Used-for, natural language generation task)(encoder-decoder framework, Used-for, natural language generation task)(natural language generation task, Used-for, learning commonsense knowledge from natural language text)(neural models, Used-for, natural language generation task)(contrastive focus, Used-for, natural language generation task)(natural language generation task, Hyponym-Of, natural language processing tasks)
EMPTY G1.various information retrieval
EMPTY G1.large language model
(Aspect sentiment classification, Used-for, sentiment classifier)(sentiment classifier, Evaluate-for, sentiment polarity)(Domain adaptation, Used-for, sentiment classifier)(Cross-domain sentiment classification, Used-for, sentiment classifier)(sentiment classifier, Used-for, Target-oriented sentiment classification)(active sentiment domain adaptation, Used-for, sentiment classifier)(sentiment classifier, Evaluate-for, Aspect sentiment classification)(sentiment classifier, Evaluate-for, Multimodal sentiment analysis)(Multimodal sentiment analysis, Used-for, sentiment classifier)(Aspect sentiment classification, Hyponym-Of, sentiment classifier)(cross-lingual sentiment classifier, Hyponym-Of, sentiment classifier)(Aspect-level sentiment classification, Hyponym-Of, sentiment classifier)(sentence-level annotation, Used-for, sentiment classifier)(Domain-sensitive word embeddings, Used-for, sentiment classifier)(Generic word embeddings, Used-for, sentiment classifier)(sentiment classifier, Evaluate-for, unlabeled target domain)
(cross lingual word embeddings, Used-for, bilingual lexicon induction)(Domain Adaptation, Is-a-Prerequisite-of, bilingual lexicon induction)(bilingual lexicon induction, Evaluate-for, bilingual tasks)(bilingual lexicon induction, Used-for, domain adaptation)(machine translation, Evaluate-for, bilingual lexicon induction)(adversarial unsupervised cross-lingual word embedding, Used-for, bilingual lexicon induction)(Bilingual Sent?ment Embeddings, Used-for, bilingual lexicon induction)(unsupervised bilingual lexicon induction, Part-of, bilingual lexicon induction)(morphological awareness, Evaluate-for, bilingual lexicon induction)(edit distance, Used-for, bilingual lexicon induction)(Hubless Nearest Neighbor, Used-for, bilingual lexicon induction)(manuscript editing, Evaluate-for, bilingual lexicon induction)(graph-based paradigm, Used-for, bilingual lexicon induction)(InstaMap, Used-for, bilingual lexicon induction)(language model downstream task, Evaluate-for, bilingual lexicon induction)(word embeddings, Used-for, bilingual lexicon induction)(lingual embeddings, Used-for, bilingual lexicon induction)(cross-lingual word_embeddings, Is-a-Prerequisite-of, bilingual lexicon induction)(cross lingual embeddings, Used-for, bilingual lexicon induction)(bilingual word_embeddings, Used-for, bilingual lexicon induction)
(multi-task learning, Part-of, relation learning)  (relation extraction, Part-of, relation learning)  (Neural network models, Used-for, relation learning)  (adversarial multi-task learning framework, Used-for, relation learning)  (None, Is-a-Prerequisite-of, relation learning)  
(spelling error, Part-of, relation extraction task)
(event ontology, Compare, domain ontology)(event ontology, Hyponym-Of, ontology)(event ontology, Used-for, type mapping in event extraction)
EMPTY G1.modal datasets
(Neural Symbolic Machine, Used-for, language understanding task)(language understanding task, Evaluate-for, statistical power of neural networks)(language understanding systems, Part-of, language understanding task)(morph-fitted vectors, Used-for, language understanding task)(RC datasets, Used-for, language understanding task)(Neural Belief Tracking, Used-for, language understanding task)(Neural Belief Tracking, Evaluate-for, language understanding task)(morph-fitting procedure, Used-for, language understanding task)(reading comprehension datasets, Evaluate-for, language understanding task)(multimodal posts, Evaluate-for, language understanding task)(machine comprehension task, Is-a-Prerequisite-of, language understanding task)
(multilingual translation, Used-for, language commonality exploitation)
EMPTY G1.bias pretrained language
(neural summarization model, Evaluate-for, factual correctness)(Semantic Relevance Based neural model, Is-a-Prerequisite-of, neural summarization model)(neural summarization model, Hyponym-Of, encoder-decoder framework)(neural summarization model, Used-for, improving semantic relevance)(sentence scoring, Part-of, neural summarization model)(sentence selection, Part-of, neural summarization model)(Global Optimization method under length constraint, Used-for, neural summarization model)(NeuralDater, Compare, neural summarization model)(Hibert, Used-for, neural summarization model)(Bi-directional Selective Encoding with Template, Used-for, neural summarization model)
(unsupervised bilingual lexicon induction, Evaluate-for, word translation accuracy)(WordNet, Used-for, unsupervised bilingual lexicon induction)(unsupervised bilingual lexicon induction, Evaluate-for, unsupervised bilingual word embeddings)(unsupervised bilingual lexicon induction, Part-of, bilingual lexicon induction)(unsupervised bilingual lexicon induction, Is-a-Prerequisite-of, cross-lingual transfer of NLP models)(morphological variation, Evaluate-for, unsupervised bilingual lexicon induction)(unsupervised bilingual lexicon induction, Used-for, bilingual dictionary induction)(cross-lingual word embeddings, Used-for, unsupervised bilingual lexicon induction)(unsupervised machine translation, Part-of, unsupervised bilingual lexicon induction)(adversarial training, Used-for, unsupervised bilingual lexicon induction)(distribution matching, Used-for, unsupervised bilingual lexicon induction)(alignment of word embeddings, Used-for, unsupervised bilingual lexicon induction)(unsupervised bilingual lexicon induction, Used-for, word translation from monolingual corpora)(unsupervised bilingual lexicon induction, Part-of, unsupervised cross-lingual learning)(unsupervised bilingual lexon induction, Compare, supervised bilingual lexicon induction)(unsupervised bilingual lexicon induction, Evaluate-for, performance of monolingual embeddings without supervision)(unsupervised bilingual lexicon induction,Evaluate
(cross lingual semantic parsing, Is-a-Prerequisite-of, distributed representations of logical forms)(cross lingual semantic parsing, Evaluate-for, performance on multilingual datasets)(cross lingual semantic parsing, Part-of, multilingual natural language processing)(distributed representations of logical forms, Used-for, cross lingual semantic parsing)
(predicate-argument structures,Part-of,semantic parser)(semantic parser,Used-for,Abstract Meaning Representations)(natural language explanation NLEs,Part-of,semantic parser)(semantic parser,Used-for,converting natural language utterances to predicate-argument structures)(semantic parser,Is-a-Prerequisite-of,executable programs generation)(semantic parser,Used-for,converting natural language question)(semantic parser,Used-for,semantic parsing)(semantic parser,Used-for,mapping natural language to logical forms)(neural semantic parser,Hyponym-Of,semantic parser)(intermediate representations,Part-of,semantic parser)(semantic representation,Part-of,semantic parser)(semantic parser,Used-for,converting natural language utterances to intermediate representations)(semantic parser,Is-a-Prerequisite-of,domain mapping)(semantic parser,Evaluate-for,semantic parsing efficiency)(semantic parser,Evaluate-for,achieving state-of-the-art results)(semantic parser,Used-for,natural language understanding)(semantic parser,Used-for,converting human annotated explanations into programmatic labels)
EMPTY G1.shot cross lingual transfer
(multimodal language, Used-for, multimodal machine translation)
(summarization dataset, Used-for, testing query-based summarization models)(query-based summarization, Is-a-Prerequisite-of, summarization dataset)(debatepedia, Part-of, summarization dataset)
EMPTY G1.dependency parse
(VisualSparta, Is-a-Prerequisite-of, image text retrieval)(cross-modal retrieval, Hyponym-Of, image text retrieval)
EMPTY G1.modified natural question dataset
EMPTY G1.political debate 50
EMPTY G1.prompt based model
(sentence representation learning, Evaluate-for, reading comprehension)(sentence representation learning, Evaluate-for, question answering)(sentence representation flashcards, Effective-for, emoji creation)(sentence representation learning, Used-for, generating embeddings)(sentence representation learning, Used-for, discriminative feature learning)(sentence representation learning, Compare, word representation learning)(sentence representation learning, Evaluate-for, rumor detection)(sentence representation learning, Evaluate-for, syntactic information extraction)(sentence representation learning, Evaluate-for, style and content disentangling)(sentence embeddings, Part-of, sentence representation learning)(sentence representation learning, Is-a-Prerequisite-of, downstream NLP tasks)(sentence representation learning, Used-for, abstractive text summarization)(sentence representation learning, Evaluate-for, universal sentence encoding)(neural machine translation, Compare, sentence representation learning)
EMPTY G1.shot relation extraction
(named entity relation, Used-for, joint extraction of entity mentions and relations)(named entity relation, Used-for, extract semantic relations between entity mentions)(named entity relation, Evaluate-for, Automatic Content Extraction (ACE))(named entity relation, Part-of, named entity recognition)(Agent-Artifact relations, Part-of, named entity relation)(Physical relations, Part-of, named entity relation)(Part-Whole relations, Part-of, named entity relation)
(cloze-style reading comprehension, Is-a-Prerequisite-of, reading comprehension mrc model)(Distantly supervised open-domain question answering, Compare, reading comprehension mrc model)(reading comprehension mrc model, Used-for, multi-passage question answering)
EMPTY G1.reading comprehension rc
EMPTY G1.complex named entity
(recognition relation extraction, Is-a-Prerequisite-of, Knowledge Base Population)(recognition relation extraction, Used-for, identifying relations between two entities)(recognition relation extraction, Evaluate-for, models performance in classification tasks)(recognition relation extraction, Part-of, Natural Language Processing)(recognition relation extraction, Used-for, automated knowledge extraction from text)(recognition relation extraction, Compare, manual relation extraction)(recognition relation extraction, Hyponym-Of, relation extraction)
(fact check,Used-for,verifying the truthfulness of a claim)(fact check,Used-for,retrieving authoritative evidence)(claim verification,Is-a-Prerequisite-of,fact check)(FEVER dataset,Used-for,fact check)(multi-task model,Evaluate-for,fact check)(LogicalFactChecker,Used-for,fact check)(neural retrieval model,Evaluate-for,fact check)(graph module network,Used-for,fact check)(veracity of a claim,Evaluate-for,fact check)(claim context,Evaluate-for,fact check)(evidence retrieval,Used-for,fact check)
EMPTY G1.network rntn
(context word vector, Used-for, unsupervised anomaly detection)(context word vector, Used-for, pre-trained model)(context word vector, Compare, target word vector)(GloVe, Is-a-Prerequisite-of, context word vector)(word embedding model, Is-a-Prerequisite-of, context word vector)(context word vector, Used-for, capturing linguistic variations)
EMPTY G1.word embeddings different language
(word-embedding models, Used-for, dialog generation)(Skip-Gram model, Used-for, dialog generation)(local coherence model, Used-for, dialog generation)(encoder-decoder model, Used-for, dialog generation)(dialog generation, Is-a-Prerequisite-of, interpretable response generation)(neural dialogue generation, Used-for, dialog generation)(dialog systems, Used-for, dialog generation)(encoder-decoder dialog model, Used-for, dialog generation)(deep latent variable models, Used-for, dialog generation)(dialog generation, Evaluate-for, response generation)
(controlled text the generation, Used-for, conditional text generation)(conditional text generation, Hyponym-Of, controllable text generation)
EMPTY G1.unsupervised cross lingual
(race nlp,Is-a-Prerequisite-of,racial justice in NLP research practices)(race nlp,Hyponym-Of,NLP model development stages)(race nlp,Used-for,surveying papers on race)(race nlp,Evaluate-for,upholding racial hierarchies)(racial hierarchies,Evaluate-for,race nlp)(race nlp,Is-a-Prerequisite-of,inclusion in NLP research)
(Abstractive summarization, Evaluate-for, summarization datasets)(Extractive summarization, Evaluate-for, summarization datasets)(English Gigaword, Hyponym-Of, summarization datasets)(DUC 2004, Hyponym-Of, summarization datasets)(MSR, Hyponym-Of, summarization datasets)(CNN/Daily Mail dataset, Hyponym-Of, summarization datasets)(AMI corpus, Hyponym-Of, summarization datasets)(ICSI corpus, Hyponym-Of, summarization datasets)(WikiQA, Hyponym-Of, summarization datasets)
(adversarial training, Used-for, mapping monolingual embeddings to a shared space)(adversarial training, Part-of, adversarial search)(adversarial training, Used-for, domain adaptation)(adversarial training, Used-for, cross lingual word embeddings)(adversarial training, Used-for, unsupervised bilingual lexicon induction)(adversarial training, Used-for, learning dialect invariant features in morphological tagging)(adversarial training, Used-for, mitigating negative transfer in cross-lingual learning)(adversarial training, Used-for, enhancing multi-dimensional emotion regression)(adversarial training, Used-for, filtering distant supervision noise in relation extraction)(adversarial training, Part-of, Japanese PAS analysis model)(adversarial training, Part-of, disentangled latent representation learning)(adversarial training, Used-for, preventing task interference in multi-task learning)(adversarial training, Used-for, boosting model generalization)(adversarial training, Evaluate-for, generated summary)(adversarial training, Used-for, knowledge-transfer scheme)(adversarial training, Used-for, improving robustness)(adversarial training, Used-for, enhancing model robustness against adversarial attacks)(adversarial attack, Is-a-Prerequisite-of, adversarial training)(adversarial training, Used-for, knowledge-transfer in dialectal variants)(adversarial training, Used-for, learning bilingual word embeddings without parallel data)(adversarial example, Used-for
(multimodal language, Used-for, sentiment analysis)(multimodal language, Used-for, emotion recognition)(multimodal language, Used-for, multimodal dialogue systems)(multimodal language, Used-for, multimodal machine translation)(multimodal language, Used-for, multimodal named entity recognition)(multimodal language, Hyponym-Of, human language processing)
(morphological analyzer, Used-for, tokenization)(morphological analyzer, Used-for, tokenization of sentences in unsegmented languages)(morphological analyzer, Used-for, generating morphological constraints)(morphological analyzer, Used-for, encoding sentences with word or subword representations)(morphological analyzer, Used-for, two-tier BERT architecture)
(long form question answering, Used-for, machine comprehension)(long form question answering, Used-for, evaluating generation systems)(Semantic parsing, Used-for, long form question answering)(Open-domain question answering, Compare, long form question answering)(Knowledge bases, Used-for, long form question answering)(Machine reading, Is-a-Prerequisite-of, long form question answering)(Neural network models, Used-for, long form question answering)(Distant supervision, Used-for, long form question answering)(Adversarial inputs, Evaluate-for, long form question answering)(Web text, Used-for, long form question answering)
(sentence generation, Used-for, code generation)(sentence generation, Used-status, semantic parsing)(decoder, Used-for, sentence generation)(sentence generation, Used-for, dialog response generation)(sentence generation, Used-for, question generation)(sentence generation, Used-for, automatic pun generation)(neural machine translation, Used-for, sentence generation)(Seq2Seq models, Used-for, sentence generation)(latent variable models, Used-for, sentence generation)(encoder-dedecoder dialog models, Used-for, sentence generation)(neural models, Used-for, sentence generation)(controlled sentence function, Used-for, sentence generation)(iterative training process, Used-for, sentence generation)(sentence generation, Used-for, email subject line generation)(neural language model, Used-for, sentence generation)
(dialogue generation model, Used-for, generating fluent natural language responses in conversational systems)(dialogue generation model, Used-for, end-to-end neural dialogue generation tasks)(dialogue generation model, Evaluate-for, quality of response)(dialogue generation model, Used-for, real-world industrial applications)(neural knowledge diffusion model, Part-of, dialogue generation model)(two-step generation architecture, Part-of, dialogue generation model)(dialogue generation model, Evaluate-for, diversity and appropriateness of responses)(dialogue generation model, Used-for, multi-turn dialogue generation)(dialogue generation model, Used-for, producing suitable responses based on detected relevant contexts)
(discourse coherence, Evaluate-for, discourse parsing)
(underrepresented language, Is-a-Prerequisite-of, linguistic diversity challenges)  (underrepresented language, Used-for, improving NLP systems)  (underrepresented language, Hyponym-Of, indigenous languages)
(neural retrieval model, Evaluate-for, fact check)(neural retrieval model, Used-for, context-dependent semantic parsing)(neural retrieval model, Used-for, generating source code conditioned on class environment)(neural retrieval model, Evaluate-for, fast adaptation in model learning)(neural retrieval model, Part-of, context-aware encoder-decoder systems)(neural retrieval model, Hyponym-Of, information retrieval systems)
EMPTY G1.zero shot text classification
(entity recognition ner aim, Is-a-Prerequisite-of, model generalization)(entity recognition ner aim, Is-a-Prerequisite-of, multilingual learning)(entity recognition ner aim, Is-a-Prerequisite-of, cross-domain domain adaptation)(tag hierarchy, Used-for, entity recognition ner aim)(external knowledge, Used-for, entity recognition ner aim)(child models, Part-of, entity recognition ner aim)(gazetteers, Evaluate-for, entity recognition ner aim)(dictionaries, Used-for, entity region ner aim)(named entity dictionaries, Used-for, entity recognition ner aim)(multitasking approach, Part-of, entity recognition ner aim)(labeled data, Used-for, entity recognition ner aim)
(relation extraction model, Evaluate-for, multi-lingual texts)(relation extraction model, Is-a-Prerequisite-of, distant supervision)(relation extraction model, Evaluate-for, noise characterization in training data)(multi-lingual neural relation extraction framework, Part-of, relation extraction model)(joint extraction of entities and relations, Part-of, relation extraction model)
(pre trained model, Used-for, enhancing performance of neural network architectures for NLP tasks)(pre trained model, Used-for, sentence and corpus-level evaluation in MT metrics)(pre trained which, Used-for, keyphrase extraction from scholarly documents)
(monotonic transformation,Part-of,morphological inflection)(morphological inflection,Is-a-Prerequisite-of,morphological tagging)(morphological inflection,Hyponym-Of,morphological analysis)(morphological inflection,Used-for,language understanding systems)(morphological inflection,Part-of,morphology)(morphological inflection,Is-a-Prerequisite-of,morphological tagging accuracy improvements)(morphological inflection,Evaluate-for,morphological richness and dialectal variations handling)(morphological inflection,Evaluate-for,semantic and morphological information incorporation)(morphological inflection generation,Hyponym-Of,morphological inflection)(morphological inflection,Is-a-Prerequisite-of,dialogue state tracking)(morphological inflection,Hyponym-Of,morphological learning)(morphological inflection,Is-a-Prerequisite-of,morphological task)
(social bias frame, Is-a-Prerequisite-of, commonsense reasoning on social implications)(social bias frame, Used-for, modeling pragmatic frames in expressing social biases)(social bias frame, Used-for, large-scale modelling and evaluation)
(translation model, Used-for, interpreting agents messages)(neural machine translation, Used-for, translation model)(chunk-based decoders, Part-of, translation model)(Multi-modal Neural Machine Translation model, Is-a-Prerequisite-of, translation model)(zero-resource NMT, Used-for, translation model)(NMT+RNNG, Used-for, translation model)(bidirectional tree encoder, Part-of, translation model)(sequence-to-dependency NMT, Used-for, translation
(evaluation of text classification, Evaluate-for, adversarial robustness)(adversarial robustness, Compare, transfer accuracy)(adversarial robustness, Compare, content preservation)(adversarial robustness, Compare, language fluency)(adversarial robustness, Used-for, defending adversarial attacks)(adversarial robustness, Compare, task accuracy)
EMPTY G1.word embeddings widely
EMPTY G1.question answering dataset squad
EMPTY G1.answering dataset
EMPTY G1.representation morphological
EMPTY G1.multi task learning
(detect hate speech, Part-of, online hate identification systems)(racial bias, Is-a-Prerequisite-of, detect hate speech)
(Mean opinion score, Used-for, learning morph-related inflection)(Labeled sequence transduction, Used-for, learning morphological inflection)(Multi-space variational encoder-decoders, Used-for, learning morphological inflection)(Generative model, Used-for, learning morphological inflection)(Semi-supervised learning, Used-for, learning morphological inflection)(Morphological inflection, Is-a-Prerequisite-of, learning morphological inflection)(Hard attention mechanism, Used-for, learning morphological inflection)(Soft attention mechanism, Used-for, learning morphological inflection)(Neural networks, Used-for, learning morphological inflection)(Morphological inflection generation datasets, Evaluate-for, learning morphological inflection)(Latent Meaning Models, Used-for, learning morphological inflection)(Statistical morphological inflectors, Used-for, learning morphological inflection)
(unsupervised selective rationalization, Used-for, producing rationales alongside predictions)(rationale generator, Part-of, unsupervised selective rationalization)(predictor, Part-of, unsupervised selective rationalization)(unsupervised selective rationalization, Evaluate-for, rationale plausibility)(unsupervised selective rationalization, Evaluate-for, task accuracy)(unsupervised selective rationalization, Used-for, improving model faithfulness)
(multi passage reading comprehension,Used-for,inferring answers from multiple documents)(multi passage reading evidence,Part-of,multi passage reading comprehension)(Dynamic Self-attention Network,Used-for,multi passage reading comprehension)(cross-passage information,Used-for,multi passage reading comprehension)(multi-hop reading comprehension,Hyponym-Of,multi passage reading comprehension)
(KB-InfoBot, Is-a-Prerequisite-of, oriented dialogue)
EMPTY G1.chinese spelling correction csc
(convolutional layers, Compare, recurrent network)(recurrent network, Hyponym-Of, neural networks)(bi-directional LSTM, Part-of, recurrent network)(recurrent network, Evaluate-for, machine translation)(recurrent network, Used-for, reading comprehension)(recurrent network, Used-for, dialog systems)(recurrent network, Evaluate-for, handling long documents)(recurrent network, Used-for, automatic question answering)(gated self-matching networks, Part-of, recurrent network)(stochastic gradient Markov Chain Monte Carlo, Used-for, recurrent network)(recurrent network, Evaluate-for, knowledge base question answering)(recurrent network, Used-for, sentiment analysis)(Hybrid Code Networks, Part-of, recurrent network)
(text generation, Hyponym-Of, language generation task)(language generation task, Used-for, generation of complex programs from natural language descriptions)(language generation, Used-for, corrective referring expressions generation)(sequence-to-sequence model, Used-for, language generation task)(neural networks, Used-for, language generation task)(grammar model, Used-for, language generation task)(grammatical representations, Used-for, language generation task)(statistical approaches, Evaluate-for, language generation oid task)
(social bias encoded, Part-of, biases in word embeddings)
EMPTY G1.direct speech speech translation
(contextualized embeddings, Used-for, capturing affect dimensions in portrayals of people)(contextualized embeddings, Used-for, encoding translation and reference sentences in machine translation evaluation)(contextualized embeddings, Compare, traditional word embeddings)(contextualized embeddings, Used-for, learning sense-level embeddings)(contextualized embeddings, Used-for, analyzing portrayals of men and women)(contextualized embeddings, Hyponym-Of, Neural Language Modeling outputs)(contextualized embeddings, Used-for, deducing finer-grained sense distributions)(contextualized embeddings, Evaluate-for, robustness against full sense inventory)(contextualized embeddings, Used-for, detecting temporal changes in word meaning)
EMPTY G1.oriented spoken dialogue system
(emotion cause extraction, Compare, emotion cause pair extraction)(emotion cause extraction, Used-for, emotion cause pair abl extraction)(benchmark emotion cause corpus, Evaluate-for, emotion cause pair extraction)(emotion cause pair extraction, Used-for, synthetic question answering corpora generation)(emotion-cause pairing and filtering, Part-of, emotion cause pair extraction)(multi-task seasoning learning, Used-for, emotion cause pair extraction)
(contextual embeddings, Used-for, word sense disambiguation)(contextual embeddings, Used-for, understanding word usage in context)(contextual embeddings, Is-a-Prerequisite-of, accurate sentiment analysis)(contextual embeddings, Used-for, improving machine translation performance)(contextual embeddings, Compare, non-contextual embeddings)(contextual embeddings, Hyponym-Of, word embeddings)(contextual embeddings, Evaluate-for, effectiveness in NLP tasks)(non-contextual embeddings, Compare, contextual embeddings)(lexical features, Compare, contextual embeddings)(contextual embeddings, Evaluate-for, domain adaptation effectiveness)(contextual embeddings, Used-for, enhancing bilingual word embeddings)(contextual embeddings, Used-for, enhancing NER)
(annotated training data, Used-for, building NLP models for low-resource languages)(neural network models, Evaluate-for, annotated training ated data)(annotated training data, Is-a-Prerequisite-of, reliably estimating lexical feature weights)(annotated training data, Used-for, Spoken Language Understanding models)(neural networks, Used-for, annotated training data)(regular training data, Compare, annotated training data)(Spoken Language Understanding models, Evaluate-for, annotated training data)(lexical features, Evaluate-for, annotated training data)(neural natural language inference models, Used-for, annotated training data)(annotated training data, Used-for, supervised learning)(small hand-labeled data, Part-of, annotated training data)(annotated training data, Part-of, Spoken Language Understanding models)
EMPTY G1.relation linking
(comprehension datasets, Used-for, natural-language understanding systems development)(comprehension datasets, Part-of, reading comprehension)(comprehension datasets, Evaluate-for, prerequisite skills)(comprehension datasets, Evaluate-for, readability)(comprehension datasets, Is-a-Prerequisite-of, machine comprehension)(comprehension datasets, Hyponym-Of, natural language processing datasets)
EMPTY G1.model semantic parsing
(professional fact checker, Used-for, verify the veracity of claims)
(summarization system, Used-for, generating summaries from text)(extractive summarization, Hyponym-Of, summarization system)(abstractive summarization, Hyponym-Of, summarization system)(query-based summarization, Hyponym-Of, summarization system)(extractive summarization, Part-of, summarization system)(abstractive summarization, Part-of, summarization system)(query-based summarization, Part-of, summarizationemme m system)(neural semantic parser, Used-for, summarization system)(multi-document summarization, Part-of, summarization system)(Integer Linear Programming, Evaluate-for, summarization system)
EMPTY G1.adversarial purification
EMPTY G1.self attentive parser
(language model lm, Used-for, language modeling)(language model lm, Part-of, natural language processing)(language model lm, Evaluate-for, generating conversational text)(neural language model, Hyponym-Of, language model lm)(recurrent neural networks, Used-for, language model lm)(language model perplexity, Evaluate-for, language model lm)(language model lm, Used-for, enhancing semantic understanding of natural language)(language model lm, Evaluate-for, improving coherence in topic models)
EMPTY G1.hate speech detection model
EMPTY G1.argument generation
(graph embedding, Used-for, network analysis)(graph embedding, Used-for, vertex representation)(graph embedding, Compare, traditional network models)(link prediction, Evaluate-for, graph embedding)(vertex classification, Evaluateican text, Evaluate-for, graph embedding)(semantic relationships, Evaluate-for, graph embedding)
EMPTY G1.based explanation
EMPTY G1.vision language
(dependency parser,Used-for,semantic tree construction)(dependency parser,Used-for,semantic dependency parsing)(dependency parser,Used-for,dependency parsing task)(dependency parser,Used-for,drawing syntactic structures)(arc-swift,Is-a-Prerequisite-of,dependency parser)(dependency parser,Evaluate-for,performance accuracy)(stack-pointer networks,Hyponym-Of,dependency parser)
(conversational corpus, Used-for, dialogue system development)(conversational corpus, Part-of, natural language processing)(conversational corpus, Evaluate-for, model performance)(conversational corpus, Hyponym-Of, text corpus)(conversational corpus, Is-a-Prerequisite-of, task-oriented dialogue system framework)(dialogue system development, Evaluate-for, conversational corpus)(natural language processing, Part-of, conversational corpus)(model performance, Evaluate-for, conversational corpus)(task-oriented dialogue system framework, Evaluate-for, conversational corpus)
(event knowledge, Is-a-Prerequisite-of, machine reading)(KBLSTM, Used-for, machine reading)(machine reading, Evaluate-for, event extraction)(recurrent neural networks, Used-for, machine reading)(machine reading, Used-for, entity extraction)(machine reading, Used-for, event extraction)
EMPTY G1.contemporary language model
(sentiment classifiers, Used-for, predict sentiment)(target-sensitive memory networks, Used-for, predict sentiment)(Multi-sentiment-resource Enhanced Attention Network, Used-for, predict sentiment)(Transfer Capsule Network, Used-for, predict sentiment)(Hybrid Contextualized Sentiment Classifier, Used-for, predict sentiment)(Bilingual Sentiment Embeddings, Used-for, predict sentiment)(deep learning, Used-for, predict sentiment)(attention mechanism, Used-for, predict sentiment)(neural word embeddings, Used-for, predict sentiment)(active learning, Used-for, predict sentiment)
(task sentiment classification,Used-for,classifying sentiment polarities over individual opinion targets in a sentence)(task sentiment classification,Used-for,classifying sentiment in texts from different domains)(word embeddings,Used-for,task sentiment classification)(Aspect sentiment classification,Part-of,task sentiment classification)(Target-oriented sentiment classification,Part-of,task sentiment classification)(cross-domain sentiment analysis,Is-a-Prerequisite-of,task sentiment classification)(task-specific modifications,Evaluate-for,task sentiment classification)(training from scratch,Evaluate-for,task sentiment classification)(Universal Language Model Fine-tuning,Used-for,task sentiment classification)(neural network models,Used-for,task sentiment classification)(linguistic features,Evaluate-for,task sentiment classification)(social interaction features,Evaluate-for,task sentiment classification)
(lexical, Is-a-Prerequisite-of, news summarization)(Document modeling, Used-for, news summarization)(Sentence extraction, Used-for, news summarization)(CNN/Daily Mail summarization task, Evaluate-for, news summarization)(Neural sequence-to-sequence models, Used-for, news summarization)(SWAP-NET, Used-for, news summarization)(Extractive summarization, Hyponym-Of, news summarization)(Abstractive summarization, Hyponym-Of, news summarization)(Sequence-to-sequence framework, Used-for, news summarization)(Template-based summarization, Compare, news summarization)(Integer Linear Programming, Used-for, news summarization)(Automatic metrics, Evaluate-for, news summarization)(Human judgment, Evaluate-for, news summarization)
(parser trained, Used-for, syntax tree generation)(parser trained, Evaluate-for, generalization to other domains)(parser trained, Evaluate-for, high exact match accuracy)(neural parser, Is-a-Prerequisite-of, parser trained)(pre-trained encoder representations, Used-for, parser trained)(parser trained, Evaluate-for, state-of-the-art parsing results)(parser trained, Evaluate-for, F1 score optimization)(domain adaptation, Used-for, parser trained)
(adversarial sample, Is-a-Prerequisite-of, adversarial attack)(adversarial sample, Used-for, decreasing accuracy)(adversarial sample, Evaluate-for, model robustness)(adversarial sample, Used-for, neural machine translation)
EMPTY G1.better language
EMPTY G1.machine translation nmt model
EMPTY G1.multi lingual
EMPTY G1.aspect level sentiment classification
EMPTY G1.language generation nlg system
(story ending generation, Part-of, natural language generation)(story ending generation, Used-for, controlling the sentiment of story endings)(natural language generation, Is-a-Prerequisite-of, story ending generation)(story corpus, Used-for, story ending generation)(fine-grained sentiment intensity, Evaluate-for, story ending generation)
EMPTY G1.dialogue generation
EMPTY G1.translation task demonstrate
EMPTY G1.supervised ner
(multi document summarization, Hyponym-Of, document summarization)(multi document summarization, Compare, single document summarization)(multi document summarization, Used-for, summarizing documents created by multiple authors)(multi document summarization, Used-for, compressing content in large document collections)(email subject line generation, Compare, multi document summarization)
(bias mention, Evaluate-for, coreference resolution improvement)
EMPTY G1.zero shot cross lingual
(multi modal sarcarcasm detection, Part-of, natural language processing applications)(multi modal sarcasm detection, Evaluate-for, effectiveness by dynamic network modeling)(cross-modal graph, Used-for, multi modal sarcasm detection)(sarcasm detection, Hyponym-Of, multi modal sarcasm detection)(sarcasm detection, Part-of, multi modal sarcasm detection)(textual sarcasm detection, Compare, multi modal sarcasm detection)
(document level sentiment, Evaluate-for, Aspect SentLength Classification effectiveness)
(dialogue agent, Used-for, searching Knowledge Bases (KBs))(dialogue agent, Part-of, multi-turn dialogue systems)(dialogue agent, Used-for, task success prediction)(dialogue agent, Used-for, chat detection with users)(reinforcement learner, Part-of, dialogue agent)
(supervised relation, Is-a-Prerequisite-of, supervised learning methods)(supervised relation, Evaluate-for, event extraction)(supervised relation, Evaluate-for, relation classification tasks)(supervised relation, Used-for, classifying discourse relations)(supervised relation, Evaluate-for, distantly supervised scenario)
(automatic dialogue evaluation, Used-for, assessing performance of neural dialogue models)
(aspect term extraction, Used-for, opinion summarizing)(aspect term extraction, Is-a-Prerequisite-of, aspect sentiment classification)(Supervised learning, Used-for, aspect term extraction)(Deep learning, Used-for, aspect term extraction)(Conditional Random Fields, Evaluate-for, aspect term extraction)(neural approach, Used-for, aspect term extraction)(pre-trained embeddings, Used-for, aspect term extraction)(NeuralREG, Compare, aspect term extraction)(aspect term extraction, Used-for, fine-grained opinion analysis)(aspect term extraction, Part-of, fine-grained sentiment analysis)(aspect term extraction, Used-for, extracting aspects from opinion document)(aspect term extraction, Evaluate-for, sentiment analysis absa)(aspect term extraction, Part-of, aspect-based sentiment analysis)
(fact checked claim, Used-for, saving effort in manual fact-checking)(fact checked claim, Part-of, FC-articles)(fake news, Is-a-Prerequisite-of, fact checked claim)
(domain sentiment lexicon, Used-for, sentiment analysis)(domain sentiment lexicon, Is-a-Prerequisite-of, cross-domain sentiment analysis)(domain sentiment lexicon, Used-for, characterizing word semantics)
EMPTY G1.explainability method
EMPTY G1.question dataset
(joint entity relation extraction, Used-for, identifying entity mention spans and relations in  raw text)  (joint entity relation extraction, Part-of, end-to-end approaches in NLP)  (neural encoder-decoder model, Used-for, joint entity relation extraction)  (entity-relation bipartite graph, Used-for, joint entity relation extraction)  (joint entity relation extraction, Evaluate-for, ACE05 benchmark)  (entity detection, Is-a-Prerequisite-of, joint entity relation extraction)
EMPTY G1.debate topic expansion
(generating natural $language, Used-for, real-world question answering systems)(generating natural $language, Used-for, creating answer rationales)
(semantic parsing, Used-for, question answering system)
(open domain question answering, Used-for, answering factoid questions using Wikipedia articles)(open domain question answering, Is-a-Prerequisite-of, machine reading at scale)(machine reading at scale, Part-of, open domain question answering)(question answering, Part-of, open domain question answering)(open domain question answering, Used-for, employing Memory networks in question answering)(text span detection, Used-for, open domain question answering)(open domain question answering, Evaluate-for, exploiting universal schema for question answering)(open domain question answering, Hyponym-Of, question answering)
(image text pair, Used-for, multimodal sentiment detection)(image text her, Used-for, cross-modal downstream tasks)(image text her, Used-for, vision-language pre-training)
(WikiReading dataset,Is-a-Prerequisite-of,question answering dataset)(WikiQA,Is-a-Prerequisite-of,question answering dataset)(SemEval-2016 Task 3A,Is-a-Prerequisite-of,question answering dataset)(Spades fill-in-the-blank,Is-a-Prerequisite-of,question answering dataset)
EMPTY G1.pre trained language
(aspect level sentiment, Is-a-Prerequisite-of, opinion term extraction)(aspect level sentiment, Is-a-Prerequisite-of, aspect-level sentiment classification)(aspect level sentiment, Evaluate-for, transfer learning effectiveness)(aspect level sentiment, Evaluate-for, model performance on ABSA tasks)
(conditional response generation, Part-of, controllable text generation)(Emotion-controllable response generation, Hyponym-Of, controllable text generation)(Pre-train and Plug-in Variational Auto-Encoder, Used-for, controllable text generation)(conditional text generation, Hyponym-Of, controllable text generation)
EMPTY G1.speech text translation
(language generation, Evaluate-for, scaling up to generation of complex programs)(Neural Symbolic Machine, Used-for, language generation)(language generation, Used-for, parsing natural language descriptions)(language generation, Used-for, producing referring expressions)(language generation, Used-for, opinionated responses)(language generation, Hyponym-Of, natural language processing)(language generation, Evaluate-for, generating corrective referring expressions)(language generation, Used-for, code-switched text analysis)(language generation, Evaluate-for, SQL query generation)(code generation, Hyponym-Of, language generation)(source code, Used-for, language generation)
EMPTY G1.improve translation
EMPTY G1.dialogue data
EMPTY G1.learns event
(semantic dependency parsing, Used-for, graph-based nlp)(semantic dependency parsing, Is-a-Prerequisite-of, policy gradient method)(semantic dependency parsing, Used-for, Maximum Subgraph algorithms)(semantic dependency parsing, Used-for, Lagrangian Relaxation-based algorithm)(dependency parser, Used-for, semantic dependency parsing)(semantic dependency parsing, Used-for, capturing semantic relations between words)(semantic dependency on, Hyponym-Of, dependency parsing)(dependency parsing, Is-a-Prerequisite-of, semantic dependency parsing)(deep neural architecture, Used-for, semantic dependency parsing)(Bidirectional LSTM, Used-for, semantic dependency parsing)(multi-layer perceptron, Used-for, semantic dependency parsing)
(discourse relation, Used-for, discourse coherence assessment)(discourse relation, Used-for, stance dimension validation)(discourse relation, Part-of, sociolinguistic construct)(discourse relation, Used-for, implicit discourse relation classification)(discourse relation, Used-for, modeling argument pairs in sequential data)(discourse relation, Used-for, discourse-specific word embedding learning)(discourse relation, Used-for, creating differentiable approximations in seq2seq models)
EMPTY G1.knowledge inference
EMPTY G1.data text generation model
EMPTY G1.percentage point ibm debater
(morphological paradigm, Used-for, morphological analysis)(morphological paradigm, Part-of, language modeling)(inflectional forms, Part-of, morphological paradigm)(declension classes, Hyponym-Of, morphological paradigm)(morphological typology, Conjunction, morphological paradigm)
(dialogue state tracking, Used-for, task oriented dialogue system)(dialogue state tracking, Evaluate-for, dialogue systems)(dialogue state, Part-of, dialogue state tracking)(morphological inflection, Is-a-Prerequisite-of, dialogue state tracking)(dialogue state tracking, Used-for, managing user interactions in oriented dialogue)(dialogue state tracking, Used-for, estimate user goals and requests)(GLAD, Used-for, dialogue state tracking)(natural language understanding, Is-a-Prerequisite-of, dialogue state tracking)(spoken dialogue system, Used-for, dialogue state tracking)(dialogue state tracking, Used-for, testing linguistic representation effectiveness)(dialogue state tracking, Evaluate-for, handling unknown slot values)(distributional semantics model, Used-for, dialogue state tracking)(morphological task, Used-for, dialogue state tracking)(dialogue state tracking, Evaluate-for, morph-fitted vectors)(event mention, Part-of, dialogue state tracking)(dialogue state tracking, Used-for, inferring slot correlations)
(event relation, Used-for, event detection)(relation extraction, Hyponym-Of, event relation)
(BPEmb, Hyponym-Of, non contextual subword embeddings)(FastText, Hyponym-Of, non contextual subword embeddings)(non contextual subword embeddings, Used-for, low-resource language tasks)
EMPTY G1.level sentiment classification
(autoencoders, Used-for, natural language generation)(deep learning tool, Used-for, natural, language generation)(Variational autoencoders, Used-for, natural language generation)(neural language model, Used-for, natural language generation)(story ending generation, Part-of, natural language generation)(natural language generation, Is-a-Prerequisite-of, story ending generation)(data-to-text generation model, Part-of, natural language generation)(natural language generation, Used-for, answer generation in question answering systems)(neural semantic parser, Part-of, natural language generation)(natural language generation, Used-for, generating rhythmic poetry)(natural language generation, Used-for, generating corrective referring expressions)(COREQA, Used-for, natural language generation)(language model downstream task, Evaluate-for, natural language generation)(distributional semantics, Used-for, natural language generation)(oriented dialogue summarization, Is-a-Prerequisite-of, natural language generation)(autoregressive language model, Used-for, natural language generation)(language model, Used-for, natural language generation)(natural language generation, Part-of, downstream tasks)(natural language generation, Is-a-Prerequisite-of, semantic parsing)(natural language generation, Used-for, producing referring expressions (REs))(natural language generation, Used-for, ONLG)(natural language generation, Used-for, generating natural language descriptions of chess games)(natural language understanding, Compare, natural language generation)
(language model downstream task,Used-for,named entity recognition)(language model downstream task,Evaluate-for,named entity recognition)(language model downstream task,Evaluate-for,cross-lingual transfer of NLP models)(language model downstream task,Evaluate-for,natural language generation)(language model downstream task,Used-for,diachronic accuracy assessment)(language model downstream task,Evaluate-for,multilingual modeling)(language model downstream task,Evaluate-for,bilingual lexicon induction)(language model downstream task,Used-for,language normalization)(language model downstream task,Evaluate-for,user profiling)(language model downstream task,Evaluate-for,semantic drift measurement)(language model downstream task,Used-for,query auto-completion)
(domain adaptation, Evaluate-for, distributional semantics)(distributional semantics, Is-a-Prerequisite-of, sentiment analysis)(semantic lexicons, Part-of, distributional semantics)(compositional distributional semantics, Hyponym-Of, distributional semantics)(distributional semantics, Used-for, natural language generation)
(neural text generation, Used-for, video captioning)(neural text generation, Used-for, dialog systems)(neural text generation, Used-for, pun generation)(neural text generation, Used-for, derived word generation)(neural text generation, Used-for, information-seeking conversation systems)(neural text generation, Used-for, data-to-text generation)(neural text generation, Used-for, table-to-text generation)(neural text generation, Is-a-Prerequisite-of, effective data-to-text generation)(neural models, Part-of, neural text generation)(sequence-to-sequence models, Part-of, neural: text generation)
EMPTY G1.aspect sentiment
(speech translation e2e st, Compare, cascade speech translation models)(encoder pre-training, Used-for, speech translation e2e st)(Target syntax incorporation, Used-for, speech translation e2e st)(context-aware modeling, Used-for, speech translation e2e st)(multitask learning, Used-for, speech translation e2e st)(phone feature incorporation, Used-for, speech translation e2e st)(context concatenation, Used-for, speech translation e2e st)(in-model ensemble decoding, Used-for, speech here translation e2e st)
EMPTY G1.word embeddings build
(context sensitive embeddings, Evaluate-for, improving accuracy in NLP tasks)(context sensitive embeddings, Part-of, deep neural network architectures)(context sensitive embeddings, Compare, type-level word embeddings)(pre-trained context embeddings, Is-a-Prerequisite-of, context sensitive embeddings)(deep neural network architectures, Is-a-Prerequisite-of, context sensitive embeddings)(neural network architectures, Hyponym-Of, context sensitive embeddings)(context sensitive embeddings, Used-for, domain-sensitive and sentiment-aware word embeddings generation)(context sensitive embeddings, Evaluate-for, domain-common embeddings differentiation)(context specific embeddings, Hyponym-Of, neural vector representations)
EMPTY G1.nli model
EMPTY G1.improves adversarial robustness
(syntactic information, Evaluate-for, domain learning)(cross-domain setting, Hyponym-Of, domain learning)
EMPTY G1.nested named entity
EMPTY G1.approach visual question answering
(automatic argument generation, Used-for, generating arguments of a different stance)(encoder-decoder neural network, Used-for, automatic argument generation)(talking point phrases, Used-for, automatic argument, generation)(automatic argument generation, Used-for, providing topic-relevant content)(retrieval system, Used-for, automatic argument generation)(Wikipedia, Used-for, automatic argument generation)(automatic argument generation, Evaluate-for, stance determination)(automatic argument generation, Evaluate-for, claim specificity assessment)
EMPTY G1.neural machine translation model
(keyphrase extraction, Is-a-Prerequisite-of, human attention keyphrase extraction)(human attention, Used-for, human attention keyphrase extraction)(human attention, Is-a-Prerequisite-of, human attention keyphrase extraction)(human attention keyphrase extraction, Evaluate-for, effectiveness on Twitter datasets)
EMPTY G1.speech translation e2e
EMPTY G1.shot learning
(category opinion sentiment, Part-of, Aspect-Category-Opinion-Sentiment Quadruple Extraction)
(lingual word embeddings,Used-for,connecting separate monolingual embeddings)(bilingual dictionaries,Used-for,lingual word embeddings)
EMPTY G1.free text relation
(sentence embeddings, Is-a-Prerequisite-of, textual similarity sts task)(ConSERT, Evaluate-for, textual similarity sts task)(semantic textual similarity tasks, Part-of, textual similarity sts task)
EMPTY G1.speech translation s2st
EMPTY G1.semantic role labeling srl
(chain thought prompting, Compare, Zero-shot-CoT)(chain thought prompting, Hyponym-Of, NLP task enhancement methods)(few-shot chain-of-thought prompting, Part-of, chain thought prompting)
(unsupervised constituency parsing, Evaluate-for, standardized settings)(structural consistency, Used-for, unsupervised constituency parsing)(constituency parsers, Part-of, unsupervised constituency parsing)
(COREQA, Is-a-Prerequisite-of, domain question answering qa)(semantic units, Used-for, domain question answering qa)(knowledge base, Used-for, domain question answering qa)(neural network-based methods, Used-for, domain question answering qa)(derived word generation, Evaluate-for, domain question answering qa)(dynamic neural semantic parsing, Used-for, domain question answering qa)(convolutional neural network, Used-for, domain question answering qa)
EMPTY G1.cross lingual transfer
(embeddings represent word,Used-for,sentiment classification)(embeddings represent word,Used-for,sequence modeling)(embeddings represent word,Used-for,detecting temporal changes)(Contextual embeddings,Hyponym-Of,embeddings represent word)(Dynamic contextualized word embeddings,Hyponym-Of,embeddings represent word)
(summary generation, Used-for, content planning)(summary generation, Used-for, surface realization)(structured convolutional decoder, Used-for, summary generation)(automatic evaluation, Evaluate-for, summary generation)
EMPTY G1.text generation text style
EMPTY G1.entity recognition relation extraction
EMPTY G1.dialog datasets
EMPTY G1.visually grounded language
EMPTY G1.unintended bias text
EMPTY G1.continual relation extraction
EMPTY G1.neural semantic
(generated summary,Hyponym-Of,summary)(encode-attend-decode,Used-for,generated summary)(data-to-text generation model,Used-for,generated summary)(query-based summarization,Used-for,generated summary)(abstractive summarization,Used-for,generated summary)(scientific paper summarization,Used-for,generated summary)(multi-sentence compression,Used-for,generated summary)(sequence-to-sequence model,Used-for,generated summary)(diversity based attention model,Evaluate-for,generated summary)(adversarial training,Evaluate-for,generated summary)
EMPTY G1.level event argument extraction
(aspect based sentiment analysis, Is-a-Prerequisite-of, aspect extraction)(aspect based sentiment analysis, Is-a-Prerequisite-of, aspect sentiment classification)(aspect based sentiment analysis, Used-for, determine sentiment polarity towards specific aspect in online reviews)(aspect extraction, Part-of, aspect based sentiment analysis)(aspect sentiment classification, Part-of, aspect based sentiment analysis)(aspect term-polarity co-extraction, Part-of, aspect based sentiment analysis)
(neural dialogue generation, Used-for, dialog generation)
(Distant Supervision, Used-for, entity relation extraction)(entity relation extraction, Evaluate-for, model performance)(entity relation extraction, Used-for, knowledge base enrichment)(semantic information, Part-of, entity relation extraction)(BIO tag embeddings, Used-for, entity relation extraction)
(linguistically diverse conversational corpus, Used-for, natural language understanding)  (linguistically diverse conversational corpus, Used-for, design of conversational interfaces)  (linguistically diverse conversational corpus, Used-for, language technology)  (linguistically diverse conversational corpus, Hyponym-Of, conversational corpora)  (linguistically diverse conversational corpus, Is-a-Prerequisite-of, flexible language technologies)  
EMPTY G1.political debate 50 year
EMPTY G1.natural language explanation prediction
(text generation model, Used-for, Question generation)(Generative Domain-Adaptive Nets, Used-for, text generation model)(text generation model, Used-for, Generating SQL queries)(neural network, Used-for, text generation model)(Generative Domain-Adaptive Nets, Part-of, text generation model)(Abstract syntax networks, Used-for, text generation model)(neural abstractive method, Hyponym-Of, text generation model)(neural knowledge diffusion model, Hyponym-Of, text generation model)(pun generation, Used-for, text generation model)(derivational morphology, Evaluate-for, text generation model)
(response generation, Is-a-Prequisite-of, text to speech generation)(Seq2Seq, Used-for, response generation)(response generation, Part-of, conversation model)(dialog generation, Evaluate-for, response generation)(response generation, Used-for, open-domain dialog systems)(conditional response generation, Part-of, response generation)(response generation, Used-for, chatbots)(response generation, Used-for, customer service)(Seq2Seq models, Used-for, response generation)(deep latent variable models, Used-for, response generation)(encoder-decoder models, Used-for, response Response selection, Is-a-Prerequisite-of, response generation)(dialogue model, Used-for, response generation)
(trained language model, Used-for, constraint satisfaction in poetry generation)
EMPTY G1.binomial neural topic model
EMPTY G1.question answering model
(pretrained multilingual, Used-for, improving NLP task performance)(ERNIE, Hyponym-Of, pretrained multonical)(pretrained multilingual, Used-for, multilingual document classification)(pretrained multilingual, Evaluate-for, language representation)(pretrained multilingual, Used-for, zero-shot translation enhancement)(pretrained multilingual, Used-for, language identification)
(Neural Machine Translation Model, Evaluate-for, translation quality)(NMT, Evaluate-for, translation routine)(Machine Translation, Evaluate-for, translation quality)
(knowledge graph completion kgc, Is-a-Prerequisite-of, relation prediction)
EMPTY G1.event argument extraction eae
(reasoning task, Used-for, comprehending natural language)(reasoning task, Is-a-Prerequisite-of, effective human decision-making)(reasoning task, Is-a-Prerequisite-of, human intelligence)(reasoning task, Is-a-Prerequisite-of, artificial intelligence)
(natural language explanation, Used-for, solving algebraic word problems)  (answer rationales, Part-of, natural language explanation)  (arithmetic operations, Part-of, natural language explanation)  (natural language explanation, Evaluate-for, learning arithmetic programs)  (natural language explanation, Used-for, understanding question-answer systems)  
EMPTY G1.recurrent neural tensor network
EMPTY G1.machine translation paper
(crossmodal attention, Compare, multimodal attention)(crossmodal attention, Used-for, bridging semantic gap between modalities)(crossmodal attention, Is-a-Prerequisite-of, multimodal emotion regression)(crossmodal attention, Used-for, improving performance on text-image retrieval tasks)(crossmodal attention, Used-for, enhancing contextualized representation of image-text pairs)(crossmodal attention, Evaluate-for, performance on Flickr30k and MS COCO datasets)(crossmodal attention, Part-of, inter-modal alignment)(crossmodal attention, Used-for, enhancing model interpretability by calibrating intra-modal self-attentions)(crossmodal attention, Hyponym-Of, attention mechanisms)
EMPTY G1.contrastive visual semantic
(text classification task, Used-for, categorizing documents in different languages)(text classification task, Is-a-Prerequisite-of, learning representations from text)(BiLSTMs, Used-for, text classification task)(text classification task, Used-for, classification of utterances in videos)(Lexico-syntactic features, Used-for, text classification task)(Neural network models, Used-for, text classification task)(text classification task, Used-for, argument mining)
(topic-like architecture, Part-of, neural language model)(neural language to
(biomedical entity linking, Is-a-Prerequisite-of, cross-lingual biomedical entity linking task)(biomedical entity linking, Used-for, handling specialised in-domain tasks)(biomedical entity linking, Evaluate-for, advancing capabilities of language models to handle specialized tasks)(cross-lingual transfer methods, Used-for, biomedical entity linking)
EMPTY G1.document level event extraction
EMPTY G1.sentiment classification asc
(latent topic, Is-a-Prerequisite-of, matrix factorization)(latent topic, Used-for, modeling inter-topic preferences)(latent topic, Hyponym-Of, user-topic matrix)(latent topic, Hyponym-Of, Document structure)(latent topic, Part-of, LDA)(latent topic, Hyponym-Of, Gaussian mixtures)(latent topic, Part-of, neural topic model)(latent feature space, Hyponym-Of, latent topic)(latent topic, Hyponym-Of, latent variable)(latent topic, Hyponym-Of, disentangled syntactic and semantic spaces)(latent topic, Hyponym-Of, latent lexical meaning)(latent topic, Used-for, coherence between topics)
(aware newsuld outperforms vanilla encode-attend-decode models used-for, improving dialogue state tracking)
EMPTY G1.generate sentence
(compositional distributional semantics, Hyponym-Of, distributional semantics)(compositional distributional semantics, Is-a-Prerequisite-of, evaluating semantic relatedness)(compositional distributional semantics, Is-a-Prerequisite-of, evaluating entailment)(compositional distributional semantics, Used-for, analyzing multiword expressions)(semantic composition, Used-for, compositional distributional semantics)(compositional distributional semantics, Evaluate-for, negation function in fMRI studies)(Functional Distributional Semantics, Hyponym-Of, compositional distributional semantics)
(named entity recognition, Used-for, named entity)(named entity, Used-for, specific entity identification)(named by-entity, Part-of, multimodal named entity disambiguation task)(cross-lingual named entity recognition, Used-for, named entity)(named entity, Part-of, knowledge graph embeddings)(entity linking, Evaluate-for, named entity)
(task oriented dialogue, Used-for, automatic diagnosis)(task oriented dialogue, Used-for, multimodal dialogue systems)(task oriented dialogue, Is-a-Prerequisite-of, robust dialogue belief tracking)(task oriented dialogue, Is-a-Prerequisite-of, knowledge base integration)(task oriented dialogue, Part-of, Multi-turn dialogue agent system)(task oriented dialogue, Part-of, dialogue systems)(multimodal dialogue systems, Is-a-Prerequisite-of, task oriented dialogue)(automatic diagnosis, Used-for, task oriented dialogue)
(spoken dialogue system, Part-of, task-oriented dialogue system)(spoken dialogue system, Evaluate-for, user adaptivity)(spoken dialogue system, Part-of, multimodal dialogue system)(spoken dialogue system, Used-for, dialogue state tracking)
(sentence extraction, Used-for, document modeling)(document modeling, Used-for, natural language understanding tasks)(Sentence extraction, Part-of, document modeling)(document modeling, Used-for, event detection)(document modeling, Used-for, document retrieval)(document modeling, Used-for, extractive document summarization)
EMPTY G1.multi party dialogue
EMPTY G1.domain sentiment
(oriented dialogue summarization, Is-a-Prerequisite-of, natural language generation)
EMPTY G1.recurrent neural
EMPTY G1.bias text
EMPTY G1.level language modeling
EMPTY G1.multilingual representation
(textual adversarial, Used-for, improving model robustness)(textual adversarial, Used-for, generating adversarial examples)(adversarial examples, Hyponym-Of, textual adversarial)
(text classification, Used-for, graph-based nlp)(convolutional neural network, Used-for, text classification)(classification, Is-a-Prerequisite-of, text classification)(text classification, Used-for, classifying documents)(text classification, Evaluate-for, capsule network)(adversarial attack, Used-for, text classification)(text classification, Hyponym-Of, NLP tasks)(multi-task learning, Used-for, text classification)(text classification, Used-for, sentiment analysis)(text classification, Used-for, sarcasm detection)(sentiment analysis, Is-a-Prerequisite-of, text classification)(sarcasm detection, Is-a-Prerequisite-of, text classification)(BiLSTMs, Used-for, text classification)(attention-based methods, Used-for, text classification)(automatic speech recognition, Used-for, text classification)(LSTM-based model, Used-for, text classification)(feature extraction, Used-for, text classification)(topic distribution, Used-for, text training)(neural text classifier, Used-for, text classification)(MixText, Used-for, text classification)(sentiment classification, Compare, text classification)(text classification, Is-a-Prerequisite-of, understanding word embeddings)(structured attention, Used-for, text classification)(text classification, Evaluate-for, effectiveness of pretrained models)(perplexity language model, Evaluate-for, text classification)(neural language model, Used-for, text classification)
(Bayesian Network Ensembles, Part-of, neural network interpretability bayesian)(neural network interpretability bayesian, Evaluate-for, AI-empowered healthcare)
EMPTY G1.end end relation extraction
(long short term memory,Used-for,predicting sequences in unannotated text)(long short term memory,Evaluate-for,language model prediction)(long short term memory,Used-for,extracting semantic relations between entity mentions)(long short term memory,Used-for,disfluency detection in spontaneous speech transcripts)(long short term memory,Used-for,scoring n-best candidate disfluency analyses)(long short term memory,Hyponym-Of,neural approaches)(semantic relation extraction,Is-a-Prerequisite-of,long short term memory)(long short term memory,Used-for,joint extraction of entity mentions and relations)(long short term memory,Used-for,aspect-level sentiment classification)
(Commonsense knowledge bases, Used-for, event commonsense evaluation)(ACCENT, Used-for, event commonsense evaluation)(event commonsense evaluation, Evaluate-for, compatibility with commonsense knowledge bases)
(text generation task, Used-for, natural language descriptions into source code)(text generation task, Used-for, generation of complex programs from natural language descriptions)(text generation task, Used-for, neural abstractive document summarization)(text generation task, Used-for, automatic question generation)(text generation task, Compare, semantic parsing)(text generation task, Is-a-Prerequisite-of, sequential models in natural language generation)(text generation task, Conjunction, neural models)
EMPTY G1.cross lingual continual learning
(Sememes, Used-for, lexical sememe prediction)(lexical sememe prediction, Used-for, improving annotation efficiency)(lexical sememe prediction, Used-for, improving annotation consistency)(lexical sememe prediction, Evaluate-for, low-frequency words)(lexical sememe prediction, Evaluate-for, out-of-vocabulary words)(HowNet, Used-for, lexical sememe prediction)
(sentence relation extraction, Used-for, identifying relation class)(sentence relation capture, Is-a-Prerequisite-of, DocRED)(sentence relation recognition, Evaluate-for, document-level RE)(sentence relation extraction, Used-for, mapping entities to knowledge bases)
EMPTY G1.speech translation model
(pretrained language, Used-for, machine reading comprehension)(pretrained language, Used-for, neural language translation)(pretrained language, Is-a-Prerequisite-of, natural language interface)
(adversarially trained, Used-for, robustness of NMT models)(adversarially trained, Used-for, robustness of self-attentive models)(adversarially trained, Used-for, neural image captioning systems)(adversarially trained, Used-for, generalizing classifiers in stance classification)(adversarially trained, Is-a-Prerequisite-of, learning cross-lingual word embeddings)(adversarially trained, Used-for, improving translation quality)(adversarially trained, Used-for, evaluation of style transfer)(adversarially trained, Used-for, extractive reading comprehension systems)
(neural question answering, Used-for, factoid question answering)(EviNets, Used-for, factoid question answering)(factoid question answering, Used-for, leveraging structured knowledge bases)(factoid question answering, Used-for, evaluating open-domain questions using Wikipedia)
(entity linking, Evaluate-for, named entity disambiguation)
(topic distribution, Hyponym-Of, document specific topic distributions)(topic distribution, Used-for, topic assignments in documents)(topic distribution, Is-a-Prerequisite-of, Normalized Pointwise Mutual Information calculation)(topic distribution, Used-for, text classification)
(pretraining language, Used-for, neural word segmentation)(pretraining language, Evaluate-for, learning feature representations)(pretraining language, Part-of, modular segmentation model)(pretraining language, Used-for, building large-scale raw text corpora)
(mention detection, Hyponym-Of, entity recognition)(entity recognition, Used-for, zero-shot learning)(lexical resources, Used-for, entity recognition)(entity recognition, Hyponym-Of, natural language processing)(entity recognition, Part-of, automatic content extraction)(automatic content extraction, Evaluate-for, entity recognition)(entity recognition, Used-for, multilingual learning)(entity recognition, Used-for, domain adaptation for named-entity recognition)(entity recognition, Used-for, emotion recognition in conversations)(entity recognition, Part-of, discourse relation recognition)(entity recognition, Used-for, identifying important elements from text)
(Neural Machine Translation, Used-for, translation task)(Neural Machine Translation models, Evaluate-for, translation task)(Multi-modal Neural Machine Translation, Used-for, translation task)(Sequence-to-Dependency Neural Machine Translation, Used-for, translation task)(Parallel RNN encoder, Used-for, translation task)
EMPTY G1.automatic fake news
(aspect extraction, Used-for, aspect-based sentiment analysis)(neural word embeddings, Used-for, aspect extraction)(aspect extraction, Is-a-Prerequisite-of, aspect sentiment classification)(Neural word embeddings, Evaluate-for, aspect extraction)(aspect extraction, Used-for, ABSA)(topic models, Used-for, aspect extraction)(aspect based sentiment analysis, Is-a-Prerequisite-of, aspect extraction)(aspect extraction, Part-of, aspect based sentiment analysis)(aspect extraction, Used-for, identifying specific aspects or entities in the text)(aspect extraction, Conjunction, aspect sentiment classification)
EMPTY G1.generated text
(neural language model lm, Used-for, learning implicit representation of poetic form and content)(neural language language lm, Evaluate-for, language model perplexity improvement)(neural language model lm, Used-for, generation of conversational text conditioned on affect categories)(neural language model lm, Is-a-Prerequisite-of, affective language generation)(neural language model lm, Hyponym-Of, language models)(neural language model lm, Part-of, language processing technologies)(neural language model lm, Used-for, generating related sentences for a topic)
(lingual embeddings, Used-for, bilingual lexicon induction)(lingual embeddings, Used-for, cross-lingual classification)(bilingual text embeddings, Hyponym-Of, lingual embeddings)(lingual embeddings, Compare, monolingual embeddings)(lingual embeddings, Compare, cross-lingual word embeddings)(lingual embeddings, Part-of, bilingual tasks)(structural similarity, Evaluate-for, lingual embeddings)(lingual embeddings, Used-for, cross-lingual image description retrieval)(bilingual dictionaries, Used-for, lingual embeddings)(Unsupervised bilingual lexicon induction, Evaluate-for, lingual embeddings)(domain adaptation, Evaluate-for, lingual embeddings)(neural word embeddings, Used-for, lingual embeddings)
EMPTY G1.multilingual neural machine
EMPTY G1.dialogue summarization
EMPTY G1.pretrained multilingual model
EMPTY G1.entity recognition multiple
(neural text classifier, Used-for, text classification)(neural text primarily, Is-a-Prerequisite-of, feature attribution)(neural text classifier, Is-a-Prerequisite-of, character-level manipulation)(neural text classifier, Used-for, weighting input components)(tokenization, Used-for, neural text classifier)(visual character embedding, Used-for, neural text classifier)(cross-lingual text classification, Used-for, neural text classifier)(attention mechanisms, Used-for, neural text classifier)(model distillation, Compare, neural text classifier)
EMPTY G1.coherent paragraph summary
(image text, Hyponym-Of, multimodal data)(image text, Used-for, visual grounding in speech perception)(image text, Used-for, joint semantic space projection)
EMPTY G1.multi modal datasets
(topic discovery, Used-for, classifying document corpora)(topic discovery, Part-of, natural language processing tasks)(topic discovery, Evaluate-for, discovery of spatially distinct topics)(topic discovery, Evaluate-for, discovery of temporal topics)(spatial aggregation, Used-for, topic discovery)(matrix factorization, Used-for, topic discovery)(topic discovery, Used-for, learning text representations)(NB-NTM, Used-for, topic discovery)(GNB-NTM, Used-for, topic discovery)(topic models, Used-for, topic discovery)(topic discovery, Used-for, insight into document corpora)(topic discovery, Is-a-Prerequisite-of, topic guided keyphrase generation)(topic discovery, Evaluate-for, understanding document content)
(neural model, Used-for, morphological inflection generation)(neural model, Used-for, semantic parsing over structured data)(neural model, Used-for, generated sentence)(neural model, Used-for, goal achievement in dialogue)(neural model, Used-for, Chinese poem generation)(neural model, Hyponym-Of, nlg model)(neural model, Used-for, task oriented dialogue system)(neural model, Used-for, semantic parsing)(neural model, Used-for, generation of rhythmic poetry)(neural model, Used-for, language modeling)(neural model, Evaluate-for, handling discourse-level diversity in conversations)(neural model, Used-for, automatically generating market comments from stock prices)(neural model, Evaluate-for, learning to predict surrounding words for every word in the dataset)(neural model, Is-a-Prerequisite-of, multi-task learning framework)(neural model, Part-of, encoder-decoder framework)(neural model, Hyponym-Of, machine learning models)(neural model, Used-for, knowledge graph embedding)
EMPTY G1.deep syntactic
EMPTY G1.word embeddings represent
(aspect implicit opinion, Used-for, Aspect-Category-Opinion-Sentiment Quadruple Extraction)
EMPTY G1.dialogue pre training
EMPTY G1.nlp task especially low
EMPTY G1.language model like bert
(natural language processing, Used-for, automatic fake news detection)
EMPTY G1.named entity recognition ner
EMPTY G1.parsing idea treebank embedding
(fact checking model, Used-for, verifying the truthfulness of claims)(fact checking model, Is-a-Prerequisite-of, manual evaluation)(fact checking model, Used-for, retrieving authoritative evidence)(fact checking model, Evaluate-for, claim veracity)(fact checking model, Used-for, detecting misinformation and disinformation)(manual fact checking, Part-of, fact checking model)(automatic fact checking, Part-of, fact checking model)(claim veracity, Evaluate-for, fact checking model)
(encoder-decoder framework, Used-for, generating summary)(multi-modal hierarchical attention, Used-for, generating summary)(COREQA, Used-for, generating summary)(data-to-text generation model, Used-for, generating especially)(Gaussian focal bias, Used-for, generating summary)(saliency-selection network, Used-for, generating summary)(generating summary, Is-a-Prerequisite-of, salient information selection)(generating summary, Used-for, multi-document summarization)(generating summary, Used-for, single document summarization)
(diverese conversational corpus,Used-for,language technology)(linguistically diverse conversational corpus,Used-for,language technology)(language technology,Used-for,promoting multilingualism)(language technology,Used-for,linguistic diversity)(speech translation,Part-of,language actually)(language technology,Used-for,machine translation)(language technology,Evaluate-for,NLP systems quality)(NLP systems,Hyponym-Of,language technology)(language technology,Is-a-Prerequisite-of,computational linguistics)
(seq2seq text generation, Used-for, language entailment generation)(seq2seq text generation, Hyponym-Of, text generation)(video captioning, Is-a-Prerequisite-of, seq2seq text generation)(machine translation, Used-for, seq2seq text generation)(formality style transfer, Used-for, seq2seq text generation)(sentence compression, Used-for, seq2seq text generation)(sentence simplification, Used-for, seq2seq text generation)(NLP, Used-for, seq2seq text generation)(automatic grammatical error correction, Evaluate-for, seq2seq text generation)
EMPTY G1.vision language model
(neural network rnns, Part-of, Natural Language Processing)(neural network rnns, Used-for, dialog systems)(Neural Symbolic Machine, Part-of, neural network rnns)(neural network rnns, Used-for, sequence transduction)
EMPTY G1.cross domain sentiment
(identifiability attention, Part-of, Attention mechanism)(identifiability attention, Evaluate-for, trustworthiness of a modelâ€™s predictions)(identifiability attention, Is-a-Prerequisite-of, unique Attention weights)
(dialogue model, Used-for, building dialog systems)(dialogue model, Used-for, response generation)(dialogue model, Is-a-Prerequisite-of, encoder-decoder architecture)(dialogue model, Is-a-Prerequisite-of, belief tracker)(dialogue model, Hyponym-Of, task-oriented dialogue systems)(dialogue model, Hyponym-Of, open-domain dialogue systems)
EMPTY G1.language model plms
(embedding model, Used-for, knowledge base completion)(embedding model, Used-for, multimodal word distributions)(embedding model, Used-for, self-learning approach for bilingual embeddings)(embedding model, Used-for, document clustering)(embedding model, Evaluate-for, performance on NER and chunking tasks)(embedding model, Evaluate-for, semantic relationship modeling in networks)(ITransF, Hyponym-Of, embedding model)
(Chinese Spelling Correction, Is-a-Prerequisite-of, language understanding)(neural network models, Used-for, language understanding)(Syntax, Used-for, language understanding)(pre trained language model, Evaluate-for, language understanding)(Vision-and-Language Navigation, Is-a-Prerequisite-of, language understanding)(language understanding, Is-a-Prerequisite-of, natural language utterances mapping into executable programs)(language understanding, Is-a-Prerequisite-of, development of natural-language understanding systems)(statistical power of neural networks, Used-for, language understanding)(Neural Symbolic Machine, Used-for, language understanding)(natural language, Hyponym-Of, language understanding)(semantic parsing, Used-for, language understanding)(transformer language model, Is-a-Prerequisite-of, language understanding)
(neural machine translation,Used-for,paraphrase generation)(paraphrase generation,Used-for,downstream tasks)(crowdsourcing,Used-for,paraphrase generation)(paraphrase generation,Used-for,training robust machine aearning systems)(paraphrase generation,Used-for,improving downstream natural language understanding tasks)(ParaNMT-50M,Used-for,paraphrase generation)(unsupervised domain adaptation,Evaluate-for,paraphrase generation)
EMPTY G1.unsupervised bilingual word
EMPTY G1.aspect based sentiment
(bias nlp,Used-for,mitigating effects of bias in machine translation)(bias nlp,Used-for,improving fairness in automatic hate speech detection)(bias nlp,Evaluate-for,predictive bias framework assessment)(predictive bias framework,Is-a-Prerequisite-of,bias nlp)(bias nlp,Hyponym-Of,natural language processing)
EMPTY G1.classification task
(multilingual model, Is-a-Prerequisite-of, parsing evaluation)(Machine translation, Used-for, multilingual model)(multilingual model, Used-for, language identification on diverse linguistic data)(embedding learning by concept induction, Part-of, multilingual model)(multilingual connotation frames, Part-of, multilingual model)(multi-task learning framework, Is-a-Prerequisite-of, multilingual model)(semantic parsing, Evaluate-for, multilingual model)(language identification, Evaluate-for, multilingual model)(multilingual model, Hyponym-Of, machine learning models)(public sentiment analysis, Used-for, multilingual model)(multilingual neural machine translation, Hyponym-Of, multilingual model)(learning multilingual distributed representations of text, Used-for, multilingual model)(multilingual model, Used-for, semantic parsing)
(generation semantic parsing, Is-a-Prerequisite-of, mapping natural language utterances into structured meaning representations)(abstract syntax trees, Used-for, generation semantic parsing)(neural network models, Used-for, generation semantic parsing)(parse, Used-for, generation semantic parsing)(code generation, Conjunction, generation semantic parsing)(natural language utterances, Evaluate-for, generation semantic parsing)(executable programs, Evaluate-for, generation semantic parsing)
EMPTY G1.lingual openqa
(chinese word segmentation cws, Part-of, natural language processing)(chinese word segmentation cws, Is-a-Prerequisite-of, chinese NLP tasks)(statistical segmentation, Used-for, chinese word segmentation cws)(neural models, Used-for, chinese word segmentation cws)(cross-domain CWS, Used-for, chinese word segmentation cws)
EMPTY G1.unsupervised parsing
(semantic representation, Is-a-Prerequisite-of, linguistic representation)(Long-tail phenomena, Evaluate-for, linguistic representation)(semantic parsing, Part-of, linguistic representation)(Word representation, Part-of, linguistic representation)
EMPTY G1.argument extraction eae
EMPTY G1.orthography morphological feature lexicalized
(unsupervised semantic parsing, Is-a-Prerequisite-of, learning from unlabeled data)(unsupervised semantic parsing, Used-for, semantic frame induction)(semantic frame induction, Used-for, unsupervised semantic parsing)(unsupervised semantic parsing, Hyponym-Of, semantic parsing)(unsupervised semantic parsing, Evaluate-for, parsing natural language into formal meaning representations)(unsupervised semantic parsing, Compare, supervised semantic parsing)(unsupervised semantic parsing, Used-for, dependency tree inference)(dependency tree, Part-of, unsupervised semantic parsing)(unsupervised semantic parsing, Used-for, Semantic Textual Similarity tasks)(unsupervised semantic parsing, Used-for, generating formal meaning representations without labeled data)(unsupervised semantic parsing, Part-of, natural language processing)(unsupervised semantic parsing, Evaluate-for, limited labeled data scenarios)(unsupervised syntactic parsing, Compare, unsupervised semantic parsing)
EMPTY G1.free text rationale
EMPTY G1.video question answering
(language model trained, Used-for, generation of rhythmic poetry)(language model trained, Used-for, learning a representation of content)(phonetic encoding, Part-of, language model trained)(neural language model, Hyponym-Of, language model trained)(language model trained, Used-for, generation of conversational text conditioned on affect categories)(language model trained, Used-for, creating artificial code-mixed language data)
EMPTY G1.nlp task especially
(contextualized word embeddings, Evaluate-for, classifying topic-dependent arguments)(contextualized word embeddings, Evaluate-for, clustering topic-dependent arguments)(BERT, Hyponym-Of, contextualized word embeddings)(ELMo, Hyponym-Of, contextualized word embeddings)(contextualized word embeddings, Compare, non-contextual subword embeddings)(contextualized word embeddings, Used-for, multilingual named entity recognition)(contextualized word embeddings, Used-for, part-of-speech tagging)
(multi-lingual neural relation extraction framework, Used-for, lingual transfer)(cross-lingual attention, Used-for, lingual transfer)(cross-lingual syntactic variation, Is-a-Prerequisite-of, lingual transfer)
(deep pyramid CNN, Used-for, sentiment classification)(sentiment classification, Used-for, evaluating polarity in text)(transfer learning, Used-for, sentiment classification)(neural network architectures, Used-for, sentiment classification)(embeddings represent word, Used-for, sentiment classification)(sentiment classification, Is-a-Prerequisite-of, polarity detection)(sentiment classification, Is-a-Prerequisite-of, sarcasm detection)(sentiment classification, Used-for, domain adaptation)(sentiment classification, Used-for, active sentiment domain adaptation)(sentiment classification, Used-for, discourse classification)(sentiment classification, Used-for, polarity detection)(sentiment classification, Compare, text classification)(sentiment classification, Compare, sarcasm detection)(Domain adaptation, Used-for, sentiment classification)(active learning, Used-for, sentiment classification)(sentiment words, Used-for, sentiment classification)(cognitive features, Used-for, sentiment classification)(gaze data, Used-for, sentiment classification)(domain-specific sentiment similarities, Used-for, sentiment classification)(sentiment lexicons, Used-for, sentiment classification)(word embeddings, Evaluate-for, sentiment classification)(Opinion entity extraction, Used-for, sentiment classification)(language model fine tuning, Used-for, sentiment classification)(sentiment classification, Evaluate-for, Multi-sentiment-resource Enhanced Attention Network)(sentiment classification, Hyponym-Of, sentiment analysis absa)
(None, Used-for, chinese spelling correction)(chinese spelling correction, Used-for, detect and correct spelling errors in Chinese natural language)(SpellGCN, Used-for, chinese spelling correction)(BERT, Evaluate-for, chinese spelling correction)(PLOME, Used-for, chinese spelling correction)(PHMOSpell, Used-for, chinese spelling correction)(UMRSpell, Used-for, chinese spelling correction)(Soft-Masked BERT, Used-for, chinese spelling correction)(phonological and visual similarity knowledge, Used-for, chinese spelling correction)(character prediction, Part-of, chinese spelling correction)(confusion set, Used-for, chinese spelling correction)(phonetic information, Used-for, chinese spelling correction)(error detection, Part-of, chinese spelling correction)(error correction, Part-of, chinese spelling correction)
EMPTY G1.task word segmentation
EMPTY G1.language vision
(generative language model, Used-for, learning representations of content)(generative language model, Used-for, constraining based on form)(weighted finite state machine, Part-of, generative language model)(neural language model, Part-of, generative language model)
(lingual embeddings, Compare, monolingual embeddings)
EMPTY G1.embeddings domain specific embeddings
EMPTY G1.stanford question answering
EMPTY G1.domain question answering
EMPTY G1.learning morphological
(generalization semantic parsing, Compare, compositional generalization semantic parsing)(generalization semantic parsing, Used-for, mapping user utterances to executable programs)(Hierarchical Semantic Parsing, Is-a-Prerequisite-of, generalization semantic parsing)(semantic parsing, Part-of, generalization semantic parsing)
EMPTY G1.improve summarization system
EMPTY G1.cross lingual dependency parsing
EMPTY G1.outperform strong baseline dialogue
EMPTY G1.improves adversarial
(textual adversarial attack, Evaluate-for, reducing classification accuracy)(textual adversarial attack, Used-for, generating adversarial examples)(textual adversarial attack, Used-for, vulnerability testing)(adversarial examples, Part-of, textual adversarial attack)(probability weighted word saliency, Used-for, textual adversarial attack)(textual adversarial attack, Used-for, evaluating NLP system reliability)(textual adversarial attack, Used-for, exposing pitfalls in NMT)(adversarial attack, Hyponym-Of, textual adversarial attack)
(entity embeddings, Used-for, neural entity linking models)(entity embeddings, Used-for, representing semantic destruction)(entity embeddings, Hyponym-Of, word embeddings)(entity embeddings, Used-for, link prediction)(entity linking, Evaluate-for, entity embeddings)
(dialogue system, Is-a-Prerequisite-of, task oriented dialogue system)(dialogue system, Used-for, automatic diagnosis)(Hybrid Code Networks, Used-for, dialogue system)(dialogue system, Part-of, spoken dialogue systems)(dialogue system, Part-of, task-oriented dialogue systems)(encoder-decoder dialog model, Used-for, dialogue system)(dialogue system, Used-for, chat detection)(Neural Belief Tracking, Used-for, dialogue system)(task-oriented dialogue systems, Is-a-Prerequisite-of, dialogue system)(open-domain non-task-oriented dialogue systems, Hyponym-Of, dialogue system)(bAbI dialog dataset, Evaluate-for, dialogue system)(dialogue states, Part-of, dialogue system)(Global-Locally Self-Attentive Dialogue State Tracker, Used-for, dialogue system)(Mem2Seq, Used-for, dialogue system)(chit-chat models, Used-for, dialogue system)
(multilingual masked language modeling, Used-for, cross-lingual transfer tasks)  (multilingual masked language modeling, Hyponym-Of, language modeling)  (BERT, Is-a-Prerequisite-of, multilingual masked language modeling)  (language-agnostic information, Used-for, multilingual masked language modeling)  (shared parameters, Evaluate-for, multilingual masked language modeling)  (XLM-R, Is-a-Prerequisite-of, multilingual masked language modeling)  (multilingual masked language modeling, Used-for, zero-shot training)  
EMPTY G1.shot text classification
(interactive semantic parsing, Is-a-Prerequisite-of, natural language feedback correction)  (interactive semantic parsing, Hyponym-Of, semantic parsing)  (interactive semantic parsing, Used-for, semantic parse correction with natural language feedback)  (interactive semantic parsing, Used-for, improving semantic parsing models)  
(machine reading comprehension, Part-of, natural language understanding)(logical reasoning, Is-a-Prerequisite-of, machine reading comprehension)(pretrained language, Used-for, machine reading comprehension)(machine reading comprehension, Used-for, knowledge extraction)(machine reading comprehension, Used-for, answer prediction)(gated self-matching networks, Used-for, machine reading comprehension)(question-aware passage representation, Part-of, machine reading comprehension)(machine reading comprehension, Used-for, natural question dataset evaluation)
(language model downstream, Is-a-Prerequisite-of, crypable ential performance)(language model downstream, Is-a-Prerequisite-of, generalization performance)
(discourse structure, Used-for, text categorization)  (discourse structure, Hyponym-Of, Rhetorical Structure Theory)  
EMPTY G1.natural language generation nlg
(logical reasoning,Is-a-Prerequisite-of,conversational machine reading)(conversational machine reading,Used-for,asking clarification questions)(conversational machine reading,Evaluate-for,determining user qualification for government benefits)(decision rules,Part-of,conversational machine reading)(E3,Is-a-Prerequisite-of,conversational machine reading)(conversational history,Used-for,conversational machine reading)(procedural text,Used-for,conversational machine reading)
EMPTY G1.news representation
EMPTY G1.coherent summary
(semantic parsing, Part-of, task natural language generation)(ONLG, Part-dark, task natural language generation)(generating natural language descriptions of chess games, Part-of, task natural language generation)
(contrastive visual semantic pretraining, Used-for, mitigating anisotropy in word embeddings)(GPT-2, Part-of, contrastive visual semantic pretraining)(CLIP, Part-of, contrastive visual semantic pretraining)(contrastive visual semantic pretraining, Evaluate-for, semantic properties of contextualized language representations)(CLIP, Is-a-Prerequisite-of, contrastive visual semantic pretraining)(contrastive visual semantic pretraining, Used-for, mitigating anisotropy in contextualized word embeddings)(contrastive visual semantic pretraining, Evaluate-for, zero-shot multimodal image classification)
EMPTY G1.orthography morphological feature
(generate coherent informative comment, Used-for, user engagement on online news platforms)(graph-to-sequence model, Used-for, generate coherent informative comment)
EMPTY G1.hate detection
EMPTY G1.word embeddings according
(text summarization, Is-a-Prerequisite-of, extractive summarization)(extractive summarization, Used-for, selecting and rearranging passages)(abstractive summarization, Compare, extractive summarization)(extractive summarization, Hyponym-Of, summarization evaluation)(extractive summarization, Is-a-Prerequisite-of, summarization task)(extractive summarization, Part-of, single document summarization)(extractive summarization, Hyponym-Of, summarization system)(extractive summarization, Part-of, summarization system)(SWAP-NET, Is-a-Prerequisite-of, extractive summarization)(extractive summarization, Used-for, selecting key sentences)
(distributional semantics model,Is-a-Prerequisite-of,language understanding systems)(distributional semantics model,Used-for,evaluation of compositional semantics)(Morphologically rich languages,Evaluate-for,distributional semantics model)(distributional semantics model,Used-for,tackling long-tail phenomena)(distributional semantics model,Evaluate-for,semantic quality of word vector collection)(distributional semantics model,Used-for,dialogue state tracking)(distributional semantics model,Used-for,decoding brain activity patterns)
(sarcasm detection, Is-a-Prerequisite-of, satire detection)(automatic classification, Used-for, satire detection)
EMPTY G1.trained language
EMPTY G1.multilingual pre
EMPTY G1.text generation text
(extractive summarization model, Is-a-Prerequisite-of, sentence selection)(extractive summarization model, Conjunction, abstractive summarization model)(sentence scoring, Part-of, extractive summarization model)(sentence selection, Part-of, extractive summarization model)(extractive summarization model, Used-for, generating summaries from key sentences)(extractive summarization model, Evaluate-for, ROUGE scores)(SWAP-NET, Hyponym-Of, extractive summarization model)(extractive summarization model, Used-for, summarizing multi-document news articles)
(treebank embeddings, Compare, concatenation strategies)(treebank embeddings, Evaluate-for, flexibility and extensibility)(concatenation strategies, Is-a-Prerequisite-of, treebank embeddings)
(Mono-lingual data, Used-for, multilingual neural machine translation)(Multi-lingual texts, Used-for, multilingual neural machine translation)(None, Is-a-Prerequisite-of, multilingual neural machine translation)(multilingual neural machine translation, Hyponym-Of, multilingual model)
(commonsense knowledge base, Used-for, storing loosely structured open-text descriptions of knowledge)(commons and sense knowledge base, Is-a-Prerequisite-of, automatic commonsense KB completion)(ATOMIC, Hyponym-Of, commonsense knowledge base)(ConceptNet, Hyponym-Of, commonsense knowledge base)
(adversarial sample, Is-a-Prerequisite-of, adversarial attack)(adversarial attack, Used-for, text classification)(adversarial attack, Used-for, question deduplication)(adversarial attack, Used-for, textual entailment)(adversarial attack, Evaluate-for, lexical correctness maintenance)(adversarial attack, Hyponym-Of, textual adversarial attack)(adversarial attack, Used-for, generating adversarial examples)(adversarial attack, Is-a-Prerequisite-of, adversarial training)(adversarial attack, Is-a-Prerequisite-of, creating robust neural networks)
(stereotypical human biases, Is-a-Prerequisite-of, bias category)(gender bias, Hyponym-Of, bias category)(ageism, Hyponym-Of, bias category)(appearance biases, Hyponym-Of, bias category)
EMPTY G1.word embeddings represent word
(text summarization, Is-a-Prerequisite-of, abstractive summarization)(abstractive summarization, Used-for, generating shorter versions of documents)(abstractive summarization, Compare, extractive summarization)(abstractive summarization, Hyponym-Of, summarization evaluation)(ROUGE, Used-for, abstractive summarization)(neural models, Used-for, abstractive summarization)(abstractive summarization, Is-a-Prerequisite-of, summarization task)(query-based summarization, Compare, abstractive summarization)(abstractive summarization, Part-of, single document summarization)(abstractive summarization, Hyponym-Of, summarization system)(abstractive summarization, Part-of, summarization system)(abstractive summarization, Hyponym-Of, document summarization)(text generation, Compare, abstractive summarization)(abstractive summarization, Used-for, text generation)(abstractive summarization, Used-for, generated summary)(abstractive summarization, Is-a-Prerequisite-of, neural sequence-to-sequence models)(abstractive summarization, Used-for, generating concise summaries)
EMPTY G1.topic aware
(language model, Used-for, advising single-step actions)(language model, Used-for, sentence level prediction)(language model, Used-for, context incorporation)(language model, Evaluate-for, diachronic accuracy)(language model, Used-for, natural language generation)(language model, Used-for, query auto-completion)(language model, Used-for, text generation)(language model, Used-for, sentence level processing)(language model, Used-for, document context incorporation)(language language model, Part-of, Neural Symbolic Machine)(language model, Part-of, COREQA)(language model, Used-for, generation of conversational text)(Affect-LM, Hyponym-Of, language model)(language model, Is-a-Prerequisite-of, language model like)
EMPTY G1.vision language navigation
(fact-checking datasets, Used-for, training factual error correction systems)(fact-checking datasets, Used-for, developing automated fact-checking models)(fact-checking datasets, Used-for, evaluate multilingual fact-checking models)(factual error correction, Part-of, fact-checking datasets)(veracity assessment, Evaluate-for, fact-checking datasets)(FACTIFY-5WQA, Part-of, fact-checking datasets)(DialFact, Part-of, fact-checking datasets)(FAVIQ, Part-of, fact-checking datasets)
(grammatical error correction gec, Used-for, correcting both global errors in word order and usage and local errors in spelling and inflection)(grammatical error correction gec, Used-for, facilitating language learning)(neural machine translation, Is-a-Prerequisite-of, grammatical error correction gec)
(sentiment analysis absa, Used-for, predicting sentiment polarities)(aspect classification, Is-a-Prerequisite-of, sentiment analysis absa)(neural networks, Used-for, sentiment analysis absa)(text processing, Used-for, sentiment analysis absa)(sentiment classification, Hyponym-Of, sentiment analysis absa)(aspect term extraction, Evaluate-for, sentiment analysis absa)(sentiment expression, Used-for, sentiment analysis absa)(machine learning models, Used-for, sentiment analysis absa)(domain-specific embeddings, Used-for, sentiment analysis absa)(multimodal aspect identification, Is-a-Prerequisite-of, sentiment analysis absa)
(Bidirectional Adversarial Topic model, Used-for, neural topic modeling)
EMPTY G1.relation extraction method
(gated self-matching networks, Used-for, task question answering)  (dynamic neural semantic parsing, Used-for, task question answering)  (universal schema, Used-for, task question answering)  (TriviaQA, Used-for, task question answering)  (EviNets, Used-for, task question answering)  (hierarchical attention network, Used-for, task question answering)  
EMPTY G1.deep learning based chinese
EMPTY G1.neural word
EMPTY G1.extractive qa
EMPTY G1.chinese named entity recognition
(annotated data, Used-for, natural language inference)
(CommonsenseQA dataset, Evaluate-for, explanation prediction)(explanation prediction, Used-for, refuting incorrect answers)(positive and negative properties, Part-of, explanation prediction)(ECQA dataset, Evaluate-for, explanation prediction)(semantic-based evaluation metric, Evaluate-for, explanation prediction)
EMPTY G1.detecting correcting
EMPTY G1.named entity recognition relation
(dataset bias, Is-a-Prerequisite-of, dataset difficulty)(dataset bias, Evaluate-for, annotation artifacts)(dataset bias, Evaluate-for, partial-input baselines)(dataset bias, Evaluate-for, generalization and transfer in RC datasets)(dataset bias, Used-for, improving dataset creation)
(semantic parsing, Compare, syntactic parsing)(reasoning ability, Evaluate-for, syntactic parsing)(syntactic parsing, Is-a-Prerequisite-of, reasoning ability)(A* CCG parsing model, Part-of, syntactic parsing)(Neural semantic parsing approach, Used-for, syntactic parsing)(Grammar model, Used-for, syntactic parsing)(Morpheme segmentation, Part-of, syntactic parsing)(Morpho-syntactic regularities, Used-for, syntactic parsing)(Bidirectional tree encoder, Used-for, syntactic parsing)(Statistical discourse segmenters, Used-for, syntactic parsing)(Constituency parsing scheme, Part-of, syntactic parsing)(Structure-aware neural architecture, Used-for, syntactic parsing)(Sequential encoder-decoder framework, Is-a-Prerequisite-of, syntactic parsing)
(word embeddings trained, Used-for, discovering coherent aspects)(neural word embeddings, Part-of, word embeddings trained)(word embeddings trained, Used-for, creating synsets)(word embeddings trained, Used-for, translation model training)(word embeddings trained, Used-for, text-based user geolocation)(word embeddings trained, Used-for, discourse-specific embedding learning)(word embeddings trained, Used-for, sentiment analysis for volatility prediction)(word embeddings trained, Used-for, improving text clustering methods)(word embeddings trained, Used-for, MCI detection in neuropsychological assessments)(distribution of word co-occurrences, Evaluate-for, word embeddings trained)
(dialogue learning, Is-a-Prerequisite-of, Hybrid Code Networks (HCNs))(dialogue learning, Used-for, Semantic parsing)(end-to-end neural dialogue generation, Used-for, dialogue learning)(dialogue learning, Is-a-Prerequisite-of, Natural Language Understanding in dialog systems)(dialogue learning, Used-for, Improving dialog systems interaction)(dialogue learning, Is-a-Prerequisite-of, Task-completion dialogue agent training)(dialogue learning, Evaluate-for, dialog state latency representation)(dialogue learning, Used-for, dialogue state optimization)(dialogue learning, Is-a-Prerequisite-of, Conversational model training)(dialogue learning, Is-a-Prerequisite-of, Deep Dyna-Q framework application)(dialogue learning, Used-for, adaptive conversational agents)
EMPTY G1.automatic argument
EMPTY G1.trained language model plms
EMPTY G1.interpretable description
EMPTY G1.current state nlp
EMPTY G1.language unseen pretraining
(modular code generation,Used-for,visual question answering)(logical reasoning,Is-a-Prerequisite-of,visual question answering)(visual question answering,Compare,tabular question answering)(visual question answering,Compare,paragraph comprehension)(visual question answering,Is-a-Prerequisite-of,understanding visual content)
EMPTY G1.chinese relation extraction
(domain sentiment classification, Is-a-Prerequisite-of, domain adaptation)(domain sentiment classification, Used-for, sentiment feature analysis)(sentiment feature analysis, Evaluate-for, domain sentiment classification)(domain sentiment classification, Part-of, sentiment analysis)(active sentiment domain adaptation, Used-for, domain sentiment classification)(cross-lingual classification, Hyponym-Of, domain sentiment classification)
EMPTY G1.visual semantic
(multitask learning, Used-for, speech translation)(lattice transformer, Used-for, speech translation)(encoder pre-training, Used-for, speech translation)(Stacked Acoustic-and-Textual Encoding, Used-for, speech translation)(speech translation, Used-for, reducing gender bias)(speech translation, Part-of, language technology)
EMPTY G1.performance neural machine translation
(attention weight, Used-for, weighting words in sentiment analysis)(attention weight, Used-for, determining importance of tokens in sequence tasks)(attention weight, Used-for, controlling focus in multi-dimensional emotion regression)
EMPTY G1.learning word embedding
EMPTY G1.oriented visual dialogue
(lemma identification,Used-for,language processing)(language processing,Part-of,Natural Language Processing)(language processing,Used-for,parsing sentences)(language processing,Used-for,error detection)(language processing,Evaluate-for,generating empathetic natural language processing agents)(language processing,Compare,modeling documents)
(parsing model, Used-for, recovering long-distance dependencies)(parsing model, Used-for, syntactic analysis)
(form question answering, Used-for, accessing knowledge from knowledge bases)(form question answering, Evaluate-for, efficiency in large document contexts)(neural networks, Used-for, form question answering)(form question answering, Is-a-Prerequisite-of, understanding and using underlying knowledge bases)(form question answering, Used-for, leveraging semi-structured knowledge in Open IE for answering complex questions)(form question answering, Used-for, achieving state-of-the-art performance on QA datasets)
(Socratic pretraining, Used-for, controllable summarization)(EntSUM, Part-of, controllable summarization)(semantic units, Used-for, controllable summarization)(aspect-specific summaries, Part-of, controllable summarization)(latent variables, Used-for, controllable summarization)(conditional variational auto-encoder, Used-for, controllable summarization)(user preferences, Evaluate-for, controllable summarization)
EMPTY G1.inter sentence relation extraction
(morphological task, Is-a-Prerequisite-of, language understanding systems)(morphological task, Used-for, dialogue state tracking)(morphological task, Used-for, morphological inflection generation)(morphological task, Used-for, semantic role labeling)(morphological task, Used-for, cross-lingual transfer)(morphological task, Used-for, morphological disambiguation)(morphological task, Used-for, morphological tagging)(morphological inflection, Is-a-Prerequisite-of, morphological task)(morphological tagging, Is-a-Prerequisite-of, morphological task)
(contextualized representation, Used-for, chunking)(contextualized representation, Evaluate-for, capturing affect dimensions)(contextualized representation, Is-a-Prerequisite-of, examining differences in portrayals of men and women)(contextualized representation, Part-of, NLP systems)(contextualized representation, Used-for, language understanding benchmarks)
(extractive summary, Hyponym-Of, document summarization)(extractive summary, Used-for, conveying salient points)(abstractive summarization, Compare, extractive summary)
EMPTY G1.topic aware news representation
EMPTY G1.approach topic aware
(entity extraction, Part-of, information extraction)(machine reading, Used-for, entity extraction)(entity extraction, Used-For, knowledge base population)
(transformer language model, Compare, LSTM-based models)  (transformer language model, Is-a-Prerequisite-of, language understanding)  (transformer language model, Part-of, neural language representation models)  (transformer language model, Evaluate-for, improving NLP task performance)  
(error correction, Part-of, chinese spelling correction)(error correction, Part of, multilingual machine translation)
EMPTY G1.sentiment analysis absa aim
(sentiment classifier, Evaluate-for, sentiment polarity)(sentiment polarity, Used-for, Aspect sentiment classification)(sentiment polarity, Used-for, Target-oriented sentiment classification)(sentiment polarity, Used-for, Open-domain targeted sentiment analysis)(sentiment polarity, Part-of, sentiment analysis)(Aspect sentiment classification, Evaluate-for, sentiment polarity)(sentiment polarity, Used-for, Aspect Sentiment Classification towards Question-Answering)
(None, Is-a-Prerequisite-of, goal oriented visual dialogue)(goal oriented visual dialogue, Used-for, generating questions about an image)(End-to-end training, Evaluate-for, goal oriented visual dialogue)(Reinforcement learning, Used-for, goal oriented visual dialogue)(goal oriented visual dialogue, Compare, chatbots)(Multimodal dialogue systems, Evaluate-for, goal oriented visual dialogue)(Rational Speech Act framework, Used-for, goal oriented visual dialogue)(goal oriented visual several systems, Hyponym-Of, dialogue systems)
EMPTY G1.structured sentiment
EMPTY G1.rnn cnn model
EMPTY G1.multi modal dialogue
EMPTY G1.advance neural text generation
(fact checking,Used-for,verifying the truthfulness of a claim)(fact checking,Used-for,saving effort in validating already checked claims)(fact checking,Used-for,increasing the efficiency of news validation processes)(fact checking,Is-a-Prerequisite-of,claim verification)(claim verification,Part-of,fact checking)(fact checking,Evaluate-for,factual correctness of generated summaries)(fact checking,Evaluate-for,stance detection in social media)
EMPTY G1.represent max likelihood parse
EMPTY G1.lingual continual learning
(visual question answering vqa,Used-for,answering questions based on images)(visual question answering vqa,Evaluate-for,accuracy using attributions)
(question answer, Used-for, caption generation)(question answer, Used.
EMPTY G1.machine reading comprehension mrc
(sentiment knowledge,Used-for,enhance hate speech detection)(sentiment knowledge,Part-of,SemAxis)
EMPTY G1.category opinion sentiment quadruple
(computational psycholinguistics, Used-for, evaluating human reading behavior)(computational psycholinguistics, Used-for, building human-like computational models)(computational psycholinguistics, Is-a-Prerequisite-of, understanding human processing times through language models)(neural language models, Evaluate-for, computational psycholinguistics)
(lingual cross modal, Used-for, aligning different views into a common semantic space)
EMPTY G1.structured topic model
(self-attention mechanism, Used-for, character level language modeling)(transformer with self-attention mechanism, Used-for, character level language modeling)(character level language modeling, Used-for, creating word types not attested in the training corpus)(character level language modeling, Used-for, handling new words with bursty distribution)(character level language modeling, Evaluate-for, effectiveness across a range of languages)(character level language modeling, Part-of, open-vocabulary language modeling)
EMPTY G1.multilingual pre training
(cross lingual word embeddings, Compare, monolingual word embeddings)(monolingual word embeddings, Used-for, document analysis)(monolingual word embeddings, Hyponym-Of, word embeddings)
(neural abstractive summarization, Used-for, document summarization)(neural abstractive summarization, Used-for, generating summaries)(neural abstractive summarization, Hyponym-Of, text summarization)(neural abstractive summarization, Part-of, natural language processing)(encoder-decoder model, Used-for, neural abstractive summarization)
(gender bias, Hyponom-Of, bias category)(gender bias, Evaluate-for, translation quality in NMT)(gender bias, Hyponym-Of, bias)(gender bias, Used-for, increased privacy in learned representations)
EMPTY G1.unsupervised semantic
(recurrent neural network rnns, Used-for, language modeling)  (recurrent neural network rnys, Hyponym-Of, neural networks)  (recurrent neural network rnns, Part-of, Hybrid Code Networks)  (recurrent neural network rnns, Evaluate-for, performance in dialog systems)  (recurrent neural network rnns, Used-for, dialog state representation)  (recurrent neural network rnns, Used-for, sentence parsing)  (recurrent neural network rnns, Used-for, automatic question answering)  
EMPTY G1.topic aware news
EMPTY G1.entity mention relation
(cross lingual embeddings, Used-for, bilingual lexicon induction)(cross lingual embeddings, Used-for, cross-lingual classification)(cross lingual embeddings, Used-for, cross-lingual sentiment classification)(cross lingual embeddings, Used-for, cross-lingual word similarity)(cross lingual embeddings, Used-for, cross-lingual document classification)(cross lingual embeddings, Used-for, cross-lingual gender prediction)(cross lingual embeddings, Used-for, cross-lingual coreference resolution)(cross lingual embeddings, Is-a-Prerequisite-of, low-resource language NLP tasks)(word embeddings, Compare, cross lingual embeddings)
EMPTY G1.graph neural
EMPTY G1.visual language
(neural dialogue, Part-of, task-oriented dialogue systems)(neural dialogue, Evaluate-for, naturalness of generated responses)(multi-turn dialogue agent, Is-a-Prerequisite-of, neural dialogue)(KB-InfoBot, Hyponym-Of, neural dialogue)
EMPTY G1.prototype mention embeddings
EMPTY G1.annotated semantic relatedness
(morphological typology, Conjunction, morphological paradigm)(Linguistic typology, Is-a-Prerequisite-of, morphological typology)(morphological typology, Used-for, predicting syntactic traits of words)(morphological typology, Used-for, morphological disambiguation)(morphological typology, Used-for, cross-lingual morphological tagging)(morphological typology, Evaluate-for, language modeling)(morphological typology, Evaluate-for, improving information sharing between languages)(morphological analysis, Part-of, morphological typology)(morphological tagging, Part-of, morphological typology)(statistical morphological inflectors, Part-of, morphological typology)(morpheme, Hyponym-Of, morphological typological)
EMPTY G1.morphologically rich
EMPTY G1.specializing word embeddings according
(None, Part-of, word embedding model)(word embedding model, Used-for, capturing semantic regularities)(word embedding model, Is-a-Prerequisite-of, context word vector)(word embedding model, Used-for, word analogy questions)(word embedding model, Used-for, caption generation)(word embedding model, Part-of, Skip-Gram model)(word embedding model, Part-of, Sufficient Dimensionality Reduction framework)(Skip-Gram model, Hyponym-Of, word embedding model)
(contextual entity, Used-for, capturing semantic nuances in sentences)(contextual entity, Part-of, LSTM-based model)(contextual entity, Hyponym-Of, entity)
EMPTY G1.knowledge graph completion
(seq2seq models, Used-for, grammatical error correction)(Neural machine translation, Used-for, grammatical error correction)
(offensive language classifier, Evaluate-for, detecting out-of-domain explicitly abusive utterances)(offensive language classifier, Used-for, content moderation)(offensive language classifier, Evaluate-for, robustness against adversarial attacks)
(knowledge graph embedding, Used-for, KG link prediction)(knowledge graph embedding, Used-for, incomplete KG question answering)(knowledge graph embedding, Is-a-Prerequisite-of, link prediction)(knowledge graph embedding, Used-for, achieving common goals in dialogues)(neural model, Used-for, knowledge graph embedding)(knowledge graph embedding, Hyponym-Of, information extraction techniques)(knowledge graph embedding, Evaluate-for, machine learning performance)(knowledge graph embedding, Used-for, joint entity and relation representation learning)(WordNet, Evaluate-for, knowledge graph embedding)
(explanation attention, Is-a-Prerequisite-of, explainability in ML models)(explanation attention, Used-for, offering plausible explanations for model predictions)(explanation attention, Compare, LIME)(explanation attention, Compare, input perturbation)(explanation attention, Used-for, generating natural language explanations for predictions)
EMPTY G1.large language model llm
(dynamic programming, Used-for, constituency parsing)(constituency parsing, Is-a-Prerequisite-of, state-of-the-art single-model performance)(greedy top-down inference algorithm, Part-of, constituency parsing)(shift-reduce parsing, Part-of, constituency parsing)(constituency parsing, Used-for, syntax analysis)(top-down inference, Part-of, constituency parsing)(constituency parsing, Used-for, natural language processing)(prediction schemes, Used-for, constituency parsing)(supertagger, Used-for, constituency parsing)(generative neural models, Used-for, constituency parsing)(constituency parsing, Is-a-Prerequisite-of, generative neural models)(parse tree annotations, Hyponym-Of, constituency parsing)(constituency parsing, Used-for, identifying syntactic structures in sentences)(constituency parsing, Evaluate-for, performance on Penn Treebank)(constituency parsing, Evaluate-for, performance on French Treebank)(shift-reduce parsing schemes, Is-a-Prerequisite-of, constituency parsing)(constituency parsing, Evaluate-for, single model F1 score)
(event causality identification, Used-for, Document-level Event Causality Identification)  (event causality identification, Used-for, understanding implicit associations)  (event causality identification, Used-for, modeling semantic structures)  (event-centric structure, Part-of, event causality identification)  (event-associated structure, Part-of, event causality identification)  (event causality identification, Evaluate-for, improving causal interpretation in texts)  (event causality identification, Hyponym-Of, Document-level Event Causality Identification)  (data augmentation, Used-for, event causality identification)  
(morphological compositionality, Used-for, capturing morphological structure in BERT)(morphological compositionality, Compare, semantic compositionality)(morphological compositionality, Hyponym-Of, semantic compositionality)
EMPTY G1.embeddings semantic
EMPTY G1.vision language pre
EMPTY G1.shot continual relation extraction
(pretrained language model, Used-for, query auto-completion)(pretrained language model, Is-a-Prerequisite-of, Universal Language Model Fine-tuning (ULMFiT))(pretrained language model, Used-for, text classification tasks)(pretrained nature model, Evaluate-for, diachronic semantic drifts analysis)
(compositional generalization semantic parsing, Compare, generalization semantic parsing)(SpanBasedSP, Is-a-Prerequisite-of, compositional generalization semantic parsing)(compositional generalization semantic parsing, Part-of, natural language processing)(Seq2seq models, Evaluate-for, compositional generalization semantic parsing)(compositional generalization semantic parsing, Evaluate-for, SCAN dataset)(compositional generalization semantic parsing, Part-of, semantic parsing)(compositional generalization semantic parsing, Evaluate-for, new train-test splits)
(multi-hop QA, Is-a-Prerequisite-of, explainable nlp)
EMPTY G1.speech text translation st
(multimodal embeddings, Used-for, cross-lingual image description retrieval)(multimodal embeddings, Used-for, multilingual word similarity)(multimodal embeddings, Used-for, sentiment analysis)(multimodal embeddings, Used-for, Multimodal Named Entity Disambiguation)(multimodal embeddings, Is-a-Prerequisite-of, Multimodal Named Entity Disambiguation)(DPCCA, Used-for, multimodal embeddings)(PCCA, Used-for, multimodal embeddings)(semantic information, Used-for, multimodal embeddings)
EMPTY G1.summarization largely
(bias mitigation, Used-for, developing fairer AI systems)  (predictive bias, Is-a-Prerequisite-of, bias mitigation)  (bias symptoms/effects, Evaluate-for, bias mitigation)  (SCM-based debiasing, Used-for, bias mitigation)  (pretrained models, Used-for, bias mitigation)  
(language target language, Part-of, Neural Machine Presence Translation model)(language target language, Used-for, evaluate bilingual tasks like cross-lingual classification)(language target language, Hyponym-Of, linguistics)(language target language, Is-a-Prerequisite-of, cross-lingual transfer learning)(language target language, Evaluate-for, sentiment analysis in bilingual context)(language target language, Evaluate-for, cross-lingual information retrieval)
(natural question dataset, Used-for, machine reading comprehension benchmark)(natural question dataset, Is-a-Prerequisite-of, developing effective question answering systems)(short answer, Part-of, natural question dataset)(long answer, Part-of, natural question dataset)(RikiNet, Evaluate-for, natural question dataset)(SQuAD, Compare, natural question dataset)
(transcribed speech, Used-for, training ASR systems)(transcribed speech, Used-for, language model training)(QASR, Part-of, transcribed speech)(transcribed state, Is-a-Prerequisite-of, speech-to-text translation)(transcribed speech, Used-for, evaluating speech recognition systems)(transcribed speech, Used-for, linguistics-based Arabic dialect identification)(transcribed speech, Used-for, punctuation restoration)(transcribed speech, Used-for, speaker identification)(transcribed speech, Used-for, speaker linking)(VoxPopuli, Part-of, transcribed speech)(transcribed speech, Used-for, NLP modules for spoken data)
(reading comprehension datasets, Evaluate-for, language understanding task)(SQuAD, Part-of, reading comprehension datasets)(MCTest, Part-of, reading comprehension datasets)(WikiReading, Part-of, reading comprehension datasets)(Childrens Book Test, Part-of, reading comprehension datasets)(TriviaQA, Part-of, reading comprehension datasets)(Common Nouns dataset, Part-of, reading comprehension datasets)(DuoRC, Part-of, reading_utilitarianation datasets)
EMPTY G1.machine translation using
(A* CCG parsing model, Used-for, dialogue modeling)(dialogue modeling, Compare, open-ended dialogue state)(dialogue modeling, Used-for, symmetric collaborative dialogue setting)(dialogue modeling, Used-for, neural model with dynamic knowledge graph embeddings)(dialogue modeling, Evaluate-for, achieving common goals)(dialogue modeling, Evaluate-for, improving dialogue systems)(neural conversational systems, Part-of, dialogue modeling)(encoder-decoder dialog model, Part-of, dialogue modeling)(dialogue modeling, Used-for, multi-turn dialogues handling)(response selection, Used-for, dialogue modeling)(dialogue modeling, Evaluate-for, interpreting user responses)(dialogue modeling, Used-for, handling multi-domain dialogues)(dialogue systems, Hyponym-Of, dialogue modeling)(retrieval-based dialogues, Part-of, dialogue modeling)
EMPTY G1.cross lingual cross modal
(latent topic, Part-of, neural topic model)(neural topic model, Is-a-Prerequisite-of, language model perplexity evaluation)(neural topic model, Used-for, document context representation)(neural topic model, Part-of, neural language model)(neural topic model, Hyponym-Of, topic model)(neural topic model, Used-for, generating related sentences)(language model perplexity evaluation, Evaluate-for, neural topic model)(document context representation, Used-for, neural topic model)
EMPTY G1.word vector
(textual feature, Used-for, sentiment analysis)(textual feature, Used-for, sarcasm detection)(textual feature, Used-for, part-of-speech induction)(textual feature, Used-for, named-entity recognition)(textual feature, Part-of, Natural Language Processing)(lexical feature, Compare, textual feature)(textual feature, Is-a-Prerequisite-of, domain adaptation)(textual feature, Used-for, coreference resolution)(textual feature, Used-for, relation extraction)(textual feature, Used-for, text classification tasks)
(language model like, Used-for, generating rhythmic poetry)  (language model like, Used-for, constraint satisfaction in poetry generation)  (discriminative weighted finite state machine, Part-of, language model like)  (language model perplexity, Evaluate-for, language model like)  (language model, Is-a-Prerequisite-of, language model like)  (Affect-LM, Is-a-Prerequisite-of, language model like)  (character-level language models, Is-a-Prerequisite-of, language model like)  (hierarchical LSTM language model, Is-a-Prerequisite-of, language model like)
(output attention weight, Hyponym-Of, attention weights)(output attention weight, Used-for, auditing algorithms in fairness and accountability contexts)(deceptive attention masks, Evaluate-for, output attention weight)(output attention weight, Compare, gradient-based rankings of attention weights)(output attention weight, Compare, identifiable weights)
(word segmentation, Used-for, neural word segmentation)(pretraining character and word embeddings, Part-of, neural word segmentation)(external training sources, Used-for, neural word segmentation)(statistical word segmentation, Compare, neural word segmentation)(pretraining language, Used-for, neural word segmentation)(neural word segmentation, Evaluate-for, improving Arabic NLP applications)(neural word segmentation, Used-for, Machine Translation)(neural word segmentation, Used-for, POS tagging)(character CNN, Hyponym-Of, neural word segmentation)(data-driven sub-word units, Hyponym-Of, neural word segmentation)(neural word segmentation, Used-for, improving NLP task accuracies)(neural word segmentation, Used-for, pretraining character and word embeddings)(neural word segmentation, Evaluate-for, improving model accuracies)(neural word segmentation, Is-a-Prerequisite-of, Chinese word segmentation as part of NLP)(neural word segmentation, Part-of, neural network-based joint models)(neural word segmentation, Hyponym-Of, text segmentation)(statistical segmentation, Compare, neural word segmentation)(modular segmentation model, Used-for, neural word segmentation)(lattice-based encoders, Used-for, neural word at segmentation)
(nested named entity recognition, Is-a-Prerequisite-of, Named Entity Recognition)(nested named entity recognition, Used-for, identifying entities of interest in a narrative)(nested named entity recognition, Part-of, Named Entity Recognition tasks)(nested named entity recognition, Hyponym-Of, Named Entity Recognition)
(Multimodal word distributions, Used-for, grained sentiment)(Aspect-based sentiment analysis, Used-for, grained sentiment)(Cross-domain sentiment classification, Used-for, grained sentiment)(Bilingual Sentiment Embeddings, Used-for, grained sentiment)(Opinionated Natural Language Generation, Evaluate-for, grained sentiment)(Active sentiment domain adaptation, Used-for, grained sentiment)(Domain adaptation, Used-for, grained sentiment)(Domain-specific information, Used-for, grained sentiment)(Fine-grained sentiment analysis, Hyponym-Of, grained sentiment)(Fine-grained entity categorization, Used-for, grained sentiment)
EMPTY G1.neural text
EMPTY G1.entity recognition ner
(perplexity language model, Evaluate-for, text classification)(perplexity language model, Is-a-Prerequisite-of, natural language understanding)
(Context-Aware Network Embedding, Used-for, link prediction)(link prediction, Conjunction, knowledge graph completion kgc)(knowledge graph kg, Used-for, link prediction)(link prediction, Evaluate-for, incomplete knowledge graph applications)(link prediction, Evaluate-for, graph embedding)(Knowledge Graph embedding, Used-for, link prediction)(entity embeddings, Used-for, link prediction)(knowledge graph embedding, Is-a-Prerequisite-of, link prediction)
EMPTY G1.bias multilingual representation
(model generated summary, Is-a-Prerequisite-of, evaluate for effectiveness)(model generated summary, Evaluate-for, semantic relevance)(encoder-decoder framework, Used-for, model generated summary)(model generated summary, Part-of, NMT pipeline)(Sequence-to-Dependency Neural Machine Translation, Used-for, model generated summary)(model generated summary, Hyponym-Of, summarization output)(statistical machine translation, Used-for, model generated summary)(Sequence-to-sequence network, Used-for, model generated summary)
(information retrieval task, Used-for, detecting review spam)(information Independent Background knowledge, Used-for, volatility prediction in financial markets)(information retrieval task, Used-for, open-domain question answering)(information retrieval task, Used-for, detecting concealed information)(information retrieval, Is-a-Prerequisite-of, information retrieval task)(neural networks, Used-for, information retrieval task)(Entity-Duet Neural Ranking Model, Used-for, information retrieval task)(AliMe Chat, Used-for, information retrieval task)
EMPTY G1.opinion entity extraction
EMPTY G1.transformer based language
(dependency parsing performance,Evaluate-for,multi-task learning)(dependency parsing performance,Evaluate-for,dynamic oracles for Covington parser)(dependency parsing performance,Evaluate-for,non-monotonic transition system)(dependency parsing performance,Part-of,computational argumentation mining)(dependency parsing performance,Evaluate-for,book-embedding framework)(dependency parsing performance,Evaluate-for,BiLSTM models)(dependency parsing performance,Evaluate-for,neural network models for Chinese linguistic analysis)(dependency parsing performance,Part-of,semantics)(dependency parsing performance,Evaluate-for,joint Chinese analysis)(dependency parsing performance,Evaluate-for,stack-pointer networks (StackPtr))(dependency parsing performance,Evaluate-for,parser adaptation using normalization)(dependency parsing performance,Evaluate-for,cross-lingual dependency parsing on low-resource languages)
(modeling morphological, Evaluate-for, morphological well-formedness)
(embeddings pre,Part-of,Neural Networks)(embeddings pre,Used-for,Word Embedding)(embeddings pre,Used-for,Language Models)(embeddings pre,Is-a-Prerequisite-of,Semantic Similarity Modeling)(embeddings pre,Used-for,Vector Calculus)(embeddings pre,Is-a-Prerequisite-of,Sequence Labeling Tasks)(embeddings pre,Is-a-Prerequisite-of,Answering Cloze-Style Questions)(embeddings pre,Is-a-Prerequisite-of,Named Entity Recognition)(embeddings pre,Is-a-Prerequisite-of,Word Segmentation)(embeddings pre,Used-for,Knowledge Base Completion)(embeddings pre,Is-a-Prerequisite-of,Joint Task for Chinese Analysis)(embeddings pre,Is-a-Prerequisite-of,Prepositional Phrase Attachments)
(multilingual machine translation, Used-for, bidirectional translation tasks)(multilingual machine translation, Hyponym-Of, neural machine translation)(multilingual machine translation, Used-for, generating translations)(neural machine translation, Is-a-Prerequisite-of, multilingual machine translation)(statistical machine translation, Is-a-Prerequisite-of, multilingual machine translation)(error correction, Part-of, multilingual machine translation)
(discourse segmenters, Used-for, detecting intra-sentential segment boundaries)(discourse segmenters, Evaluate-for, building end-to-end discourse parsers)(statistical discourse segmenters, Hyponym-Of, discourse segmenters)
(unsupervised morphological,Is-a-Prerequisite-of,morphological inflection generation)(unsupervised morphological,Used-for,analyzing word formation in NMT)(unsupervised morphological,Used-for,morphological paradigm completion)(unsupervised morphological,Hyponym-Of,morphological analysis)
EMPTY G1.natural language text
EMPTY G1.advance neural text
(Universal Language Model Fine-tuning, Used-for, language model fine tuning)(language model fine tuning, Used-for, reducing error rates on datasets)(pretrained language models, Used-for, language model fine tuning)(language model pretraining, Compare, language model fine tuning)(transfer learning, Hyponym-Of, language model fine tuning)(language model fine logging, Used-for, sentiment classification)
EMPTY G1.semantic retrieval
(dictionary-based mapping, Used-for, bilingual word embeddings)(bilingual word dpsdin, Part-of, machine translation)(unsupervised bilingual word embeddings, Compare, bilingual word embeddings)(bilingual word embeddings, Part-of, contextual embeddings strategies)(monolingural embeddings, Is-a-Prerequisite-of, bilingual word embeddings)(bilingual word embeddings, Hyponym-Of, word embeddings)(word embeddings, Is-a-Prerequisite-of, bilingual word embeddings)(cross-lingual word embeddings, Is-a-Prerequisite-of, bilingual word embeddings)(bilingual word embeddings, Part-of, unsupervised machine translation models)(bilingual word embeddings, Hyponym-Of, linguistic features)(bilingual word embeddings, Used-for, cross-lingual twitter sentiment classification)(bilingual word embeddings, Used-for, medical bilingual lexicon induction)(bilingual word embeddings, Used-for, learning bilingual lexicon induction)(bilingual word embeddings, Used-for, bilingual lexicon induction)(bilingual word embeddings, Used-for, cross-lingual classification)(bilingual word embeddings, Used-for, domain adaptation)(bilingual word embeddings, Evaluate-for, inducing bilingual dictionaries)(bilingual word embeddings, Used-for, improving bilingual text embeddings)(bilingual word embeddings, Evaluate-for, cross-lingual sentiment classification)(bilingual word embeddings, Evaluate-for, medical bilingual lexicon induction)
(event extraction, Used-for, knowledge base population)(machine reading, Used-for, event extraction)(supervised learning, Used-for, event extraction)(supervised relation, Evaluate-for, event extraction)(Supervised learning, Is-a-Prerequisite-of, event extraction)(data labeling, Used-for, event extraction)(event extraction, Hyponym-Of, knowledge base population)(automatically label training data, Used-for, event extraction)(event extraction, Used-for, populating knowledge bases with events)(distant supervision, Used-for, event extraction)(dependency parsing, Evaluate-for, event extraction)(event extraction, Compare, supervised learning)(event detection, Part-of, event extraction)(event extraction, Hyponym-Of, information extraction)(event extraction, Compare, relation extraction)(multi-lingual multi-task architecture, Used-for, event extraction)(event mention, Part-of, event extraction)(event extraction, Compare, dense video event captioning)(event extraction, Evaluate-for, generic grounding problem)
(entity detection, Used-for, natural language processing)(entity detection, Part-of, naturalaters the joint type inference task, we propose a novel graph convolutional network (GCN) running on an entity-relation bipartite graph. By introducing a binary relation classification task, we are able to utilize the structure of entity-relation bipartite graph in a more efficient and interpretable way. Experiments on ACE05 show that our model outperforms existing joint models in entity performance and is competitive with the state-of-the-art in relation performance. There exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. In this paper we introduce a new anomaly detection methodâ€”Context Vector Data Description (CVDD)â€”which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. language processing)(mention detection, Part Ta of, entity detection)
(sentence representation learning, Evaluate-for, rumor detection)(rumor detection, Used-for, classifying tweets)(progressive neural networks, Used-for, rumor detection)
(language generation nlg, Is-a-Prerequisite-of, natural language interfaces to databases)(language generation nlg, Part-of, natural language processing)(neural networks, Used-for, language generation nlg)(semantic parsing, Is-a-Prerequisite-of, language generation nlg)(sequential models, Used-for, language generation nlg)(statistical models, Used-for, language generation nlg)(syntax analysis, Used-for, language generation nlg)(neural symbolic machine, Used-for, language generation nlg)(contrastive focus, Used-for, language generation nlg)(Opinionated Natural Language Generation ONLG, Hyponym-Of, language generation nlg)
(sequence classification, Used-for, natural language understanding)(machine reading comprehension, Part-of, natural world)
(natural language utterances, Part-of, lingual semantic parsing)(formal meaning representations, Part-of, lingual semantic parsing)(lingual semantic parsing, Is-a-Prerequisite-of, generating structured queries from natural language)(semantic parsing task, Hyponym-Of, lingual semantic parsing)(lingual semantic parsing, Is-a-Prerequisite-of, knowledge base querying)(lingual semantic parsing, Evaluate-for, accuracy in structured data interpretation)(structured data interpretation, Part-of, lingual semantic parsing)(lingual semantic parsing, Used-for, machine understanding of human language)(machine understanding of human language, Part-of, lingual semantic parsing)
(COREQA, Is-a-Prerequisite-of, answering system)(answering system, Used-for, generating natural answers)(COREQA, Used-for, answering system)(EviNets, Is-a-Prerequisite-of, answering system)(answering system, Used-for, selecting final answer in QA)(EviNets, Used-for, answering system)(answering system, Used-for, multimodal question answering)
(aspect extraction, Is-a-Prerequisite-of, aspect sentiment classification)  (aspect sentiment classification, Used-for, classify sentiment based on specific targets in sentences)  (aspect term extraction, Is-a-Prerequisite-of, aspect sentiment classification)  (aspect based sentiment analysis, Is-a-Prerequisite-of, aspect sentiment classification)  (aspect sentiment classification, Part-of, aspect based sentiment analysis)  (aspect sentiment classification, Used-for, classify sentiment expressed toward extracted aspect)  (aspect sentiment classification, Used-for, identifying sentiment about extracted aspect)  (aspect sentiment classification, Used-for, determining sentiment of specific aspects)  
(hierarchical attention network, Used-for, progress)(hierarchical attention network, Used-for, task question answering)(hierarchical attention  network, Used-for, reading comprehension)(hierarchical attention network, Used-for, answering questions for narrative paragraphs)(hierarchical attention network, Used-for, modeling long-range dependencies in texts)(hierarchical attention network, Used-for, learning word embeddings of OOV words)(hierarchical attention network, Used-for, claim verification)
(spelling error, Used-for, detecting the native language of a writer)(spelling error, Part-of, spelling error features)(spelling error, Part-of, Chinese Spelling Check)(spelling error, Hyponym-Of, linguistic errors)(spelling error, Part-of, relation extraction task)(spelling error, Part-of, grammatical error correction systems)
(adversarial example, Used-for, adversarial training)(adversarial example, Used-for, adversarial attacks on text classification)(adversarial example, Used-for, adversarial generation techniques)(adversarial example, Hyponym-Of, adversarial perturbation)(adversarial example, Evaluate-for, showing the weakness of NLP systems)(adversarial example, Evaluate-for, robustness testing)(adversarial example, Hyponym-Of, adversarial stability training)(adversarial example, Is-a-Prerequisite-of, adversarial stability training)(adversarial example, Compare, adversarial perturbation)
(attention based explanation, Used-for, facilitating user understanding of model decision-making)
(morphological inflection, Hyponym-Of, morphological analysis)(morphological paradigm, Used-for, morphological analysis)(morphological analysis, Used-for, predicting syntactic traits)(language-independent model, Used-for, morphological analysis)(morphological analysis, Evaluate-for, linguistic structure)(morphological analysis, Part-of, morphological typology)(morphological inflection generation, Used-for, morphological analysis)(morphological tagging, Is-a-Prerequisite-of, morphological analysis)(unsupervised morphological, Hyponym-Of, morphological analysis)(morphological analysis, Used-for, predicting syntactic traits of words)(morphological analysis, Evaluate-for, language understanding systems)
EMPTY G1.attention based
(sentence compression model, Compare, non-neural-network-based model)
EMPTY G1.machine translation system
EMPTY G1.evaluation benchmark korean
EMPTY G1.event language model
(chain reasoning, Used-for, event argument extraction)
(syntax-aware models, Used-for, unsupervised syntactic parsing)(Parsing-Reading-Predict architecture, Used-for, unsupervised syntactic parsing)(unsupervised syntactic parsing, Used-for, learning natural language syntax)(unsupervised syntactic parsing, Evaluate-for, discrepancy in datasets)(unsupervised syntactic parsing, Compare, unsupervised semantic parsing)(statistical methods, Used-for, unsupervised syntactic parsing)(neural methods, Used-for, unsupervised syntactic parsing)(unsupervised syntactic parsing, Is-a-Prerequisite-of, better comparability between methods)(Order Neuron LSTM, Used-for, unsupervised syntactic parsing)(unlabeled data, Used-for, unsupervised syntactic parsing)(unsupervised syntactic parsing, Evaluate-for, F-1 score)
(Long Short Term Memory, Hyponym-Of, short term memory)(Bi-LSTM, Hyponym-Of, short term memory)(Tree-LSTMs, Hyponym-Of, short term memory)(bidirectional-LSTMs, Hyponym-Of, short term memory)(Gated Recurrent Units, Hyponym-Of, short term strength)
EMPTY G1.neural topic
EMPTY G1.speech speech translation s2st
(character n-grams, Used-for, word representation)(word representation, Used-for, language modeling)(word embedding, Hyponym-Of, word representation)(morphological segmentation, Used-for, word representation)(semantic quality, Evaluate-for, word representation)(multimodal word distributions, Hyponym-Of, word representation)(distributional vector space, Used-for, word representation)(subword units, Part-of, word representation)(character n-grams, Part-of, word representation)
(extractive summarization model,Conjunction,abstractive summarization model)(abstractive summarization model,Is-a-Prerequisite-of,abstractive document summarization)(encode-attend-decode paradigm,Used-for,abstractive summarization model)(global encoding framework,Used-for,abstractive summarization model)(query attention model,Part-of,abstractive summarization model)(Transformer-based encoder-decoder framework,Used-for,abstractive summarization model)(email subject line generation,Evaluate-for,abstractive summarization model)(hybrid pointer-generator network,Part-of,abstractive summarization model)(coverage mechanism,Part-of,abstractive summarization model)
(semantic decoding, Is-a-Prerequisite-of, semantic parsing)
(entity grid representation, Hyponym-Of, entity representation)(semantic information, Part-of, entity representation)(word embeddings, Used-for, entity representation)(multimodal word distributions, Used-for, entity representation)(distributed representation, Part-of, entity representation)(local coherence model, Evaluate-for, entity representation)
EMPTY G1.phonological representation
(deep learning tool, Used-for, fake news detection)(fake news detection, Used-for, preventing misinformation)(social medium analysis, Used-for, fake news detection)(fake news detection, Is-a-Prerequisite-of, reasoning over evidence)(fake news detection, Used-for, preventing misinformation on social media)(fake news detection, Used-for, maintaining a healthy society)(LIAR dataset, Used-for, fake news detection)(stylometry, Evaluate-for, fake news detection)(graph neural model, Used-for, fake news format)(fake news detection, Used-for, employing fact-checking datasets)(None, Is-a-Prerequisite-of, fake news detection)(fake news detection, Used-for, political fact-checking)(fake news detection, Evaluate-for, social media posts)(fake news detection, Used-for, preventing misinformation dissemination)(fake news detection, Evaluate-for, comparing language patterns)(fake news detection, Evaluate-for, environmental signals)(News Environment Perception Framework, Used-for, fake news detection)(Graph-aware Co-Attention Networks, Used-for, fake news detection)(CompareNet, Used-for, fake news detection)(hybrid convolutional neural network, Used-for, fake news detection)(statistical approaches, Part-of, fake news detection)(social impacts, Part-of, fake news detection)(PolitiFact.com, Used-for, fake news detection)(benchmark datasets, Used-for, fake news detection)
(event mention, Is-a-Prerequisite-of, event coreference resolution)(event mention, Part-of, dense video event captioning)(event mention, Part-of, event extraction)(event mention, Part-of, event ontology mapping)(event mention, Part-of, dialogue state tracking)(event mention, Part-of, coreference resolution)(event mention, Is-a-Prerequisite-of, semantic relation extraction)
(document level relation extraction, Part-of, Neural Relation Eyes Extraction)(inter-sentence relation extraction, Compare, document level relation extraction)(inter-sentence relation extraction, Is-a-Prerequisite-of, document level relation extraction)(document level relation extraction, Used-for, multi-hop reasoning)(latent document-level graph, Part-of, document level relation extraction)
EMPTY G1.cross lingual summarization
(sentence embeddings, Part-of, vector semantics)(word embeddings, Compare, sentence embeddings)(sentence embeddings, Part-of, sentence representation learning)(sentence embeddings, Is-a-Prerequisite-of, textual similarity sts task)(sentence embeddings, Used-for, natural language processing tasks)(sentence embeddings, Used-for, sentence classification)(sentence embeddings, Hyponym-Of, word embeddings)(sentence embeddings, Evaluate-for, probing tasks)(sentence embeddings, Evaluate-for, downstream tasks)(sentence embeddings, Is-a-Prerequisite-of, semantic similarity prediction)(sentence embeddings, Used-for, embedding sequences of sentences)(sentence embeddings, Used-for, thematic clustering of sentences)(sentence embeddings, Used-for, capturing linguistic features)(sentence embeddings, Used-for, improving machine translation performance)(sentence embeddings, Part-of, NLP models)(sentence embeddings, Part-of, multilingual distributed representations)(sentence embeddings, Hyponym-Of, vector representations)
EMPTY G1.shot semantic parsing
EMPTY G1.multi hop reading comprehension
EMPTY G1.human annotated explanation
(annotated data, Is-a-Prerequisite-of, zero pronoun resolution task)(annotated data, Is-a-Prerequisite-of, supervised machine learning models)(annotated data, Used-for, training Grammatical Error Correction systems)(annotated data, Used-for, training neural network based inference models)(annotated data, Used-for, natural language inference)(annotated data, Used-for, Japanese predicate-argument structure analysis)(annotated data, Used-for, sentiment analysis)(annotated data, Used-for, error detection in automatically annotated text)(annotated data, Hyponym-Of, pseudo training data)(annotated data, Part-of, Natural Language Processing tasks)
EMPTY G1.question answering qa
